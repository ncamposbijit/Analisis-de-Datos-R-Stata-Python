[["index.html", "Análisis de Datos con Stata, R y Python 1 Introducción", " Análisis de Datos con Stata, R y Python Nicolás Campos Bijit 2024-06-27 1 Introducción "],["buenas-prácticas-de-programación.html", "2 Buenas prácticas de programación 2.1 Programación defensiva 2.2 Estilo de codificación en Stata 2.3 Organización de un proyecto en Stata", " 2 Buenas prácticas de programación 2.1 Programación defensiva Las personas somos malas escribiendo códigos Esta sección esta basada en los trabajos y recursos publicados por Nick Eubank. La filosofía de escribir un código esta motivada por el hecho de que las personas somos malas escribiendo códigos. Si queremos evitar errores no es suficiente con ser cuidadoso(a) Es necesario diseñar estrategias que tomen en cuenta el hecho de que vamos a cometer errores al escribir un código. Tener buenas estrategias es más relevante cuando existe trabajo colaborativo, que puede amplificar los errores. En esta sección revisaremos cuatro buenas practicas de programación. Estas practicas no son solo aplicables a Stata, más bien son guías generales para escribir códigos en cualquier lenguaje de programación. Usar testeos o pruebas. No duplicar información. No transcribir información, siempre exportarla. Usar un estilo correcto. Con estos elementos podremos: Minimizar las oportunidades de generar errores al codificar. Maximizar la probabilidad de que cuando se cometa un error (lo que ocurrirá!) se pueda detectar con mayor facilidad. ¿Necesito entender estos principios? Chang and Li (2015) testearon la reproducibilidad de 67 artículos publicados. Todos con alguna sección empírica. Encuentran que: Solo 29 artículos fueron totalmente replicables. 29 artículos tenían problemas al tratar missing values. 9 poseen datos incorrectos o errores de codificación. Christensen and Miguel (2018) indican que Chang and Li (2015) utilizan una versión poco estricta de replicabilidad. (…) Chang and Li (2015) use a qualitative definition of replication, and test only key results of the paper, and this appears to lead to a fairly generous interpretation of replicability. (…) (…) To our minds, this is evidence that even when data are available (which they sometimes are not) a non-negligible fraction of empirical economics research cannot be reproduced, even when using the original data and a relatively non-stringent conceptual understanding of what constitutes replication success. ¿Qué podemos aprender de esta historia? Todos y todas tienen problemas para codificar (pasa hasta en las mejores familias). Hoy el trabajo es colaborativo. También tienes que confiar en los demás. Hay cosas que no se pueden resolver con ser cuidadoso(a) al codificar. ¿Si los datos están mal? Reproducibilidad y replicabilidad son claves. Buenas referencias sobre estos temas se encuentran en la página de Julia Schulte-Cloos. 2.1.1 Practica I: escribir una prueba o test ¿Qué significa escribir una prueba o test? Nick Eubank indica que si el tuviese que elegir solo una practica elegiría agregar pruebas o tests a los códigos. ¿Qué es una prueba? Son afirmaciones que tienen como respuesta un valor lógico (verdadero o falso). Buscan chequear si una condición particular es cierta. En el caso de que no lo sea, es decir de que no se cumpla esta condición, la ejecución del código se detiene. ¿Por qué agregar estos chequeos, si yo siempre chequeo o reviso mis códigos? 1. Test se ejecutan cada vez que corres tu código La mayoría testea la primera vez que escribe un código. Si hay algún apuro por terminar el testeo del código disminuye rápidamente. Si ya ha pasado algún tiempo desde la última vez que lo ejecuté. Únicamente ejecuto el código y espero los resultados. Entender un código (incluso el que uno(a) misma escribió es difícil después de un tiempo). Un error que se comete en una de las múltiples ediciones del código pueda ser detectado con facilidad al testear. 2. Genera el hábito de chequear permanentemente a un costo razonable La mayoría se para a chequear los datos cuando detecta algún error o bien sospecha que existe un problema. Si nos acostumbramos a escribir pruebas al final de los códigos o bien cada vez que se hagan cierto tipo de operaciones (ej. siempre después de unir bases de datos), iremos generando el hábito de chequear constantemente, pero sin invertir demasiado tiempo en revisiones posteriores. 3. Ayuda a detectar problemas rápidamente Se donde esta el error. Puedo corregirlo rápidamente. 4. Escribir pruebas ayuda a detectar más problemas de los previstos Los errores de un código se manifiestan de múltiples formas y muchas veces son consecuencia de errores anteriores. Si tengo valores duplicados mi estadística descriptiva estará mala. El problema surge de no darle un tratamiento correcto a los valores duplicados. No surge de algún problema en el código que genera la estadística descriptiva. 2.1.1.1 El comando |assert| Ejemplo: Para escribir pruebas en Stata utilizaremos el comando |assert|. Notar que este comando detiene la ejecución del código cuando la condición no se cumple. Testear si la base de datos tiene 100 observaciones: count assert `r(N)&#39; == 100 Testear si la variable tasa_desempleo esta en su rango: assert tasa_desempleo| &gt; 0 &amp; tasa_desempleo| &lt; 100 Testear si la variable edad es siempre positiva: edad &gt; 0 Ejercicio: Instrucciones: Descargar los datos Preguntas: La base de datos World Development Indicator (WDI) tiene observaciones duplicadas. Escriba un test que falle cuando existen valores duplicados. Los países en la base de datos Polity deberían ser un subconjunto de los países que están en la base WDI. Escriba un test que falle debido a este problema. Las soluciones de todos los ejercicios se encuentran aquí ¿Cuando escribir tests? Después de juntar bases: Siempre incluir test después de utilizar un merge o bien de append . Después manipulaciones complejas de datos: Si piensas que fue difícil hacer un proceso, incluye un test. Después de eliminar observaciones: Siempre incluir test después de utilizar un drop. Regla general siempre que te veas a ti misma(o) revisando o chequeando algún segmento de código interactivamente escribe un test. 2.1.2 Practica II: no duplicar información Información debe estar expresada una sola vez en un código. Si la información esta en muchas partes en el código, tendrás que buscar todos esos lugares. Ejemplo: Imaginemos que quiero eliminar todas las observaciones que toman un valor mayor a un cierto límite, 110 en este caso. drop if var1 &gt; 110 drop if var2 &gt; 110 drop if var3 &gt; 110 Lo anterior funciona… ¿Cual es el problema entonces? Imaginemos que queremos cambiar el límite de 110 a 100. Del modo en que esta escrito, vamos a tener que hacer cambios 3 veces! Si estas restricciones se encuentran en distintas partes de tú código, se puede inducir un error. Ejemplo: Para evitar el problema anterior, es mejor escribir: local lim = 110 drop if var1 &gt; `lim&#39; drop if var2 &gt; `lim&#39; drop if var3 &gt; `lim&#39; De esta forma, en caso de decidir cambiar la restricción, solo tendrás que hacer un cambio. Minimizando la posibilidad de cometer un error. Ejercicio 2: Todas las regresiones que se encuentran en el código tienen la misma base, es decir, el mismo conjunto de variables que se repite. La idea es consolidar la base con el fin de evitar duplicación de información. Con este fin, agregar population como control en todas las regresiones. Nuevamente, no dupliques información. 2.1.2.1 Practica III: nunca transcribir directamente información, todo automatizado Nunca se debe transcribir resultados. Todo debe ser exportado de forma tal que pueda ser actualizado automáticamente. No cumplir con esta practica es una importante fuente de errores Errores de transcripción: Si me equivoco en el décimo decimal no es tan grave, si cambio un signo si lo es. Problemas para actualizar: Es probable que al actualizar un código (como sucede usualmente en cualquier proyecto), olvidemos traspasar los resultados. ¿Como evitar este problema? Para usuarios de \\(\\LaTeX\\) cada resultado debe ser guardado en archivos .tex. Hacer esto no solo con las tablas (ej. putexcel). También hacerlo con cualquier estadística que aparezca en el texto. (Por ejemplo, ``El promedio de edad de la muestra es de 43 años”. Este 43 debe obtenerse automáticamente desde tú código, no debes transcribirlo.) Para esto: debes generar el número que se desea citar. Convertirlo a string y guardarlo (ej. midato.tex). Llamarlo desde \\(\\LaTeX\\) con \\input{midato.tex}. Para usuarios de Word es un poco más complejo, pero posible putdocx. Otra alternativa es utilizar Stata Markdown. Independiente de la estrategia que utilizes la buena practica guarda relación con el hecho de que el producto final (artículo academico, reporte, nota, etc) debe estar conectada con cualquier estadística que se genere dentro de un proceso de análisis de datos. Ejemplo: importar una tabla desde Stata a \\(\\LaTeX\\) directamente eststo clear eststo: reg polity gdp_per_cap eststo: reg polity gdp_per_cap under5_mortality esttab using nombredelarchivo.tex, replace /// title(&quot;Mis estimaciones&quot;) Lo importante es que todo lo que se produzca en Stata debe ir directamente a el producto final. Para una revisión detallada ver Stata to LaTeX Guide. Una excelente referencia para hacer tablas mucho más avanzadas. Ejercicio 3: Calcula el promedio de literacy_rate y guárdalo en una macro local. Generar un formato tal que solo tenga dos decimales. Guarda el valor como string en un archivo .tex en la misma carpeta. 2.1.3 Practica IV: estilo importa Estilo no se refiere a la estética del código ni al orden. Tiene que ver con la facilidad con el que este se puede leer. Al igual que cuando se escribe, lo importante es ser claro. Para ello tres aspectos son claves: 1.Utilizar espacios. Usar espacios siempre después de utilizar un operador. Por ejemplo: gen var1=var2+var3 /// Mal gen var1 = var2 + var3 /// Bien Con criterio… hours + minutes / 60 + seconds / 3600 hours + minutes/60 + seconds/3600 2.Nombrar las variables para que sean informativas en si mismas. No escribir var1, si puedes escribir: tasa_desempleo. Requiere más trabajo, pero facilita la lectura. Agregar las unidades en las etiquetas facilita la lectura (en porcentaje). o bien (== 1) para indicar una variable dicotomica. 3.Escribir comentarios Ejercicio para explicar en pocas palabras lo que se ha hecho. Existe un equilibrio entre parsimonia y calidad del comentario. Establecer el objetivo del comentario claramente es muy importante. No hay una regla para escribir comentarios, pero si algunos consejos (ver, Gentzkow and Shapiro (2014)) Utilizar imperativo si lo tiene que corregir otra persona (ej. modifica parámetros aquí, fija directorio). Debe incluir suficiente contexto para que alguien que no trabaje directamente en eso entienda que se debe hacer. Los comentarios largos deben explicar secciones de código. Importante escribir el código por bloques. La brevedad no debe afectar nunca la comprensión. Esto también aplica para los comandos (ejemplo de Michael Shill). Ejemplo: summarize var1 sum var1 su var1 2.2 Estilo de codificación en Stata 2.2.1 Ser ordenado no es condición suficiente para tener un buen estilo de codificación Primera idea: Un buen estilo de codificación es seguir una estrategia. Hay ciertos guías, pero lo relevante es seguir una y se consistente. Los códigos tienen distintas audiencias. La primera es tu computadora. Si tu computadora no entiende tú código es el peor de los mundos. Una de las audiencias más importantes son tus colaboradores y colaboradoras. Si ellos/as no lo entienden. El problema es tuyo en primer lugar. No hay un solo buen estilo. No hay una guía que pueda sustituir experiencia o estrategias ya probadas de trabajo de equipo. Lo importante es tener estrategias y utilizarlas en la medida de que sea positivo para el producto final. Ahora veremos algunas estrategias y buenas practicas que pueden ayudar a mejorar el estilo de codificación individual y de tu equipo en base a dos referencias adicionales: y de una guía confeccionada por Michael Stepner. 2.2.2 Escribe códigos cortos Ningún segmento de código debiese ser mayor a 100 - 150 carácteres. Códigos largos, debiesen ser escritos de forma más pequeña como ado-files. Si tienes dificultades para reducir un código a estos estándares, es probable que tengas que pensar en como estructurarlo de mejor forma. Cada segmento de código o funciones debiesen tener un propósito claro e intuitivo para lectores experimentados en el lenguaje de programación que se este utilizando. Los do - files deben ser cortos y deben tener un objetivo claro. Todo do-file debe tener un propósito que pueda ser explicado en una oración. De hecho, hay que hacerlo. En el preámbulo. Cada do-file debe indicar claramente su objetivo. Un proyecto debería tener varios do-files con las siguientes características. Cortos: Debes ser capaz de leer un do-file completo en poco minutos y entender el objetivo. Una buena regla es que no sean más largos de 250 - 350 lineas. No es estricta esta regla. Hay ocasiones que se justifican códigos más largos. Lo importante es la claridad. No sacrificarla nunca. Autocontenidos: Cada do-file interactúa con otros solo a través de los archivos que carga y de los que guarda. Es importante entender tu proyecto como un flujo estructurado con pasos predeterminados. Enfocados: Un do-file cumple un propósito determinado. Cada parte se debe entender como un paso intuitivo para lograr ese propósito. 2.2.3 El nombre de las variables debe caracterizarla Nombres adecuados remplazan comentarios y hacen el código auto contenido. Utilizar nombres completos. Abreviaciones solo en el caso de estar seguro que cualquier lector puede entenderlo sin ambigüedad. Si se utilizan abreviaciones. Ser consistente. Utilizar siempre las mismas a lo largo del código. Documentarlas, especialmente con bases de datos complejas. Cuando no se pueda ser suficientemente claro en un nombre, utilizar si o si para dar una descripción (ej. agregar unidad de medida, o que tipo de variable es). labels especifican información. (ej, edad con el label edad al 31 de enero es más informativa que edad y más corto que edad_31enero. No utilizar nombres en dos (o más) archivos que no expliciten porque son diferentes. Por ejemplo: version_final.do y version_final_ahorasiquesiestavezsi.do. Los nombres pueden ser más cortos cuando se utilicen con frecuencia o el objeto este inmediatamente después. Ejemplo: beta_mco = (educacion&#39; * educacion)^(-1) * /// (educacion&#39; * log_salarios) Mejor escrito sería: X = educacion Y = log_salarios beta_educ_salarios = (X&#39;*X)^(-1)*(X&#39;*Y) No ser tacaño(a) al nombrar variables, archivos, carpetas. Probablemente lo escribas solo una vez. No ahorres espacio. Siempre pensar en la claridad. Para el caso de una figura: c_1.pdf es un pesimo nombre. correlaciones.pdf es un mal nombre. T1_CORRELACIONES_INGRESO_MORTALIDAD.PDF es un mejor nombre. Para el caso de una carpeta: datos/brutos/interregional es un mal nombre. datos/brutos/Estimaciones Censo Interregional 2010 - 2021 es un buen nombre. Estos nombres nos permiten pensar en el flujo lógico del proyecto sin tener que mirar los códigos nuevamente. Ayudate constanmente. 2.2.4 Ten especial cuidado cuando uses algebra en los códigos. Separa el álgebra en partes. Por ejemplo, el siguiente código puede inducir a un error, dado que no esta separado en partes. gen pib_percapita_real = /// (cons + ggob + export - import - impuestos)* 10^6 /// /(indice_precios * poblacion_miles * 1000) Este código es más largo, pero mucho más entendible: gen pib_millones_nominal = /// (cons + ggob + export - import - impuestos) gen pib_total_real = pib_millones_nominal * 10^6 /// / indice_precios gen pop_total = poblacion_miles * 1000 gen pib_percapita_real = pib_total_real / pob_total 2.2.5 Ser consistentes Hay cosas de estilo que son solo un tema de gusto. Por ejemplo, hay quienes escriben nivel_empleo_anual y otras que escriben NivelEmpleoAnual. No importa la elección de un estilo u otro. Lo que importa es que todas y todos los miembros del equipo utilicen la misma convención. 2.2.6 Chequear errores Hemos aprendido que es importante escribir pruebas o testeos. Especialmente relevante después de operaciones complicadas o pegado de bases de datos. Stata en algunas ocasiones indica los errores que se comenten, sin la necesidad de colocar estas pruebas. Por ejemplo: gen string x = &quot;hola&quot; gen y = x^2 Stata retorna el siguiente error: type mismatch r(109); Normalmente el error da suficiente información. A pesar de lo anterior, en algunas circunstancias, no es tan claro saber cual es el error. Cuando ustedes se hayan encontrado con un error, que no les fue fácil descifrar, ** tienen que escribir un comentario que lo indique y explique**. Facilitara mucho el trabajo colaborativo. Eviten que otras personas tengan que invertir tiempo descifrando los mismos errores que ustedes. 2.2.7 Separar tipos de códigos Separar códigos lentos de los rápidos Códigos lentos que rara vez se piensan ejecutar idealmente debiesen estar separados de los códigos rápidos que se quieren ejecutar múltiples ocasiones. Evito tener que ejecutar un código lento cada vez que necesito actualizar algún resultado. Este criterio cambia en caso de que todos los códigos sean rápidos. Si todos son computacionalmente no demandantes, lo importante es respetar el flujo de trabajo y la claridad de los procesos de análisis. 2.2.8 Automatizar todo lo que se pueda automatizar Todo lo que se pueda automatizar debe ser automatizado. Por ejemplo, escriban ado-files que cambien para subconjuntos. Que los iteradores tengan números que se obtengan a partir del código, no por algo que ustedes tengan que escribir. Nunca hacer trabajo que vaya a obtener resultados de forma interactiva. Lo interactivo es siempre y unicamente para inspeccionar. En todo caso, al inspeccionar deberían hacer un código de todas formas. Esto les va a ayudar a saber que estaban pensando en el primer momento que vieron los datos que quieren analizar. 2.2.9 Abstraer para eliminar pasos redundantes. Abstraer con fines de hacer códigos más claros. No por otras razones. A nadie le importa saber que puedes abstraer demasiado, si nadie entiende la razón de hacerlo. La claridad es lo que mñas importa. Abstracción es esencial para escribir un buen código por al menos dos razones: Al eliminar la redundancia se reducen las posibilidades de cometer errores. Aumenta la claridad. Para cualquier lector será más facil leer un código no redundante. Ejemplo: Supongamos que queremos ver la correlación espacial del consumo de papas fritas. Queremos testear si el consumo per-capita de papas fritas esta correlacionado con el consumo promedio percapita de las otras comunas de la misma región. Primero tenemos que calcular el consumo per-capita del resto: egen total_pc_papitas = total(pc_papitas), by(region) egen total_obs = count(pc_papitas), by(region) gen consumo_papitas_resto_pc = /// (total_pc_papitas - pc_papitas)/(total_obs - 1) Ahora podemos ver si existe correlación. ¿Pero si queremos cambiar el nivel de agregación? Tal vez si existe correlación, pero a nivel de área metropolitana. Copiemos el código de nuevo y calculemos esto. egen total_pc_papitas = total(pc_papitas), by(metroarea) egen total_obs = count(pc_papitas), by(region) gen consumo_papitas_restometro_pc = /// (total_pc_papitas - pc_papitas)/(total_obs - 1) Noten que hay un error. Se nos olvido remplazar región por metroarea. Este error se puede propagar si seguimos haciendo operaciones. Una alternativa al copiar y pega es escribir una función con propósito general (ado - file, programa, función, etc) que calcule la variable que deseamos bajo distintos parámetros (noten que esto se relaciona con automatizar) program consumo_papitas_resto syntax, invar(varname) outvar(name) byvar(varname) tempvar tot_invar count_invar egen `tot_invar&#39; = total(`invar&#39;), by(`byvar&#39;) egen `count_invar&#39; = count(&#39;invar&#39;), by(&#39;byvar&#39;) gen `outvar&#39; = (`tot_invar&#39; - `invar&#39;) /// / (`count_invar&#39; - 1) end Con el programa podemos escribir los bloques de código anteriores como: * Caso 1 : a nivel de región consumo_papitas_resto, invar(pc_papitas) /// outvar(consumo_papitas_resto_pc) byvar(region) * Caso 2 : a nivel de area metropolitana consumo_papitas_resto, invar(pc_papitas) /// outvar(consumo_papitas_restometro_pc) byvar(metroarea) Hemos escrito la función de forma totalmente general. Podemos cambiar el nivel de agregación sin inducir errores por ir copiando y pegando códigos, además automatizamos un comando y evitamos repetición. 2.2.10 Crear códigos autoexplicativos que complementen a la documentación No escribas documentación que no vas a mantener. Lo mejor es que los códigos deben sean autoexplicativos. Veamos un ejemplo. Imaginemos queremos estimar una elasticidad de demanda para hacer un análisis de bienestar. * Elasticidad = Cambio porcentual en la cantidad / Cambio Porcentual en precio * Elasticidad = -0,4 / 0,2 = -2 * Mirar Informe 1, Tabla 2A. calcular_perdida_bienestar, elasticidad(3) Problema: código contradice comentario. Cuando tenemos dos fuentes o más fuentes que informan sobre el mismo objeto, es probable que estas se puedan contradecir. El problema de inconsistencia interna es especialmente importante para documentación, comentarios, notas, etc. Para evitar este problema es necesario mantener los comentarios al día tal como se hace con los códigos. Hacer esto es costoso. Mantener un código extenso al mismo tiempo que una larga documentación no es sencillo. Dado esta restricción, es importante escribir códigos que sean claros sin la necesidad de escribir comentarios extensos. Volviendo al ejemplo. ¿Como le decimos al lector del código debe tener una elasticidad de 2 y no de 3? Ejemplo de un código auto-explicativo: * Mirar Informe 1: Tabla 2A. local cambio_porcentual_cantidad = -0,4 local cambio_porcentual_precio = 0,2 local elasticidad = `cambio_porcentual_cantidad&#39; /// / `cambio_porcentual_precio&#39; calcular_perdida_bienestar, elasticidad(`elasticidad&#39;) Menos comentarios que el anterior. Sin números mágicos, los parámetros estan definidos por variables locales, en vez de imputados directamente. Este código es más informativo y es internamente consistente dado que no puedo cambiar algunos de los parámetros sin modificar el resultado. Comentarios sirven en su justa medida: es imposible desprender del mismo código sin saber la tabla. 2.2.10.1 Pensar siempre en la unidad de observación e identificar los id que identifican unicamente a la unidad de observación ID permiten saber la unidad de observación (hogar, empresas, lugares, hogar-año, empresa-mes-lugar). Conocer ID únicos es esencial para entender los datos. Indican que es lo que describen los datos. También nos sirven para entender como juntar estos datos con otras fuentes de información. Dos reglas que siempre tener en cuenta: No tienen que existir observaciones duplicadas que tengan el mismo ID único. Los ID no pueden tener missings. Una forma de verificar estos dos hechos es utilizando el comando isid. Este comando chequea si las variables especificadas identifican únicamente a las observaciones. Si esto no ocurre arrojara el siguiente mensaje: variable does not uniquely identify the observations Si encuentro plausible que alguna variable tenga missing puedo colocar la opción missok. 2.2.11 No ser repetitivo Hemos hablado de evitar ser repetitivo. ¿Cuándo hacerlo? Repetir en una operación simple con dos categorías es razonable. xtile decil_ing_ga = ing if grupo==&quot;A&quot;, nq(10) xtile decil_ing_gb = ing if grupo==&quot;B&quot;, nq(10) Si tengo que operar 15 veces, mejor hacerlo iterativamente: forvalues y = 2001/2015 { xtile decil_ing_`y&#39; = inn if año == `y&#39;, nq(10) } Si involucra múltiples operaciones. A pesar de que solo existan dos categorías, es mejor hacerlo iterativo. foreach g in &quot;A&quot; &quot;B&quot; { assert ingreso &gt;= 0 if grupo == &quot;`g&#39;&quot; xtile decil_ingreso_`g&#39; = ingreso /// if grupo==&quot;`g&#39;&quot; &amp; ingreso &gt; 0, nq(10) replace decil_ingreso_`g&#39; = 0 /// if grupo==&quot;`g&#39;&quot; &amp; ingreso == 0 label var decil_ingreso_`g&#39; &quot;Deciles de ingreso&quot; } ¿Como evitar ser repetitivo? Cuando necesites cambiar solo una cosa en el código utilizar un loop. foreach y forvalues iteran por el mismo código múltiples veces, cambiando el valor de una variable local cada vez. Cuando necesites cambiar múltiples cosas dentro de un código, define un programa dentro del do-file utilizando program. De este modo tu podrás especificar más de un argumento. Cuando necesites repetir un código en múltiples do.files, pueden definir un programa en un ado-file, guardarlo y llamarlo cuando lo necesites. Es importante que el código agregue automáticamente la ruta en donde se encuentran guardados estos programas. Algunas ayudas: Cuando algunas lineas de código necesiten cambiar entre repeticiones utilizar if/else y forvalues. Estos iteran por el mismo código múltiples veces, cambiando el valor de una variable local cada vez. Cuando definas programas dentro de un do-file, agrega una linea que diga cap program drop &lt;program_name&gt; antes de program define. Automáticamente se eliminará y redefinirá el programa cada vez que ejecutes el código. 2.2.12 Mensajes finales sobre estilo de codificación Haz las cosas lo más simples posibles para ti (y tu equipo) de hoy y del futuro. No confíes tanto en ti de hoy y menos en el del futuro. Es mejor programar defensivamente. Ser consistente en formato, estilo, organización y nombres. Sigue estrategias. Reducir en lo posible pegado de códigos y repetición innecesaria. Testear regularmente. Documentar a menudo, estratégicamente, pero mínimamente. 2.3 Organización de un proyecto en Stata 2.3.1 Flujo de trabajo en Stata Hasta el momento uno de los mayores mensajes es adoptar un sistema de organización compartirlo en el equipo y adherirse a él. Este debe incluir: Estructuras de directorio. Estilo de codificación. Convenciones para escribir nombres. Convenciones para exportar datos. Hemos visto en detalle (2) y (3). Algunas cosas de (4) (más detalles en otro capítulo). En esta sección nos enfocaremos en (1). Esta sección se basa en una de las guías de Asjad Nqvi. Muy buenos recursos. Muy recomendado. También revisaremos algunos elementos de (). Un proyecto de análisis de datos contempla: planificar el trabajo, documentar las actividades, crear y verificar variables, generar y presentar análisis estadístico, replicar resultados y archivar. Todo lo anterior es a lo que nos referiremos como flujo de trabajo. Un buen flujo de trabajo es esencial para que tu y tus colaboradores(as) y los interesados en tu trabajo puedan replicarlo. Replicar es clave para generar información confiable y correcta. La administración del flujo de trabajo se hace al inicio de un proyecto. No al final. No hacerlo así tiene costos en tiempo y posiblemente induzca errores no forzados. Veremos algunos aspectos relacionados con la administración de datos y scripts. Nuevamente, más que reglas a seguir, lo presentado aquí son una serie de buenas practicas que pueden ayudar a mejorar la gestión del trabajo con Stata. La idea de esto es que finalmente podamos dedicarle más y mejor tiempo al análisis de datos. * Nuevamente, tener un flujo de trabajo es clave, incluso los mayores expertos en estos temas tienen problemas con esto. Por ejemplo, en el prólogo de Long and Long (2009) When I started, I thought my workflow was very good and that it was simply a matter of recording what I did” As writing proceeded (this book), I discovered gaps, inefficiencies, and inconsistencies in what I did. Sometimes these involved procedures that I knew were awkward, but where “I never took the time to find a better approach”. Some problems were due to oversights where I had not realized the consequences of the things I did or failed to do. 2.3.2 Organización de archivos y carpetas Algunos problemas que son recurrentes: Tener múltiples versiones de un archivo y no saber cual es cual. No poder encontrar un archivo y pensar que lo has borrado. Tu equipo (y tú) no saben cual es el informe final. Hay dos archivos que con el nombre archivo final, pero con distinto contenido. Para evitar los problemas anteriores es importante diseñar la estructura de la carpeta de trabajo y comprometerse a seguir una alerta. 2.3.2.1 Carpetas Todo se guarda en carpetas. TODO. Estas deben seguir un orden lógico e intuitivo. Como consejo. Colocar números en la carpeta. Al ordenar las carpetas seguirán este orden y no el alfabético. No tener espacios en el nombre de las carpetas. Nunca utilizar (-) entre nombres de una carpeta. Utilizar (_). Evitar nombrar las carpetas con mayúsculas. Siempre escribir un README.txt. Puede ser tedioso, pero muy necesario. Corto, informativo, que explique el flujo de trabajo y las carpetas. Sugerencia: colocar las rutas. En esta figura se ve la idea de como deberia estructurarse las carpetas de un proyecto pequeño Lo mismo, pero para un proyecto grande Lo mismo, pero presentado de otra forma: En la medida que sus carpetas se parezcan más a estas, mucho mejor. Es importante generar dos tipos de carpetas para los resultados trabajo y otras que diga compartir. trabajo guarda todos los resultados (ej. tablas, gráficos, documentos) que están siendo trabajados. compartir guarda resultados terminados y listos para compartir en alguna etapa del proyecto. Por ejemplo, cuando se quieran compartir resultados intermedios del proyecto. Dos reglas: Resultados se comparten solo después que los archivos que lo generan pasen a la carpeta compartir. Una vez traspasados a la carpeta compartir. No se modifican. Incluso cuando puedan existir errores. Ayuda a garantizar que los resultados que se comparte se puedan replicar. Si encuentras errores o decides cambiar algo no cambies los ya posteados. Genera nuevos. Esto también ayuda a la replicabilidad: permite saber que cambios existieron durante un proyecto. Un directorio de trabajo estructurado es especialmente importante para proyectos colaborativos donde todo se puede desordenar mas fácilmente. Dos directorios son sugeridos cuando se trabaja de forma colaborativa: correo y otras que diga privados. correo guarda intercambios entre colaboradoras mientras que privados guarda el trabajo propio de cada colaborador(a). Otras carpetas que pueden complementar trabajo son porhacer que sea una carpeta que sirva como una lista de tareas poraprender. En cualquier caso, estas carpetas pueden ser parte de privados. Cuando se tengan versiones se pueden utilizar dos modos: v1, v2, v3, etc. Fechas. Si se utilizan estas escribirlo en año/mes/día. De esta forma se ordenara automáticamente desde el último al primero. Cuando versiones anteriores no te sean útiles. Borralas! No acumules archivos y códigos porque sí. Si por alguna razón deseas dejarlos (ej. necesidad de respaldar lo hecho). Genera una carpeta temporal. Esta puede estar en tú versión personal del proyecto, pero no en el archivo final. 2.3.3 Master do-files Un Master do-file es un do-file que ejecuta todos los códigos relevantes para el proyecto. Se basa en la idea de que diferentes do-files se enfocan en diferentes tareas o subtareas. No es buena practica tener un solo código enorme que haga todo el proceso. Es importante separar tanto todos los do-files (incluyendo el master) en secciones y subsecciones. Por ejemplo: Preámbulo, cargar datos, generar variables, figura I, Tabla 3, etc. Crear master_do.files cada proyecto o subproyecto debe tener un archivo maestro que ejecute todos los códigos de inicio a fin y genere todos los resultados. Cada vez que se termine un proyecto o se obtenga algún resultado es importante ejecutar el master_do.files. Ejemplo de master do-file: *** Información del proyecto aquí *** *** Información sobre directorios *** *** Ejecutar códigos *** * Generación de datos do ./dofiles/01_datos_v11.do do ./dofiles/02_merge_v2.do * Generación de tablas do ./dofiles/03_tablas_v4.do * Estimación principal do ./dofiles/04_estimaciones_v11.do * Figuras principales do ./dofiles/05_graficos_v11.do 2.3.4 Rutas de las carpetas: Es importante tener una carpeta principal y referenciar todas las rutas a esa dirección. Utilizar siempre / no \\textbackslash. Puede causar problemas según el sistema operativo que utilices. Utilizar macros para nombrar a las carpetas. Que estos nombres tengan sentido y se relacionen con el flujo de trabajo. Recordar que una variable local guarda información temporalmente mientras que global guarda información permanentemente. Esto último hace de los global algo no recomendable de utilizar constantemente. No obstante, es útil para los directorios. *** Nota 1: Remplazar esta ruta *** global miproyecto &quot;C://Program Files/Dropbox/miproyecto/&quot; *** Nota 2: Directorios de subcarpetas *** clear global datosbrutos &quot;$miproyecto/datos/brutos&quot; cd &quot;$datosbrutos&quot; use datos1.dta, replace 2.3.5 Utilizar algún auxiliar de generación de código: Sublime Text Esta sección esta basada en la página web de Alvaro Carril. 2.3.5.1 ¿Qué es Sublime? Sublime Text es un editor de códigos gratuito. Este editor permite compilar códigos de distintos lenguajes de programación tales como Stata, Python, R, , entre muchos otros. Tiene algunas herramientas que facilitan el trabajo de programación tales como atajos (shortcuts) o multicolumnas. Veremos varias en esta sección. Es muy útil cuando tienes un proyecto grande con códigos de distintos tipos de lenguaje. Instalar Sublime Dirígete a sublimetext.com. Descarga e instala Sublime Text 4 (ST4) para tu sistema operativo. Es esencial que también instales Package Control, que te permite añadir y eliminar fácilmente complementos adicionales. Si esta bien instalado debiese aparecer en preferences. Para installar hay que entrar a sublime, apretar ctrl + shift + p. Tipear Install Package Control y presionar enter. 2.3.5.2 Ejecutar Stata desde Sublime Para Windows Instalar StataEditor: En ST4 aprete ctrl+shift+p. Escriba install y presione Enter. Ahora busque StataEditor y haga clic en él para instalarlo. Instale Pywin32: repita el proceso anterior pero instalando el complemento Pywin32. Configure StataEditor: En ST4 vaya a Preferences -&gt; Package Settings -&gt; StataEditor -&gt; Settings - Users En este archivo en blanco debe escribir la ruta de su ejecutable de Stata y la versión que posee. Mi archivo de configuración en Windows se ve así: { &quot;stata_path&quot;: &quot;C:/Program Files/Stata17/StataMP-64.exe&quot;, &quot;stata_version&quot;: 17, } Registre la biblioteca de automatización de Stata: Vaya al ejecutable de Stata del que ha copiado la ruta (por ejemplo, StataMP-64.exe). Cree un acceso directo en su escritorio. Ahora haga clic con el botón derecho en este nuevo acceso directo y seleccione ``Propiedades”. En el campo ``Destino”, añada /Registro al final (con un espacio precedente). En mi caso es: &quot;C:\\Program Files\\Stata17\\StataMP-64.exe&quot; /Register Aplicar y aceptar los cambios. Apretar sobre el acceso directo el click derecho y ejecutar como administrador. Y listo! Para Mac Para instalar el Editor Mejorado de Stata en ST4, inicie la paleta de comandos con Cmd+shift+p, escriba `install\" y presioneEnter`. Ahora busque Stata Improved Editor y haga clic en él para instalarlo. Reinicie ST4 y listo! Autoguardado Sublime permite auto-guardar los códigos. De esta forma no hay que preocuparse en casos de que no hayas respaldado tu código. Para esto hay que escribir vamos a instalar el paquete autosave y adicionalmente vamos a agregar a ``preferencias”: ``save_on_focus_lost&quot;: true. References "],["id_01-chap02.html", "3 Programar do - files 3.1 Programar en Stata", " 3 Programar do - files 3.1 Programar en Stata Esta sección esta basada en: An introduction to Stata Programing (), en notas de clase de la profesora Erin Hengel, en Advanced Stata Topics del profesor Alexander C. Lembcke, en Seventy-six Stata tips , en Top 10 ``gotchas” de y en Data Management Using Stata. A Practical Handbook de . 3.1.0.1 ¿Qué es programar en Stata? Programar en Stata es: Escribir do - files: una secuencia de comandos ejecutables a través de un archivo .do. Escribir los que formalmente en Stata es un programa: un conjunto de comandos que incluyen el comando program. Un programa en Stata se guarda como un ado - file. Escribir lenguaje de programación matricial: denominado mata. 3.1.0.2 do - files El uso de do-file garantiza la replicabilidad del análisis de datos utilizando Stata. Recordar que un do-file puede llamar a otros (ej. master do.file. La jerarquización de los do-file puede ser importante para proyectos grandes o complejos. Es importante evitar trabajar en Stata interactivamente. Únicamente hay que hacerlo para inspeccionar datos (recordar practicas Sección 3.1). 3.1.0.3 ado - files Sirven para crear tus propios en Stata. Una vez que armes tu programa y lo guardes en la carpeta de ado-file puedes utilizarlo como cualquier otro comando de Stata. Por ejemplo, puedes agregar las opciones if, in range y otras. También puedes escribir un documento de ayuda (help} que explique el programa. Crear tus propios comandos es una muy buena forma de ser más eficiente al trabajar con Stata (recordar Sección 1). Mata para ado-files: Los ado-files pueden realizar tareas más complejas que involucren ejecutar el comando múltiples veces. El lenguaje de programación mata es mucho más rápido que un ado-file. Útil para realizar tareas que sean intensivas computacionalmente. No es solo un lenguajes de programación que utiliza matrices. También sirve para tareas que involucran texto o listas. Próximante discutiremos este lenguaje. 3.1.1 Comandos y funciones claves (y algunos detalles)} Revisaremos algunos aspectos necesarios para mejorar la eficiencia al trabajar en Stata. Directorios y uso de profile.do. Tipos de comandos. Tipos de datos y uso de compress. Uso de capture, preserve y restore. 3.1.1.1 Directorios cd y pwd sirven para fijar y conocer mis directorios. Es importante utilizar dobles comillas en los casos en que la ruta del directorio tenga espacios. El comando sysdir provee una lista de los directorios importantes para Stata. El comando update sirve para actualizar los comandos. La carpeta PLUS es el directorio donde se guardan comandos descargados. Si utilizas ssc, se guardara en esta carpeta. PERSONAL para tus propios ado-files. 3.1.1.2 Profile do ¿Qué es profile do? Una opción no tan conocida es la utilización de profile.do Este archivo se ejecuta cada vez que Stata se inicia. Si no tienes uno guardado no pasa nada. Este do-file permite, por ejemplo, fijar tu directorio de trabajo inicial, cambiar características de los gráficos, generar atajos. También se puede hacer que Stata abra un log-file y lo guarde siempre en el mismo lugar. Cada vez que inicies sesión Stata ira por profile.do. Si lo encuentra lo va a ejecutar. Stata recomienda guardar profile.do en home directory (ver help profilew para ususarios de windows y profilem para Mac). Vamos a ver como modificar profile.do. En particular: Ajustes generales. Establecer las características de los gráficos. Establecer atajos Abrir, cerrar y guardar un log-file con la fecha. Ejercicio 3.3.1: Instrucciones Descargar carpeta ejercicios-clase3. Abrir profile.do e inspeccionar cada una de las lineas del código. Preguntas Agrega dos elementos a alguna sección. Modifica las carpetas según las características de tu computadora. Guarda profile.do en el home directory. Ejecutar nuevamente Stata y verificar que el código se ha ejecutado. 3.1.1.3 Tipos de comandos Comandos r-class y e-class Muchos comandos en Stata (ej. summarize, correlate, regress) hacen que sea posible utilizar sus resultados una vez que estos son ejecutados. summarize y correlate son comandos r-class. Es decir, comandos que guardan sus resultados en r(). regress es un comando e-class. Es decir, comandos que guardan sus resultados en e(). return list retorna los valores guardados en r() * r-class sysuse auto.dta, clear summarize mpg return list summarize mpg, detail return list display &quot;La asimetría de mpg es&quot; r(skewness) summarize price, detail return list Su limitación es que solo están disponibles los valores del último comando r-class ejecutado. Los comandos e-class son comandos de estimación. Para verlos ereturn list y para llamarlos individualmente e(nombre). Guardan más información que los de r-class : matrices, vectores y funciones. La información de los comandos e-class sigue estando disponible una vez que utilizamos algún comando r-class`. Aquí una diferencia! Un ejemplo clásico son las estimaciones de regresión lineal. regress mpg weight length rep78 display &quot;La regresión se estimo para &quot; e(N) &quot; observaciones.&quot; ereturn list En general cualquier comando de estimación se guarda en esta clase de formato. 3.1.1.4 Tipos de datos numeric y string La mayor distinción entre tipos de datos es entre numeric y string. Al trabajar con datos muchas veces es necesario realizar conversiones entre estos dos formatos. Los comandos destring, tostring y encode son útiles para estas tareas (si alguien tiene dudas con alguno me escriben). strings pueden soportar un máximo de 244 caracteres con un byte por cada carácter. Por ejemplo, una variable del tipo str20 requiere 20 bytes por observación. Clasificación de los datos Para las variables en formato numeric los tipos de datos son: byte, int, long, float y double. Los tres primeros solo puede almacenar valores enteros. long puede almacenar todos los números de 9 dígitos, pero es limitado para 10 dígitos. float y double pueden almacenar números grandes. No asumir que float será aritméticamente exacto. Por ejemplo: display float(16777216) 16777216 display float(16777217) 16777216 ¿Como utilizar esto para programar efectivamente en Stata? IDs con muchos dígitos (y caracteres) guardarlos como string. No como integers, float o doubles. No confiar en test exactos contra una constante con datos en formato float. Utilizar formato double para cualquier serie que necesita ser precisa (ej. suma de los residuos de una regresión). Utiliza integers cuando sea apropiado (ej. variables dicotómicas). Guardar valores como int o byte ayuda a utilizar de forma más eficiente el espacio en el disco. compress examina cada variable y determina si estas pueden ser guardadas de forma más eficiente. Utilizarlo. Si se especifican valores iniciales fuera de los rangos permitidos para cada tipo de dato el resultado será un missing. clear all set obs 10 generate byte var1 = 101 summarize var1 Notar que no se genera ninguna alerta de missing cuando este se crea. Esto es distinto para variables que ya existen. clear all set obs 10 generate byte var1 = 1 replace var1 = 101 Si el valor esta fuera de los rangos permitidos, la variable se guarda en un formato mayor. byte a int, int a long, a float a double. En este caso un mensaje aparece. 3.1.1.5 capture, preserve y restore Manejando errores: capture Sirve para evitar que Stata aborte cuando detecta un error. Bueno para cuando quieres borrar algo que ya no se encuentra. Lo malo es que suprime todos los errores y oculta todo lo que puede ir mal. set obs 10 generate byte var1 = 5 generate byte var2 = 10 capture drop var1 var2 var3 describe var1 var2 Código crea var1 y var2, luego las elimina incluyendo una variable que no existe. Al contrario de lo que se intuye, no se borran las variables, dado que hay una variable que no existe. La recomendación es siempre utilizar capture con moderación. Puede ser utilizado en bloque para así no tener que utilizarlo en cada linea. sysuse auto.dta, clear capture{ reg price mpg-trunk reg price mpg-weight reg price mpg-foreign } ereturn list preserve y restore Algunos comandos en Stata remplazan la base actual por una nueva (ej. collapse o contract). Utilizar preserve y restore es útil en estos casos. En caso de que queramos obtener estadística descriptiva agregada y asociarla a observaciones podemos utilizar estos comandos. * Ejemplo con collapse sysuse auto.dta, clear generate lprice = log(price) preserve collapse (max) max_lprice=lprice max_mpg=mpg /// (iqr) iqr_lprince = lprice iqr_mpg = mpg if !missing(rep78), by(rep78) sort rep78 tempfile repstats save `repstats&#39; restore sort rep78 merge m:1 rep78 using `repstats&#39; assert _merge != 2 summarize lprice max_lprice max_mpg * Ejemplo con contract sysuse auto.dta, clear preserve contract mpg, cfreq(cumfreq) percent(percentage) cpercent(cumpercent) sort mpg tempfile mpgfreq save `mpgfreq&#39; restore sort mpg merge m:1 mpg using `mpgfreq&#39; assert _merge != 2 summarize _freq cumfreq percentage cumpercent 3.1.1.6 Missing values En general uno utiliza if variable !=. para evitar incluir missings. Mejor practica es variable &lt;. para excluir todos los valores en missing. Otra opción es if !missing(variable). Los missing están codificados internamente como valores mayores a cualquier número. El menor valor de todos los missing es el punto. Al utilizar if variable &lt;. es como decir, solo utiliza los números. Como vimos en la Sección 3.1 hay muchos problemas al no tratar bien los missings. Este problema se incrementa al momento de trabajar con bases de datos imposibles de inspeccionar. Veremos algunas recomendaciones que pueden ser utiles para evitar estos errores. Los comandos tratan distinto a los missings Cualquier función de datos missing será missing. Cuando se calcula un promedio o una desviación estándar solo valores no missing son considerados (ej. sum). Algunos comandos en Stata manejan los missing de otras formas. Por ejemplo, las funciones max, min y las funciones para filas de egen: rowmax(), rowmean(), rowmin(), rowsd() y rowtotal() ignoran los missing. Por ejemplo, rowmean(x1,x2,x3) calcula el promedio de las variables y solo retornara missing si todas lo son. Por ejemplo, rowmean(x1,x2,x3) calcula el promedio de las variables y solo retornara missing si todas lo son. collapse (sum) trata a los missing como ceros. Calcular un promedio con missings clear set obs 10 gen var1 = rnormal() gen var2 = 5 gen var3 = . gen promedio = (var1 + var2 + var3)/3 egen promedio1 = rowmean(var1 var2 var3) PromedioSinMissing da como resultado missings. PromedioConMissing no considera missings. Es importante tratar de entender como funcionan los missings de los comandos que utilizas. Generar variables considerando missings: Al crear una variable dicotómica, gen y gen byte tratan a los missing de formas distintas. local N = 500 set obs `N&#39; gen indicador = uniform() &lt; .5 replace indicador =. if mod(_n, 2) == 0 * Si indicador es missing, variable sera missing gen variable_primercaso = 1 if indicador == 1 replace variable_primercaso = 0 if indicador == 0 * Si indicador es missing, variable sera cero. gen variable_segundocaso=(indicador==1) \\end{minted} \\end{itemize} sum() considera missing como ceros. clear all set obs 4 generate byte var1 = cond(mod(_n,2)==1, 1, .) generate byte var1sum = sum(var1) list, noobs max() trata a los missing como si no estuviesen allí. display max(-5,.) tabmiss, mvdecode y mvencode En ocasiones los missing difieren en notación (ej. al importar datos de otro paquete). Siempre que trabajes con una base de datos nueva es importante recodificar. Notar que no todos tienen que ir a un punto. Todo depende del origen de los missing. mvdecode y mvencode pueden ser útiles en este tipo de casos. mvdecode permite recodificar valores numéricos como missing. Útil cuando valores son representados como -99, -999. mvencode hace lo inverso. Mapea missing como numéricos. El comando tabmiss inspecciona todas las variables de una base de datos y reporta los missing totales y como fracción del total de observaciones. clear all set seed 1234 local N = 50 set obs `N&#39; gen income = abs(int(rnormal(0,5))) assert income &gt;= 0 replace income =. if mod(_n, 2) == 0 * Para ver los missing en variables tabmiss * Transformar un valor númerico a missing mvdecode income, mv(2) * Transformar varios valores númericos a missing. mvdecode income, mv(2 5) * Transformar varios a missing, pero identific ́andolos. mvdecode income, mv(3 = .a \\ 6 = .b) * Missings se cambian de vuelta a su valor original. mvencode income, mv(.a = 3 \\ .b = 6) Ejercicio 3.3.2: Instrucciones Abrir ejercicios-clase3.do y ejecute las líneas correspondientes al ejercicio 2. Pregunta Compare las opciones 1, 2 y 3 en relación a como tratan los missing. Explique las diferencias entre cada una de estas opciones. generate, replace y missing: Sólo unas pequeños detalles. clear cd &quot;$ejercicio3&quot; use census2c, clear * Opción 1 gen smallpop_o1 = 1 if pop&lt;=5000 replace smallpop_o1 = 0 if pop&gt;5000 * Opción 2 gen smallpop_o2 = (pop &lt;= 5000) * Opcion 3 gen smallpop_o3 = (pop &lt;= 5000) if !missing(pop) Opción 1 es la típica. Ojo no esta considerando missing. Es necesario agregar &amp; !missing(pop). Escrito de esta forma, si pop es missing, smallpop será cero. Opción 2 es más simple, pero si cualquier valor de pop es missing será evaluado como un cero también. La razón es que los missing en Stata son considerados como números muy grandes para el programa. La Opción 3 soluciona el problema. 3.1.1.7 String a numeric y al revés De string a numeric Si las variables han sido mal clasificadas como string puedes utilizar la función real(). Por ejemplo: generate idpaciente = real(pacienteid). El comando anterior genera missing para todas las observaciones que no puedan ser interpretadas como numéricas. Mucho mejor es utilizar destring, replace. Otro caso usual es que tenemos datos en formato string y queremos que tengan un equivalente. El comando encode. No es aconsejable utilizar este comando para valores numéricos guardados como string. De numeric a string Hay veces en las que se quiere generar un equivalente string a valores numéricos. Tres comandos: string() , tostring() y decode(). Un ejemplo es querer mantener los 0 que estan al inicio de un código ID. El comando tostring zip, format(\\%05.0f) generate(idstring) genera un string de cinco digitos con los ceros al inicio. decode() sirve para un caso en que tengas un id en numérico, pero que no la tengas en string. Strings entre comillas: importa poner las comillas bien. display &quot;Este es un string normal&quot; display &quot;Este no es un string con &quot;comillas&quot; &quot; display `&quot;Este si es un string con &quot;comillas&quot;&quot;&#39; 3.1.1.8 Funciones para generar variables generate y replace: función cond() Si quiero que un resultado sea “a” si una condición es verdadera y “b” si es falsa. La función cond(x,a,b) posee esta capacidad sin la necesidad de utilizar if. * Útil para construir una tabla generate netmarr2x = cond(marr/divr &gt; 2, 1, 2) label define netmarr2xc 1 &quot;marr &gt; 2 divr&quot; 2 &quot;marr &lt;= 2 divr&quot; label values netmarr2x netmarr2xc tabstat pop medage, by(netmarr2x) Las observaciones en Stata estás numeradas desde el 1. _N es el mayor número de observaciones, mientras que el actual es _n. sort (ascendete) y gsort (ascendete o descendente) alteran el orden de las observaciones. Como recomendación eviten generar variables o condiciones que dependan de la posición especifica de una observación. * Ejemplo 1: uso de gsort gsort region -pop by region: generate totpop = sum(pop) * Ejemplo 2: uso de _n y _N by region: list region totpop if _n == _N * Ejemplo 3: sort generate largepop = 0 replace largepop = 1 if pop &gt; 5000 &amp; !missing(pop) gen smallpop = (pop &lt;= 5000) if !missing(pop) generate popsize = smallpop + 2*largepop label variable popsize &quot;Population size code&quot; label define popsize 1 &quot;&lt;= 5 million&quot; 2 &quot;&gt; 5 million&quot;, modify label values popsize popsize bysort region popsize: egen meanpop2 = mean(pop) recode para variables discretas: recode crea una nueva variable basada en otra variable. * Esta no es una buena opción replace newcode = 5 if oldcode == 2 replace newcode = 8 if oldcode == 3 replace newcode = 12 if inlist(oldcode, 5, 6, 7) * Esta si es una buena opción recode oldcode (2 = 5) (3 = 8) (5/7 = 12), gen(newcode) El signo (\\(=\\)) es para indicar valor antiguo a valor nuevo. No es necesario aplicarlo linea por linea. recode produce un código más eficiente. recode para variables continuas: recode(x,x1,x2,x3,x4,xn) para variables continuas de forma tal de generar intervalos tal que \\(x \\leq x_1 ; x_1 \\leq x \\leq x_2\\) y así sucesivamente. Los resultados son iguales a los límites creados. use census2c, clear generate breaks = recode(medage, 29, 30, 31, 32, 33) Otros comandos que cumplen una función parecida son floor y ceil(). Ambos sirven para generar un valor entero. El primero para redondear hacia abajo y el otro hacia arriba. floor(x) retorna el entero \\(n\\) tal que \\(n \\leq x &lt; n + 1\\) mientras que ceil(x) es tal que \\(n - 1 &lt; x \\leq n\\). use census2c, clear generate popurbfloor = floor(popurb) generate popurbceil = ceil(popurb) irecode para variables continuas: irecode(x,x1,x2,x3,x4,x_n) es una alternativa para categorizar grupos también. Por ejemplo: generate size = irecode(pop, 1000, 4000, 8000, 20000) label define popsize 0 &quot;&lt;1m&quot; 1 &quot;1-4m&quot; 2 &quot;4-8m&quot; 3 &quot;&gt;8m&quot; label values size popsize tabstat pop, stat(mean min max) by(size) Categorizara cada grupo según el intervalo en el que este. Parte del cero!. $x x_1 $, \\(x_1 \\leq x \\leq x_2 \\rightarrow 1\\) y así sucesivamente. Crear cuartiles con xtile: Con xtile podemos querer clasificar las variables según cuantiles (quintiles, deciles, cuartiles, etc). * Creamos cuartiles para población xtile popcuart = pop , nq(4) tabstat pop, stat(n mean min max) by(popcuart) Ejercicio 3.3.3: Instrucciones Cargar la base auto.dta Pregunta Genere una variable que sirva para redondear hacia abajo la variable mpg en en múltiplos de 5, de modo que cualquier valor de 10 a 14 se redondee a 10, cualquier valor de 15 a 19 a 15 y así sucesivamente. 3.1.1.9 Funciones de egen Todos y todas conocemos algunas de las típicas funciones de egen. clear * Ejemplo del uso de egen generate size = irecode(pop, 1000, 4000, 8000, 20000) label define popsize 0 &quot;&lt;1m&quot; 1 &quot;1-4m&quot; 2 &quot;4-8m&quot; 3 &quot;&gt;8m&quot; label values size popsize bysort size: egen avgpop = mean(pop) generate popratio = 100 * pop / avgpop format popratio \\%7.2f list state pop avgpop popratio if size == 0 Otras funciones son iqr(), kurt(),mad(), mdev(), median(), mode(), pc(), pctile(), rank(), sd(), skwe(), std(). egenmore Menos conocida es la colección de funciones adicionales de egen hechas por Nicholas J. Cox. Estas funciones están contenidas en el comando egenmore. bom() y eom() crean variables de fechas que corresponde al primer y último día de un mes determinado. corr() calcula correlaciones y covarianzas mientras que var() calcular la varianza. semean() calcula la desviación estándar del error de una media. record() permite calcular el valor más alto o más bajo de una serie. * Ejemplo 1: Generar variable que tenga la primera palabra de una frase (wordof) egen firstword = wordof(make), word(1) list firstword make in 1/15 * Ejemplo 2: Para generar automáticamente valores extremos (outside, 1,5 RIQ) egen extrmpg = outside(mpg) tab extrmpg, missing rall() y rany() son útiles para el análisis de datos. Evaluan una condición y genera un indicador si todas o alguna observación la cumple. set obs 12 gen a = 1 in 1 gen b = 2 in 2/4 gen c = -3 in 5/7 gen d = 4 in 8/10 gen e = . in 11/12 egen any = rany(a b c d e) , c(@ &gt; 0 &amp; !missing(@)) egen all = rall(a b c d e) , c(@ &gt; 0 &amp; !missing(@)) 3.1.2 Macros locales y globales 3.1.2.1 Nombrar macros Una macro es un contenedor que puede almacenar números o nombres de variables. Puede ser local o global. La primera es temporal, la segunda no. Un ejemplo de variable local es: local NivelEstres Nada Medio Moderado Severo display &quot;Los niveles de estrés son: `NivelEstres&#39;&quot; El primer comando define la macro y sus valores. Para llamar a la macro hay que utilizar las comillas (“). local nombre texto \\(\\rightarrow\\) local nombre = text \\(\\rightarrow\\) local nombre = \"text\". Ojo con las rutas: En ocasiones voy a querer utilizar una macro dentro de la ruta de una carpeta. Es importante utilizar siempre / o bien \\\\. De otra forma no lo reconocerá. local filename base.dta use &quot;H:\\ECStata\\`filename&#39;&quot; r(601); * Para corregir el error, dos caminos: use &quot;H:\\ECStata\\\\`filename&#39;&quot; use &quot;H:/ECStata/`filename&#39;&quot; Ejercicio 3.3.4: Compare los siguientes comandos y comente las diferencias: display \"Dos mas dos = 2 + 2\" display \"Dos mas dos= 2 + 2’“` Ojo con el signo igual: Algunas veces es bueno colocar un signo \\(=\\) al definir macros. Por ejemplo, cuando redefinimos variables. local contador 0 local NivelEstres Nada Medio Moderado Severo foreach a of local NivelEstres { local contador = `contador&#39; + 1 display &quot;Nivel de Estres `contador&#39; : `a&#39;&quot; } La primera parte sirve para definir la macro mientras que la segunda sirve para dar cuenta de su valor actual. Al actualizar, ocupen igual. Sin signo igual: En algunas ocasiones queremos escribir una macro dentro de un loop. En estos casos es conveniente evitar el signo igual. local contador 0 local NivelEstres Nada Medio Moderado Severo foreach a of local NivelEstres { local contador = `contador&#39; + 1 local nuevalista `nuevalista&#39; `contador&#39; &#39;a&#39; } display &quot;`nuevalista&#39;&quot; El local nuevalista define una macro como string que posee su propio contenido, el valor de contar y el valor del iterador. 3.1.2.2 Generar variables, contadores y condiciones con macros Podemos utilizar macros para renombrar variables. clear forvalues a = 10/20{ gen v`a&#39; = rnormal() } * Renombramos variables forvalues i = 11/15 { rename v`i&#39; x`=1960 + `i&#39;&#39; } En este fragmento de código, Stata evalúa la expresión 1960 + 'i' antes de evaluar la macro externa. Por ejemplo, cuando pase por el iterador i = 11, el nuevo nombre de la variable será x1971. Resumir condiciones: Podemos utilizar macros para resumir condiciones. Esto es útil para estimar modelos o generar estadística descriptiva. clear sysuse auto local cond &quot;if foreign==0&quot; local varlist &quot;mpg rep78 trunk weight turn&quot; * Estimar la regresión considerando la condicíòn reg price `varlist&#39; `cond&#39; Agrupar en base a una condición: Imaginemos que ahora queremos estimar una regresión para todas las compañías de auto que empiezan empiezan con B. local autoname B reg price mpg weight if substr(make,1,1)==&quot;`autoname&#39;&quot; La mejor manera de pensar en esto es hacer lo que hace Stata: reemplazar \"ctyname\" por su contenido substr(país,1,1)==\"ctyname’“. Al hacerlo, se convierte ensubstr(país,1,1)==”B”. Si se omiten las comillas dobles, se obtienesubstr(country,1,1)==B`, lo que da lugar a un error. Contadores: Las macros también pueden ser útiles para contadores. * Para adelante local i = 1 local ++i di `i&#39; * Para atrás local i = 1 local --i di `i&#39; Muy útil para hacer gráficos o guardar datos en matrices. Utilizar una macro para estimar regresiones: Imaginen que desean estimar regresiones sobre un conjunto de variables donde una parte de este conjunto esta fija y la otra parte es variable. local rhs mpg weight reg price `rhs&#39; if foreign == 0 local rhs &quot;`rhs&#39; headroom trunk&quot; reg price `rhs&#39; if foreign == 0 ¿Qué ocurre si queremos hacerlo por separado? ¿Uno a uno? local rhs &quot;mpg weight \\`add_var&#39;&quot; local add_var &quot;headroom&quot; reg price `rhs&#39; if foreign == 0 local add_var &quot;trunk&quot; reg price `rhs&#39; if foreign == 0 local add_var &quot;turn&quot; reg price `rhs&#39; if foreign == 0 Lo que ocurre cuando hacemos referencia a una macro es que su valor se introduce en ese punto. El uso de una barra invertida en su lugar hace que se introduzca la referencia de la macro, es decir, no se sustituirá el valor de add_var sino el término `add_var'. Así que cada vez que llamamos a la local rhs el valor actual de el local add_var es sustituido. Ejercicio 3.3.5: Instrucciones Cargar la base auto.dta Preguntas Defina una variable macro llamada control que contenga mpg, rep78 y headroom. Estime una regresión entre price y control para vehículos extranjeros y de nuevo para vehículos domésticos. Ejecute summarize mpg junto con return list. Defina dos macros display local mean1 r(mean) y local mean2 = r(mean). ¿Son iguales? 3.1.2.3 Globales Crear macros globales Se crean con el comando global. Útiles para fijar directorios o programas. En otros casos es mejor utilizar local. * Para generarla global variable * En caso de querer llamarla display $variable 3.1.2.4 Funciones extendidas de macros ¿Qué son las funciones de macros extendidas? Stata también define ciertas macro. Estas se denominan extended macro functions o macros extendidas. En algunos casos contienen información sobre tu sistema operativo, sobre la última estimación que se realizo o sobre la base de datos. El help extended_fcn y la documentación que la acompaña proporcionan una descripción completa de la sintaxis de cada función de macro extendida (hay muchas). Muchas tienen ligeras variaciones de sintaxis entre ellas (por ejemplo, algunas requieren que las macros estén entre comillas dobles; otras no lo permiten). * La sintaxis general es: local nombremacro: función macro extendida Mirar labels * Para mirar los labels de trunk sysuse auto, clear local tlab : variable label trunk display &quot;`tlab&#39;&quot; variable label recupera el nombre asignado a una variable. Contar dentro de un local * Para contar dentro de un local local NivelEstres Nada Medio Moderado Severo local wds: word count `NivelEstres&#39; display &quot;Hay `wds&#39; niveles de estres:&quot; forvalues i = 1/`wds&#39; { local wd: word `i&#39; of `NivelEstres&#39; display &quot;Nivel `i&#39; is `wd&#39;&quot; } word count y word como funciones de extensión que operan sobre strings. Conocer tipos de datos * Para conocer tipos de datos sysuse auto, clear local stortype : type make display &quot;`stortype&#39;&quot; Ojos con las comillas dobles (nuevamente): Algunas veces las macros contienen comillas dobles. Para poder escribirlos sin errores es necesario modificar levemente la forma en que se llama a la variable local * Con error local answers yes no &quot;do not know&quot; display &quot;`answers&#39;&quot; * Sin error local answers yes no &quot;do not know&quot; display `&quot;`answers&#39;&quot;&#39; Ejercicio 3.3.6: Preguntas Use una macro extendida para mostrar el tipo de dato (ej. int, float, long,…) de mpg. Revise el help de extended_fcn. Use una macro extendida para retornar el valor del label asociado a foreign cuando es igual a 1. Use una macro extendida para mostrar solo la primera variable de `controls'. Utilice una función extendida de macros para mostrar todos los archivos de su directorio actual (sugerencia: utilice comillas compuestas cuando muestre los nombres de los archivos). 3.1.2.5 Funciones de macro extendidas para listas Stata también define ciertas macro para operar sobre listas. Estas funciones permiten combinar listas, buscar elementos dentro de una lista o bien buscar elementos comunes entre dos listas. Conveniente revisar help macrolist para más funciones. * Sintaxis: local nombre macro: list función * Ejemplo de lista local animales &quot;gato perro gato loro loro&quot; local uniqanimales : list uniq animales display &quot;`uniqanimales&#39;&quot; Función levelsof: El comando levelsof lista los valores distintos de una variable. Al agregar la opción local(nombremacro) esto se guardara como una macro. * Sintaxis básica sysuse auto, clear levelsof rep78 display &quot;`r(levels)&#39;&quot; * Sintaxis cuando hay variables categóricas. levelsof foreign, local(levels) foreach l of local levels { di &quot;-&gt; foreign = `: label (foreign) `l&#39;&#39;&quot; reg price mpg if foreign == `l&#39; } 3.1.2.6 Manipulación de locales vía listas de macros Listas de macros: Las macro lists permiten obtener el número de elementos de una macro, trabajar con valores duplicados, ordenar elementos. Veremos cuatro aplicaciones: Elementos duplicados. Agregar y remover elementos. Uniones e intersecciones. Ordenar elementos. Elementos duplicados: dups extrae todos los elementos sobrantes. * Deja solo los unicos local fib 0 1 1 2 3 local fib_nodups : list uniq fib display &quot;`fib_nodups&#39;&quot; * Quita todo los duplicados local fib 0 1 1 2 3 local fib_dups : list dups fib display &quot;`fib_dups&#39;&quot; Agregar y remover elementos: Es básicamente pegar elementos de una macro con otra. Definamos dos variables locales: vars y coef y peguémoslos. local vars x y z local coefs a b c local vars_coefs `vars&#39; `coefs&#39; display &quot;`vars_coefs&#39;&quot; Para remover tenemos que definir un nuevo local con los elementos que queremos quitar y sustraerlo del original. Supongamos que queremos actualizar el contenido de vars eliminado “y”. local not y local vars : list vars - not display &quot;`vars&#39;&quot; Unión de elementos: Podemos pegar todos los elementos diferentes entre dos listas. local A house tree car local B computer car bike local all_things : list A | B display &quot;`all_things&#39;&quot; Noten que los elementos de car no fueron pegados. Notar que esto es distinto a simplemente pegar macros entre sí. Intersección de elementos: Podemos hacer la intersección entre dos elementos de una lista. Esto corresponde a los elementos que pertenecen a ambas macros. local A house tree car local B computer car bike local common_things : list A &amp; B display &quot;`common_things&#39;&quot; Noten que en este caso solo car se mantiene. Ordenar elementos: En ocasiones queremos ordenar los elementos de una lista contenida en una macro. local names camila camilo pedro paula local names : list sort names display &quot;`names&#39;&quot; Para hacer que los elementos de una macro se ordenen aleatoriamente es conveniente utilizar mata local nums 1 2 3 4 5 mata : st_local(&quot;random_nums&quot;, /// invtokens(jumble(tokens(st_local(&quot;nums&quot;))&#39;)&#39;)) display &quot;`random_nums&#39;&quot; 3.1.2.7 creturn Macros con creturn En algunos casos al utilizar local o global vamos a querer fijar algunos parámetros. Para este propósito utilizar creturn. Algunos ejemplos son: c(current_date), c(pwd), c(current_time), c(stata_version), c(pi), c(alpha), c(Wdays). Ejercicio 3.3.7: Preguntas Define una macro llamada comestibles con peras, manzanas, fresas, yogur, vino y queso en ella. Ponla en orden alfabético. Define una macro llamada unión que contenga los miembros de la macro animales y comestibles y luego utiliza una función de lista extendida de la macro para mostrar el número de palabras que contiene. Ordena unión y muestra la posición de la palabra “vino” utilizando una función de lista extendida de macros. 3.1.3 Estructuras de datos 3.1.3.1 Escalares Los escalares pueden contener valores numéricos o strings. Un escalar solo puede contener un valor. La sintaxis para generar un escalar es scalar scalar_name = exp. quietly: summarize mpg scalar mean_mpg = r(mean) quietly: summarize rep78 scalar mean_rep78 = r(mean) display &quot;r(mean) guarda el promedio de rep78: &quot; r(mean) display &quot;Pero tambien podemos recuperarlo:&quot; mean_mpg Para utilizar un escalar en una operación solo es necesario llamarlo por su nombre. Para listar el contenido de todos los escalares scalar list. Para borrar scalar drop scalar_name o si quiero eliminar todo scalar drop _all. Los escalares permiten parametrizar un do-file. use fem2, clear scalar lb1 = 80 scalar ub1 = 88 scalar lb2 = 89 scalar ub2 = 97 forvalues i = 1/2 { display _n &quot;IQ&quot; &quot;lb`i&#39;&quot; &quot;-&quot; &quot;ub`i&#39;&quot; tabulate anxiety if inrange(iq, lb`i&#39;, ub`i&#39;) } 3.1.3.2 Elementos y operaciones con matrices Stata puede generar matrices. reg weight age age2 matrix b = e(b) matrix list b matrix V = e(V) matrix list V Las matrices son muy importantes para guardar resultados y exportarlos de forma conveniente. También son útiles para cuando se quieren hacer estimaciones de muchos parámetros y para distintos conjuntos de datos. Operaciones con matrices: Se puede operar con matrices. Por ejemplo, sabemos que \\(b = (X&#39;X)^{-1}X&#39;y\\) y que \\(V = \\sigma^{2}(X&#39;X)^{-1}\\). Si queremos extraer solo la matriz \\(X&#39;y\\) tenemos que operar, calculando \\(\\frac{1}{\\sigma^{2}}V^{-1}b\\). matrix define b = e(b)&#39; matrix define xty = inv(V) * b /e(rmse)^2 matrix list xty e(b)’ indica la traspuesta, mientras que inv() es para calcular la matriz inversa. Llamar a los elementos de una matriz: Se puede acceder a los escalares contenidos en las matrices. Por ejemplo, si queremos obtener los elementos de la matriz \\(e(b)\\) tenemos: display &quot;The coefficient on weight is: &quot; _b[weight] display &quot;Its standard error is: &quot; _se[weight] * Valores predichos generate anxietyhat = _b[_cons] + _b[weight] * weight + /// _b[age] * age + _b[age2] * age2 También podemos utilizar las posiciones de los elementos de la matriz (b[i,k)]). 3.1.3.3 Funciones Solo disponibles para comandos e-class. Por ejemplo, si estimamos una regresión veremos que la unica función disponible es e(sample). sysuse auto regress mpg weight length rep78 ereturn list e(sample) nos indica si una determinada observación se utilizó para estimar la regresión. Es decir, es igual a uno si una observación estaba en la muestra de estimación y 0 si fue excluida. Ejercicio 3.4.1: Preguntas: Estime una regresión de mpg contra weight length rep78 utilizando base de datos auto, pero solo para los autos extranjeros (foreign == 1). Genere una nueva variable, llamada enmuestra que tome los valores dados por la función e(sample). Utilizando br, observe los valores de enmuestra. ¿Qué observa? Calcule la estadística de mpg solo para las observaciones que fueron incluidas en la regresión. 3.1.4 Iteradores 3.1.4.1 Foreach foreach y forvalues: forvalues itera sobre una lista de números y foreach recorre los elementos de una macro, o los nombres de las variables de una lista de variables, o los elementos de una lista de números. * Foreach foreach animal in cats and dogs { display &quot;`animal&#39;&quot; } * Forvalues forvalues i = 1(1)100 { generate x`i&#39; = runiform() } Hay algunas variaciones en foreach según el tipo de lista. La sintaxis es similar a la recien presentada, pero difiere en dos aspectos: in se remplaza por of. Hay que llamar al identificador. Veamos como iterar sobre una lista de globales y locales. \\end{itemize} * Sobre locales y globales local money &quot;Franc Dollar Lira Pound&quot; foreach currency of local money { display &quot;`currency&#39;&quot; } Veamos como iterar sobre una lista de variables: * Sobre lista de variables foreach var of varlist mpg weight-turn { quietly summarize `var&#39; summarize `var&#39; if `var&#39; &gt; r(mean) } Veamos como iterar sobre una lista de de nuevas variables: * Sobre lista de nuevas variables foreach var of newlist z1-z20 { generate `var&#39; = runiform() } Veamos como iterar sobre una lista de números: foreach num of numlist 1 4/8 13(2)21 103 { display `num&#39; } 3.1.4.2 Combinar macros con iteradores foreach y forvalues combinados con macros pueden utilizarse para ahorrarnos mucho trabajo. Podemos generar tun conjunto de locales a partir de iteraciones. use replicate.dta, replace levelsof cty local ctries &quot;`r(levels)&#39;&quot; foreach ctr in `ctries&#39; { sum hours_t if cty == &quot;`ctr&#39;&quot; local nombre `ctr&#39; = `r(mean)&#39; } Como ya hemos visto, el comando levelsof devuelve una lista de todos los valores distintos de una variable categórica y los guarda en la macro r(levels). Esto lo hace en el caso de que nosotros no le asignemos un nombre. Podemos utilizar esta lista para los países y años de nuestra muestra para definir dos iteraciones que recorran todos los valores posibles. Para cada valor resumimos la población y definimos una macro local compuesta por el código de país y el año (por ejemplo, USA1990) que toma el valor de la población en ese año para ese país. Una aplicación del forvalues: use gdp4cty, clear forvalues i = 1/4 { generate double lngdp&#39;i&#39; = log(gdp&#39;i&#39;) summarize lngdp&#39;i&#39; } Utilizando dos forvalues: forvalues y = 1995(2)1999 { forvalues i = 2(2)4 { summarize gdp`i&#39;_`y&#39; } } foreach y recode: use gdp4cty, clear local ctycode 111 112 136 134 local i 0 foreach c of local ctycode{ local ++i local rc &quot;`rc&#39; (`i&#39;=`c&#39;)&quot; } display &quot;`rc&#39;&quot; recode cc `rc&#39;, gen(newcc) Loops anidados: foreach y forvalues use gdp4cty, clear local country US UK DE FR local yrlist 1995 1999 forvalues i = 1/4 { local cnaine: word `i&#39; of `country&#39; display &quot;`cnaine&#39;&quot; foreach y of local yrlist { rename gdp`i&#39;_`y&#39; gdp`cnaine&#39;_`y&#39; } } En estos casos es bueno utilizar espacios para hacer el código más amigable. Ojo que a Stata no le interesa esto para ejecutar, es solo una cuestión de estilo. sysuse auto, clear foreach y of varlist mpg rep78 headroom trunk weight length { foreach x of varlist rep78 price displacement gear_ratio foreign { regress `y&#39; `x&#39; } También útil para estimar regresiones. Tokenize: Podemos almacenar los elementos de la lista de países en macros numeradas con tokenize. use gdp4cty, clear local country US UK DE FR local yrlist 1995 1999 local ncty: word count `country&#39; display &quot;`ncty&#39;&quot; tokenize `country&#39; forvalues i = 1/`ncty&#39;{ foreach y of local yrlist { rename gdp`i&#39;_`y&#39; gdp``i&#39;&#39;_`y&#39; } } Aquí los nombres de los países se almacenan como valores de las macros numeradas. Debemos referenciar doblemente la macro \\(i\\). El contenido de esa macro la primera vez que se pasa por el bucle es el número 1. Para acceder al primer código de país, debemos referenciar la macro \\(`1&#39;\\). 3.1.4.3 While loop Realiza la iteración o se repite una lista de comandos mientras la condición while sea verdadera. La sintaxis es: while exp { hace algo } ¿Cuando es útil? : Cuando no este seguro(a) cuantas veces se realizará la iteración. Notar que si no hay convergencia, va iterar infinitamente. while reldif (nueva, antigua) &gt; 0.001 { } También se puede combinar con macros. Es importante utilizar los incrementales: local i=1 while `i&#39;&lt;=5 { display &quot;loop number&quot; `i&#39; local i = `i&#39;+1 } La primera parte define el inicio del contador, mientras que la segunda indica la condición para que sea ejecutado. El local final actualiza (incremental). Si el incremento es unitario podemos utilizar local ++i. 3.1.4.4 Branching Hacer una cosa en caso de que alguna condición sea cierta y otra cosa en caso de que sea falsa. La sintaxis básica es: if algo es verdadero { hacer esto } else { hacer lo contrario } Ejercicio 3.4.2: Preguntas Defina una macro llamada mimacro que sea igual a un número entero aleatorio entre 1 y 99. Utilizando if y else muestre un mensaje que diga si es par o impar. 3.1.4.5 Aplicaciones Seguir secuencias especiales: creturn posee varias constantes y valores a los que se puede acceder. Por ejemplo: c(filename) nombre del último nombre del archivo guardado. c(alpha/ALPHA) lista de letras minúsculas/mayusculas. c(Mons) lista de nombres de los meses abreviados. c(Months) lista de los nombres de los meses no abreviados. c(Wdays) lista de los dias de la semana abreviados a tres caracteres. c(Weekdays) lista de los días de la semana no abreviados. Con un iterador es posible aplicar estas listas para agregar labels. Suponga que tenemos valores de 1 a 12 que representan meses. clear all set obs 12 gen month = _n tokenize `c(Months)&#39; forvalues i = 1/12 { label define monthlab `i&#39; &quot;``i&#39;&#39;&quot; , modify } label val month monthlab 3.1.4.6 Monitorear un loop Ejemplo proveniente de (Stata tip 41). Cualquier loop puede ser modificado para que muestre su progreso con el comando _dots. Esto es importante para cuando se requieren hacer procesos que toman varias horas y es necesario monitorear avances. _dots 0, title(Loop ejecutando) reps(75) forvalues i = 1/75 { _dots ‘i’ 0 } ----+--- 1 ---+--- 2 ---+--- 3 ---+--- 4 ---+--- 5 .................................................. 50 ......................... El primer comando _dots establece las lineas. Titulo y número de repeticiones son opcionales. reps solo acepta enteros como argumento. _dots ‘i’ 0 tiene dos elementos: El primer argumento es el número de repetición, que registra el número de intentos en curso. En el ejemplo, esta automáticamente determinado por el loop. El segundo argumento es el código de retorno, el cual indica el tipo de símbolo. En el ejemplo tenemos un 0. Los códigos de retorno alternativos producen una “s” (-1), “.” (), “x” roja (1), una “e” (2), una “n” (3) o un “?” (cualquier otro valor). Ejemplo no numérico: La idea es definir una macro local para que actue como un contador. sysuse auto _dots 0, reps(10) foreach var of varlist price - gear_ratio { sum `var&#39;, d local i = `i&#39;+1 _dots `i&#39; 0 } Contar repeticiones: El número de repeticiones en rep no es calculado por _dots. Es necesario contar manualmente las variables e introducir el número. Esto lo vamos a hacer con la ayuda de una función de macro extendida sizeof. unab permite ingresar una lista de variable abreviada y expandirla, de forma tal de que la pueda contar. sysuse auto unab myvars : price - gear_ratio local N : list sizeof myvars _dots 0, reps(`N&#39;) foreach var of varlist `myvars&#39; { ... local i = `i&#39;+1 _dots `i&#39; 0 } Un ejemplo más complejo es: noisily _dots 0, title(Looping until 70 successes...) local rep 1 local nsuccess 0 while ‘nsuccess’ &lt; 70 { local fail = uniform() &lt; .2 local nsuccess = ‘nsuccess’ + (‘fail’ == 0) noisily _dots ‘rep++’ ‘fail’ } Se ejecuta hasta que logre 70 aciertos. En este ejemplo artificial, cada iteración tiene un éxito aleatorio con una probabilidad del 80%. Los éxitos se indican con un punto (.) y los fracasos con una x. 3.1.5 Manejo de bases de datos 3.1.5.1 Prefijo: by Los prefijos en Stata ejecutan tareas repetitivas sin la necesidad de especificar el rango de valores sobre la tarea que es ejecutada. Un prefijo muy conocido es by. Por ejemplo, by varlist [, sort]: command. command es repetido para cada valor de la variable. Es más, repeticiones siguen el orden de la variable sea string o numeric. use bpress, clear bysort sex agegrp: summarize bp 3.1.5.2 Prefijo: xi xi es útil para cuando queremos producir un variable indicador para las observaciones que son distintas entre si. xi i.agegrp El ejemplo le esta diciendo a Stata que genere variables indicadores. Esto es muy útil para reducir los códigos. Interpretando prefijo xi: xi es comúnmente utilizado como un prefijo. La principal ventaja es cuando existen múltiples interacciones entre las variables. * Caso 1: incluye indicadores de ambas variables. xi: regress bp i.agegrp i.sex * Caso 2: incluye además interacciones entre ellas. xi: regress bp i.agegrp*i.sex * Caso 3: Interactúa una variable continua con una discreta. xi: regress bp i.agegrp*bp0 * Caso 4: Incluye solo interacciones con variable continua (además de principal). xi: regress bp i.agegrp|bp0 3.1.5.3 Prefijo: statsby statsby permite ampliar by. Este último tiene la limitación de permitir únicamente un comando. statsby mean=r(mean) sd=r(sd) n=r(N), by(agegrp sex): summarize bp Se produce una nueva base de datos con una observación por grupo con los estadísticos incorporados. Útil para calcular estadística descriptiva. 3.1.5.4 Prefijo: rolling statsby permite obtener estadísticas para sub-muestras que no se traslapan. rolling sirve para sub-muestras traslapadas. Por ejemplo, al trabajar con series de tiempo se quiere calcular estadísticas para datos que están traslapados (el. calcular una media móvil). Vamos a calcular medias y medianas utilizando una ventana de 90 días: use ibm, clear rolling mean=r(mean) median=r(p50), window(90): summarize spx, d tsset end tsline mean start y end indican el inicio y fin de la ventana. 3.1.5.5 Merge y Append Recomendaciones para utilizar merge Especifique siempre el tipo de fusión (1:1, m:1 o 1:m). Si no se especifica el tipo de fusión, se llama a la versión antigua y no robusta de la fusión. Nunca haga fusiones de muchos a muchos (m:m), o al menos, sólo hágalo cuando tenga una muy buena razón. Incluya siempre la opción assert() para indicar qué patrón de observaciones coincidentes espera. Incluya siempre la opción keep() para indicar qué observaciones deben conservarse del conjunto de datos fusionados. Siempre que sea posible, incluya la opción keepusing() y enumere explícitamente qué variables pretende añadir al conjunto de datos; puede incluir esta opción incluso cuando mantenga todas las variables de los datos utilizados. Utilice la opción nogen, excepto cuando piense utilizar explícitamente la variable _merge más adelante. Nunca debe guardar un conjunto de datos que tenga _merge; si necesita esta variable más adelante, dele un nombre más informativo. Con respecto a especificar _merge y assert notar que: merge ..., assert(match master) keep(match) * Es equivalente a: merge ... assert _merge==1 | _merge==3 keep if _merge==3 Append con ciudado: El comando append es muy útil para manejos de bases de datos. Una precaución común es con respecto al nombre de las variables. Si dos variables (ej. PRECIO y precio) difieren, se generaran dos columnas nuevas al hacer el append en vez de una. Una precaución un poco menos conocida guarda relación con el tipo de variables. ¿Qué ocurre si dos variables se llaman igual, pero estan guardadas en formatos distintos? En este caso, el orden en el cual se combine la base de datos va a importar y puede generar diferencias al momento de pegar datos. Esto es especialmente importante cuando una variable esta guardada en numérico en una base de datos y en string en la otra. Veamos un ejemplo con la base auto.dta. Vamos a crear dos bases de datos según la procedencia de los autos y ejecutar el comando append. sysuse auto drop if foreign save autodom sysuse auto drop if !foreign rename foreign nondom generate str foreign = &quot;foreign&quot; if nondom save autofor use autdom append using autofor describe foreign Notar que append genera el siguiente mensaje: foreign is str 7 in using data byt will be byte now. Noten que el contenido de la variable string se ha perdido: 22 casos son ahora missing. ¿Qué ocurre si hacemos el proceso al revés? Vamos a cargar autos extranjeros y le vamos a pegar autos domésticos: use autfor append using autodom, force describe foreign codebook foreign El formato de los datos de la primer base de datos manda. * No utilizar force sin cuidado. * Con distintos tipos de datos, append es sensible al orden en que los archivos son pegados. Tener cuidado y revisar consistencia en los datos. Hacer test aquí tambíen es importante. "],["programar-ado---files.html", "4 Programar ado - files", " 4 Programar ado - files 4.0.1 Escribir programas en Stata En esta sección aprenderemos a escribir nuestros propios comandos en Stata. Escribir programas en Stata tiene muchas ventajas relacionadas con las buenas practicas que vimos en la Sección 1. Vamos a utilizar program para escribir los programas. Es posible equipar cualquier comando con las opciones típicas usuales (ej. if o inrange. Discutiremos mayormente programas del tipo r-class y algunos comentarios con respecto a los del tipo e-class. ¿Por qué debiese escribir mis propios programas en Stata? Automatizar procesos que se ejecutan frecuentemente y donde los resultados dependen de algún tipo de heterogeneidad. 4.0.2 ¿Por qué escribir programas?: Abstracción Abstraer para eliminar pasos redundantes. Abstraer con fines de hacer códigos más claros. No por otras razones. Abstracción es esencial para escribir un buen código por al menos dos razones: Al eliminar la redundancia se reducen las posibilidades de cometer errores. Aumenta la claridad. Para cualquier lector será más facil leer un código no redundante. Veamos un ejemplo: Supongamos que queremos ver la correlación espacial del consumo de papas fritas. Queremos testear si el consumo per-capita de papas fritas esta correlacionado con el consumo promedio percapita de las otras comunas de la misma región. Primero tenemos que calcular el consumo per-capita del resto: egen total_pc_papitas = total(pc_papitas), by(region) egen total_obs = count(pc_papitas), by(region) gen consumo_papitas_resto_pc = /// (total_pc_papitas - pc_papitas)/(total_obs - 1) Ahora podemos ver si existe correlación. ¿Pero si queremos cambiar el nivel de agregación? Tal vez si existe correlación, pero a nivel de área metropolitana. Copiemos el código de nuevo y calculemos esto. egen total_pc_papitas = total(pc_papitas), by(metroarea) egen total_obs = count(pc_papitas), by(region) gen consumo_papitas_restometro_pc = /// (total_pc_papitas - pc_papitas)/(total_obs - 1) Noten que hay un error. Se nos olvido remplazar región por metroarea. Este error se puede propagar si seguimos haciendo operaciones. Una alternativa al copiar y pegar es escribir una función con propósito general que calcule la variable que deseamos bajo distintos parámetros. program consumo_papitas_resto syntax, invar(varname) outvar(name) byvar(varname) tempvar tot_invar count_invar egen `tot_invar&#39; = total(`invar&#39;), by(`byvar&#39;) egen `count_invar&#39; = count(&#39;invar&#39;), by(&#39;byvar&#39;) gen `outvar&#39; = (`tot_invar&#39; - `invar&#39;) /// / (`count_invar&#39; - 1) end Con el programa podemos escribir los bloques de código anteriores como: * Caso 1 consumo_papitas_resto, invar(pc_papitas) /// outvar(consumo_papitas_resto_pc) byvar(region) * Caso 2 consumo_papitas_resto, invar(pc_papitas) /// outvar(consumo_papitas_restometro_pc) byvar(metroarea) Hemos escrito la función de forma totalmente general. Podemos cambiar el nivel de agregación sin inducir errores. Estructura de un programa en Stata: La sintaxis más simple es: program nombredelprograma display &quot;Lo que va a hacer el programa&quot; end Cuando se ha definido un programa con program, este se vuelve indistinguible de cualquier otro comando de Stata. Es importante estar seguros(as) de que no estoy escribiendo el mismo nombre que otro programa. Para garantizar lo anterior, es bueno utilizar el comando which. which tabmiss Guardar un programa en Stata Hay dos lugares en donde puedes guardar tus ado-files. Cuando se ha definido un programa con program, este se vuelve indistinguible de cualquier otro comando de Stata. En el directorio de trabajo del proyecto. Se puede hacer una carpeta nueva en la sección de códigos que indique los programas. Otra opción es guardarlo en el directorio Personal de Stata. Para entrar escriban personal en la consola. Nombrar un programa en Stata Es posible darle cualquier nombre a un programa mientras no sea un nombre que ya es utilizado por Stata. Si por ejemplo, creas un programa llamado summarize Stata lo va a ignorar y utilizará su propio comando. 4.0.3 Mis primeros programas en Stata 4.0.3.1 Programa 1 Vamos escribir nuestro primer programa: * Mi primer programa en Stata program minombrees display &quot;Hola, mi nombre es &quot; end Noten que al ejecutarlo no se genera ningún resultado. Lo que hemos hecho es definir un comando llamado minombrees con una simple función. Esta es la idea principal de un programa. Ejecuten nuevamente el programa. Al hacer esto observaran que se genera un error. Esto es porque al igual que las variables, no es posible asignar dos nombres iguales a un programa. Para evitar esto, es importante utilizar program drop minombrees antes de cargar el comando nuevamente. Este es un buen momento para utilizar capture. 4.0.3.2 Programa 2 Vamos escribir un programa que permita calcular un promedio. Vamos a escribir un comando que nos permita crear una nueva variable que contenga los valores promedio y que muestre el resultado en la pantalla. Esto sería igual que escribir: sysuse auto, clear egen mimedia_mpg = mean(mpg) tab mean_mpg Podemos evitar la repetición de estos dos comandos armando un programa: program drop _all capture program drop mymean program mymean egen media_`1&#39; = mean(`1&#39;) tab media_`1&#39; end * Aplicamos este programa sysuse auto, clear mimedia mpg Noten que hemos utilizado `1'. Este nos indica cualquier variable que este en la primera posición. Si incluimos más de una variable. Solo va a considerar la primera variable. Ejercicio 3.5.1: Preguntas Escriba un programa que permita ver todas las etiquetas (labels) de una base de datos. Aplique este programa a la base auto.dta. 4.0.3.3 Programa 3 Vamos a reescribir el programa para permitir un número arbitrario de variables. La macro ‘0’ contiene toda la cadena, ‘1’ el primer elemento, ‘2’ el segundo, etc. Podemos hacer una iteración sobre todos los elementos sin saber cuántos hay utilizando la técnica de desplazamiento incremental que implementaremos con macro_shift. capture program drop mimedia program define mimedia while &quot;`1&#39;&quot;!=&quot;&quot; { egen mean_`1&#39;=mean(`1&#39;) tab mean_`1&#39; macro shift } end sysuse auto, clear mimedia price mpg rep78 El comando macro shift sirve como incremental. Termina cuando encuentra un vacio, lo que explica la presencia del while. Notar que \"1’“` habla de la posición. Es una buena tecnica para garantizar que el iterador seguirá cuando este vacio. 4.0.3.4 Programa 4 Modificamos un poco el programa para que despliegue los resultados en la consola de Stata. Adicionalmente, agregamos un quietly. capture program drop mimedia program define mimedia while &quot;`1&#39;&quot;!=&quot;&quot; { qui: egen `1&#39;_mean = mean(`1&#39;) display &quot;Media de `1&#39; = &quot; `1&#39;_mean macro shift } end sysuse auto, clear mimedia price mpg rep78 4.0.3.5 Programa 5 El comando macro shift es útil, pero puede ser lento. Agregando variables locales usuales y un incremental es mucho mejor. Ojo con las dobles comillas. capture program drop mimedia program define mimedia local i = 1 while &quot;``i&#39;&#39;&quot;~=&quot;&quot; { qui: egen ``i&#39;&#39;_mean = mean(``i&#39;&#39;) display &quot;Media de `i&#39; = &quot; ``i&#39;&#39;_mean local ++i } end 4.0.4 Programa con distintos argumentos El programa de los ejemplos anteriores soporta un solo argumento. Ahora vamos a ver un programa que considere explícitamente que los argumentos de un programa pueden tomar roles distintos. program drop _all capture program drop show program define show tempvar obs quietly gen `obs&#39; = `1&#39; /// if (ctycode == &quot;`2&#39;&quot; &amp; year ==`3&#39;) sort `obs&#39; display &quot;`1&#39; of country `2&#39; in `3&#39; is: &quot; `obs&#39; end tempvar crea una variable temporal que existe mientras el programa se ejecuta pero que se elimina automáticamente una vez que el programa termina su ejecución. Es importante estar seguros(as) de que cualquier string están en comillas dobles. Ejercicio 3.5.2: Preguntas Escriba un programa que reporte la mediana de la diferencia entre dos variables. Aplique este programa a la base auto.dta. 4.0.5 Opción Syntax Renombrando argumentos: De momento hemos escrito los programas utilizando '1', '2', '3' con el fin de introducir el uso de programas en Stata. Sin embargo, esta notación puede ser un poco confusa y provocar errores en la codificación. Podemos asignar nombres de mayor significado a los argumentos del programa. program drop _all capture program drop show program define show args var cty yr tempvar obs quietly gen `obs&#39; = `var&#39; /// if (ctycode == &quot;`cty&#39;&quot; &amp; year ==`yr&#39;) sort `obs&#39; display &quot;`1&#39; of country `2&#39; in `3&#39; is: &quot; `obs&#39; end El comandoargs asigna a las variables locales var, cty, yr los valores de '1', '2', '3'. Noten que si llaman al programa con cuatro argumentos no retorna un error. Un método mucho más robusto y mejor para llamar a los argumentos de un programa es utilizar syntax. En vez de referirnos a cada elemento de un programa por su posición, vamos a especificar la “gramática” del programa. La sintaxis de Stata es: [by varlist:] command [varlist] [=exp] [in range] [, options] varlist denota la lista de variables, command el comando a ejecutar, exp denota una expresión algebraica, range denota un rango para las observaciones mientras que ,options denota la lista de opciones propias de un comando. Utilizar syntax en un programa hace que Stata verifique si un programa satisface la sintaxis. En caso de que no la cumpla, arrojara un error. Syntax * El comando syntax almacena en macros locales todos los elementos típicos de un comando en Stata. * Por ejemplo, syntax también puede definir condicionales como if o in. program ... syntax varlist(min = 2) [if] [in] Si quieren hacer programas más complejos, con otras características es importante revisar syntax. Uno de los primeros elementos de syntax es varlist. Es posible indicar el mínimo o máximo de variables. * Por ejemplo: varlist(min = 2 max = 2). * Si tienes solo una variable puedes utilizar varname. * Es equivalente a varlist(min = 1 max = 1). * La macro que guarda las variables siempre se llama varlist. Ejemplo 1: programa para calcular percentiles program pctrange, rclass version 17 syntax varlist(max = 1 numeric) quietly summarize `varlist&#39;, detail scalar range = r(max) - r(min) scalar p7525 = r(p75) - r(p25) scalar p9010 = r(p90) - r(p10) display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; display as txt &quot;75-25 : &quot; p7525 display as txt &quot;90-10: &quot; p9010 display as txt &quot;Range: &quot; range end El programa anterior nos permite obtener los percentiles de alguna variable al mismo tiempo que los muestra en la consola. Noten que tambíen he incluido que tipo de comando es. En este caso es un comando r-class. program es quien determina el nombre del programa. syntax permite determinar los elementos de tu programa. En el ejemplo define el tipo y el límite de variables. También puede definir condicionales como if o in. Los escalares definidos durante el programa se pueden utilizar. Esto no siempre es conveniente. Como recomendación es bueno guardar los programas como variables temporales. Ejemplo 2: programa para calcular percentiles con variables locales program pctrange, rclass version 17 syntax varlist(max = 1 numeric) local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010&#39; display as txt &quot;Range: &quot; `range&#39; 4.0.6 Return Una característica importante de los comandos de Stata es su capacidad de reportar los resultados de forma tal de que los usuarios y usuarias podamos utilizarlos posteriormente. El comando return nos permite guardar los escalares y hacerlos accesibles, sin tener el problema que vimos en el ejemplo anterior. Notar que el lado izquierdo del escalar de retorno se refiere al nombre de la macro, el lado derecho debe hacer referencia a la macro una vez más para extraer el valor almacenado en ese nombre, por lo que debe utilizar dos comillas. Ejemplo 3: programa para calcular percentiles con return program pctrange, rclass version 17 syntax varlist(max = 1 numeric) local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } end 4.0.7 Implementar opciones al programa Podemos agregar distintas opciones al programa. Una opción es agregar como opcional que el programa de un resultado en la consola. Al incluir \\([ ]\\) en syntax significa un componente opcional en el comando. Ejemplo 4: implementar opciones al programa (imprimir por defecto) program pctrange, rclass version 17 syntax varlist(max = 1 numeric) [, PRINT] local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; == &quot;print&quot; { display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } end Ejemplo 5: implementar opciones al programa (no imprimir por defecto) program pctrange, rclass version 17 syntax varlist(max = 1 numeric) [, noPRINT] local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; != &quot;noprint&quot; { display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } end 4.0.8 Incluir if/in al programa Incluir un subconjunto de observaciones Cualquier comando debiese incluir if o in range. Nuevamente, estas opciones son manejadas dentro del comando syntax. Para incluir estas opciones hay que agregar [if] y [in]. Con estos comandos puedo ejecutar el programa para sub-muestras. Es importante asegurar que la sub-muestra no es vacía. Para ello es importante calcular r(N), chequear que sea distinto de cero y agregar un título que lo indique. El comando marksample touse utiliza la información provista por if o in en caso de que estos sean indicados en el programa. Este comando genera una variable local touse que es igual a 1 si las variables entran en el calculo que hace el programa y 0 en caso contrario. Utilizaremos 'touse' para calcular el número de observaciones que se utilizan después de aplicar los condicionales. Es necesario agregar 'touse' en cada parte del programa que trabaje con la variable de input. Ejemplo 6: Incluir un subconjunto de observaciones program pctrange, rclass version 17 syntax varlist(max = 1 numeric) [, noPRINT] local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; != &quot;noprint&quot; { display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } end 4.0.9 Generalizar el comando para incluir múltiples variables Algunas consideraciones Si quiero ejecutar el programa sobre múltiples variables, es necesario ajustar un poco el programa. Tengo que indicarle a syntax que hay más de una variable. Además, tengo que indicarle al programa que en caso de que existan más variables muestre los resultados en una tabla. Guardaremos los resultados en matrices (no escalares) y vamos a aplicar una función de macro extendida con el fin de contar el número de filas que esta matriz debiese tener. Agregaremos también una la opción para cambiar el formato y la opción mat que permite a la matriz ser guardada automáticamente con el nombre indicado. Ejemplo 7 program drop _all program pctrange, rclass version 17 syntax varlist(min = 1 numeric ts) [if] [in] [, noPRINT FORmat(passthru) MATrix(string)] marksample touse quietly count if `touse&#39; if `r(N)&#39; == 0 { error 2000 } local nvar: word count `varlist&#39; if `nvar&#39; == 1 { local res range p7525 p9010 tempname `res&#39; quietly summarize `varlist&#39; if `touse&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; != &quot;noprint&quot; { display as result _n &quot;Rangos de percentiles para `varlist&#39;, N = `r(N)&#39;&quot; display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } return scalar N = r(N) } else { tempname rmat matrix `rmat&#39; = J(`nvar&#39;, 3, .) local i 0 foreach v of varlist `varlist&#39;{ local ++i quietly summarize `v&#39; if `touse&#39;, detail matrix `rmat&#39;[`i&#39; ,1] = r(max) - r(min) matrix `rmat&#39;[`i&#39;, 2] = r(p75) - r(p25) matrix `rmat&#39;[`i&#39;, 3] = r(p90) - r(p10) local rown `rown&#39; `v&#39; } matrix colnames `rmat&#39; = Range P75-P25 P90-P10 matrix rownames `rmat&#39; = `rown&#39; if &quot;`print&#39;&quot; != &quot;noprint&quot; { local form &quot;, noheader&quot; if &quot;`format&#39;&quot; != &quot;&quot; { local form &quot;`form&#39; `format&#39;&quot; } matrix list `rmat&#39; `form&#39; } if &quot;`matrix&#39;&quot; != &quot;&quot; { matrix `matrix&#39; = `rmat&#39; } return matrix rmat = `rmat&#39; } return local varname `varlist&#39; end 4.0.10 Agregar prefijos a los programas Prefijo by Ahora vamos a hacer que nuestro programa pueda utilizar el prefijo by. Para agregar esta opción simplemente hay qyue modificar program. program pctrange, rclass byable(recall) También podemos permitir que la lista de variables incluya operadores de series de tiempo (ej. L.pib, D.ingreso). Para incorporar estos elementos tenemos que modificar syntax. syntax varlist(min = 1 numeric ts) [if] [in] /// [, noPRINT FORmat(passthru) MATrix(string)] 4.0.11 Programas para complementar función egen Es posible programar funciones adicionales de egen. El nombre de estos programas deben empezar con _g. Una diferencia importante entre este tipo de programas y los que ya hemos escrito guarda relación con el hecho de que hay que tener en cuenta la nueva variable que se va a crear. La sintaxis es la siguiente: egen [type] newvarname = fcn(arguments) [if] [in] [, options] El cambio que haremos en la sintaxis del programa será que incluiremos un touse para hacer la nueva variable. Ejemplo 8: programa función egen * Programas de egen program drop _all program _gpct9010 syntax newvarname =/exp [if] [in] tempvar touse mark `touse&#39; `if&#39; `in&#39; quietly summarize `exp&#39; if `touse&#39;, detail quietly generate `typlist&#39; `varlist&#39; = r(p90) - r(p10) if `touse&#39; end * Aplicación del programa sysuse auto, clear egen rango9010 = pct9010(price) Programas con funciones de egen con prefijo by Agregamos [, *] que corresponde a las opciones. En egen el prefijo by es una opción. Con el fin de permitirle al programa que pueda producir un rango de percentil separado para cada grupos utilizaremos pctile en vez de summarize. El cambio que haremos en la sintaxis del programa será que incluiremos un touse para hacer la nueva variable. Ejemplo 9: programa función egen con prefijo by * Programa con opción by. program drop _all program _gpct9010 syntax newvarname =/exp [if] [in] [, *] tempvar touse p90 p10 mark `touse&#39; `if&#39; `in&#39; quietly { egen double `p90&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(90) egen double `p10&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(10) generate `typlist&#39; `varlist&#39; = `p90&#39; - `p10&#39; if `touse&#39; } end * Aplicación del programa sysuse auto, clear egen rango9010 = pct9010(price) bysort rep78 foreign: egen rango9010_prefijoby = pct9010(price) Generalización de la función egen para soportar todos los pares de cuantiles Hemos desarrollado una función de egen que permite calcular un rango entre percentiles para una lista de variables especifica. Puede ser util tomar ventaja de egen pctile() para poder calcular cualquier percentil de la lista de variables especificadas. Vamos a agregar dos opciones a la función egen : lo() y hi(). En caso de que no especifiquemos, por defecto se calcula el rango interquartil. La función de egen ahora se llamará _gpctrange.ado. Ejemplo 10: función egen más general * Programa con opción by. program drop _all program _gpctrange syntax newvarname =/exp [if] [in] [, LO(integer 25) HI(integer 75) *] if `hi&#39; &gt; 99 | `lo&#39; &lt; 1 { display as error /// &quot;Percentiles `lo&#39; `hi&#39; deben estar entre 1 y 99.&quot; error 198 } if `hi&#39; &lt;= `lo&#39; { display as error /// &quot;Percentiles `lo&#39; `hi&#39; deben estar en orden ascendente&quot; error 198 } tempvar touse phi plo mark `touse&#39; `if&#39; `in&#39; quietly { egen double `phi&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(`hi&#39;) egen double `plo&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(`lo&#39;) generate `typlist&#39; `varlist&#39; = `phi&#39; - `plo&#39; if `touse&#39; } end sysuse auto, clear bysort rep78: egen iqr = pctrange(price) if inrange(rep78,3,5) bysort rep78: egen p8020 = pctrange(price) if inrange(rep78,3,5), hi(80) lo(20) tabstat iqr if inrange(rep78, 3, 5), by(rep78) tabstat p8020 if inrange(rep78, 3, 5), by(rep78) 4.0.12 Syntax Algunas consideraciones Como hemos visto hay dos formas en los que un programa de Stata puede interpretar lo que ingresamos. Por posición tal como lo hace args o como lo hicimos con los números entre las comillas. De acuerdo a la gramática del programa utilizando syntax. syntax guarda los componentes en macros locales particulares a las cuales podemos acceder posteriormente. Por ejemplo 'if' 'in' 'varlist' son macros locales a las que podemos acceder tal como vimos la clase pasada. Ahora vamos a ver algunas opciones de syntax que nos permitirán tener más herramientas para escribir nuestros programas. Al utilizar dentro de un programa paréntesis cuadrados estoy indicando que esas partes son opcionales. Por ejemplo, estas dos versiones son equivalentes, salvo que en la segunda linea todo es opcional: * Nada opcional syntax varlist if in title(string) adjust(real 1) * Todo opcional syntax [varlist] [if] [, adjust (real 1) title(string)] Vamos a mirar las macros generadas por syntax: capture program drop myprog program myprog syntax varlist [if] [in] [, adjust(real 1) title(string)] display &quot;varlist contiene |`varlist&#39;|&quot; display &quot;if contiene |`if&#39;|&quot; display &quot;in contiene |`in&#39;|&quot; display &quot;adjust contiene |`adjust&#39;|&quot; display &quot;title contiene |`title&#39;|&quot; end Vamos a aplicar lo aprendido en un ejemplo sencillo: capture program drop miprograma program miprograma syntax varlist [if] [in] [, adjust(real 1) title(string)] display if &quot;`title&#39;&quot; != &quot;&quot; { display &quot;`title&#39;:&quot; } foreach var of local varlist{ quietly summarize `var&#39; `if&#39; `in&#39; display &quot;`var&#39;&quot; &quot; &quot;%9.0g r(mean)*`adjust&#39; } end marksample y touse: Un error común es utilizar una muestra en una parte del programa y otra distinta en otra parte. La solución es crear una variable que contiene un 1 si la observación fue utilizada y un 0 en caso contrario. capture program drop miprograma program miprograma syntax varlist [if] [in] [, adjust(real 1) title(string)] marksample touse display if &quot;`title&#39;&quot; != &quot;&quot; { display &quot;`title&#39;:&quot; } foreach var of local varlist{ quietly summarize `var&#39; `if&#39; `touse&#39; display &quot;`var&#39;&quot; &quot; &quot; r(mean)*`adjust&#39; } end 4.0.13 Varlist varlist específica la macro que contiene las variables que van a ingresar al programa como inputs. Las opciones de varlist son: * default = none. Especifica como la varlist se va a llenar. Por defecto se llena con todas las variables. * min, max especifica el número de variables permitidas. * numeric, string especifican que criterio deben cumplir todas las variables que ingresas al programa. * ts permite que la varlist contenga operadores de series de tiempo. * fv permite que la varlist contenga variables categóricas. 4.0.14 Opciones Las opciones permiten hacer requeridos o opcionales. Por ejemplo, regress, noconstant. Las opciones pueden ser una numlist, una varlist (ej. by(varlist) option, una namelist tal como el nombre de una matriz o de una nueva variable. Como regla general, cualquier característica que pudieran encontrar en un comando de Stata, la pueden agregar en un programa. Para las opciones es importante recordar que las mayúsculas indican la menor abreviación posible. replace, detail, constant son opciones de on. noreplace, nodetail, noconstant son opciones de off. Ojo las macros que retornan tienen los mismos nombres que en el caso anterior. title y adjust también son otros opcionales. El primero permite ingresar un título al comando mientras que el segundo permite ajustar los resultados por algún escalar. Hay muchas otras. Veamos un ejemplo… capture program drop miprograma program miprograma syntax varlist [if] [in] [, adjust(real 1) title(string)] display if &quot;`title&#39;&quot; != &quot;&quot; { displa &quot;`title&#39;:&quot; } foreach var of local varlist{ quietly summarize `var&#39; `if&#39; `in&#39; display &quot;`var&#39;&quot; &quot; &quot;%9.0g r(mean)*`adjust&#39; } end 4.0.15 Programa para calcular percentiles Una versión simple para una variable: program pctrange, rclass version 17 syntax varlist(max = 1 numeric) quietly summarize `varlist&#39;, detail scalar range = r(max) - r(min) scalar p7525 = r(p75) - r(p25) scalar p9010 = r(p90) - r(p10) display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; display as txt &quot;75-25 : &quot; p7525 display as txt &quot;90-10: &quot; p9010 display as txt &quot;Range: &quot; range end Algunas consideraciones El programa anterior nos permite obtener los percentiles de alguna variable al mismo tiempo que los muestra en la consola. Noten que también he incluido que tipo de comando es. En este caso es un comando r-class. Los escalares definidos en el programa se pueden utilizar. Esto no siempre es conveniente. Como recomendación es bueno guardar los resultados esperados de los programas como variables temporales. Versión que incluye variables guardadas localmente: program pctrange, rclass version 17 syntax varlist(max = 1 numeric) local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010&#39; display as txt &quot;Range: &quot; `range&#39; 4.0.16 Programa con opción return Una característica importante de los comandos de Stata es su capacidad de reportar los resultados de forma tal de que los usuarios y usuarias podamos utilizarlos posteriormente. El comando return nos permite guardar los escalares y hacerlos accesibles, sin tener el problema de los escalares que vimos en los ejemplos anteriores. Programa para calcular percentiles con return: program pctrange, rclass version 17 syntax varlist(max = 1 numeric) local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } end Sobre la forma en que llamamos a return: foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } Notar que el lado izquierdo de scalar('r') se refiere al nombre de los elementos de la macro res. El lado derecho hace referencia a la macro una vez más para extraer el valor almacenado en ese nombre. En este caso es importante notar que se deben utilizar dos comillas (ej. \"range\"). Noten también que si utilizamos scalar list no hay resultados, sin embargo, al utilizar la opción return ahora podemos rescatar los resultados con return list. Ejercicio 3.6.3: Preguntas Escriba un programa llamado misuma que sea del tipo r-class. Este programa debe entregar en la lista de return list el número total de observaciones, la suma total y el promedio de la variable. Aplicarla en auto.dta. 4.0.17 Agregar opciones al programa Podemos agregar distintas opciones al programa. Para este ejemplo vamos a agregar como opcional que el programa de un resultado en la consola. Recordemos que incluir \\([ ]\\) en syntax significa un componente opcional en el comando. Implementar opciones al programa (imprimir por defecto): program pctrange, rclass version 17 syntax varlist(max = 1 numeric) [, PRINT] local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; == &quot;print&quot; { display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } end Implementar opciones al programa (no imprimir por defecto): program pctrange, rclass version 17 syntax varlist(max = 1 numeric) [, noPRINT] local res &quot;range p7525 p9010&quot; tempname `res&#39; display as result _n &quot;Rangos de percentiles para `varlist&#39;&quot; quietly summarize `varlist&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; != &quot;noprint&quot; { display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } end Incluir un subconjunto de observaciones Cualquier comando debiese incluir if o in range. Nuevamente, estas opciones son manejadas dentro del comando syntax. Para incluir estas opciones hay que agregar [if] y [in]. Con estos comandos puedo ejecutar el programa para sub-muestras. Es importante asegurar que la sub-muestra no este vacía. Para ello es importante calcular r(N), chequear que sea distinto de cero y agregar un título que lo indique. El comando marksample touse utiliza la información provista por if o in en caso de que estos sean indicados en el programa. Este comando genera una variable local touse que es igual a 1 si las variables entran en el calculo que hace el programa y 0 en caso contrario. Utilizaremos `touse' para calcular el número de observaciones que se utilizan después de aplicar los condicionales. Es necesario agregar `touse' en cada parte del programa que trabaje con la variable de input. Incluir un subconjunto de observaciones: program drop _all program pctrange, rclass syntax varlist(max = 1 numeric) [if] [in] [, noPRINT] marksample touse quietly count if `touse&#39; if `r(N)&#39; == 0 { error 2000 } local res range p7525 p9010 tempname `res&#39; quietly summarize `varlist&#39; if `touse&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; != &quot;noprint&quot; { display as result _n &quot;Rangos de percentiles para `varlist&#39;, N = `r(N)&#39;&quot; display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } return scalar N = r(N) return local varname `varlist&#39; end Ejercicio 3.6.2: Preguntas Ajuste el programa del ejercicio 1 con el fin de incluir la opción if. Guárdelo como misuma2. Aplicarla en auto.dta. 4.0.18 Generalizar el comando para incluir múltiples variables Algunas consideraciones Si quiero ejecutar el programa sobre múltiples variables, es necesario ajustar un poco el programa. Tengo que indicarle a syntax que hay más de una variable. Además, tengo que indicarle al programa que en caso de que existan más variables muestre los resultados en una tabla. Guardaremos los resultados en matrices (no escalares) y vamos a aplicar una función de macro extendida con el fin de contar el número de filas que esta matriz debiese tener. Agregaremos también una la opción format para cambiar el formato y la opción mat que permite a la matriz ser guardada automáticamente con el nombre indicado. program drop _all program pctrange, rclass version 17 syntax varlist(min = 1 numeric ts) [if] [in] [, noPRINT FORmat(passthru) MATrix(string)] marksample touse quietly count if `touse&#39; if `r(N)&#39; == 0 { error 2000 } local nvar: word count `varlist&#39; if `nvar&#39; == 1 { local res range p7525 p9010 tempname `res&#39; quietly summarize `varlist&#39; if `touse&#39;, detail scalar `range&#39; = r(max) - r(min) scalar `p7525&#39; = r(p75) - r(p25) scalar `p9010&#39; = r(p90) - r(p10) if &quot;`print&#39;&quot; != &quot;noprint&quot; { display as result _n &quot;Rangos de percentiles para `varlist&#39;, N = `r(N)&#39;&quot; display as txt &quot;75-25 : &quot; `p7525&#39; display as txt &quot;90-10: &quot; `p9010 &#39; display as txt &quot;Range: &quot; `range&#39; } foreach r of local res { return scalar `r&#39; = ``r&#39;&#39; } return scalar N = r(N) } else { tempname rmat matrix `rmat&#39; = J(`nvar&#39;, 3, .) local i 0 foreach v of varlist `varlist&#39;{ local ++i quietly summarize `v&#39; if `touse&#39;, detail matrix `rmat&#39;[`i&#39; ,1] = r(max) - r(min) matrix `rmat&#39;[`i&#39;, 2] = r(p75) - r(p25) matrix `rmat&#39;[`i&#39;, 3] = r(p90) - r(p10) local rown `rown&#39; `v&#39; } matrix colnames `rmat&#39; = Range P75-P25 P90-P10 matrix rownames `rmat&#39; = `rown&#39; if &quot;`print&#39;&quot; != &quot;noprint&quot; { local form &quot;, noheader&quot; if &quot;`format&#39;&quot; != &quot;&quot; { local form &quot;`form&#39; `format&#39;&quot; } matrix list `rmat&#39; `form&#39; } if &quot;`matrix&#39;&quot; != &quot;&quot; { matrix `matrix&#39; = `rmat&#39; } return matrix rmat = `rmat&#39; } return local varname `varlist&#39; end 4.0.19 Agregar prefijos a los programas prefijo by: Ahora vamos a hacer que nuestro programa pueda utilizar el prefijo by. Para agregar esta opción simplemente hay que modificar program. program pctrange, rclass byable(recall) También podemos permitir que la lista de variables incluya operadores de series de tiempo (ej. L.pib, D.ingreso). syntax varlist(min = 1 numeric ts) 4.0.20 Programas para complementar función egen Es posible programar funciones adicionales de egen (extended generate). El nombre de estos programas deben empezar con _g. Una diferencia entre este tipo de programas y los ya hechos es que en estos hay que tener en cuenta la nueva variable que se va a crear. La sintaxis es la siguiente: egen [type] newvarname = fcn(arguments) [if] [in] [, options] El cambio que vamos a hacer en la sintaxis del programa será que incluiremos un touse para hacer la nueva variable. Vamos a escribir un programa para calcular un rango en particular: * Programas de egen program drop _all program _gpct9010 syntax newvarname =/exp [if] [in] tempvar touse mark `touse&#39; `if&#39; `in&#39; quietly summarize `exp&#39; if `touse&#39;, detail quietly generate `typlist&#39; `varlist&#39; = r(p90) - r(p10) if `touse&#39; end * Aplicación del programa sysuse auto, clear egen rango9010 = pct9010(price) Programas con funciones de egen con prefijo by: Agregamos [, *] que corresponde a las opciones. En egenel prefijo by es una opción. Con el fin de permitirle al programa que pueda producir un rango de percentil separado para distintos grupos utilizaremos pctile en vez de summarize. * Programa con opción by. program drop _all program _gpct9010 syntax newvarname =/exp [if] [in] [, *] tempvar touse p90 p10 mark `touse&#39; `if&#39; `in&#39; quietly { egen double `p90&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(90) egen double `p10&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(10) generate `typlist&#39; `varlist&#39; = `p90&#39; - `p10&#39; if `touse&#39; } end * Aplicación del programa sysuse auto, clear egen rango9010 = pct9010(price) bysort rep78 foreign: egen rango9010_prefijoby = pct9010(price) 4.0.21 Generalización de la función egen Hemos desarrollado una función de egen que permite calcular un rango entre percentiles para una lista de variables especifica. Puede ser útil tomar ventaja de egen pctile() para poder calcular cualquier percentil de la lista de variables especificadas. Vamos a agregar dos opciones a la función egen : lo() y hi(). En caso de que no especifiquemos, por defecto se calcula el rango interquartil. La función de egen ahora se llamará _gpctrange.ado. Función egen más general para percentiles: * Programa con opción by. program drop _all program _gpctrange syntax newvarname =/exp [if] [in] [, LO(integer 25) HI(integer 75) *] if `hi&#39; &gt; 99 | `lo&#39; &lt; 1 { display as error /// &quot;Percentiles `lo&#39; `hi&#39; deben estar entre 1 y 99.&quot; error 198 } if `hi&#39; &lt;= `lo&#39; { display as error /// &quot;Percentiles `lo&#39; `hi&#39; deben estar en orden ascendente&quot; error 198 } tempvar touse phi plo mark `touse&#39; `if&#39; `in&#39; quietly { egen double `phi&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(`hi&#39;) egen double `plo&#39; = pctile(`exp&#39;) if `touse&#39;, `options&#39; p(`lo&#39;) generate `typlist&#39; `varlist&#39; = `phi&#39; - `plo&#39; if `touse&#39; } end sysuse auto, clear bysort rep78: egen iqr = pctrange(price) if inrange(rep78,3,5) bysort rep78: egen p8020 = pctrange(price) if inrange(rep78,3,5), hi(80) lo(20) bysort rep78: egen p8020 = pctrange(price) if inrange(rep78,3,5) /// &amp; foreign==1, hi(80) lo(20) * Utilizar estas nuevas variables con otros comandos tabstat iqr if inrange(rep78, 3, 5), by(rep78) tabstat p8020 if inrange(rep78, 3, 5), by(rep78) 4.0.22 Documentar tu programa Escribir un help Es necesario y recomendado mantener una documentación de los programas que se escriban para un proyecto. Esta documentación debe estar actualizada e incluir cualquier modificación. Importante hacerlo mientras se hace el programa y no al final. Vamos a aprender un poco de SMCL (Stata Markup and Control Language file). Básicamente es el lenguaje con el que se escriben los help en Stata. También es el lenguaje con el que se muestran los resultados de display. Los archivos se pueden escribir en cualquier procesador de texto, pero deben ser guardados en formato .smcl. También deben ser guardados en la misma carpeta en donde se encuentra el ado-file relacionado con el archivo. Hay que empezar los códigos con \\(\\{smcl\\}\\) con el fin de indicarle a Stata que el texto que viene será en formato SMCL. Las etiquetas de SMCL van entre llaves (\\(\\{\\}\\)) y se pueden leer de dos formas principalmente: \\(\\{tag:text\\}\\) etiquetar el texto que se esta escribiendo. \\(\\{tag\\}\\) etiquetar todo lo que viene hasta que se cambie en otra parte del texto. Por ejemplo, si quiero poner texto en itálica tengo que ocupar la etiqueta it: * Una palabra/frase en particular {it:este texto aparecerá en itálica} * Todo el bloque de texto {it} Todo lo que este aquí aparecerá en itálica. /// Esto va a ocurrir hasta que aparezca un nuevo /// tipo de etiqueta. Etiquetas de SMCL Texto: {it}, {bf}, {sf}, {ul}. Itálica, negrita, texto normal, subrayado, repectivamente. Texto en formato Stata: {cmd}, {error}, {result}, {text}. {Destacar una referencia: {hi}. Opciones de comando: {opt}. Insertar linea horizontal: {hline}. Volver a dejar el texto a su estado normal: {reset}. Formato de documento: {title:text}, {center}, {ralign}, {lalign}, {tab}. Párrafos: Hay dos opciones para escribirlos. {p #1 #2 #3 #4}. {p} = {p 0 0 0 0} Los números indican los siguientes elementos de un párrafo: El primer número (#1) es cuántos caracteres hay que sangrar en la primera línea. El segundo número (#2) es cuántos caracteres hay que sangrar en la segunda y tercera línea. El tercer número (#3) es cuán lejos de la derecha debe estar el margen. El cuarto número (#4) es para el ancho total del párrafo. {phang}: es equivalente a {p 4 8 2}. {pstd}: es equivalente a {p 4 4 2}. {phang2}: es equivalente a {p 8 12 2}. {p2col}: Para separar el texto en dos columnas. {p_end}: Para terminar un párrafo. Útil cuando tienes dos parrafos en formatos distintos. 4.0.23 Escribir programas e-class Vamos a aprender algunos elementos que nos van a permitir escribir nuestros propios comandos de estimación en Stata. Muchos de los conceptos que hemos visto aplican también para este tipo de comandos. Es necesario recordar algunas convenciones que nos van a ser útiles para poder escribir nuestros programas. Los resultados se guardan en |e() y se puede acceder a ellos con ereturn list. El número de observaciones es e(N) y para identificar que observaciones fueron incluidas en la estimación es necesario utilizar la función e(sample). Los coeficientes estimados se guardan en un vector e(b) y la matriz de varianza covarianza se guarda en e(V). El comando ereturn name = exp retorna un escalar, mientras que ereturn local name value y ereturn matrix name matname retorna una macro y una matriz respectivamente. El comando ereturn post envía las estimaciones de b y V a sus ubicaciones oficiales. Para devolver el vector de coeficientes y su matriz de varianza, es necesario crear el vector de coeficientes, digamos \\(beta\\), y su matriz de varianza-covarianza, digamos \\(vce\\). ereturn post `beta&#39; &#39;vce&#39;, esample(`touse&#39;) También podemos definir la muestra de estimación incluida en la estimación con touse. Ahora es posible guardar los elementos en e(). Por ejemplo, es posible utilizar ereturn scalar, ereturn local o ereturn matrix. Es conveniente utilizar los nombres típicamente asignados para guardar resultados de los programas e(df_m) o e(df_r). Sin embargo, se pueden nombrar como deseen. 4.0.24 Marksample, Mark y Markout marksample y mark son alternativas. mark no es muy utilizado. Ambos comandos crean un indicador que marcan que observaciones será utilizadas. Los ocupadmos en los programas en Stata. La idea es indicarle al programa la muestra relevante. markout marca la variable con un indicador igual a 0 si cualquier variable en varlist indicada contiene un missing. Marksample marksample se utiliza en programas en los que los argumentos se analizan mediante el comando syntax. Crea una variable temporal, almacena el nombre de la variable temporal en un local, y rellena la variable temporal con 0 y 1 según si la observación debe ser utilizada. program .... syntax ... marksample touse rest of code .... if `touse&#39; end Mark mark utiliza la variable temporal touse basada en las expresiones de if e in. Si no hay expresiones de if e in, touse será 1 para cada observación en los datos. Si indico una condición, solo las observaciones que cumplan esta condición tendrán un 1 en touse. Mark actualiza touse de forma tal de que revisa missing. Mark y Markout mark parte con una variable temporal previamente creada. markout modifica la variable creada por mark poniéndola a cero en las observaciones que tienen valores perdidos registrados para cualquiera de las variables en varlist. program .... tempvar touse mark `touse&#39; ... markout `touse&#39; ... rest of code ... if `touse&#39; end Marksample vs. Markout marksample es mejor que mark. Disminuye la probabilidad de que se olvide alguna restricción. markout puede ser utilizados después de mark o bien marksample. program ... tempvar touse mark `touse&#39; ... markout `touse&#39; ... rest of code ... if `touse&#39; end prgraom myprog syntax varlist [if] [in] marksample touse ... end * Equivale a: program myprog version 17.0 syntax varlist [if] [in] tempvar touse mark `touse&#39; `if&#39; `in&#39; markout `touse&#39; `varlist&#39; ... end markout también puede ser usado con marksample: program ... syntax ... [, Denom(varname) ... ] marksample touse markout `touse&#39; `denom&#39; rest of code ... if `touse&#39; end Ejemplo Marksample y Markout: program cwsumm syntax [varlist(fv ts)] [if] [in] [aweight fweight] [, Detail noFormat] marksample touse summarize `varlist&#39; [`weight&#39;`exp&#39;] if `touse&#39;, `detail&#39; `format&#39; end 4.0.25 Sortpreserve Si está escribiendo un programa de Stata que cambia temporalmente el orden de los datos y quieres que los datos se ordenen en su orden original al final de la ejecución, puede ahorrar un poco de programación incluyendo sortpreserve. Para ellos debemos escribir: program miprograma, sortpreserve. Stata automáticamente reordenara las variables como estaban originalmente. Al agregar esta opción se genera una variable temporal llamada _sortindex la que contiene el orden original de los datos. "],["exportar-información.html", "5 Exportar Información", " 5 Exportar Información Exportar información es parte importante del proceso de análisis de datos. En esta sección veremos algunas herramientas que permiten conectar los resultados del análisis hecho en Stata con los reportes que queremos crear en Word. 5.0.1 Stata Markdown Herramienta para crear informes que sean reproducibles y en donde los datos, códigos y operaciones están conectadas. Básicamente una intersección entre texto narrativo y Es posible generar informes en distintos formatos (ej. html, docx, pdf). Hoy veremos como generar un informe en Word y una presentación en HTML. Stata Markdown es especialmente útil para: Informes rutinarios: Informe semanas/mensual sobre un conjunto de datos que se actualizan constantemente. Documentar análisis: Es posible integrar reportes intermedios en un trabajo de análisis de datos. Estos informes pueden ser de utilidad para detectar errores y para supervisión en equipos de trabajo. Stata Markdown y otras herramientas similares son claves para mejorar en términos de . Algunos argumentos basados en : Ayuda a documentación: En la fase de procesamiento de datos podemos describir todos los pasos utilizados para convertir los datos brutos en variables de análisis, produciendo un documento con un buen formato, más claro y legible. Es posible pensar en estos informes como productos intermedios. En la fase de análisis de los datos, podemos incluir el código, explicar las razones para probar determinados modelos, incluir los resultados, las tablas y las figuras, y comentar los resultados, todo ello sin tener que cortar y pegar de forma tediosa y propensa a errores. En la fase de presentación, podemos elaborar un informe centrado en los resultados, con la opción de ocultar los comandos reales utilizados para que no aparezcan en el documento final. Algunos conceptos Markdown: Es un lenguaje que permite escribir documentos en texto plano. Los archivos escritos en Markdown tienen la extensión md. Stata Markdown: Es la variación especifica para Stata. Se implementa a través del comando markstat hecho por Germán Rodriguez. Estos archivos tiene extensión .stmd. 5.0.1.1 Instalación En Stata, ejecuta estos comandos: ssc install markstat ssc install whereis Instalar padcoc desde pandoc.org/installing Decirle a markstat donde encontrar pandoc. En mi caso es: whereis pandoc &quot;C:\\Users\\nicol\\AppData\\Local\\Pandoc\\pandoc.exe&quot; 5.0.1.2 Funcionalidades Algunos elementos que vamos a utilizar en Stata Markdown: Ecuaciones y notación matemática utilizando (para los que lo sepan utilizar). Hacer títulos y encabezados. Enfatizar texto (negritas e itálicas). Armas listas numeradas y no numeradas. Poder mostrar líneas de código, resultados directamente de Stata. Esto incluye Mata. Insertar quiebres de páginas: \\newpage. 5.0.1.3 Generar un documento Como mencionamos anteriormente es posible generar un documento de word, pdf o html que contenga todos los códigos, resultados, tablas y figuras hechas en Stata. Para generar un documento es necesario: Escribir en tu editor de códigos preferido un archivo en formato stmd. En nuestro caso escribiremos un archivo que se llama ejemplo1.stmd. Guardar el archivo en tu carpeta de trabajo. Ejecutar el comando markstat según el tipo de archivo que desee generar. En nuestro caso será: markstat using ejemplo1, strict docx 5.0.2 Putdocx La ventaja de putdocx radica en que es posible personalizar un poco más las tablas. Esto permite conectar el informe final con el trabajo de análisis de datos. 5.0.2.1 Algunos comandos útiles Crear, pegar y guardar documentos: putdocx begin: Crea un archivo docx para exportar. putdocx describe: Describe los contenidos de archivo. putdocx save: Guarda y cierra el archivo. putdocx clear: Cierra el archivo sin guardar. putdocx append: Combina el contenido de múltiples archivos. Insertar quiebres de páginas: * putdocx pagebreak: Agrega una nueva página. * putdocx sectionbreak: Agrega una nueva sección. Agregar párrafos con texto e imágenes: * putdocx paragraph: Agrega un nuevo párrafo. * putdocx text: Agrega un bloque de texto a un párrafo. * putdocx image: Agrega una imagen al párrafo. 5.0.2.2 Tablas: putdocx table: Crea una nueva tabla en el documento la que puede contener resultados de estimaciones, estadística descriptiva o datos. putdocx sectionbreak: Agrega una nueva sección. "],["manejo-de-grandes-bases-de-datos-en-stata.html", "6 Manejo de grandes bases de datos en Stata", " 6 Manejo de grandes bases de datos en Stata 6.0.1 Estilo de codificación: mejorar velocidad y eficiencia Esta introducción esta basada en Suggestions on Stata programming style de Nicholas J. Cox. Un buen estilo de codificación es por sobre todo claridad. Lo más importante es tener una estrategia y seguirla. En esta sección nos enfocaremos en algunos consejos de codificación que nos van a permitir mejorar la velocidad y eficiencia en el uso de Stata. Este punto es especialmente relevante cuando se trabaja con grandes bases de datos. Una lista de formas básicas de aumentar la velocidad y eficiencia al manejar grandes bases de datos: Testear siempre las condiciones claves. Hacerlo lo antes posible. Utilizar summarize, meanonly cuando solo necesite este valor. Como regla general siempre he de preguntarme si lo que estoy obteniendo es útil o no. Preferir foreach y forvalues sobre while. Son más rápidos. Evitar el uso de macro shift. Con muchas variables, se vuelve muy lento. Mejor ocuapar un forvalues. Evitar siempre que sea posible iterar sobre observaciones. Mata puede ser útil en este aspecto. Evite usar preserve si es posible. Es atractivo para el programar pero puede ser costoso en tiempo cuando se utilizan grandes bases de datos. Es bueno profundizar en el uso de marksample con el fin de hacer programas efectivos. Las variables temporales se eliminarán automáticamente al final de un programa, pero también considere la posibilidad de eliminarlas cuando ya no sean necesarias para minimizar la sobrecarga de memoria y reducir las posibilidades de que su programa se detenga porque no hay espacio para añadir más variables. Especifique el tipo de las variables temporales para minimizar la sobrecarga de memoria. Si se puede utilizar una variable de bytes, especifique: generate bytes 'myvar' en lugar de dejar que se utilice el tipo por defecto, que desperdiciaría espacio de almacenamiento. Evite utilizar una variable para mantener una constante; una macro o un escalar suele ser todo lo que se necesita. 6.0.2 Cargar grandes bases de datos Cuatro aspectos que considerar siempre que se desee cargar en Stata alguna base de datos de un tamaño considerable: ¿Necesita todas las variables del conjunto de datos? Si no es así, cargue sólo las variables que necesite: use var1 var2 var3 var4 using data1, clear ¿Necesita todas las observaciones del conjunto de datos? Si no es así, importe sólo las observaciones que necesite: use data1 if state &lt;= 9, clear ¿Su conjunto de datos ocupa más espacio de almacenamiento del necesario?: Intenta leer tu conjunto de datos poco a poco y optimizarlo. Además de solo importar determinadas observaciones o variables, se optimiza el espacio de almacenamiento utilizado compress. Es posible inspeccionar la base de datos sin cargarla. describe using data.1.dta ¿Su conjunto de datos contiene muchas observaciones idénticas?: Debe transformar el conjunto de datos en un conjunto de datos ponderado por frecuencia. 6.0.3 Reducir el uso de memoria Cuando usted trabaja con un conjunto de datos en Stata, Stata debe cargar todo el conjunto de datos en la memoria de la computadora (RAM). Afortunadamente, las computadoras portátiles de hoy tienen más memoria que la mayoría de los servidores de hace 20 años, y la mayoría de la gente nunca tiene que preocuparse por la cantidad de memoria que Stata está utilizando. ¿Me tengo que preocupar de la memoria? Sólo tienes que preocuparte por la memoria si el tamaño de tu conjunto de datos se aproxima a la cantidad de memoria del ordenador que utilizas, y si es mayor, definitivamente tienes un problema. Si usted trabaja con grandes conjuntos de datos, debe tener cuidado: tratar de usar más memoria de la que tiene terminará mal. ¿Cuando es mucho?: un Laptop tipico tiene 16gb. 6.0.4 Reducir el tamaño de la base de datos Elimina datos innecesarios. Utiliza tipos de variables pequeños: help datatypes. Siempre que crees una variable es una buena practica especificar el tipo de dato. Acortar cadenas o codificarlas: strings requieren un byte por caracteres. Sin embargo, para las observaciones todas tienen el mismo tamaño. Si tengo una variable que contiene: “Si”, “No”, “No lo se”. La variable utilizara 8 bytes por observación tal como si solo tuviese “No lo se”. Si tu cambias “no lo se” por “ns”. Ahora solo se utilizaran 2 bytes por observacion. Si tu cambias a: “S”, “N”, “I” solo utilizara un byte por observación. Codificar la variable de string como una variable númerica tambien reduce el espacio en memerio a un byte por observación. Se recomiendo agregar labels y trabajar los string de esta forma cuando sea posible. 6.0.5 Eliminar siempre resultados intermedios, incluso temporales Elimina resultados intermedios. Si creas variables para almacenar resultados intermedios, elimínelas tan pronto como haya terminado con ellas. Por ejemplo, el siguiente código crea una variable llamada incomePovertyRatio sólo para poder crear una variable indicadora lowIncome que identifica a los sujetos cuyos ingresos son inferiores al 150% del nivel de pobreza gen incomePovertyRatio = income/povertyLevel gen lowIncome = (incomePovertyRatio &lt; 1.5) drop incomePovertyRatio Debes eliminar la variable que no utilizas. Hacer esto siempre. 6.0.6 Dividir en trozos cuando sea posible Si un conjunto de datos es demasiado grande para cargarlo en la memoria, para algunas tareas puede dividirlo en un conjunto de conjuntos de datos más pequeños y trabajar con ellos de uno en uno. Puede haber una variable categórica en el conjunto de datos de tal manera que un conjunto de datos separado para cada categoría funcionaría bien, o puede dividirlo por número de observación. Dividir el conjunto de datos en trozos más pequeños probablemente sólo tiene sentido si puedes reducir el tamaño de cada trozo para que al final puedas combinarlos todos en un único conjunto de datos que pueda cargarse en la memoria. 6.0.7 Sort sort en Stata es razonablemente eficiente: un millón de valores aleatorios pueden ponerse en orden creciente en menos de 3 segundos con sort x. Sin embargo, el comando no tiene una opción inversa para ordenar de mayor a menor. gsort hace una ordenación decreciente de forma ineficiente - ordena de forma creciente en x, y luego ordena de forma creciente en menos _n. Esencialmente está haciendo: sort x gen long sortvar = -_n sort sortvar drop sortvar Es mejor negar por usted mismo antes de una ordenación creciente: generate negx = -x sort negx 6.0.8 Selección de muestra Separar la selección de variables de la inclusión. Para minimizar la cantidad de memoria utilizada, necesitamos separar la decisión de selección de la muestra de la decisión de inclusión de la variable. 6.0.9 Precaución con reshape El comando reshape es inexplicablemente lento. 13 segundos por millón de observaciones en mi computadora. Es importante pensar en codificación y buscar más opciones para hacer reshape. Se puede escribir un archivo separado para cada año de datos, y luego concatenarlos en un largo conjunto de datos en unos 2 segundos. 6.0.10 Subexpresiones comunes A menudo, varias sentencias generate o replace tendrán subexpresiones comunes. generate y = a if c==d &amp; e==f generate x = b if c==d &amp; e==f Una mejor opcion es precalcularlo. generate smpl = c==d &amp; e==f generate y = a if smpl generate x = b if smpl 6.0.11 Recomendaciones Adicionales 6.0.11.1 Collapse collapse no es muy rápido. El autor, sin duda, supuso que aunque se utilizara con grandes conjuntos de datos, no estaría dentro de una iteración. Pero a veces lo está, y puede convertirse en el paso limitante de la velocidad en un programa de larga duración. Se puede reemplazar fácilmente con código más rápido, pero el beneficio total no es tan grande como uno esperaría. Mejor ocupar gtools. 6.0.11.2 Egen egen tambien puede ser adecuado para que funcione más rapido. Vamos a ver un ejemplo calculando el máximo de una variable. 6.0.11.3 Regresiones en sub-grupos Exiten varias opciones para calcular regresiones según el tipo de datos que queramos incluir en nuestra muestra. Vamos a ver distintas opciones y ver su desempeño en bases de datos grandes. 6.0.11.4 Recode recode puede ser modificado utilizando matrices. Otra opción es utilizar ggtools 6.0.12 Resumen Utilizar compress. Mantener solo las variables que se van a utilizar. Mantener solo las observaciones que se van a utilizar. Cargar solo las variables y observaciones necesarias. keepusing y nogen siempre para merge. ftools y gtools (más detalles sección final). Utilizar parallel (más detalles sección final). "],["mata.html", "7 Mata 7.1 ¿Qué es Mata?", " 7 Mata 7.1 ¿Qué es Mata? Esta introducción esta basada en la guía de Asjad Naqvi. Mata es un lenguaje de programación matricial. Es más rápido que Stata, pero necesita más precisión (ej. dimensiones de las matrices). Muchos de los operadores de Mata son similares a R o Matlab. Adicionalmente, incluye sus propios optimizadores y funciones útiles. Esta sección será una pequeña introducción al lenguaje. Tiene como objetivo mostrarles el abanico de herramientas que Mata posee con el fin de que puedan utilizarlo para hacer sus propios programas. La sintaxis básica de Mata * Opción 1 mata &lt; comando de mata 1 &gt; &lt; comando de mata 2 &gt; &lt; y así .... &gt; end * Opción 2 mata &lt;un comando de mata&gt; * Opción 3 : la versión estricta mata: &lt;un comando de mata&gt; Si son utilizados dos puntos y ocurre un error, Mata será abortado, volverá a Stata y aparecerá un mensaje de error. * Ejemplo 1 mata emat = 7 + 3 emat end * Ejemplo 2 mata emat = (&quot;Josefa&quot;, &quot;Perez&quot;) emat = (21\\8) mmat = (17\\6) vmat = (25,3\\3,11) emat, mmat, vmat end Para buscar ayuda, podemos hacer: * Ejemplo 1 help mata comando help mata cholesky() Comandos básicos:Al igual que Stata, Mata posee sus propios comandos. Algunos útiles de recordar son: mata describe mata clear mata rename nombre mata drop nombre1 nombre2 mata stata * Ejemplo mata mata clear emat = (&quot;Josefa&quot;, &quot;Perez&quot;) emat = (21\\8) mmat = (17\\6) vmat = (25,3\\3,11) mata describe emat, mmat, vmat mata rename vmat vmat_renombrada mata drop mmat emat, vmat_renombrada end Para hacer comentarios en Mata hay que utilizar //. No funcionan los asteriscos como en Stata. Todo los que se genera en Mata queda en la memoria a menos que Stata se cierre, se use clear allen Stata o bien mata: mata clear. Todas las matrices y funciones definidas se mantienen en el espacio de trabajo de Mata espacio de trabajo cuando se termina mata y se puede acceder a ellas cuando se vuelve a entrar en mata. 7.1.0.1 Matrices Una forma común de generar una matriz en Mata es importándola desde los datos de Stata. mata: X = st_data(.,(&quot;var1&quot;, &quot;var2&quot;)) Definimos la matriz X, que tiene todas las observaciones (filas) y dos columnas var1 y var2. st_ permite a Mata interactuar con la interfaz de Stata con el fin de pasar información de un lado a otro. Revisar help m4_stata. * Ejemplo 4: confeccionar matrices en base a mata sysuse auto.dta mata mata clear mata: X = st_data(.,(&quot;price&quot;, &quot;mpg&quot;)) X mata describe end Otra opción es definir matrices dentro de Mata. mata A = (1,2 \\ 3,4) Definimos la matriz A, la cual podemos ver escribiendo mata A. Veremos una matriz cuadrada de 2x2. Notar que la coma separa elementos entre columnas mientras que el slash mueve elementos entre filas. El uso de paréntesis es opcional en Mata, pero es más conveniente y es una buena práctica de estilo utilizarlos. También se pueden definir matrices especiales. * Vector fila o Vector columna mata 1,2,3 mata 1\\2\\3 * Vector fila o Vector columna del 1 al 4 mata 1..4 mata 1::4 Finalmente, uno puede generar matrices similar a lo que se hace en Stata. mata J(2,2,1) Este comando genera una matriz de 2x2 rellena de unos. La sintaxis genérica es: mata J(filas, columnas, constante). Para definir una matriz identidad: mata I(5). Otro operador útil es range: mata range(1,7,2) Crea un vector que empieza en 1 y termina en 7 saltándose de 2 en 2 (es decir, (1,3,5,7)). Si quiero generar una matriz con números aleatorios provenientes de una distribución uniforme (0,1) : mata runiform(3,3). Accediendo a los elementos de una matriz Podemos acceder a los elementos de una matriz. mata A[1,2]: fila 1, columna 2. mata A[1,.]: Todas las columnas de la fila 1. mata A[.,2]: Todas las filas de la columna 2. También podemos extraer sub-conjuntos de elementos: * Empezar de fila 2 columna 1 hasta fila 3 columna 3 mata A[|2,1\\3,3|] * Tomar filas 2 y 3 y columnas de la 1 a la 3. mata A[(2::3),(1..3)] Pegando elementos de matrices: mata A = runiform(1,2) B = runiform(1,2) * Opción 1 : pegar horizontalmente A,B * Opción 2 : pegar verticalmente A\\B end A,B las pega horizontalmente, es decir por columnas (filas son las mismas). A\\B las pega vertical, es decir por filas (columnas son las mismas). Operaciones básicas entre matrices y por elementos: Una vez definidas las matrices, también se dedica bastante tiempo a sumar, restar, multiplicar y dividir matrices entre sí. Vamos a ver algunas operaciones entre matrices y también entre elementos. mata A = (1,2,3\\4,5,6) B = (2,3\\4,5\\6,7) A * B end A * B es una operación entre matrices, de forma tal que el número de filas de A debe ser igual al número de columnas de B. Si queremos que cada elemento de A sea multiplicado por el mismo elemento en B, es necesario trasponer alguna matriz y cambiar el operador. mata A = (1,2,3\\4,5,6) B = (2,3\\4,5\\6,7) A&#39; :* B A :* B&#39; end En el primer caso la matriz resultante corresponde a una matriz de 3x2, en el segundo caso es una matriz de 2x3. Operadores lógicos y funciones sobre escalares: Las matrices también pueden ser comparadas a través de operadores lógicos. A == B chequea si dos matrices son iguales. Esto también puede hacerse elemento por elemento (por ejemplo A:&gt;=B). Tambien se puede utilizar !=. Mata también tiene funciones sobre escalares: mata X = (-2, 1 \\ 0, 5) abs(X) // valor absoluto de cada elemento sign(X) // signo de cada elemento exp(X) // exponencial de cada elemento sqrt(X) // raíz cuadrada de cada elemento sin(X) // seno de cada elemento end Algunas funciones para operar en matrices son: mata A = (5,4\\6,7) B = (1,6\\3,2) end * Número de filas y columnas mata rows(A) mata cols(A) * Suma de las filas/columnas mata rowsum(B) mata colsum(B) * Calcular el promedio (retorna un vector fila) mata mean(B&#39;)&#39; * Seleccionar todas las columnas sin ceros mata D = (1,0,2,3,0) mata selectindex(D) * Select para condiciones más flexibles select(B, B[.,1]:&gt;2) * Ordenar matrices mata sort(B,2) // Ordena según la columna 2 de B mata jumble(B) // Aleatoriza las filas de B Mata contiene varias funciones para extraer propiedades de las matrices: mata A = (5,4\\6,7) det(A) // Determinante de A invsym(A) // Ibversa de A trace(A) // Traza de A rank(A) // Rango de A norm(A) // Norma de A X=. // Vectores propios L=. // Valores propios end Mata contiene varias funciones para operar o manipular dos matrices: mata A = (5,4\\6,7) B = (1,6\\3,2) A&#39; * B // A x B cross(A,B) // A x B using a solver (faster) end Al igual que Stata, Mata puede utilizar iteradores. While loop: mata x = 1 // Valor inicial X = 4 // Valor final while (x &lt;= X) { // Iniciar condición del while printf(&quot;\\%g \\n&quot;, x) // Alguna operación de Mata aquí x++ // Incremental } // Terminar el while end For Loops: * Este for for (expr1; expr2; expr3) { stmts } * Es equivalente a expr1 while (expr2) { stmt expr3 } Los valores iniciales y finales pueden extraerse de algunas declaraciones condicionales como las dimensiones de las matrices. Mata sólo permite incrementos de 1. Notar que aquí es bien distinto a Stata: mata N = 4 // Valor final for (i=1; i&lt;=N; i++) { // for loop printf(&quot;%g \\n&quot;, i) // some Mata operation here } // end for loop end If/else son: mata x = 3 // Valor de x for (i=1; i&lt;=5; i++) { // Empezar el loop if (x &gt; i) { // if printf(&quot;\\%g\\n&quot;, 0) // Comando de mata si la condición se cumple } else { // en caso contrario printf(&quot;\\%g\\n&quot;, 1) // ejecutar este comando de mata } } end Mata también permite abreviar la sintaxis en caso de que existan dos o mas condiciones. (a ? b : c) “a” es la condición, “b” es el valor en caso de que sea verdadero y “c” en caso de que sea falsa. Para el ejemplo, la condición sería: (x &gt; i ? 1 : 0). 7.1.0.2 De Stata a Mata Estimar un MCO: Un ejemplo estándar es hacer un simple OLS en Mata. sysuse auto, clear mata y = st_data(.,&quot;price&quot;) X = st_data(.,(&quot;mpg&quot;, &quot;weight&quot;)) X = X, J(rows(X),1,1) beta = invsym(cross(X,X))*cross(X,y) esq = (y - X*beta) :^ 2 V = (sum(esq)/(rows(X)-cols(X)))*invsym(cross(X,X)) stderr = sqrt(diagonal(V)) st_matrix(&quot;b&quot;, beta) st_matrix(&quot;se&quot;, stderr) end For Loops En Mata, la variable dependiente se importa como el vector y las variables independientes se importan como la matriz X. A esta matriz X se le añade un vector columna de unos para el intercepto. Como el vector de unos debe tener el mismo número de filas que la matriz X, nótese el uso del operador J(). En el último paso, las betas y los errores estándar se exportan a Stata como matrices de Stata. 7.1.0.3 Funciones en Mata Vector: Un ejemplo estándar es hacer un simple OLS en Mata. * Generar un vector mata function zeros(c) { a = J(c, 1, 0) return(a) } b = zeros(3) b end Matriz: * Generar una matriz mata function zerosv1(real scalar c, real scalar r) { real matrix A A = J(c,r,0) return(A) } b = zerosv1(3,2) b end Hacer opcionales: mata function zerosv2(real scalar c,| real scalar r) { real matrix A if (args()==1) r = 1 A = J(c, r, 0) return(A) } end mata zerosv2(3,2) end help m2 syntax Guardar: mata function zerosfinal(real scalar c,| real scalar r) { real matrix A if (args()==1) r = 1 A = J(c, r, 0) return(A) } mata stata cd &quot;\\$ejercicios&quot; mata mosave zerosfinal(), replace end Dos referencias recomendadas para profundizar en Mata: An Introduction to Stata Programming, Second Edition () The Mata Book: A Book for Serious Programmers and Those Who Want to Be () "],["análisis-de-datos-con-python.html", "8 Análisis de Datos con Python 8.1 Introducción al análisis de datos con Python 8.2 Librerías principales de Python - Numpy, Matplotlib, Pandas 8.3 Herramientas de programación 8.4 Manipulación de bases de datos (Parte I) 8.5 Manipulación de bases de datos (Parte II) 8.6 Análisis de datos (Parte I) 8.7 Análisis de datos (Parte II) 8.8 Análisis de Datos (Parte III) 8.9 Web Scraping en Python", " 8 Análisis de Datos con Python 8.1 Introducción al análisis de datos con Python Objetivos de este curso: Perderle el miedo a Python. Complementar el análisis de datos utilizando las ventajas que da Python. Introducirlos al programa de forma tal de que puedan ir aprendiendo cosas nuevas por su cuenta. Ventajas de Python respecto a otros programas Código abierto. Paquetes muy potentes y en continua actualización. Múltiples usos: astronomía, física, diseños sitios web, economía, análisis de datos. Muy demandando en el mercado laboral. Rapido e intuitivo Potente con grandes bases de datos: conectar a servidores externos como Google Cloud. Algunas referencias: The Python Data Science Handbook by Jake VanderPlas (O’Reilly). Copyright 2016 Jake VanderPlas, 978-1-491-91205-8. 8.1.1 Antes de partir con Python… 8.1.1.1 Instalar Python, Jupyter Notebook y Spyder Utilizaremos Anaconda y Python 3. Aunque hay varias maneras de instalar Python, la que yo sugeriría es a través de la distribución multiplataforma Anaconda. Pueden encontrarlo en la página de [Anaconda] (https://www.continuum.io/downloads). Incluye tanto Python y otros paquetes preinstalados orientados a la computación científica. Si tuvieron algún problema con la instalación, les recomiendo ver video.. Explica detalladamente comom instalar Python con Ananconda. Para programas vamos a utilizar Jupyter Notebook para algunas clases y Spyder para otras clases. Pueden instalar Spyder directamente desde el navegador de Anaconda. La idea es que se familiarizen con la interfaz de ambos programas. Jupyter Notebook es útil para trabajar interactivamente, conectarse a servidores y trabajar en equipo más sencillmente. Spyder es útil cuando trabajamos con bases de datos debido a que la interfaz permite una programación y análisis mucho más cómodos. Jupyter Notebook es útil para desarrollar, compartir y generar procesos reproducibles, pero a mi me molesta no poder ver los datos. Por lo que hay ocasiones en la que prefiero utilizar Spyder, más cuando estoy haciendo algún trabajo más exploratorio. 8.1.1.2 Markdown para Jupyter Notebook Markdown es un lenguage de programación de texto plano. Se puede utilizar en múltiples lenguajes de programación: por ejemplo, R Markdown y Stata Markdown. Permite combinar texto plano, codificación y resultados del análisis de datos como figuras y/o tablas en un solo lugar. Aprenderemos a escribir en Markdown de forma tal de mejorar la presentación de los códigos. Les servirá tambíen si es que utilzan R o Stata. Es preferible ver esto al inicio de forma tal de que podamos ir incorporando esta codificación a lo largo del curso. 1. Títulos Los títulos pueden ser creados utilizando los símbolos ‘##’. Estos símbolos determinan el tamaño del título. Van de 1 a 6. Enfatizar texto Para enfatizar texto tenemos las opciones más típicas codigo aquí para escribir códigos. Lo que hacen estos símbolos es que esto paresca una función o comando escrita como codigo. Esto es útil si es que quieres escribir nombres de paquetes en un informe. Nombre del paquete. Por ejemplo paquete statsmodel. Para utilizar itálicas y subrayar. Tambien se pueden escribir de otra forma negrita, negrita, italica o bien italica, o bien negrita e itálica o negritaeitalica. Para tachar texto Escribir parrafos: Para escribir parrafos tengo que colocar &lt;p&gt; al inicio y &lt;/p&gt; al final. Para hacer espacios entre parrafos simplemente presionar enter. ___ 4. Escribir ecuaciones Se pueden escribir ecuaciones (para los/as que manejan Latex) \\[A = \\pi*r^{2}\\] Colocar citas anidadas Todo lo que ponga aquí se vera como una cita &gt; Todo lo que ponga aquí se vera como una cita &gt;&gt; Todo lo que ponga aquí se vera como una cita Lineas horizontales Estas líneas sirven para separar los parrafos y así tener un texto mucho más ordenado. Crear listas Listas no ordenadas item 1 item 2 importante agregar un espacio entre +! . Sino R no lo reconocerá. item 3 sub - item 1 Listas ordenadas item 1 item 2 Ejemplo 3: Letras no funcionan para listas A. List B. List + item List List 6. Agregar un link Opción 1: Mostrar link https://rmarkdown.rstudio.com/authoring_quick_tour.html#Using_Parameters Opción 2: Ocultar link aquí 7. Agregar una imagen 8. Cambiar colores de frases Cambiar el color del texto Tambien se puede especificar el color con códigos. ( Nota : Paletas de colores pueden encontrar ([aquí.])(https://www.colourlovers.com/) Otros colores: blue|red|green|pink|yellow. 9. Agregar listas de tareas alguna tarea otra tarea 10. Tablas Para hacer tablas se utilizan “|” y “-”. Titulo Titulo Titulo A B C X Y Z Para cambiar manualmente la jusfiticación del texto: :-: 1. Usar :-: para centrar 2. Usar — para la derecha 3. Usar :- para la izquierda 11. Cajas de colores Tips: Usar cajas azules para destacar cosas y/o hacer notas. Ejemplos o advertencias: Utilice los cuadros amarillos para los ejemplos que no están dentro de las celdas de código, o utilícelos para las fórmulas matemáticas si es necesario. Normalmente también se utiliza para mostrar mensajes de advertencia. Algo positivo: Este cuadro de alerta indica una acción exitosa o positiva. Peligro: Este cuadro de alerta indica una acción peligrosa o potencialmente negativa. 12. Destacar alguna línea Útil para resaltar y captar la atención del lector hacia determinados puntos. 13. Marcar algún texto con un color determinado No olvides comprar leche hoy. 14. Menu de navegación LinkedIn | Github | Medium | 8.1.2 Atajos Muy necesarios para ganar tiempo y no duplicar tareas, por ejemplo, cambiar de Markdown a código. Nos vamos a ir acostumbrando poco a poco con la práctica. Para usuarios de Mac Ctrl: command key ⌘ Shift: Shift ⇧ Alt: option ⌥ Modos Existen dos modos: comando y edición. El modo de edición le permite escribir código o texto en una celda y se indica con un borde de celda verde. El modo de comando vincula el teclado a los comandos del nivel del cuaderno y se indica con un borde de celda gris con un margen izquierdo azul. Algunos atajos que funcionan en ambos modos Shift + Enter: ejecuta la celda seleccionada y pasa a la de abajo. Ctrl + Enter: ejecuta todas las celdas seleccionadas. Alt + Enter: ejecuta una celda, pero inserta una abajo. Ctrl + S: guardar el código. Atajos: modo comando (presionar esc para activar) H : mostrar todos los accesos directos. arriba: seleccionar la celda de arriba. abajo: seleccionar la celda de abajo. Shift + arriba: extender las celdas seleccionadas arriba. Shift + abajo : extender las celdas seleccionadas por debajo. A: insertar celda arriba. B: insertar celda abajo. X: cortar las celdas seleccionadas. C: copiar celdas seleccionadas. V : pegar celdas abajo. Shift + V : pegar celdas arriba. D: (pulsar la tecla dos veces) borrar las celdas seleccionadas. Z: deshacer el borrado de celdas. S: guardar y comprobar. Y: cambiar el tipo de celda a Código. M: cambiar el tipo de celda a Markdown. P: abrir la paleta de comandos. Muy útil para ejecutar directamente atajos. Shift + espacio: desplazar el cuaderno hacia arriba. espacio: para desplazar el cuaderno hacia abajo. Alt + enter: ejecutar linea y agregar una nueva. Atajos: modo edicion(presionar enter para activar) Esc : te lleva al modo comando. tab : para completar el código o la sangría. Ctrl + A: seleccionar todo. Ctrl + Z: deshacer. Ctrl + Y: rehacer. Ctrl + flecha izquierda: ir una palabra a la izquierda. Ctrl + flecha derecha: ir una palabra a la derecha. Ctrl + Shift + p abrir la paleta de comandos. Agregar tus propios atajos Help &gt; Edit Keyboard Shortcuts 8.1.3 Extensiones de Jupyter Notebook Es posible agregar caracteristicas adicionales a Jupyter Notebook. Para ello inclimos en el Promp lo siguiente: pip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib nbextension install Varias extensiones útiles e interesantes: índice, ocultar código, observar variables, etc. 8.1.4 Sintaxis básica a = 2 a print(a) # Ejemplo introductorio: vamos a ver una ejemplo sencillo, que involucra una # típica tarea de programación. # 1.Generamos una variable variable = 5 # 2. Generamos dos listas vacias inferior = []; superior = [] # Distribuimos los números entre las dos listas for i in range(10): if (i &lt; variable): inferior.append(i) else: superior.append(i) print(&quot;inferior:&quot;, inferior) print(&quot;superior:&quot;, superior) Comentarios marcados con #. Notar que tambíen se pueden escribir al lado derecho de los códigos. Se asignan variables con signo igual. Similar a Stata, casi similar a R. En Python, si quiero avanzar a la línea de abajo tengo que ocupar slach. Entre parentesis no es necesario. ## Quebrar un código en dos. ### Sin parentesis x = 1 + 2 + 3 + 4 + 5 + \\ 6 + 7 print(x) ### Con parentesis y= (1 + 2 + 3 + 4 + 5 + 6 + 7 + 8) print(y) El punto y coma puede terminar opcionalmente un enunciado. Esto es útil cuando quiero funciones y/o tareas en una línea. El espacio en blanco es importante en Python. Muy importante Los bloques de código con sangría siempre van precedidos de dos puntos (:) en la línea anterior. En el ejemplo siguiente el resultado es distinto unicamente por los espacios en blanco. # Ejemplo 1: los espacios en blanco importan if x &lt; 4: y = x * 2 print(x) ### Ejemplo 2: los espacios en blanco verdaderamente importan. if x &lt; 4: y = x * 2 print(x) &quot;&quot;&quot;6. A pesar de lo anterior, es importante notar que **los espacios en blanco dentro de las líneas no importan**&quot;&quot;&quot; # Los espacio en blanco el interior de una linea no importan, por ejemplo, estas tres expresiones son equivalentes: x=1+2 y = 1 + 2 z = 1 + 2 print(x,y,z) Los paréntesis son para agrupar, llamar valores o ocupar funciones. # Agrupar (2 * 3) + 3 # Llamar print(&quot;valor:&quot;, 1) # Funciones L = [4,2,3,1] L.sort() print(L) # El patentesis en blanco indica que la función debe ser ejecutada. # Estos se llaman metodos, los veremos con detalle más adelante. 8.1.5 Variables y objetos 8.1.5.1 Escalares # Se asignan variables utilizando signo &quot;=&quot; a = 3 + 10 b = 2 * 4 print(a,b) # Luego, podemos operar con ellas y definir nuevas variables c = a ** b * 1.002 d = a ** b * 1.2 print(c,d) e = &quot;Hola&quot; e1 = &#39;Hola&#39; f = True f1 = False print(e,e1,f,f1) # Nota: ¿Qué ocurre con los separadores de decimales? 8.1.5.2 Tipos de escalares/valores en Python Existen distintos tipos de objetos en Python, para saber cuales existen podemos utilizar type(). Cada vez que escriba un () me estaré refieriendo a una función. 8.1.5.2.1 Float Representa un número real. Tiene una parte entera u una fracción n = 3.14126 * 3.1416 type(n) Importante tener cuidado con la precisión. Si queremos hacer algo muy preciso, es importante tener ojo con esta caracteristica del formato float. 0.1 + 0.2 == 0.3 print(&quot;0.1 = {0:.17f}&quot;.format(0.1)) print(&quot;0.2 = {0:.17f}&quot;.format(0.2)) print(&quot;0.3 = {0:.17f}&quot;.format(0.3)) 8.1.5.2.2 Números complejos. Números con una parte real y la otra imaginaria. complex(1,2) c = 3 + 4j c.real c.imag type(c.real) type(c) 8.1.5.2.3 Integers Números enteros # 1. int: números enteros dias_semana = 7 print(dias_semana) type(dias_semana) 8.1.5.2.4 Boolean Variables que toman como valor verdadero o falso. Estos resultados se obtienen al utilizar comparadores. Muy importantes para programar correctamente. z = True z1 = False type(z) type(z1) result = (4 &lt; 5) print(result) print(type(result)) # La función bool() tambíen sirve para crear estos objetos. bool(2022) bool(0) bool(None) bool(&quot;&quot;) bool(&quot;Nico&quot;) 8.1.5.2.5 Strings Texto. Se creo con dos comillas. Recordar que la versión “missing” en string son dos comillas sin texto. minombre = &#39;nico&#39; # Concatenar texto texto = &#39;a&#39; + &#39;b&#39; print(texto) # Concater multiples veces texto_multiplicado = &quot;a&quot; * 10 print(texto_multiplicado) # Tamaño len(texto) # Acceder a una letra en particular texto[0] Alerta: Los distintos tipos de datos importan debido a que se comportan distinto cuando operamos con ellos. 8.1.5.2.6 None Python incluye un tipo especial NoneType que solo tiene un valor posible “None”. type(None) 8.1.6 Operadores Como todo lenguaje de programación Python permite hacer operaciones de distinto tipo. 8.1.6.1 Operaciones básicas Operador Nombre Descripción a + b Suma Suma de a y b a - b Sustracción Diferencia de a y b a * b Multiplicación Producto de a y b a / b División verdadera Cociente de a y b a // b División mínima Cociente de a y b, eliminando las partes fraccionarias a % b Módulo Resto entero tras la división de a entre b a ** b Exponenciación a elevado a la potencia de b # 1. Suma print(7 + 3) # 2. Resta print(10 - 2) # 3. Division print(5 / 10) # 4. Multiplicación print(5 * 3) # 5. Modulo: retorna el resto de la división entre estos dos elementos print(15 % 7) # 6. Exponencial: para elevar en potencia. Esto es distinto a R o Stata, ojo ahí. print(4 ** 10) # 7. Incluir mensajes print(&quot;el resultado es&quot;, 44**3) # 8. División mínima # Floor division print(11 // 2) Alerta: La forma de hacer el exponencial en Python es distinto a R o Stata. 8.1.6.2 Operadores de asignación # Asignar valor a = 24 print(a) a + 2 # Actualizar valor a += 2 # equivalente a &quot;a = a + 2&quot; print(a) 8.1.6.3 Operadores de comparación Operación Descripción a == b a igual a b a != b a no igual a b a &lt; b a menor que b a &gt; b a mayor que b a &lt;= b a menor o igual que b a &gt;= b a mayor o igual que b # 25 es impar 25 % 2 == 1 # 66 is impar 66 % 2 == 1 # Esta entre 15 y 30? a = 25 15 &lt; a &lt; 30 # Es una negativo la negación del cero? -1 == ~0 8.1.6.4 Operadores lógicos Cuando se trabaja con valores booleanos, Python proporciona operadores para combinar los valores utilizando los conceptos estándar de “and”, “or” y “not”. Como es de esperar, estos operadores se expresan utilizando estas palabras. # y x = 4 (x &lt; 6) and (x &gt; 2) # o (x &gt; 10) or (x % 2 == 0) # Negación not (x &lt; 6) # Distinto a (x &gt; 1) != (x &lt; 10) 8.1.6.5 Operadores de Identidad y Afiliación Operador Descripción a is b Verdadero si a y b son objetos idénticos a is not b Verdadero si a y b no son objetos idénticos a in b Verdadero si a es un miembro de b a not in b Verdadero si a no es miembro de b a = [1, 2, 3] b = [1, 2, 3] a == b a is b a is not b a = [1, 2, 3] b = a a is b 1 in [1, 2, 3] 2 not in [1, 2, 3] Ejercicio 4.1.1: Genere un vecto llamado II igual a 100, un interes igual a un 1% diario y un horizonte temporal de un año. Calcule la ganancia que tendría al final del periodo si es paciente. Finalmente, genere un mensaje que diga “Partí con”YY” y ahora tengo “XX” en ganancias, donde “YY” y “XX” son los montos. 8.1.7 Estructuras de datos: listas, tuplas, conjuntos, diccionarios De momento hemos visto distintos tipos de variables y algunas funciones asociadas a ellas. Ahora vamos a ver estructuras que nos serás de mucha utilidad a lo largo del curso. La siguiente tabla muestra todos los tipos: Tipo Ejemplo Descripción list [1, 2, 3] Colección ordenada tuple (1, 2, 3) Colección ordenada inmutable dict {‘a’:1, ‘b’:2, ‘c’:3} Mapeo desordenado (clave,valor) set {1, 2, 3} Colección desordenada de valores únicos 8.1.7.1 Listas Las listas son el tipo básico de colección de datos ordenados en Python. Se pueden definir con valores separados por comas entre corchetes cuiadrados a = [1.5, 1.3, 1.4, ] print(a) len(a) # tamaño de la lista # Estas listas pueden incluir distintos tipos de variables en la lista, por ejemplo incluir númerico y texto numtext = [&quot;pepe&quot;, 1.3, &quot;pepa&quot;, 1.4] print(numtext) # Pueden tambíen incluir listas anidadas y valores lógicos (lista de listas) listasanidadas = [[1.3, 1,2], [&quot;pepita&quot;, &quot;pepito&quot;], True] print(listasanidadas) Alerta: Una lista anidada es una lista de listas. # Ahora veamos el tipo de la lista type(listasanidadas) print(type(listasanidadas)) # Tambíen podemos incluir operaciones dentro [1 + 2, &quot;a&quot; * 5, 3] 8.1.7.1.1 Subconjuntos de listas # Vamos a ver como acceder a los elememetos de una lista. # Creamos lista de ejemplo lista = [True, &quot;pepa&quot;, [1,3], 2] # Ahora vamos a ver distintos subconjuntos lista[1] # Segundo elemento! Alerta: Python cuenta el primer elemento como un cero! lista[0] # primer elemento de la lista lista[2] # tercer elemento de la lista lista[3] # cuarto elemento de la lista lista[-1] # primer elemento de derecha a izquierda! # Tambien podemos hacer subconjuntos indicado el inicio y el fin lista[0:2] Alerta: El primero es incluyente mientras que el segundo excluye # Tambíen se le puede decir a Python que el inicio sea por defecto lista[:2] Finalmente, es posible especificar un tercer entero que representa el tamaño del paso; por ejemplo, para seleccionar cada segundo elemento de la lista, podemos escribir L = [2, 3, 5, 7, 11] L[::2] # equivalent to lista[0:len(lista):2] # El tercer elemento es quien nos muestra el salto. Distinto a Stata donde es: inicio(paso)final # Pregunta: ¿Qué ocurre en este caso? lista2 = [1,2,3,4,&quot;pepe&quot;,&quot;pepa&quot;] lista2[2:] # R: Selecciona hasta final! 01figuras/image.png 8.1.7.1.2 Operar con listas anidadas # Podemos operar con los subconjuntos incluso cuando tienen listas lista3 = [1,2,3, [4,5],7] v1 = lista3[1] v2 = lista3[3] v3 = v2[1] v4 = v1 * v3 print(v4) # Pregunta: ¿Cómo llegamos al 10? ¿Qué paso con v3? # Para seleccionar los elementos de una lista de listas debemos utilziar dos parentesis cuadrados # Por ejemplo, si quiero seleccionar el segundo elemento # del tercer elemento de la lista que hemos creado lista_anidada = [[1,3], [4,5,&quot;pepa&quot;]] segundalista_tercerelemento = lista_anidada[1][2] primeralista_segundoelemento = lista_anidada[0][1] print(segundalista_tercerelemento) print(primeralista_segundoelemento) 8.1.7.1.3 Manipulación de listas # 1. Podemos cambiar los elementos de una lista, pro ejemplo cambiemos el útilmo lista = [True, &quot;pepa&quot;, [1,3], 2] lista[3] = &quot;pepito&quot; print(lista) # Agregar elementos listaconmascosas = lista + [&quot;nico&quot;, 1.7] print(listaconmascosas) # Noten que es muy parecido a cuando concatenamos valores en strings. # Remover elementos l = list(range(10)) # range: para generar 10 valores (desde el cero) print(l) del l[0] print(l) # Notar que cuando borramos un elemento de la lista, cambian los índices! # Ordenar listas # Creamos listas listaA = [11.25, 18.0, 20.0] listaB = [10.75, 9.50] # Paste together first and second: full juntas = listaA + listaB # Sort full in descending order: full_sorted ordenarjuntas = sorted(juntas, reverse = True) # Print out full_sorted print(ordenarjuntas) # Es importante tener cuidado al manipular listas y/o generamos nuevas # Ejemplo 1: cambia original x = [&quot;a&quot;, &quot;b&quot; , &quot;c&quot;] y = x # Guardamos la lista x en una lista nueva # Ahora vamos a cambiar la lista y y[1] = &quot;z&quot; # Miremos las listas print(x) print(y) # Ejemplo 2: no cambia original x = 10 y = x x += 5 # Agrego 5 al valor de 5, pero no modifico x! print(&quot;x =&quot;, x) print(&quot;y =&quot;, y) # Cuando llamamos a x += 5, no estamos modificando el valor del objeto 10 apuntado por x; # más bien estamos cambiando la variable x para que apunte a un nuevo objeto entero con valor 15. # Por esta razón, el valor de y no se ve afectado por la operación. # Para evitar este problema es necesario crear listas de otra forma x = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] y = list(x) # list() sirve para hacer una lista en base a otra y = x[:] # Subcionjunto considerando todos los elementos y[1] = &quot;z&quot; print(x) print(y) # Noten que ahora cambio la lista x, pero no la lista y. Ejercicio 4.1.2: Genere una lista llamada cada con la siguiente información: Zona Tamaño (m2) comedor 11.25 cocina 18.0 living 20.0 pieza principal 10.75 baño 9.50 Seleccione la siguiente información desde la lista casa: Primeros 6 elementos Últimos 4 elementos Cocina y living junto a sus medidas Confeccione una lista llamada casa_dos_pisos que contenga dos listas: piso 1 y piso 2. En el piso 1 deje cocina, living y comedor mientras que en el segundo piso deje pieza principal y baño. 8.1.7.2 Tuplas Las tuplas son en muchos aspectos similares a las listas, pero se definen con paréntesis en lugar de corchetes. Tambíen pueden ser generadas sin los parentesis. tupla = (1, 2, 3) print(tupla) type(tupla) tupla = 1, 2, 3 print(tupla) len(tupla) # tamaño tupla[0] # Posicion ¿Qué diferencia entonces a las tuplas de las listas? La principal característica de las tuplas es que son inmutables: esto significa que, una vez creadas, su tamaño y contenido no pueden modificarse. tupla[1] = 4 De este modo, las listas son mutables mientras que las tuplas son inmutables. 8.1.7.3 Diccionarios Los diccionarios son mapeos extremadamente flexibles de claves a valores, y forman la base de gran parte de la implementación interna de Python. Pueden crearse mediante una lista separada por comas de pares clave:valor entre llaves. numeros = {&quot;uno&quot;:1, &quot;dos&quot;:2, &quot;tres&quot;:3, &quot;cuatro&quot;: 4} print(numeros) # Para accesder debo llamar al nombre, de allí que sean como un diccionario. Son &quot;similares&quot; a las listas en R. numeros[&quot;dos&quot;] # Agregar nuevos elementos al diccionario numeros[&#39;cero&#39;] = 0 print(numeros) # Noten que no hay necesarimente un sendito del orden en los diccionarios. El cero esta al final por ejemplo. 8.1.7.4 Conjuntos (sets) Se definen de forma similar a las listas y las tuplas, salvo que utilizan las llaves de los diccionarios. primos = {2, 3, 5, 7} impares = {1, 3, 5, 7, 9} # unión primos | impares # with an operator # intersección primos &amp; impares # diferencia: que estan en el primer grupo, pero no en el segundo primos - impares # diferencia simetrica: solo aparecen en un conjunto (cualquiera de los dos) primos ^ impares 8.1.8 Funciones Una función es básicamente un conjunto de tareas que podemos reutilizar a lo largo de un código. Es una buena practica de programación, permite evitar repetición. Ya hemos visto varias: print(), type(), str(), int(), bool(), float(). En todos los programas existen funciones predeterminadas (Buil-in function). Veamos algunmos ejemplos y tambíen veamos como buscar ayuda. lista = [1.5, 1.3, 1.2] # Algunas funciones para explorar datos minimo = min(lista) maximo = max(lista) suma = sum(lista) potencia = pow(2,3) print(minimo, maximo, suma, potencia) # Redondear escalares escalar = 5.3233232 redondear = round(escalar,2) # Otras para ordenar listas y medirlas tamaño = len(lista) ordenar = sorted(lista) # Por defecto ascendente print(tamaño) print(ordenar) # Ayuda! : la más importante de todas help(len) len? len?? 8.1.9 Methods Un method es una función que pertenece a un cierto objeto. Vamos a ver varios ejemplos para listas y strings. 8.1.9.1 Methods para listas lista = [&quot;pepa&quot;, 1.45, &quot;pepe&quot;, 1.32, &quot;ema&quot;, 1.45] # 1. Indicador de un valor determinado print(lista.index(&quot;ema&quot;)) # 2. Contemos valores print(lista.count(1.45)) areas = [11.25, 18.0, 20.0, 10.75, 9.50] # 3. Agregar elementos areas.append(24.5) areas.append(15.45) print(areas) # 4. Revertir order areas.reverse print(areas) 8.1.9.2 Methods para strings # --------# # Strings # --------# string = &quot;pepa&quot; # 1. Mayusculas la primera letra string.capitalize() # 2. Remplazamos alguna letra string.replace(&quot;e&quot;, &quot;a&quot;) # 3. Todas mayusculas print(string.upper()) # 4. Contar letras print(string.count(&quot;a&quot;))b # Para buscar ayuda en metodos L = [1,2,3] L.insert? L? # Tab Completation # ¿Cómo se los métodos disponibles? Nombre objeto + &quot;.&quot; + &quot;tab&quot;. Si pongo una letra al inicio puedo # ver cuales estan disponibles. #L. #L.c # Tambien es util para importar funciones desde paquetes. # Más allá del Tab, puedo utilizar * para hacer busquedas más especiales! str.*find? En resumen… En Python todo es un objeto. Dependen del tipo de objeto. Cada objeto tiene métodos especificos asociados. Hay algunos metodos comunes en todos los objetos (ej. index), pero otros no (ej. replace). # 4. Algunos metodos pueden cambiar los objetos. Veamos un ejemplo. lista = [&quot;pepa&quot;, 1.45, &quot;pepe&quot;, 1.32, &quot;ema&quot;, 1.45] lista.append(&quot;nicolas&quot;) lista # 5. Otros &quot;methods&quot; para números reales x = 4.5 print(x.real, &quot;+&quot;, x.imag, &#39;i&#39;) x = 4.5 x.is_integer() x = 4.0 x.is_integer() type(x.is_integer) 8.1.10 Paquetes Como cualquier lenguaje de programación existen paquetes. Los paquetes son conjuntos de funciones que cumplen un objetivo común. En las clases siguientes veremos en detalles varios de ellos. Estos paquetes tienen funciones y metodos especificos. No todos los paquetes estan disponibles en Python. Hay que instalarlos. Es posible colocar el nombre que uno quiera a los paquetes. Por ejemplo import numpy as np np.array([1,2,3]) # Tambíen es posible importar solo una función de un paquete. from numpy import array # Hay ocasiones en las que hay paquetes y subpaquetes from scipy.linalg import inv as my_inv # Es mejor la primer opción. Queda claro cual paquete estamos utilizando. # Ejemplo: calcular area de una circunferencia r = 0.43 # Importamos paquete math import math as mt # Calcula C C = 2 * mt.pi * r # Calcula A A = mt.pi * r ** 2 # Build printout print(&quot;Circumferencia: &quot; + str(C)) print(&quot;Area: &quot; + str(A)) 8.1.10.1 Algunos paquete fundamentales para el análisis de datos NumPy proporciona un almacenamiento y cálculo eficientes para matrices de datos multidimensionales. Pandas proporciona un objeto DataFrame junto con un potente conjunto de métodos para manipular, filtrar, agrupar y transformar datos. Matplotlib proporciona una interfaz útil para la creación de gráficos y figuras con calidad de publicación. 8.2 Librerías principales de Python - Numpy, Matplotlib, Pandas En esta sección revisaremos todo el instrumental básico de Python: operadores, estructuras de datos, funciones y metodos. Revisaremos una de las principales librerías que se utilizan para el análisis de datos: numpy. Veremos algunos ejemplos muy básicos de matplotlib tambíen. Con estos elementos, ya pueden sentirse libres para trabajar en Python. 8.2.1 NumPy Los conjuntos de datos pueden ser de muchos tipos como colecciones de documentos, colecciones de imágenes, colecciones de clips de sonido, colecciones de medidas numéricas o casi cualquier cosa. NumPy nos ayudará a manejar datos pensados como arreglos númericos. NumPy significa numerical python. Es una alternativa a las listas y sirve para manipular arreglos. Las principales librerías de Python estan basadas en numpy: Pandas, SciPy, Matplotlib 8.2.1.1 Sintaxis básica y detalles ingreso = [890, 1023, 2500, 3223, 23434] n_hogar = [2,4,5,6,9] ingreso_por_persona = ingreso / n_hogar # Tenemos un error! Para ver más detalles de la libreria es bueno recordar: Puedo ver el contenido de np aprentando &lt;tab&gt;despues de apretar np. Puedo revisar la documentación con np? Más información sobre numerical python: https://numpy.org/ # importamos la librería o paquete import numpy as np # importamos np_ingreso = np.array(ingreso) np_nhogar = np.array(n_hogar) # Ahora, no hay problemas en calcularlo ingreso_por_persona = np_ingreso / np_nhogar print(ingreso_por_persona) type(ingreso_por_persona) En resumen Con listas no se puede operar como nos gustaría, pero con NumPy si. NumPy asume que la lista contiene los mismos elementos. NumPy es igualador! Como vimos anteriormente, array es un tipo de objeto. Por lo tanto, sus methods pueden ser distintos. ndarray: n-dimentional array. Eje x se denomina axis 0, eje y axis1. # Numpy es igualador np.array([1.2, &quot;martes&quot;, False]) # logicos + numeros: todo a números. True se convierte en uno / False en cero. # logicos/numerico + string: todo a string # NumPy cambia algunos operadores lista_numeros = [1,2,3] np_numeros = np.array(lista_numeros) print(np_numeros) # En una lista: + concatenar lista_numeros + lista_numeros # En un arreglo: + suma np_numeros + np_numeros 8.2.1.2 Crear arreglos con NumPy # Veamos los distintos tipos de arreglos que podemos hacer en numpy np.random.seed(0) # Garantizar reproducibilidad x1 = np.random.randint(10, size = 6) # Arreglo de una dimensión (dimension) x2 = np.random.randint(10, size = (3, 4)) # Arreglo de dos dimensiones (filas x columnas) x3 = np.random.randint(10, size = (3, 4, 5)) # Arreglo de tres dimensiones (numero matrices, filas x columnas) x4 = np.random.randint(10, size =(3,4,5,6)) # Arreglo de cuatro dimensiones (numero de grupos, numero matrices, filas x columnas) print(x1) print(x2) print(x3) print(x4) # Veamos más ejemplos # Arreglo de una dimensión x = np.array([1, 2, 3]) x print(x) # Arreglo de dos dimensiones # Hago una lista de listas x_2d =[[3,2,5], [9,7,1], [4,3,6]] x_2d_array = np.array(x_2d) print(x_2d_array) # Arreglo de tres dimensiones x_3d = np.array([x_2d_array,x_2d_array, x_2d_array]) print(x_3d) Arreglo de 1,2,3 dimensiones 01figuras/image.png Arreglo de 4 dimensiones 01figuras/array_4d.png # Arreglo de cuatro dimensiones x_4d = np.array([x_3d,x_3d, x_3d, x_3d]) print(x_4d) 8.2.1.3 Atributos de los arreglos # Saber la dimensión print(&quot;x3 ndim: &quot;, x3.ndim) # Saber detalles de la dimensión print(&quot;x3 shape:&quot;, x3.shape) # Saber el número de elementos del objeto print(&quot;x3 size: &quot;, x3.size) 8.2.1.4 Subconjuntos en un arreglo 8.2.1.4.1 Subconjuntos en arreglos unidimensionales np_numeros # Puedo seleccionar subconjuntos al igual que en una lista: con la posición. np_numeros[1] np_numeros[0:2] np_numeros[-1] np_numeros[np_numeros &gt; 2] # Lo anterior ocurre porque el resultado de un comparador es un valor logico np_numeros &gt; 2 Recordar : En cualquier subconjunto: parentesis cuadrados y utilizar valores lógicos. # Podemos escribirlo más general condicion = np_numeros &gt; 2 print(np_numeros[condicion]) # Noten que obtenemos el mismo resultado que antes x1 = np.array([5, 0, 3, 3, 7, 9]) print(x1) x1[0] x = np.arange(10) x # Primeros cinco elementos x[:5] # Elements despúes del indicador cinco x[5:] # todos los elementos en posiciones pares x[::2] # Todos los elementos, en saltos de 2 en 2, pero partiendo desde el 1 x[1::2] x[::-1] # Todos los elementos, pero de atras a adelante 8.2.1.4.2 Subconjuntos en arreglos multidimensionales x2 = np.array([[3, 5, 2, 4], [7, 6, 8, 8], [1, 6, 7, 7]]) x2 x2[0, 0] x2[2, 0] x2[2, -1] x2 # dos filas, tres columnas x2[:2, :3] # Dar vuelta todo x2[::-1, ::-1] print(x2[:, 0]) # primera columna print(x2[0, :]) # primera fila print(x2[0]) # equivalente a x2[0, :] 8.2.1.5 Filtrar arreglos # Filtros con mascaras uno_al_diez = np.arange(1,10) uno_al_diez # Creo la mascara mask = uno_al_diez % 2 == 0 mask # Va a dejar unicamente los elementos donde la condición # es cierta uno_al_diez[mask] y_2d = np.array([[1,22], [2,21], [3,27], [4,26]]) y_2d # Voy a colocar la condición dentro del subconjunto # Es analogo a lo que esta arriba y_2d[:,0][y_2d[:,1] % 2 == 0] # Otra opción es utilizar np.where() np.where(y_2d[:,1] % 2 == 0) # Retorna un array de INDICES en donde # se cumplen las condiciones np.where(y_2d == 2) type(np.where(y_2d == 2)) # El resultado es una tupla! (indice fila, indice columna) np.where(y_2d == 2) # Puedo ocupar np.where para modificar un arreglo según # alguna condición np.where(y_2d == 2, &quot; &quot;, y_2d) # Lo primero indica la condición, lo segundo el remplazo, # lo tercero donde se remplaza 8.2.1.6 Modificar arreglos Es importante tener ojo al modificar los arreglos # Recordemos el arreglo x2 print(x2) # Hagamos un subconjunto del arreglo x2 x2_sub = x2[:2, :2] print(x2_sub) # Modifiquemos el arreglo x2 x2_sub[0, 0] = 99 print(x2_sub) print(x2) # x2 cambio!!! # Para evitar que el objeto original se modifique utilizamos el metodo copy() x2_sub_copy = x2[:2, :2].copy() print(x2_sub_copy) x2_sub_copy[0, 0] = 42 print(x2_sub_copy) print(x2) # No se ha modificado! 8.2.1.7 Manipular arreglos También es posible combinar múltiples arrays en uno solo, y a la inversa, dividir un único array en múltiples arrays. Aquí veremos esas operaciones. La concatenación, o unión de dos arrays en NumPy, se realiza principalmente utilizando las rutinas np.concatenate, np.vstack y np.hstack. np.concatenate toma una tupla o lista de arrays como primer argumento, como podemos ver aquí 8.2.1.7.1 Concatenar arreglos x = np.array([1, 2, 3]) y = np.array([3, 2, 1]) np.concatenate([x, y]) # O de otra forma np.concatenate((x,y)) # Agregar un nuevo arreglo z = [99, 99, 99] print(np.concatenate([x, y, z])) grid = np.array([[1, 2, 3], [4, 5, 6]]) # Concatenar con matrices de dimensión 2 np.concatenate([grid, grid]) # Concatenar con matrices de dimensión 2 np.concatenate([grid, grid], axis = 0) # Concatenar con matrices de dimensión 2 np.concatenate([grid, grid], axis = 1) Para trabajar con matrices de dimensiones mixtas, puede ser más claro utilizar las funciones np.vstack (pila vertical) y np.hstack (pila horizontal): # Pegar arreglos verticalmente x = np.array([[1, 2, 3], [-11, -2, -3]]) y = np.array([[9, 8, 7], [6, 5, 4]]) print(np.vstack([x, y])) # Pegar arreglos horizontalmente np.hstack([y, x]) 8.2.1.7.2 Separar arreglos # Puedo indicarle el numero de grupos arr = np.array([1, 2, 3, 4, 5, 6]) newarr = np.array_split(arr, 4) print(newarr) # Puedo indicarle los indices de los grupos x = [1, 2, 3, 99, 99, 3, 2, 1] x1, x2, x3 = np.split(x, [4, 5]) print(x1, x2, x3) 8.2.1.7.3 Trasponer arreglos print(x) xT = np.transpose(x) print(xT) # Noten que ahora es un vector fila np_2d_t = np.transpose(np_2d) print(np_2d_t) # Tambien puedo aplicarlo como método. np_2d.T 8.2.1.7.4 Otras funciones para crear arreglos # Crea un arreglo unidimensional de zeros np.zeros(10) np.zeros((10,10)) # Crear una matriz de las dimensiones que se le indican # Notar parentesis cuadrado np.zeros([10,10]) # Crea una matriz de 3x5 puntos flotantes llena de unos np.ones((3, 5), dtype=float) # Crea una matriz de 3x5 llena de 3,14 np.full((3, 5), 3.14) # Crea un array lleno de una secuencia lineal # Empezando en 0, terminando en 20, pasando por 2 # (esto es similar a la función incorporada range()) np.arange(0, 20, 2) np.arange(-3,4) # Incluye el primer elemento, pero el último no np.arange(-3,4,3) # Crear una matriz 3x3 de valores aleatorios normalmente distribuidos # con media 0 y desviación estándar 1 np.random.normal(0, 1, (3, 3)) # Crea una matriz 3x3 de enteros aleatorios en el intervalo [0, 10) np.random.randint(0, 10, (3, 3)) # Crea una matriz identidad np.eye(3) # Crear una matriz de valores espaciados uniformemente # espaciados entre sí np.linspace(0,2,9) # Solo valores aleatorios np.random.random((2,4)) # La primera parte es el modulo de numpy, el segundo # es el nombre de la función. # Crear un arreglo vacio np.empty((3,2)) # Reshape: modifica las dimensiones de un arreglo array = np.array([[1,2], [5,7], [6,6]]) print(array) array.reshape((2,3)) # Flatten: toma todos los elementos de un arreglo y los # deja en una dimensión array.flatten() # Podemos componer funciones, volver al origina array.flatten().reshape((2,3)) 8.2.1.7.5 Ordenar arreglos # Como función x = np.array([2, 1, 4, 3, 5]) print(x) print(np.sort(x)) np.sort? # Como método x.sort() print(x) # argsort() retorna los indices x = np.array([2, 1, 4, 3, 5]) i = np.argsort(x) print(i) # El primero es el menor &quot;&quot;&quot;+ **Si queremos ordenar por filas o columnas?**&quot;&quot;&quot; # Vamos a utilizar el argumento axis a = np.random.randint(0, 10, (3, 3)) print(a) # Ordenamos por columnas print(np.sort(a, axis = 0)) # Ordenamos por filas print(np.sort(a, axis = 1)) Notar que trata a cada columna/fila como arreglos independientes. Cualquier relación entre columnas/filas se pierde al ordenarlos. # Puedo ordenar con la siguiente regla: # indica un número: total de elementos a ordenar # a la izquierda los menores # a la derecha los mayores x = np.array([7, 2, 3, 1, 6, 5, 4]) np.partition(x, 3) # Analogo para un 2d print(np.partition(a, 2, axis=1)) 8.2.1.8 Broadcasting Es una de las razones de que Python sea tan eficiente. Permite operar entre objetos (ej. matrices) que tienen distinta dimensión. La transmisión en NumPy sigue un estricto conjunto de reglas para determinar la interacción entre las dos matrices: Regla 1: Si las dos matrices difieren en su número de dimensiones, el objeto de la que tiene menos dimensiones se rellena con unos en su lado frontal (izquierdo). Regla 2: Si la forma de las dos matrices no coincide en ninguna dimensión, la matriz con forma igual a 1 en esa dimensión se estira para que coincida con la otra forma. Regla 3: Si en alguna dimensión los tamaños no coinciden y ninguno es igual a 1, se produce un error. Más info: https://numpy.org/doc/stable/user/basics.broadcasting.html 01figuras/image.png 01figuras/image.png 01figuras/image.png array2_2 = np.array([0,0,0,0]).reshape(2,2) array2_1 = np.array([0,1]).reshape((2,1)) array1_2 = np.array([0,1]).reshape((1,2)) print(array2_2) np.shape(array2_2) print(array2_1) np.shape(array2_1) print(array1_2) np.shape(array1_2) # Caso 1: sumo a cada columna print(array2_2 + array2_1) # Caso 2: sumo a cada fila print(array2_2 + array1_2) # ¿Que hizo python? : respeto siempre las filas. # Todo lo anterior aplica para otras operaciones. # Aquí tambien hay de alguna forma bc print(a) a + 5 # Es similar a transformar 5 en [5,5,5] # Otro ejemplo M = np.ones((3, 3)) print(M) print(a) M + a 01figuras/image.png M = np.ones((2, 3)) a = np.arange(3) print(M) print(a) print(M.shape) print(a.shape) Regla 1: a tiene menos dimension, por lo tanto, a la izquierda con 1. M -&gt; (2,3) a -&gt;(1,3) Regla 2: como difieren en la primera dimensión, estiramos la más pequeña hasta hacerla coincidir print(M + a) # Ejemplo 2: Cuando ambos tienen que hacer broadcasting a = np.arange(3).reshape((3, 1)) b = np.arange(3) print(a) print(b) print(a.shape) print(b.shape) Regla 1: el de dimensión más baja con 1 a la izquierda (3,1) (1,3) Regla 2: ambos deben coincidir (3,3) y (3,3). print(a + b) # Ejemplo 3 M = np.ones((3, 2)) a = np.arange(3) Regla 1: M -&gt; 3,2 a -&gt; 1,3 Regla 2: La dimensión 1 se modifica hasta que sean iguales M -&gt; 3,2 a -&gt; 3,3 M + a 8.2.1.9 Operaciones matematicas en arreglos # Para hacer operaciones np_mat = np.array([[1, 2], [3, 4], [5, 6]]) print(np_mat ** 1) print(np.array([10, 10])) print(np_mat + np.array([10, 10])) np_mat + np_mat 8.2.1.9.1 Operaciones vectorizadas en Python def dividirporvalores(valores): output = np.empty(len(valores)) for i in range(len(valores)): output[i] = 1 / valores[i] return output arreglo = np.random.randint(1, 20, size=10) dividirporvalores(arreglo) # Commented out IPython magic to ensure Python compatibility. arreglo_muygrande = np.random.randint(1, 100, size=1000000) # %timeit dividirporvalores(arreglo_muygrande) Al vectorizar operaciones se reduce el tiempo de ejecución. Las típicas operaciones que realizamos muchas veces en estos lenguajes son las que permiten disminuir el tiempo de ejecución en los programas. print(dividirporvalores(arreglo)) print(1.0 / arreglo) # Commented out IPython magic to ensure Python compatibility. # %timeit (1.0 / arreglo_muygrande) np.arange(5) / np.arange(1, 6) np.arange(100000) ** 2 np.arange(100000).sum() 8.2.1.9.2 Operaciones básicas y funciones básicas x = np.arange(4) print(&quot;x =&quot;, x) print(&quot;x + 5 =&quot;, x + 5) print(&quot;x - 5 =&quot;, x - 5) print(&quot;x * 2 =&quot;, x * 2) print(&quot;x / 2 =&quot;, x / 2) print(&quot;x // 2 =&quot;, x // 2) # floor division print(&quot;-x = &quot;, -x) print(&quot;x ** 2 = &quot;, x ** 2) print(&quot;x % 2 = &quot;, x % 2) # Todas estas funciones/operadores se encuentran como metodos # Funciones como metodos print(np.add(x, 2)) print(np.multiply(x,2)) print(np.divide(x,2)) print(np.subtract(x,2)) a = np.array([1,2,3]) b = np.array([[1.5,-32,-33], [4,5,6]]) c = 2 np.absolute(b) np.exp(b) np.sqrt(b) np.sin(a) np.cos(b) np.log(a) 8.2.1.9.3 Operaciones entre distintos tipos de objeto a = np.array([1,2,3]) b = np.array([[1.5,2,3], [4,5,6]]).reshape(2,3) c = np.array([[2,3],[3,3],[3,5]]).reshape(3,2) d = 2 e = np.array([[2,3,3]]) print(a) print(b) print(c) print(d) print(e) # Vector con escalar a + d # Matriz con escalar b - d # Matriz con matriz (suma) b + b # Matriz con matriz (producto) b * c # Si quiero multiplicar matrices normalmente np.dot(b,c) # Otro ejemplo m1 = np.array([[1,4,7],[2,5,8]]) m2 = np.array([[1,4],[2,5],[3,6]]) np.shape(m1) np.shape(m2) m3 = np.dot(m1,m2) print(m3) # ¿Por qué aquí pudimos multiplicar? b * e 8.2.1.10 Estadistica exploratoria con arreglos np_mat = np.array([[1, 2,30], [30, 4, -5], [5, 6,7 ]]) print(np_mat) np_mat[:,0] # Media np.mean(np_mat[:,0]) # Mediana np.median(np_mat[:,0]) # Coeficiente de correlación np.corrcoef(np_mat[:,1],np_mat[:,2] ) # Desviación estandar np.std(np_mat[:,1]) # Suma np.sum(np_mat[:,1]) # Suma acumulada print(np.cumsum(np_mat)) # Tambien puedo utilizarlas como metodos np_mat.sum() np_mat.min() np_mat.max() np_mat.mean() # Generar datos aleatorios a partir de una distribución normal ingreso = np.round(np.random.normal(1000,100,5000),1) n_hogar = np.round(np.random.normal(5,2,5000),0) ingreso ingreso.shape n_hogar n_hogar.shape datos = np.column_stack((ingreso,n_hogar)) print(datos) datos[:,0].mean() # Tambien podemos utilizar variables categoricas excluyentes # Fijemos colores y precios como listas # Listas : colores y precios mano = [&quot;diestro&quot;, &quot;zurdo&quot;, &quot;zurdo&quot;, &quot;diestro&quot;, &quot;zurdo&quot;] valor = [20, 21, 24, 12, 15] # Convertimos a arreglos np_mano = np.array(mano) np_valor = np.array(valor) print(np_mano, np_valor) # Hacemos una selección solo para diestros valores_diestro = np_valor[np_mano == &#39;diestro&#39;] # Hacemos una selección solo para zurdos valores_zurdos = np_valor[np_mano != &#39;diestro&#39;] # Imprimimos los resultados utilizando las estadisticas promedio print(&quot;Mediana valor diestros: &quot; + str(np.median(valores_diestro ))) # Print out the median height of other players. Replace &#39;None&#39; print(&quot;Mediana valor zurdos: &quot; + str(np.median(valores_zurdos))) np_mat # Ojo que le puedo indicar los ejes np_mat.sum(axis = 0) # Ojo que le puedo indicar los ejes np_mat.sum(axis = 1) M = np.random.random((3, 4)) print(M) M.sum() M.min(axis=0) M.max(axis=1) np.argmin(M) np.percentile(M, 10) 8.2.1.11 Comparativos en arreglos a == b a &lt; 2 Ejercicio 4.2.1: Genere una lista llamada ejercicio1 que contenga cuatro listas. La primera debe tener los números (180, 78.4, 15, 18), la segunda (215, 102.7, 17, 19), la tercera (210, 98.5, 18, 20) y la cuarta (188, 75.2, 22, 21). Importe el paquete numpy como np. Transfore ejercicio1 en un arreglo llamado np_ejercicio1. Finalmente, imprima el tipo de arreglo y el tamaño de este (shape.) Cree un arreglo que contenga unicamente la segunda columna de ejercicio1. Llame a este arreglo segunda columna. Haga el mismo procedimiento con la primera fila. Múltiplique la segunda columna por la primera fila. Llame a este arreglo resultado. Cree un mensaje que le muestre el promedio, la mediana y la desviación estadar de la tercera fila del arreglo ejercicio1. 8.2.2 Matplotlib Librería de Python para visualizar datos. Importante para explorar datos y para comunicar efectivamente el análisis de datos. 8.2.2.1 Visualizaciones básicas 8.2.2.1.1 Lineas import matplotlib.pyplot as plt ## No module named &#39;matplotlib&#39; años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.plot(años,pib) ## name &#39;plt&#39; is not defined 8.2.2.1.2 Puntos import matplotlib.pyplot as plt ## No module named &#39;matplotlib&#39; años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.scatter(años,pib) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined 8.2.2.1.3 Histograma # Histograma de la distribución de la variable ingreso ingreso = np.round(np.random.normal(1000,100,5000),1) ## name &#39;np&#39; is not defined plt.hist(ingreso, bins = 100) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Por defecto Python selecciona en 10 el número de bins. Muy pocos &quot;bins&quot; sobre simplifican. # Muchos no permiten ver algún patrón en los datos 8.2.2.1.4 Barras # Grafico de barras x = [5,6,3,7,2] y= [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;] plt.bar(y,x) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Grafico de barras horizontal #create data for plotting x = [5,6,3,7,2] y = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;] plt.barh(y,x) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined 8.2.2.2 Personalizar gráficos # Agregar escalas logaritmicas import matplotlib.pyplot as plt ## No module named &#39;matplotlib&#39; años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.scatter(años,pib) ## name &#39;plt&#39; is not defined plt.yscale(&quot;log&quot;) # Para cambiar las escalas a logaritmos ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Modificar el tamaño plt.figure(figsize=(8,6)) ## name &#39;plt&#39; is not defined plt.plot([10,11,12,13,14], [15,16,17,18,19]) ## name &#39;plt&#39; is not defined plt.grid(True) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Modificar simbolos plt.plot([10,11,12,13,14], [15,16,17,18,19], &quot;gd&quot;) ## name &#39;plt&#39; is not defined plt.grid(True) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # gd: green dot. # Modificar color x = [0,1,2,3,4,5] x2 = [0,1,4,9,16,25] plt.scatter(x,x2, s=10, color = &quot;red&quot;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Agregar titulos y nombres de los ejes import matplotlib.pyplot as plt ## No module named &#39;matplotlib&#39; años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.plot(años,pib) ## name &#39;plt&#39; is not defined plt.xlabel(&quot;Años&quot;) # Titulo eje x ## name &#39;plt&#39; is not defined plt.ylabel(&quot;Producto interno bruto&quot;) # Titulo eje y ## name &#39;plt&#39; is not defined plt.title(&quot;Evolución PIB en el tiempo&quot;) # Titulon grafico ## name &#39;plt&#39; is not defined # Modificar los números de los ejes import matplotlib.pyplot as plt ## No module named &#39;matplotlib&#39; años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.plot(años,pib) ## name &#39;plt&#39; is not defined plt.xlabel(&quot;Años&quot;) # Titulo eje x ## name &#39;plt&#39; is not defined plt.ylabel(&quot;Producto interno bruto&quot;) # Titulo eje y ## name &#39;plt&#39; is not defined plt.title(&quot;Evolución PIB en el tiempo&quot;) # Titulon grafico ## name &#39;plt&#39; is not defined plt.yticks([0,500,1000,1500,2000]) # Eje y ## name &#39;plt&#39; is not defined plt.xticks([2010, 2011, 2012, 2013, 2014])# Eje x ## name &#39;plt&#39; is not defined # Colocar una etiqueta en el eje import matplotlib.pyplot as plt ## No module named &#39;matplotlib&#39; años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.plot(años,pib) ## name &#39;plt&#39; is not defined plt.xlabel(&quot;Años&quot;) # Titulo eje x ## name &#39;plt&#39; is not defined plt.ylabel(&quot;Producto interno bruto&quot;) # TItulo eje y ## name &#39;plt&#39; is not defined plt.title(&quot;Evolución PIB en el tiempo&quot;) ## name &#39;plt&#39; is not defined plt.yticks([0,500,1000,1500,2000], [0, &quot;0.5M&quot;, &quot;1M&quot;, &quot;1.5M&quot;, &quot;2M&quot; ]) ## name &#39;plt&#39; is not defined # Cambiar la opacidad x = [2,1,6,4,2,4,8,9,4,2,4,10,6,4,5,7,7,3,2,7,5,3,5,9,2,1] plt.hist(x, bins = 10, color=&#39;blue&#39;, alpha=0.2) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Alpha: cambia la opacidad del grafico. Entre 0 y 1. # Cambiar color x = [5,6,3,7,2] y = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;] plt.bar(y,x, color = &quot;green&quot;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Grafico de barras horizontal y con color #create data for plotting x = [5,6,3,7,2] y = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;] plt.barh(y,x, color =&quot;yellowgreen&quot;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Agregar texto al grafico plt.plot([10,11,12,13,14], [15,16,17,18,19], &#39;rd&#39;) ## name &#39;plt&#39; is not defined plt.axis([10, 16, 15, 20]) ## name &#39;plt&#39; is not defined plt.xlabel(&#39;Eje x&#39;) ## name &#39;plt&#39; is not defined plt.ylabel(&#39;Eje y&#39;) ## name &#39;plt&#39; is not defined plt.text(11.2, 15.9, r&#39;Este es el segundo punto&#39;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Agregar una linea horizontal años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.plot(años,pib) ## name &#39;plt&#39; is not defined plt.axhline(y=1200, ls=&#39;--&#39;, c=&#39;r&#39;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Agregar una linea vertical años = [2010, 2011, 2012, 2013, 2014] pib = [1000, 1150, 1090, 1200, 1500] plt.plot(años,pib) ## name &#39;plt&#39; is not defined plt.axvline(x = 2011, ls=&#39;--&#39;, c=&#39;r&#39;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Agregar caja con nombres x = np.linspace(0, 10, 30) # Devuelve números espaciados uniformemente en un intervalo especificado. ## name &#39;np&#39; is not defined plt.plot(x, np.sin(x), label=&#39;seno&#39;) ## name &#39;plt&#39; is not defined plt.plot(x, np.cos(x), label=&#39;coseno&#39;) ## name &#39;plt&#39; is not defined plt.legend() ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Otra forma de agregar caja con nombres plt.plot(x, np.sin(x)) ## name &#39;plt&#39; is not defined plt.plot(x, np.cos(x)) ## name &#39;plt&#39; is not defined plt.legend([&quot;seno&quot;, &quot;cos&quot;]) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Cambiar la ubicación de la leyenda plt.plot(x, np.sin(x)) ## name &#39;plt&#39; is not defined plt.plot(x, np.cos(x)) ## name &#39;plt&#39; is not defined plt.legend([&quot;seno&quot;, &quot;cos&quot;], loc = &quot;upper right&quot;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Visualización más compleja con más de una leyanda lines = [] line_styles = [&#39;-&#39;, &#39;-.&#39;, &#39;--&#39;, &#39;:&#39;] line_colors = [&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;black&#39;] x = np.linspace(0, 10, 1000) ## name &#39;np&#39; is not defined for i in range(4): line, = plt.plot( x, np.sin(x - i * np.pi / 2), line_styles[i], color=line_colors[i] ) lines.append(line) ## name &#39;plt&#39; is not defined legend1 = plt.legend(lines[:2], [&#39;part A&#39;, &#39;part B&#39;], loc=&#39;upper right&#39;) ## name &#39;plt&#39; is not defined legend2 = plt.legend(lines[2:], [&#39;part C&#39;, &#39;part B&#39;], loc=&#39;lower right&#39;) ## name &#39;plt&#39; is not defined plt.gca().add_artist(legend1) ## name &#39;plt&#39; is not defined plt.gca().add_artist(legend2) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Visualizar cuatro elemento: variable 1 y 2 con los ejes, variable 3 con color y variable 4 con tamaño- x = np.random.randn(100) ## name &#39;np&#39; is not defined y = np.random.randn(100) ## name &#39;np&#39; is not defined colors = np.random.rand(100) ## name &#39;np&#39; is not defined sizes = 100 * np.random.rand(100) ## name &#39;np&#39; is not defined plt.scatter(x, y, c=colors, s=sizes, alpha=0.8, cmap=&#39;viridis&#39;) # cmap: color maps. Otro ejemplo: plasma ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined Más colores en: https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html # Agregar barra de colores # Visualizar cuatro elemento: variable 1 y 2 con los ejes, variable 3 con color y variable 4 con tamaño- x = np.random.randn(100) ## name &#39;np&#39; is not defined y = np.random.randn(100) ## name &#39;np&#39; is not defined colors = np.random.rand(100) ## name &#39;np&#39; is not defined sizes = 100 * np.random.rand(100) ## name &#39;np&#39; is not defined plt.scatter(x, y, c=colors, s=sizes, alpha=0.8, cmap=&#39;viridis&#39;) # cmap: color maps. Otro ejemplo: plasma ## name &#39;plt&#39; is not defined plt.colorbar(orientation = &quot;horizontal&quot;) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined Para ver más opciones de la librería: https://matplotlib.org/3.5.0/api/ x = np.linspace(0, 10, 11) ## name &#39;np&#39; is not defined x # Combinar tres histogramas en un grafico h1 = np.random.normal(0, 0.8, 1000) ## name &#39;np&#39; is not defined h2 = np.random.normal(-3, 1.2, 1000) ## name &#39;np&#39; is not defined h3 = np.random.normal(4, 0.5, 1000) ## name &#39;np&#39; is not defined plt.hist(h1, bins=40, alpha=0.5) ## name &#39;plt&#39; is not defined plt.hist(h2, bins=40, alpha=0.8) ## name &#39;plt&#39; is not defined plt.hist(h3, bins=40, alpha=0.5) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Modificar la opacidad del grafico x = [2,1,6,4,2,4,8,9,4,2,4,10,6,4,5,7,7,3,2,7,5,3,5,9,2,1] #plot for a histogram plt.hist(x, bins = 10, color=&#39;blue&#39;, alpha=0.2) ## name &#39;plt&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Alpha: cambia la opacidad del grafico. Entre 0 y 1. # Generar graficos, combinarlos, pero uno despues del otro fig = plt.figure() # Genero un espacio para la figura ## name &#39;plt&#39; is not defined ax1 = fig.add_axes([0.1, 0.5, 0.8, 0.4], ylim=(-2, 2)) ## name &#39;fig&#39; is not defined ax2 = fig.add_axes([0.1, 0.0, 0.8, 0.4], ylim=(-2, 2)) ## name &#39;fig&#39; is not defined x = np.linspace(0, 10) ## name &#39;np&#39; is not defined ax1.plot(np.sin(x)) ## name &#39;ax1&#39; is not defined ax2.plot(np.cos(x)) ## name &#39;ax2&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined # Generate normal distributed sample points in 2-D x, y = np.random.multivariate_normal([0, 0], [[1, 1], [1, 2]], 3000).T ## name &#39;np&#39; is not defined # Define subplot styles fig = plt.figure(figsize=(10, 10)) ## name &#39;plt&#39; is not defined grid = plt.GridSpec(4, 4, hspace=0.2, wspace=0.2) ## name &#39;plt&#39; is not defined ax = fig.add_subplot(grid[:-1, 1:]) ## name &#39;fig&#39; is not defined ax_x = fig.add_subplot(grid[-1, 1:]) ## name &#39;fig&#39; is not defined ax_y = fig.add_subplot(grid[:-1, 0]) ## name &#39;fig&#39; is not defined # Plot the data to subplots ax.scatter(x, y, s=3, alpha=0.5) ## name &#39;ax&#39; is not defined ax_x.hist(x, 40, orientation=&#39;vertical&#39;) ## name &#39;ax_x&#39; is not defined ax_y.hist(y, 40, orientation=&#39;horizontal&#39;) ## name &#39;ax_y&#39; is not defined ax_x.invert_yaxis() ## name &#39;ax_x&#39; is not defined ax_y.invert_xaxis() ## name &#39;ax_y&#39; is not defined plt.show() ## name &#39;plt&#39; is not defined 8.2.2.3 Ejemplo de generación de visualizaciones # Preambulo import pandas as pd # datos import matplotlib as mpl # graficos import datetime as dt # fechas import numpy as np # arreglos # Chequear todo print(&#39;Pandas version: &#39;, pd.__version__) print(&#39;Matplotlib version: &#39;, mpl.__version__) print(&#39;Today: &#39;, dt.date.today()) # PIB y consumo USA gdp = [13271.1, 13773.5, 14234.2, 14613.8, 14873.7, 14830.4, 14418.7, 14783.8, 15020.6, 15369.2, 15710.3] pce = [8867.6, 9208.2, 9531.8, 9821.7, 10041.6, 10007.2, 9847.0, 10036.3, 10263.5, 10449.7, 10699.7] year = list(range(2003,2014)) # use range for years 2003-2013 # Creamos data frame us = pd.DataFrame({&#39;gdp&#39;: gdp, &#39;pce&#39;: pce}, index=year) print(us.head(3)) # Pib percapita (World Bank data, 2013, thousands of USD) code = [&#39;USA&#39;, &#39;FRA&#39;, &#39;JPN&#39;, &#39;CHN&#39;, &#39;IND&#39;, &#39;BRA&#39;, &#39;MEX&#39;] country = [&#39;United States&#39;, &#39;France&#39;, &#39;Japan&#39;, &#39;China&#39;, &#39;India&#39;, &#39;Brazil&#39;, &#39;Mexico&#39;] gdppc = [53.1, 36.9, 36.3, 11.9, 5.4, 15.0, 16.5] wbdf = pd.DataFrame({&#39;gdppc&#39;: gdppc, &#39;country&#39;: country}, index=code) wbdf Método 1: Aplicar plot() us us.plot() # Solo pib us[&#39;gdp&#39;].plot() # ahora en barras us.plot(kind=&#39;bar&#39;) # Comp puntos # Orden importa us.plot.scatter(&#39;pce&#39;, &#39;gdp&#39;) Método 2: Aplicar la función plot(x,y) # import pyplot module of Matplotlib import matplotlib.pyplot as plt us us.index plt.plot(us.index, us[&#39;pce&#39;]) # Dos lineas juntas plt.plot(us.index, us[&#39;gdp&#39;]) plt.plot(us.index, us[&#39;pce&#39;]) # Noten como python agrega us # Grafico de barras plt.barh(us.index, us[&#39;gdp&#39;], align = &#39;center&#39;) # Agregamos nuevos elementos plt.plot(us.index, us[&#39;gdp&#39;]) plt.plot(us.index, us[&#39;pce&#39;]) plt.title(&#39;US GDP&#39;, fontsize=12, loc=&#39;left&#39;) # Títutlo plt.ylabel(&#39;Billions of 2009 USD&#39;) # Eje y plt.xlabel(&quot;Years&quot;) plt.xlim(2002.5, 2013.5) # Limites eje x plt.tick_params(labelcolor=&#39;gray&#39;) # Colores plt.legend([&#39;GDP&#39;, &#39;Consumption&#39;]) # Nombres variables plt.show() Método 3: Crear una figura vacía y aplicar metodos # Creamos figura vacía fig, ax = plt.subplots() # Creamos con más cosas fig, ax = plt.subplots() # Agregamos cosas con metodos de ax ax.plot(us.index, us[&#39;gdp&#39;], linewidth=2, color=&#39;magenta&#39;) ax.set_title(&#39;US GDP&#39;, fontsize=14, loc=&#39;left&#39;) ax.set_ylabel(&#39;Billions of USD&#39;) ax.set_xticks([2004, 2008, 2012]) ax.grid(True) # Guardamos la figura fig.savefig(&#39;resultados\\\\us_gdp.pdf&#39;) Múltiples sub-plots # Creamos un arreglo de 2d fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True) print(&#39;Objeto ax tiene dimensión&#39;, len(ax)) # Agregamos contenido fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True) ax[0].plot(us.index, us[&#39;gdp&#39;], color=&#39;green&#39;) # first plot ax[1].plot(us.index, us[&#39;pce&#39;], color=&#39;red&#39;) # second plot Otro ejemplo: combinación método 1 y método 3 # grab the axis ax = us.plot() # Aplicamos métodos ax = us.plot() ax.set_title(&#39;US GDP and Consumption&#39;, fontsize=14, loc=&#39;left&#39;) ax.set_ylabel(&#39;Billions of 2013 USD&#39;) ax.legend(loc=&#39;center right&#39;) Ejercicio 4.2.1: Escribir una función que pregunte al usuario o usuaria por las ventas de un rango de años y muestre por pantalla un diagrama de líneas con la evolución de las ventas. # Respuesta: inicio = int(input(&quot;Introduce el año inicial: &quot;)) fin = int(input(&quot;Introduce el año final: &quot;)) # Definimos un diccionario vacio donde ir guardando los datos ventas = {} # Hacer una iteración for i in range(inicio, fin + 1): ventas[i] = input(&quot;Introduce las ventas del año: &quot; + str(i)) # Armamos la figura: fig, ax = plt.subplots() ax.plot(ventas.keys(), ventas.values()) plt.show() 8.2.3 Pandas Esta librería nos permitirá analizar data como tablas, rectangular o data frames. En las que las filas y columnas se identifican con etiquetas en lugar de con simples índices enteros La documentación la encuentra aquí: https://pandas.pydata.org/docs/ 8.2.3.1 Sintaxis básica: Ejemplo 1 # Importamos librería pandas import pandas as pd columnas = [&quot;ind1&quot;, &quot;ind2&quot;, &quot;ind3&quot;] print(columnas) # Transformemoslo data = pd.DataFrame(columnas) # Ahora los datos estan en un data.frame print(data) type(data) # Ahora vamos a agregar nombres y nuevas columnas con un diccionario nombres_columna = {&quot;nombre&quot;: columnas, &quot;altura&quot;: [1.67, 1.9, 0.23], &quot;peso&quot;: [54, 100, 2]} data = pd.DataFrame(nombres_columna) print(data) # Si queremos acceder a elementos data[&quot;altura&quot;] # Si queremos acceder a un valor en particular, similar a las listas seleccion_columna = data[&quot;altura&quot;][2] print(seleccion_columna) seleccion_filas = data.iloc[2] #data1.iloc? print(seleccion_filas) # Agregar nueva información bmi = [] for i in range(len(data)): bmi_score = data[&quot;peso&quot;][i]/(data[&quot;altura&quot;][i]**2) bmi.append(bmi_score) bmi data[&quot;bmi&quot;] = bmi data # Guardemos data.to_csv(&quot;datos_ejemplo1.csv&quot;) 8.2.3.2 Sintaxis básica: Ejemplo 2 # Importamos datos que utilizaremos durante la clase from gapminder import gapminder gapminder 8.2.3.3 Inspeccionar un data frame df = gapminder df.head(5) # Mirar las primeras 10 filas df.head(5).tail(2) df.tail() # Mirar los tipos de datos df.dtypes # object se refiere a datos de texto. # Mirar las columnas print(df.columns) type(df.columns) # Mostrar N fila- M columna print(df.shape) # Mirar una columna en particular # Caso 1 df[&#39;country&#39;] # Todas: resumen estadistico df.describe() # Texto df.country.describe() # Numerica df.lifeExp.describe() print(&quot;min:&quot;, df.gdpPercap.min()) print(&quot;max:&quot;, df.gdpPercap.max()) print(&quot;mean:&quot;, df.gdpPercap.mean()) print(&quot;std:&quot;, df.gdpPercap.std()) print(&quot;count:&quot;, df.gdpPercap.count()) 8.2.3.4 Manipular Renombrar: # Renombrar variables: diccionario + rename df.rename(columns = {&#39;country&#39;: &#39;pais&#39;, &#39;continent&#39;:&#39;continent&#39;, &#39;year&#39;:&#39;año&#39;, &#39;lifExp&#39;:&#39;esperanza_vida&#39;, &#39;pop&#39;:&#39;poblacion&#39;, &#39;gdpPercap&#39;:&#39;pib_percapita&#39;}) # Ojo que no hemos asignado df df2 = df.rename(columns = {&#39;country&#39;: &#39;pais&#39;, &#39;continent&#39;:&#39;continente&#39;, &#39;year&#39;:&#39;año&#39;, &#39;lifeExp&#39;:&#39;esperanza_vida&#39;, &#39;pop&#39;:&#39;poblacion&#39;, &#39;gdpPercap&#39;:&#39;pib_percapita&#39;}) # Recomendable generar una nueva base de datos, va dejando un registro df2 Filtrar: solo_asia = df[df[&quot;continent&quot;] == &quot;Asia&quot;] # Lo que esta adentro es un vecto booleano df[&quot;continent&quot;] == &quot;Asia&quot; df_asia = solo_asia print(df_asia) Reemplazar: datos_remplazados = df.replace(&quot;Asia&quot;, &quot;asia&quot;) print(datos_remplazados) Remover: remover_columna = df.drop(&quot;pop&quot;, axis = 1) print(remover_columna) remover_dos_columnas = df.drop([&quot;gdpPercap&quot;, &quot;pop&quot;], axis = 1) print(remover_dos_columnas) remover_filas = df.iloc[0:100] print(remover_filas) Agregar nuevas filas: nuevas_filas = {&quot;country&quot;: &quot;nuevo&quot;, &quot;continent&quot;: &quot;Asia&quot;, &quot;year&quot; : &quot;2022&quot;, &quot;gdpPercap&quot; : &quot;100000000&quot;} df.append(nuevas_filas, ignore_index = True) # Para agregar un diccionario es importante la úñtima opción Describir una base de datos: # Agrupar un data frame: group_by df_agrupado = df.groupby(&#39;year&#39;) df_agrupado.mean() # Sobre una variable en particular df_agrupado = df[[&#39;gdpPercap&#39;, &#39;year&#39;]].groupby(&#39;year&#39;) df_agrupado.mean() # Podemos agrupar varias operaciones df.groupby(&quot;year&quot;).agg({&quot;lifeExp&quot;: np.mean, &quot;gdpPercap&quot;: np.size}) # Agrupar por mas de una variable: año y continente df.groupby([&quot;year&quot;, &quot;continent&quot;]).agg({&quot;lifeExp&quot;: np.mean, &quot;gdpPercap&quot;: np.size}) 8.3 Herramientas de programación 8.3.1 Controladores de flujo Controlar los caminos de los códigos que programamos. Permite sofisticar procesos de análisis de datos. Veremos algunos típicos + otros que son propiamente de Python. 8.3.2 if-elif-else # Sintaxis básica x = -100 if x == 0: print(x, &quot;es cero&quot;) elif x &gt; 0: print(x, &quot;es positivo&quot;) elif x &lt; 0: print(x, &quot;es negativo&quot;) else: print(x, &quot;cualquier cosa.&quot;) La operación if nos permite evaluar si se cumple una condición. Por ejemplo: Iteramos sobre una lista entre (0, 10), si el valor es mayor que 5 muestra el resultado, caso contrario si es que no. # Ejemplo sencillo for i in range(0, 10): #iteración if i &gt; 5: #condición print(i) #operación # Puedo puedo definir variables dentro a = 10 if a == 10: x = a print(x) # Otros operadores con texto a = &#39;texto2&#39; if a != &#39;texto&#39;: print(&quot;ok&quot;) # Dentro de una lista # Es importante entender el flujo de los códigos que escribamos a = [0, &#39;x&#39;, 3] for i in range(0, len(a)): print(a[i]) if i == 0: print(&quot;ok&quot;) elif i == 1: print(&quot;ok2&quot;) # Noten lo importante que es la sangria if a == 10: x = a print(x) # ¿Qué ocurre si movemos el print()? Como vimos en el ejemplo anterior, para poder realizar flujos de códigos que consideren varias situaciones vamos a usar if, elif y else. Vamos a crear una lista A utilizando valores de una segunda lista B con valores entre 0-10. Vamos a iterar sobre los valores de la lista A y realizar las siguientes operaciones: Si el valor está entre 0-3 guardamos ese mismo valor en B Si el valor es mayor que 3 y menor que 5 lo multiplicamos por 2 Si el valor es igual a 5 pedir ingresar un valor con la función input() Para todo el resto eleva el número al cuadrado # 1. Cargamos numpy import numpy as np # 1. Creamos las listas A = np.arange(0, 11, 1) B = [] # 2. Iteramos sobre A for i in A: # Creamos la primera condición if i &gt;= 0 and i &lt;= 3: B.append(i) # Segunda condición elif i &gt; 3 and i &lt; 5: B.append(i * 2) # Tercera condición elif i == 5: B.append(float(input(&quot;Agregar un valor &quot;))) B.append(input(&quot;cualquier cosa: &quot;)) # Para todo el resto else: B.append(i**2) # ¿Qué ocurre de especial con el formato float()? print(&quot;A:&quot;, A) print(&quot;B:&quot;, B) La función input() nos va a pedir entregar un valor. Esto es útil cuando necesitamos que el usuario nos reporte algún dato. input() # Noten que es un string. Tenemos que cambiarle el formato. x = input(&quot;Su edad aquí: &quot;) type(x) # Otro ejemplo con múltiples condiciones llueve = True salir = True temperatura = 30 # Multiples condiciones if llueve and salir: usar_paraguas = True elif llueve and not salir: usar_paraguas = False elif not llueve and salir and temperatura &gt; 25: usar_bloqueador = True else: vender_paragua = True # Noten que el if no sigue en caso de que una condición ya esta saldada. print(&quot;Paraguas:&quot;, usar_paraguas) print(&quot;Bloqueador:&quot;, usar_bloqueador) print(&quot;Vender Paraguas:&quot;, vender_paragua) # Defino variables cuarto = &quot;cocina&quot; area = 10.0 # Para habitaciones if cuarto == &quot;cocina&quot; : print(&quot;Mira en la cocina&quot;) elif cuarto == &quot;cuarto&quot;: print(&quot;Mira en el cuarto&quot;) else : print(&quot;Mira en cualquier lugar&quot;) # Para areas if area &gt; 15 : print(&quot;Gran tamaño&quot;) elif area &gt; 10 : print(&quot;Tamaño normal&quot;) else : print(&quot;Bastante pequeño&quot;) Ejercicio 4.3.1: Escriba un programa que tenga como resultado su nombre si la edad ingresada es multiplo de 5. Note que la edad debe ser ingresada por el mismo usuario. Si la edad no es multiplo de 5 el programa debe dar como resultado su apellido. edad = int(input(&quot;Diga su edad: &quot;)) if edad % 5 == 0: print(&quot;Nicolás&quot;) else: print(&quot;Campos&quot;) 8.3.3 Iteradores básicos 8.3.3.1 For loop # For loops for N in [2, 3, 5, 7, 9, 11]: print(N) # For for i in range(10): print(i, end = &quot; &quot;) # Todo en la misma linea # Podemos iterar sobre una lista: iteramos en el rango de valores que tiene la lista x = [0, 1, &#39;a&#39;, True] for i in range(0, len(x)): print(i, end = &quot; &quot;) for i in range(0, len(x)): print(x[i], end = &quot; &quot;) # Podemos iterar sobre un array import numpy as np x = np.array([0, 1, 2, 3]) for i in range(0, len(x)): print(i,end = &quot; &quot;) # Podemos iterar sobre un string for a in &quot;Nicolas&quot; : print(a.capitalize(), end = &quot; &quot;) # Podemos iterar en una lista de listas # Lista de listas casa = [[&quot;living&quot;, 11.25], [&quot;cocina&quot;, 18.0], [&quot;comedor&quot;, 20.0], [&quot;habitacion&quot;, 10.75], [&quot;baño&quot;, 9.50]] print(casa) # Iteramos for x in casa : print(&quot;La &quot; + x[0] + &quot; es de &quot; + str(x[1]) + &quot; m2&quot;) # Noten que el x en si mismo es una lista con dos elemenos. # Utilizando subconjuntos de listas podemos ir llamando a los elemenos del iterador. # Creamos x y y fuera de la iteración x = 0 y = 0 for i in range(1, 5): z = 0 # creamos z dentro de la iteración x = x + i y = i z = z + i print(&quot;i:&quot;, i, &quot;, x=&quot;, x, &quot;, y=&quot;, y, &quot;, z=&quot;, z) # Podemos generar objetos para ir iterando sobre ellos. Recordar: linspace (inicio, término, cantidad de elementos) a = np.linspace(1, 10, 5) print(&quot;type de a:&quot;, type(a)) print(&quot;a: &quot;, a) #arange(inicio, término, separador) b = np.arange(1, 10,1) print(&quot;type de b:&quot;, type(b)) print(&quot;b: &quot;, b) Veamos dos versiones alternativas de hacer el mismo for, pero anidado print(a,b) # For anidados. # 1. Creamos una lista lista1 = [] # 2. Iteramos sobre los valoes de a y b for i in a: for j in b: #3. Anexamos una operación a la lista # print((i, j), &quot;multiplicacion:&quot;, i * j) lista1.append(i * j) print(lista1) #1. Creamos una lista 2 lista2 = [] #2. Iteramos sobre el largo de las listas a y b for i in range(0, len(a)): for j in range(0, len(b)): #3. Anexamos una operación a la lista lista2.append(a[i] * b[j]) print(lista2) #Podemos sumar los valores de una lista con sum() print(&quot;suma de la lista:&quot;, sum(lista2)) lista1 == lista2 # Puedo ingresar tuplas de forma tal de recuperar el indice y el resultado! lista = [1.73, 1.68, 1.71, 1.89] for index, a in enumerate(lista): print(&quot;indice &quot; + str(index) + &quot;: &quot; + str(a)) # Noten que utilizamos un enumerate(). # Esto es un interador. # Revisaremos más de estos en la siguiente sección # Como pueden ver auenta nuestra posibilidades en Python # Se pueden saltar en saltos de más de uno? (en tuplas?) 8.3.3.2 While loop No muy común Útil para resolver problemas de optimización error = 50 while error &gt; 1: error = error/4 print(error) # Otra forma más típica de hacer la actualización i = 0 while i &lt; 10: print(i, end=&#39; &#39;) i += 1 # While loop: pueden ser eternos. ¿Utilidad practica? i = 0 while i &lt; 10: print(i, end=&#39; &#39;) i = i + 1 # Recuerden que puedo agregar condicionales # Condición inicial inicial = 10 while inicial != 0 : print(&quot;corregir...&quot;) if inicial &gt; 0 : inicial = inicial - 1 else: inicial = inicial + 1 print(inicial) 8.3.3.3 Opcionales: break y continue break: quiebra loop continue: siguiente # Se lo salta for n in range(10): if n % 2 == 0: continue print(n, end=&#39; &#39;) # No continua si se llega a un nivel predeterminado a, b = 0, 1 amax = 1000 L = [] while True: (a, b) = (b, a + b) if a &gt; amax: break L.append(a) print(L) 8.3.3.4 Iterar sobre diccionarios y arreglos Hay que tener cuidado al iterar sobre estos objetos Veamos dos ejemplos # Diccionarios mundo = {&quot;Burgos&quot;: 30.55, &quot;Villasana de Mena&quot;: 2.77, &quot;Santiago&quot;: 39} mundo for key, value in mundo: print(key + &quot;:&quot; + str(value)) # Tenemos que utilizar un metodo for key, value in mundo.items(): print(key + &quot;: &quot; + str(value)) mundo.items() type(mundo.items()) # Ejemplo: europa = {&#39;spain&#39;:&#39;madrid&#39;, &#39;france&#39;:&#39;paris&#39;, &#39;germany&#39;:&#39;berlin&#39;, &#39;norway&#39;:&#39;oslo&#39;, &#39;italy&#39;:&#39;rome&#39;, &#39;poland&#39;:&#39;warsaw&#39;, &#39;austria&#39;:&#39;vienna&#39; } # Iterar sobre un diccionario for key, value in europa.items() : print(&quot;the capital of &quot; + str(key) + &quot; is &quot; + str(value)) # Noten que hacermos el for con una tupla! # Arreglo 2d np_altura = np.array([1.73,1.68,1.71,1.89,1.79]) np_peso = np.array([65.4,32.2,42.3,42.3,89.2]) meas = np.array([np_altura,np_peso]) print(meas) for val in meas: print(val) for val in np.nditer(meas): print(val) 8.3.3.5 Iterar sobre Pandas Hay que tener cuidado al iterar sobre estos objetos Veamos dos ejemplos from gapminder import gapminder gapminder # Primer intento for val in gapminder: print(val) # No entiende que queremos... # Utilizamos iterrows method for lab,row in gapminder.iterrows(): print(lab) print(row) # Lab esta asociado al ID de un data frame # row es toda la fila asociada # Podemos hacerlo selectivamente for lab,row in gapminder.iterrows(): print(str(lab) + &quot;: &quot; + row[&quot;continent&quot;]) # Agregar una columna con un iterador for lab,row in gapminder.iterrows(): gapminder.loc[lab, &quot;tamaño_continente&quot;] = len(row[&quot;continent&quot;]) # Es solo un ejemplo, no es muy eficiente. # Otro ejemplo, utilizando una función en vez de un método for lab,row in gapminder.iterrows(): gapminder.loc[lab, &quot;continente_mayus&quot;] = row[&quot;continent&quot;].upper() # Es solo un ejemplo, no es muy eficiente. gapminder # Opcion alternativa: buscar un method y vectorizar gapminder[&quot;tamaño_continente_vect&quot;] = gapminder[&quot;continent&quot;].apply(len) # Esta es la opción eficiente. gapminder gapminder[&quot;Cont_mayus_vect&quot;] = gapminder[&quot;continent&quot;].apply(str.upper) gapminder 8.3.4 Iteradores adicionales en Python 8.3.4.1 Enumerate usar cuando: queremos tener registro tanto del indice como del valor asociado L = [2, 4, 6, 8, 10] for i in range(len(L)): print(i, L[i]) for i, val in enumerate(L): print(i, val) 8.3.4.2 Zip usar cuando: Se quiera iterar sobre múltiples listas simultaneamente L = [2, 4, 6, 8, 10] R = [3, 6, 9, 12, 15] for lval, rval in zip(L, R): print(lval, rval) # Si son de distinto tamaño? L = [2, 4, 6, 8, 10] R = [3, 6, 9] for lval, rval in zip(L, R): print(lval, rval) # Va a utilizar como referencia el más corto 8.3.4.3 Map usar cuando: quiera tomar una función y aplicarla al valor dentro de un objeto # Defino una función cuadrado = lambda x: x ** 2 for val in map(cuadrado, range(100)): print(val, end = &quot; &quot;) # Nota: Una función lambda puede tomar cualquier número de argumentos, pero sólo puede tener una expresión. # Otro ejemplo: importar funciones especificas a una librearía import math for val in map(math.sqrt, range(100)): print(val, end = &quot; &quot;) 8.3.4.4 Filter similar a map, excepto que pasa valores que son considerados verdaderos por la función de filtrado # Itera sobre los valores para los cuales se cumple la condición es_par = lambda x: x % 2 == 0 for val in filter(es_par, range(100)): print(val, end=&#39; &#39;) Ejercicio 4.3.2: Genere un código que tome una lista de strings y calcule su tamaño. lista_strings = [&quot;Rodrigo&quot;, &quot;Santiago&quot;, &quot;Tania&quot;, &quot;Laura&quot;, &quot;Nicolás&quot;] lista_final = list(map(len,lista_strings)) print(lista_final) 8.3.5 List Comprehension Es una forma de comprimir la construcción de un objeto utilizando un iterador. [i for i in range(20) if i % 2 == 0] # El resultado es una lista que solo considera los números pares. # Un for-loop L = [] for n in range(12): L.append(n ** 2) L [n ** 2 for n in range(12)] # construir una lista formada por el cuadrado de n para cada n hasta 12 01figuras/image.png 01figuras/image.png 8.3.5.1 Iteración Múltiple objetivo: armar for-loops anidados en una linea [(i, j) for i in range(2) for j in range(3)] [(i, j, z) for i in range(2) for j in range(3) for z in range(3)] 8.3.5.2 Condicionales en el iterador Idea: agregar condicionales al final de la expresión. # Construir una lista de valores para cada valor hasta 20, pero sólo si el valor no es divisible por 3&quot; [val for val in range(20) if val % 3 &gt; 0] # Equivalente a L = [] for val in range(20): if val % 3 &gt; 0: L.append(val) L val = -1000 val if val &gt;= 0 else -val # ¿Qué función es esta? # Una más compleja # construir una lista, dejando fuera los múltiplos de 3, y haciendo negativos todos los múltiplos de 2. [val if val % 2 else -val for val in range(20) if val % 3] # Se puede extender para otro tipo de objetos, por ejemplo a conjuntos {n**2 for n in range(12)} {a % 3 for a in range(1000)} Ejercicio 4.3.3: Escriba los siguientes procesos iterativos en formato de list comprehension: # Parte a numeros = [3,5,45,97,32,22,10,19,39,43] resultado = [] for n in numeros: if n % 2 == 0: resultado.append(n) print(resultado) [n for n in numeros if n % 2 == 0] # Parte b: Cuente el número de espacios en un string cualquiera string = &quot;Mi nombre es Nicolás Campos &quot; len([x for x in string if x == &quot; &quot;]) 8.3.6 Generadores Similar a los iteradores de listas, pero ahorra bastate memoria, es eficiente. # Hasta el momento [n ** 3 for n in range(12)] # Un generador (n ** 2 for n in range(12)) # No genera el contenido. # Si lo quiero expresar G = (n ** 2 for n in range(12)) list(G) Un generador es básicamente un recipiente en donde se van a colocar valores. Un generador no calcula los valores a menos que sea necesario, lo cual aumenta su eficiencia. # Generadores utilizan parentesis G = (n ** 2 for n in range(12)) list(G) # Una lista es una colección de valores L = [n ** 2 for n in range(12)] for val in L: print(val, end=&#39; &#39;) # Un generador es un recipiente para guardar valores G = (n ** 2 for n in range(12)) for val in G: print(val, end=&#39; &#39;) # El potencial del generador es mayor L1 = [n ** 2 for n in range(10)] L2 = [] for n in range(10): L2.append(n ** 2) print(L1) print(L2) G1 = (n ** 2 for n in range(10)) def gen(): for n in range(10): yield n ** 2 G2 = gen() print(*G1) print(*G2) # Utiliza yield para representar una secuencia potencialmente infinita de valores # Recordemos que estos dos aspectos son iguales L1 = [n ** 2 for n in range(12)] L2 = [] for n in range(12): L2.append(n ** 2) print(L1) print(L2) # Si queremos hacer la equivalencia, pero con generadores tenemos G1 = (n ** 2 for n in range(12)) def gen(): for n in range(12): yield n ** 2 G2 = gen() print(*G1) print(*G2) # Noten que tuve que ingresar la función yield en vez de return. # La idea es que pueda incluir potencialmente una lista infinita de valores. Un generador se puede utilizar una vez, un iteador normal muchas veces L = [n ** 2 for n in range(12)] # Normal for val in L: print(val, end=&#39; &#39;) print() # Generador G = (n ** 2 for n in range(12)) list(G) list(G) 8.3.6.1 Algoritmo de erastotenes Vamos a generar un algoritmo que nos permita obtener todos los numeros primos bajo un cierto número natural dado. # Generamos una lista de candidatos L = [n for n in range(2, 10)] print(L) # Removemos todos los múltiplos de dos salvo si mismo L = [n for n in L if n == L[0] or n % L[0] &gt; 0] print(L) # Removemos todos los múltiplos de tres salvo si mismo L = [n for n in L if n == L[1] or n % L[1] &gt; 0] print(L) # Asi sucesivamente hasta converger L = [n for n in L if n == L[2] or n % L[2] &gt; 0] print(L) # Asi sucesivamente hasta converger L = [n for n in L if n == L[3] or n % L[3] &gt; 0] print(L) # Generamos esto como un generador def gen_primos(N): &quot;&quot;&quot;Generar primos hasta N&quot;&quot;&quot; primos = set() for n in range(2, N): if all(n % p &gt; 0 for p in primos): primos.add(n) yield n print(*gen_primos(1000)) Es conveniente utilizar generadores cuando se trabajan con grandes bases de datos. Es importante recordar que el generador solo puede ser iterado una vez. No es conveniente utilizar generadores cuando necesitamos los valores previos. Recuerden que solo itera una vez. Cuando tengo bases de datos pequeñas no es necesario. ¿Puedo escribir lo anterior con un for? : Si, el generador es por la actualización y el espacio. 8.3.7 Funciones Parte importante de cualquier proceso sofisticado de análisis de datos Son lo que conocemos como programas en Stata. Tres grupos principales: Funciones incorporadas, como help() para pedir ayuda, min() para obtener el valor mínimo, print() para imprimir un objeto en la terminal. Listado: https://docs.python.org/3/library/functions.html Funciones definidas por el usuario (UDFs), que son funciones que los usuarios crean para mejorar sus procesos. Funciones anónimas, que también se llaman funciones lambda porque no se declaran con la palabra clave def estándar. Diferencia entre metodos y funciones? Metodo es especifico a una clase de objeto Todos los metodos son funciones, pero no todas las funciones son metodos. 8.3.7.1 Sintaxis básica y algunos ejemplos def miprimerafuncion(a): #declaramos la función y establecemos el input b = a * 2 #definimos qué hace la función return b #Establecemos un output # llamamos la función miprimerafuncion(&quot;nico&quot;) def miprimerafuncion_sr(a): a * 2 miprimerafuncion_sr(15) b = 100 print(miprimerafuncion(b)) print(miprimerafuncion(1000)) # Funciones con más de un argumento def ejemplo2(a, b): return a * b, a ** b type(ejemplo2(10,2)) # Funciones que puedo asignar como tuplas X, Y = ejemplo2(10,2) print(&quot;X =&quot;, X, &quot;Y =&quot;, Y) # Funciones sin argumentos def hello(): print(&quot;Hola mundo&quot;) return hello() def fibonacci(N): L = [] a, b = 0, 1 while len(L) &lt; N: a, b = b, a + b L.append(a) return L fibonacci() ## Valores por parametros y opciones por defecto def fibonacci_cd(N, a = 0, b = 1): L = [] while len(L) &lt; N: a, b = b, a + b L.append(a) return L fibonacci(10) # Por defecto fibonacci_cd(10) # Variables locales y variables globales def ejemplo3(a): AA = [] AA.append(a) return AA C = ejemplo3(&quot;hola&quot;) print(C) print(C) Una variable local va a existir en un lugar específico. Dentro de la función ejemplo3(a) definimos A=, esta variable A existe sólo dentro de la función ejemplo3(), es decir es una variable local. Cuando decimos que A = ejemplo3(“hola”) estamos definiendo una variable global, que existe en todo el espacio del Jupyter, una vez definida puede ser llamada en en cualquier parte. # Ahora vamos a crear una función que nos permita interactuar con los/las usuarias #1. Creamos la función con un input A def ejemplo4(A): #2. Creamos una lista vacía B = [] #3. Iteramos sobre A for i in A: #Creamos la primera condición if i &gt;= 0 and i&lt;= 3: B.append(i) #Segunda condición elif i &gt; 3 and i&lt;5: B.append(i*2) #Tercera condición elif i==5: B.append(input(&quot;Agregar un valor&quot;)) #Todo el resto else: B.append(i**2) #4. Output return B # Creamos una lista entre 0-10 A = np.arange(0, 11, 1) #Llamamos la función y guardamos el resultado en una variable global B = ejemplo4(A) print(&quot;A:&quot;, A) print(&quot;B:&quot;, B) # Otro ejemplo def hello(): name = str(input(&quot;Tu nombre aquí: &quot;)) if name: print (&quot;Hola &quot; + str(name)) else: print(&quot;Hola no más&quot;) return hello() # Notemos el uso de return def hola(): print(&quot;Hola mundo&quot;) return(&quot;hola&quot;) hola() * 2 def hola_sinreturn(): print(&quot;Hola mundo&quot;) hola_sinreturn() * 2 # Arroja un error # Funciones anidadas (revisar prox clase) def display(name): def message(): return &quot;Hello &quot; return name print(message(), display(&quot; Siva&quot;)) 8.3.7.2 Argumentos de las funciones en Python ## Defecto def plus(a,b = 2): return a + b plus(1) plus(1, 3) # Requeridos: son los que necesita la funcion def plus(a,b): return a + b plus() # Argumentos clave def plus(a,b): return a - b plus(2,3) plus(b = 2, a = 4) plus(2,4) 8.3.7.3 args and kwargs: argumentos flexibles idea: Querer escribir una función en la que no sepas inicialmente cuántos argumentos va a tener. *args y **kwargs permiten trabajar con este tipo de problemas def catch_all(*args, **kwargs): print(&quot;args =&quot;, args) print(&quot;kwargs = &quot;, kwargs) catch_all(1, 2, 3, 4,5,65, a = 4, b = 5) catch_all(&#39;a&#39;, keyword=2) Aquí lo importante no son los nombres args y kwargs, sino los caracteres * que los preceden. args y kwargs son sólo los nombres de las variables que se utilizan a menudo por convención, abreviatura de “argumentos” y “argumentos de palabras clave”. La diferencia operativa son los caracteres de asterisco: un solo * antes de una variable significa “expandir esto como una secuencia”, mientras que un doble ** antes de una variable significa “expandir esto como un diccionario”. # Tambien puedo ocupar esta sintaxis (* y **) al llamar variables dentro de una función inputs = (1, 2, 3) keywords = {&#39;pi&#39;: 3.14} catch_all(*inputs, **keywords) 8.3.7.4 Anonymous (lambda) Functions Idea: quiero hacer funciones que sean anonimas # Caso 1 add = lambda x, y: x + y add(1, 2) # Caso 2 def add(x, y): return x + y # Noten que esta función no tiene nombre double = lambda x: x*2 double(5) # Con dos argumentos sum = lambda x, y: x + y; sum(4,5) Caso 1 y Caso 2 son equivalentes ¿Por qué quiero escribir entonces funciones lambda?: Función por un periodo corto de tiempo. O cuando tenemos una función que tiene como input otras funciones. from functools import reduce lista = [1,2,3,4,5,6,7,8,9,10] # Filtro: filtra según un criterio. En este caso el criterio es una función creada por mi y anonima filtrado_list = list(filter(lambda x: (x*2 &gt; 10), lista)) # Mapeo: aplica una función a un objeto. En este caso una anonima mapeo_list = list(map(lambda x: x*2, lista)) # Reducir: reduce la secuencia a un simple valor. Suma el acumulado finalmente. reducedico_list = reduce(lambda x, y: x+y, lista) print(filtrado_list) print(mapeo_list) print(reducedico_list) #reduce? 8.3.7.5 Documentar funciones Es útil documentar lo que hacen las funciones que hacemos def count_letter(content, letter): &quot;&quot;&quot;Cuenta el número de veces que una `letra` aparece en `contenido`. Argumentos: contenido (str): La palabra (string) donde vamos a ir a buscar. letra (str): La letra que vamos a ir a buscar. Retorna: int &quot;&quot;&quot; return len([char for char in content if char == letter]) result = count_letter(&quot;nicolascampos&quot;, &quot;s&quot;) print(result) count_letter? Ejercicio 4.3.4: Crear una función que calcule la estimación del ingreso según la ecuación de Mincer: \\(Ln(Y)=\\beta_0+\\beta_1S+\\beta_2^2 Exp+ Exp2\\) Donde: \\(S\\): años de educación \\(Exp\\): experiencia \\(Exp2\\): experiencia al cuadrado Utilice: _0=9.7, _1=0.14, _2=0.07, _3=−0.001 def mincer(S , Exp): ß0 = 9.7 ß1 = 0.14 ß2 = 0.07 ß3 = -0.001 return ß0 + ß1*S + ß2*Exp + ß3*Exp**2 educ_list = np.array([14,8,20]) exp_list = np.array([2,20,1]) print(mincer(educ_list, exp_list)) 8.4 Manipulación de bases de datos (Parte I) 8.4.1 Importar e inspeccionar datos (repaso) ¿Cuál es mi directorio? import os os.getcwd() os.listdir() # Carpetas principal = os.getcwd() datos = principal + &quot;\\\\datos&quot; datos resultados = principal + &quot;\\\\resultados&quot; resultados # Recordar como es un dataframe de pandas import pandas as pd exam_data = {&#39;name&#39;: [&#39;Anastasia&#39;, &#39;Dima&#39;, &#39;Katherine&#39;, &#39;James&#39;, &#39;Emily&#39;, &#39;Michael&#39;, &#39;Matthew&#39;, &#39;Laura&#39;, &#39;Kevin&#39;, &#39;Jonas&#39;], &#39;score&#39;: [12.5, 9, 16.5, np.nan, 9, 20, 14.5, np.nan, 8, 19], &#39;attempts&#39;: [1, 3, 2, 3, 2, 3, 1, 1, 2, 1], &#39;qualify&#39;: [&#39;yes&#39;, &#39;no&#39;, &#39;yes&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;, &#39;yes&#39;, &#39;no&#39;, &#39;no&#39;, &#39;yes&#39;]} labels = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;] df = pd.DataFrame(exam_data , index = labels) df # Importamos datos que utilizaremos durante la clase from gapminder import gapminder gapminder df = gapminder df.head(10) # Acceder al nombre de las columnas df.columns # Acceder al nombre de las filas print(df.index) # Acceder a valores df.values # Nuevamente el tipo type(df) # Forma df.shape # Inspeccion detallada de las columnas df.info() # Mirar una sola columna df[&#39;country&#39;] # Crear un nuevo dataset country_df = df[&quot;country&quot;] country_df type(country_df) # Tenemos una serie: cada columna es una serie para python. Similar a un arreglo 1D. # Sunconjuntos de variables subset = df[[&quot;country&quot;, &quot;continent&quot;, &quot;year&quot;]] subset.head() df # Buscamos la fila con el indice 2 df.loc[3] # Buscamos las filas con el id en 0 u 2 df.loc[[3,0,100]] df.iloc[3] df.iloc[[3,0,100]] loc se refiere al nombre de la unidad de observación (labeling indexing). iloc se refiere al índice (positional indexing). En este caso son iguales dado que los nombres son efectivamente las posiciones. En casos donde esto no sea así, es importante considerar la diferencia al utilizar estas funciones. # Recordar filas y columnas subset = df.loc[:,[&quot;year&quot;, &quot;pop&quot;]] subset.head() # Para hacer filtros en base a condiciones # Filtramos todas las filas que tienen como año 1967 df.loc[df[&quot;year&quot;] == 1967, [&quot;year&quot;, &quot;pop&quot;]] # Ahora con múltiples condiciones # Ocupar paretesis redondos # Ocupar entre ellos iperadores de comparación # Note como escrbimimos el número df.loc[(df[&quot;year&quot;] == 1967) &amp; (df[&quot;pop&quot;] &gt; 1_000_000), [&quot;year&quot;, &quot;pop&quot;]] 1_000_000 scientists=pd.DataFrame( data={&#39;Occupation&#39;:[&#39;Chemist&#39;,&#39;Statistician&#39;], &#39;Born&#39;:[&#39;1920-07-25&#39;, &#39;1876-06-13&#39;], &#39;Died&#39;:[&#39;1958-04-16&#39;, &#39;1937-10-16&#39;], &#39;Age&#39;:[37,61]}, index=[&#39;Rosaline Franklin&#39;,&#39;William Gosset&#39;], columns=[&#39;Occupation&#39;, &#39;Born&#39;,&#39;Died&#39;,&#39;Age&#39;]) scientists import os os.getcwd() os.listdir() # Carpetas import os principal = os.getcwd() datos = principal + &quot;\\\\datos&quot; datos resultados = principal + &quot;\\\\resultados&quot; resultados scientists = pd.read_csv(&#39;datos\\\\scientists.csv&#39;) scientists ages = scientists[&#39;Age&#39;] ages ages = scientists.Age ages type(ages) ages.mean() ages.shape ages.min() ages.describe() ages[ages &gt; ages.mean()] ages[(ages &gt; ages.mean()) &amp; (ages &gt; 75)] ages[(ages &gt; ages.mean()) &amp; ~(ages &gt; 75)] ages.describe() ages + 100 scientists scientists[scientists[&#39;Age&#39;] &gt; scientists[&#39;Age&#39;].mean()] scientists[&#39;Age&#39;] &gt; scientists[&#39;Age&#39;].mean() scientists # Ordenar valores scientists.sort_values(by=&quot;Age&quot;, ascending = False) scientists.to_csv(&#39;resultados\\\\scientists_clean.csv&#39;, index=False) 8.4.2 Manipular (repaso) 8.4.2.1 Renombrar # Renombrar variables: diccionario + rename df.rename(columns = {&#39;country&#39;: &#39;pais&#39;, &#39;continent&#39;:&#39;continent&#39;, &#39;year&#39;:&#39;año&#39;, &#39;lifExp&#39;:&#39;esperanza_vida&#39;, &#39;pop&#39;:&#39;poblacion&#39;, &#39;gdpPercap&#39;:&#39;pib_percapita&#39;}) # Ojo que no hemos asignado df df2 = df.rename(columns = {&#39;country&#39;: &#39;pais&#39;, &#39;continent&#39;:&#39;continente&#39;, &#39;year&#39;:&#39;año&#39;, &#39;lifeExp&#39;:&#39;esperanza_vida&#39;, &#39;pop&#39;:&#39;poblacion&#39;, &#39;gdpPercap&#39;:&#39;pib_percapita&#39;}) # Recomendable generar una nueva base de datos, va dejando un registro df2 8.4.2.2 Filtrar solo_asia = df[df[&quot;continent&quot;] == &quot;Asia&quot;] # Lo que esta adentro es un vecto booleano df[&quot;continent&quot;] == &quot;Asia&quot; df_asia = solo_asia print(df_asia) 8.4.2.3 Replazar datos_remplazados = df.replace(&quot;Asia&quot;, &quot;asia&quot;) print(datos_remplazados) 8.4.2.4 Remover remover_columna = df.drop(&quot;pop&quot;, axis = 1) print(remover_columna) remover_dos_columnas = df.drop([&quot;gdpPercap&quot;, &quot;pop&quot;], axis = 1) print(remover_dos_columnas) remover_filas = df.iloc[0:100] print(remover_filas) #### Agregar nuevas filas nuevas_filas = {&quot;country&quot;: &quot;nuevo&quot;, &quot;continent&quot;: &quot;Asia&quot;, &quot;year&quot; : &quot;2022&quot;, &quot;gdpPercap&quot; : &quot;100000000&quot;} df.append(nuevas_filas, ignore_index = True) # Para agregar un diccionario es importante la úñtima opción Ejercicio 4.4.1: Escriba una función llamada ins_simpl que le permita a usted obtener un resumen exploratorio de su base de datos. La función debe tener como input un dataframe y retornar 5 outputs: Primerar y últimas 8 filas. Tipo de datos para todas las columnas Nombre de las columnas Descripción completa de la base de datos (metodo: describe) El mínimo, máximo y el promedio de la columna 4 de la base de datos. 8.4.3 Agrupar calculos df = gapminder df.head() df.groupby(&#39;year&#39;)[&#39;gdpPercap&#39;].mean() df.groupby([&#39;year&#39;, &#39;continent&#39;])[[&#39;lifeExp&#39;, &#39;gdpPercap&#39;]].mean() # Otra forma de escribir lo mismo df\\ .groupby([&#39;year&#39;, &#39;continent&#39;])[[&#39;lifeExp&#39;, &#39;gdpPercap&#39;]]\\ .mean() # Para dejarlos como un dataframe -&gt; reset_index (df .groupby([&#39;year&#39;, &#39;continent&#39;])[[&#39;lifeExp&#39;, &#39;gdpPercap&#39;]] .mean() .reset_index() ) # Combinar lo que obtengo con visualizaciones import matplotlib.pyplot as plt (df .groupby([&#39;year&#39;]) [[&#39;lifeExp&#39;]] .mean() .plot() ) plt.show() # Imprima un subconjunto de datos solo_asia = gapminder.groupby(&#39;continent&#39;) asiaDf = solo_asia.get_group(&#39;Asia&#39;) asiaDf # Contar valores por grupp gapminder[&quot;continent&quot;].value_counts() # Agrupar un data frame: group_by df_agrupado = df.groupby(&#39;year&#39;) df_agrupado.mean() # Sobre una variable en particular df_agrupado = df[[&#39;gdpPercap&#39;, &#39;year&#39;]].groupby(&#39;year&#39;) df_agrupado.mean() # Podemos agrupar varias operaciones df.groupby(&quot;year&quot;).agg({&quot;lifeExp&quot;: np.mean, &quot;gdpPercap&quot;: np.size}) # Agrupar por mas de una variable: año y continente df.groupby([&quot;year&quot;, &quot;continent&quot;]).agg({&quot;lifeExp&quot;: np.mean, &quot;gdpPercap&quot;: np.size}) 8.4.4 Aplicar funciones Idea: Aplicar funciones a columnas de una dataframe def my_sq(x): return x ** 2 my_sq(4) def avg_2(x, y): return (x + y) / 2 avg_2(10, 20) assert avg_2(10, 20) == 15.0 # Esta es una función muy importante para programar defensivamente!!! # Apliquemos funciones a un dataframe df=pd.DataFrame({&#39;a&#39;:[10,20,30], &#39;b&#39;:[20,30,40]}) df df[&#39;a&#39;] ** 2 type(df[&#39;a&#39;]) def my_sq(x): # assert isinstance(x, int) return x ** 2 # Con el metodo apply lo utilizamos df[&#39;a&#39;].apply(my_sq) def my_exp(x, e): return x ** e my_exp(2, 2) my_exp(2, 10) df[&#39;a&#39;].apply(my_exp, e=10) def print_me(x): print(x) df df.apply(print_me, axis=1) import numpy as np df.apply(np.mean) Ejercicio 4.4.2: Importe Automobile data. Llamela df_auto Imprima las últimas dos filas de las primeras cinco filas ¿Cuál es el auto más caro de la compañía? Cree un data frame con toda la info de los autos toyota. ¿Cuál es el número total de autos de la compañia? ¿Cuàl es el auto más caro de cada compañia? ¿Cuales son las millas promedio por compañia? Ordene la base de datos por precio de los autos. 8.4.5 Datos como tablas: problemas y soluciones 8.4.5.1 Ejemplo 1: encabezados de las columnas son valores y no nombres de variables # Fijar directorio principal main = os.getcwd() main # Fijar directorios secundarios en base al principal datos = main + &quot;\\\\datos&quot; datos pew = pd.read_csv(&#39;datos\\\\pew-raw.csv&#39;) pew pew.head() Noten que esta es una base de datos donde tenemos información en las columnas # Formato wide a formato 2 pew_long = pd.melt(pew, id_vars = &quot;religion&quot;) pew_long.head() pew_long = pd.melt(pew, id_vars = &quot;religion&quot;, var_name = &quot;ingreso&quot;) pew_long.head() pew_long = pd.melt(pew, id_vars = &quot;religion&quot;, var_name = &quot;ingreso&quot;, value_name = &quot;Casos&quot;) pew_long.head() pew billboard = pd.read_csv(&quot;datos\\\\billboard.csv&quot;) billboard billboard_long = pd.melt( billboard, id_vars = [&quot;year&quot;, &quot;artist&quot;, &quot;track&quot;, &quot;time&quot;, &quot;date.entered&quot;], var_name = &quot;week&quot;, value_name = &quot;rating&quot; ) billboard_long billboard.shape billboard_long.shape 8.4.5.2 Ejemplo 2: múltiples variables guardadas en la columna 1 ebola = pd.read_csv(&quot;datos\\\\country_timeseries.csv&quot;) ebola.head() ebola_long = pd.melt(ebola, id_vars = [&quot;Date&quot;, &quot;Day&quot;]) ebola_long.head() # Vamos a quitar &quot;Cases&quot; en variables &quot;cases_Guinea&quot;.split(&quot;_&quot;) # Columna + tipo de acceso + metodo variable_split = ebola_long[&quot;variable&quot;].str.split(&quot;_&quot;) variable_split # Veamos que tiene por partes type(variable_split) type(variable_split[0]) type(variable_split[0][0]) variable_split.str.get(0) variable_split.str.get(1) # Ahora vamos a utilizar estos dos para actualizar base de datos ebola_long[&quot;stats&quot;] = variable_split.str.get(0) ebola_long[&quot;country&quot;] = variable_split.str.get(1) ebola_long # Otra forma de hacer lo anterior ebola_long[[&quot;stats_e&quot;, &quot;country_e&quot;]] = (ebola_long[&quot;variable&quot;].str.split(&quot;_&quot;, expand = True)) ebola_long 8.4.5.3 Ejemplo 3: Mútiples tipos de unidades de observación en la misma tabla # Variables guradads en filas y columnas weather = pd.read_csv(&quot;datos\\\\weather.csv&quot;) weather weather_melt = pd.melt( weather, id_vars = [&quot;id&quot;, &quot;year&quot;, &quot;month&quot;, &quot;element&quot;], var_name = &quot;day&quot;, value_name = &quot;temp&quot; ) weather_melt weather_tidy = weather_melt.pivot_table( index = [&quot;id&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;], columns = &quot;element&quot;, values = &quot;temp&quot; ) weather_tidy # Nos permite ver la jerarquí ad elos datos que tenemos weather_tidy.reset_index() # Multiple Types of Observational Unit in Same Table (i.e De-nomalized Table) billboard_long.head() billboard_long.loc[billboard_long[&quot;track&quot;] == &quot;Loser&quot;] billboard_songs = billboard_long[[&quot;year&quot;, &quot;artist&quot;, &quot;track&quot;, &quot;time&quot;]] billboard_songs.shape billboard_songs.head() # Eliminar duplicados billboard_songs = billboard_songs.drop_duplicates() billboard_songs[&quot;id&quot;] = range(len(billboard_songs)) billboard_songs.head() # Guardamos resultados = main + &quot;\\\\resultados&quot; billboard_songs.to_csv(&quot;resultados\\\\billboard_songs.csv&quot;, index = False) # Combinemos dos bases de datos billboard_ratings = billboard_long.merge( billboard_songs, on = [&quot;year&quot;, &quot;artist&quot;, &quot;track&quot;, &quot;time&quot;] ) billboard_ratings.head() billboard_ratings = billboard_ratings[[&quot;id&quot;, &quot;date.entered&quot;, &quot;week&quot;, &quot;rating&quot;]] billboard_ratings.head() billboard_ratings.sample(20) billboard_songs.to_csv(&quot;resultados\\\\billboard_ratings.csv&quot;, index = False) billboard_ratings.info() billboard_long.info() 8.4.6 Concatenar data frames import pandas as pd df1 = pd.read_csv(&#39;datos\\\\concat_1.csv&#39;) df2 = pd.read_csv(&#39;datos\\\\concat_2.csv&#39;) df3 = pd.read_csv(&#39;datos\\\\concat_3.csv&#39;) df1 df2 df3 row_concat = pd.concat([df1, df2, df3]) row_concat row_concat.loc[0] row_concat.iloc[0] row_concat.ix[0] row_concat_reset = pd.concat([df1, df2, df3], ignore_index=True) row_concat_reset new_row_series = pd.Series([&#39;n1&#39;, &#39;n2&#39;, &#39;n3&#39;, &#39;n4&#39;]) new_row_series pd.concat([df1, new_row_series]) new_row_data = pd.DataFrame([[&#39;n1&#39;, &#39;n2&#39;, &#39;n3&#39;, &#39;n4&#39;]], columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) new_row_data pd.concat([df1, new_row_data]) col_concat = pd.concat([df1, df2, df3], axis=1) col_concat col_concat[&#39;A&#39;] col_concat_ignore = pd.concat([df1, df2, df3], axis=1, ignore_index=True) col_concat_ignore df1.columns=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;] df2.columns=[&#39;E&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;] df3.columns=[&#39;A&#39;,&#39;H&#39;,&#39;F&#39;,&#39;C&#39;] df1 df2 df3 pd.concat([df1, df2, df3]) 8.5 Manipulación de bases de datos (Parte II) # Preambulo # Librerías: import pandas as pd # datos import matplotlib as mpl # graficos import numpy as np # arreglos import datetime as dt # fechas import os # directorios # Fecha print(dt.date.today()) 8.5.1 Aplicar funciones a columnas de una base de datos Idea: Aplicar funciones a columnas de una dataframe Recuerden todo el instrumental que aprendimos en las clases anteriores respecto al uso de funciones! # Apliquemos funciones a un dataframe df = pd.DataFrame({&#39;a&#39;:[10,20,30], &#39;b&#39;:[20,30,40]}) df # Aplico directamente a una columna, la cual llamo por su nombre df2 = df[&#39;a&#39;] ** 2 df2 df[&#39;b&#39;] ** 2 type(df[&#39;a&#39;]) # Defino una función cualquiera, tan compleja como lo desee def my_sq(x): return x ** 2 df[&#39;b&#39;].apply(my_sq) # Podemos hacer lo mismo con funciones que tengan más de un parámetro def my_exp(x, e): return x ** e my_exp(2, 2) my_exp(2, 10) # Noten que puedo incluir valores para los argumentos adicionales al principal! df[&#39;a&#39;].apply(my_exp, e=5) def print_me(x): print(x) df df.apply(print_me, axis = 1) # Tambien puedo incluir funciones de otras librerías de python df.apply(np.mean) Ejercicio 4.5.1: + Escribir una función que reciba una muestra de números en una lista y devuelva un diccionario con su media, varianza y desviación estandar. import math def statistics(sample): &quot;&quot;&quot; Función que calcula media, varianza y desv.estandar Parámetros: sample: Es una lista de números Output Diccionario con media, varianza y desv. estandar. &quot;&quot;&quot; stat = {} stat[&#39;media&#39;] = sum(sample)/len(sample) stat[&#39;varianza&#39;] = sum(np.square(sample))/len(sample)-stat[&#39;media&#39;]**2 stat[&#39;desviacion estandar&#39;] = stat[&#39;varianza&#39;]**0.5 return stat print(statistics([1, 2, 3, 4, 5])) 8.5.2 Dividir una base de datos por grupos df = pd.read_csv( &#39;https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv&#39;) df df.groupby(&#39;sexo&#39;).groups # groups devuelve un diccionario cuyas claves categoricas y valores son indices # Si quiero obtener uno de estos grupos utilizo get_group df.groupby(&#39;sexo&#39;).get_group(&#39;M&#39;) df.groupby(&#39;sexo&#39;).get_group(&#39;H&#39;) # Hacer esto es útil si es que quiero aplicar una función a ciertos subconjuntos de observaciones # Voy a agrupar por sexo y voy a calcular el promedio de cada variable df.groupby(&#39;sexo&#39;).agg(np.mean) # Noten que utilizamos agg. Este metodo aplica la función a todo el dataframe df.groupby(&#39;sexo&#39;).agg(np.sum) df.groupby(&#39;sexo&#39;).agg(np.std) Ejercicio 4.5.2: Análisis exploratorio de una base de datos Importe desde su carpeta de trabajo auto.csv, llame a esta base de datos df. Imprima las últimas dos filas de las primeras cinco filas de df. ¿Cuál es el auto más caro de la compañía? Cree un data frame con toda la información disponible para los autos toyota. ¿Cuál es el número total de autos de la compañia? ¿Cuál es el auto más caro de cada compañia? ¿Cuales son las millas promedio por compañia? Ordene la base de datos por precio de los autos. # Importar datos main = os.getcwd() datos = main + &quot;\\\\datos&quot; datos df = pd.read_csv(&quot;datos\\\\auto.csv&quot;) df df.tail() # 2. Inspeccionar df.head(5).tail(2) # El auto más caro df[[&#39;company&#39;,&#39;price&#39;]][df.price==df[&#39;price&#39;].max()] # 4. Imprima todos los autos de toyota y su información toyotaDf = df.groupby(&#39;company&#39;).get_group(&#39;toyota&#39;) toyotaDf # 5. ¿Cuál es el número total de autos por compañia? df[&#39;company&#39;].value_counts() # 6. ¿Cuál es el auto más caro de cada compañia? companias = df.groupby(&#39;company&#39;) precioMaxComp = companias[&#39;price&#39;].max() precioMaxComp # 7. Millas promedio por cada compañia grupo = df.groupby(&#39;company&#39;) millasDF = grupo[&#39;company&#39;,&#39;average-mileage&#39;].mean() millasDF # 8. Ordene por precio columna df = df.sort_values(by=[&#39;price&#39;], ascending=True) df.head(5) 8.5.3 Librería datetime Para manejar fechas en Python se suele utilizar la librería datetime Esta incluye los tipos de datos date, time y datetime para representar fechas y funciones para manejarlas. Algunas de las operaciones más habituales que permite son: Acceder a los distintos componentes de una fecha (año, mes, día, hora, minutos, segundos y microsegundos). Convertir cadenas con formato de fecha en los tipos date, time o datetime. Convertir fechas de los tipos date, time o datetime en cadenas formateadas de acuerdo a diferentes formatos de fechas. Hacer aritmética de fechas (sumar o restar fechas). Comparar fechas. Los tipos de datos date, time y datetime date(año, mes, dia) : Devuelve un objeto de tipo date que representa la fecha con el año, mes y dia indicados. time(hora, minutos, segundos, microsegundos) : Devuelve un objeto de tipo time que representa un tiempo la hora, minutos, segundos y microsegundos indicados. datetime(año, mes, dia, hora, minutos, segundos, microsegundos) : Devuelve un objeto de tipo datetime que representa una fecha y hora con el año, mes, dia, hora, minutos, segundos y microsegundos indicados. # Devuelve un objeto de tipo date que representa la fecha con el año, mes y dia indicados. fecha1 = dt.date(2020, 12, 25) print(fecha1) type(dt.time(13,30,5)) # Devuelve un objeto de tipo time que representa un tiempo la hora, minutos, segundos y microsegundos indicados. print(dt.datetime(2020, 12, 25, 13, 30, 5)) # Representa una fecha y hora con el año, mes, dia, hora, minutos, segundos y microsegundos indicados. # Acceder a los elementos de una fecha print(dt.date.today()) from datetime import date, time, datetime dt.month dt.day dt.hour dt.minute dt.second dt.microsecond Usualmente vamos a querer cambiar formato de fechas, para hacerlo ocupamos marcadores de posición: %Y (año completo), %y (últimos dos dígitos del año), %m (mes en número), %B (mes en palabra), %d (día), %A (día de la semana), %a (día de la semana abrevidado), %H (hora en formato 24 horas), %I (hora en formato 12 horas), %M (minutos), %S (segundos), %p (AM o PM), %C (fecha y hora completas), %x (fecha completa), %X (hora completa). # Conversión de fechas en cadenas con diferentes formatos from datetime import date, time, datetime d = datetime.now() print(d.strftime(&#39;%d-%m-%Y&#39;)) print(d.strftime(&#39;%A, %d %B, %y&#39;)) print(d.strftime(&#39;%H:%M:%S&#39;)) print(d.strftime(&#39;%H horas, %M minutos y %S segundos&#39;)) # Función strptime(s, formato): print(datetime.strptime(&#39;15/4/2020&#39;, &#39;%d/%m/%Y&#39;)) print(datetime.strptime(&#39;2020-4-15 20:50:30&#39;, &#39;%Y-%m-%d %H:%M:%S&#39;)) # Para hacer aritmetica de fechas utilizar # timedelta(dias, segundos, microsegundos) : intervalo de tiempo con los dias, segundos y micorsegundos indicados. from datetime import date, time, datetime, timedelta d1 = datetime(2020, 1, 1) d2 = d1 + timedelta(200000, 3600) print(d2) ## Convertir una columna con to_datetime para que sea del tipo fecha # to_datetime(columna, formato): devuelve la serie que resulta de convertir las cadenas de la columna con el # nombre columna en fechas del tipo datetime con el formado especificado en formato df = pd.DataFrame({&#39;Name&#39;: [&#39;María&#39;, &#39;Carlos&#39;, &#39;Carmen&#39;], &#39;Nacimiento&#39;:[&#39;05-03-2000&#39;, &#39;20-05-2001&#39;, &#39;10-12-1999&#39;]}) df df[&#39;Nacimiento&#39;] df1 = pd.to_datetime(df.Nacimiento, format = &#39;%d-%m-%Y&#39;) df1 8.5.4 Reestructurar un DataFrame Formato ancho a largo y viceversa Diferencias entre ambos? 01figuras/image.png datos = {&#39;nombre&#39;:[&#39;Pepe&#39;, &#39;Pepa&#39;, &#39;Nico&#39;], &#39;edad&#39;:[18, 22, 20], &#39;Matemáticas&#39;:[8.5, 9, 3.5], &#39;Economía&#39;:[8, 8.5, 5], &#39;Programación&#39;:[6.5, 9.2, 9]} df = pd.DataFrame(datos) df # Esta en formato ancho # Para pasar a formato largo utilizo melt() df1 = df.melt(id_vars=[&#39;nombre&#39;, &#39;edad&#39;], var_name=&#39;asignatura&#39;, value_name=&#39;nota&#39;) df1 # Ahora pasemosla de vuelta a un formato ancho el metodo pivot() df2 = df1.pivot(index=[&#39;nombre&#39;, &#39;edad&#39;], columns=&#39;asignatura&#39;, values= &quot;nota&quot;) df2 8.5.5 Reestructurar datos para arreglar posibles problemas 8.5.5.1 Caso 1: encabezados de las columnas son valores y no nombres de variables # Fijar directorio principal main = os.getcwd() main # Fijar directorios secundarios en base al principal datos = main + &quot;\\\\datos&quot; datos pew = pd.read_csv(&#39;datos\\\\pew-raw.csv&#39;) pew pew.head() Noten que esta es una base de datos donde tenemos información en las columnas # Formato wide a formato 2 pew_long = pd.melt(pew, id_vars = &quot;religion&quot;) pew_long.head(10) pew_long = pd.melt(pew, id_vars = &quot;religion&quot;, var_name = &quot;ingreso&quot;) pew_long.head() pew_long = pd.melt(pew, id_vars = &quot;religion&quot;, var_name = &quot;ingreso&quot;, value_name = &quot;Casos&quot;) pew_long.head() pew billboard = pd.read_csv(&quot;datos\\\\billboard.csv&quot;) billboard billboard_long = pd.melt( billboard, id_vars = [&quot;year&quot;, &quot;artist&quot;, &quot;track&quot;, &quot;time&quot;, &quot;date.entered&quot;], var_name = &quot;week&quot;, value_name = &quot;rating&quot; ) billboard_long billboard.shape billboard_long.shape 317*76 8.5.5.2 Caso 2: múltiples tipos de datos en una misma columna ebola = pd.read_csv(&quot;datos\\\\country_timeseries.csv&quot;) ebola.head() ebola_long = pd.melt(ebola, id_vars = [&quot;Date&quot;, &quot;Day&quot;]) ebola_long # Vamos a quitar &quot;Cases&quot; en variables &quot;cases_Guinea&quot;.split(&quot;_&quot;) # Columna + tipo de acceso + metodo variable_split = ebola_long[&quot;variable&quot;].str.split(&quot;_&quot;) variable_split # Veamos que tiene por partes type(variable_split) type(variable_split[0]) variable_split[0][1] variable_split.str.get(0) variable_split.str.get(1) # Ahora vamos a utilizar estos dos para actualizar base de datos ebola_long[&quot;stats&quot;] = variable_split.str.get(0) ebola_long[&quot;country&quot;] = variable_split.str.get(1) ebola_long # Otra forma de hacer lo anterior ebola_long[[&quot;stats_e&quot;, &quot;country_e&quot;]] = (ebola_long[&quot;variable&quot;].str.split(&quot;_&quot;, expand = True)) ebola_long 8.5.5.3 Caso 3: Multiples variables guardadas en una columna A few notes on the raw data set: The columns starting with “m” or “f” contain multiple variables: Sex (“m” or “f”) Age Group (“0-14”,“15-24”, “25-34”, “45-54”, “55-64”, “65”, “unknown”) Mixture of 0s and missing values(“NaN”). This is due to the data collection process and the distinction is important for this dataset. df = pd.read_csv(&quot;datos\\\\tb-raw.csv&quot;) df df = pd.melt(df, id_vars=[&quot;country&quot;,&quot;year&quot;], value_name=&quot;cases&quot;, var_name=&quot;sex_and_age&quot;) # Extract Sex, Age lower bound and Age upper bound group tmp_df = df[&quot;sex_and_age&quot;].str.extract(&quot;(\\D)(\\d+)(\\d{2})&quot;, expand=False) # Name columns tmp_df.columns = [&quot;sex&quot;, &quot;age_lower&quot;, &quot;age_upper&quot;] # Create `age`column based on `age_lower` and `age_upper` tmp_df[&quot;age&quot;] = tmp_df[&quot;age_lower&quot;] + &quot;-&quot; + tmp_df[&quot;age_upper&quot;] # Merge df = pd.concat([df, tmp_df], axis=1) # Drop unnecessary columns and rows df = df.drop([&#39;sex_and_age&#39;,&quot;age_lower&quot;,&quot;age_upper&quot;], axis=1) df = df.dropna() df = df.sort_values(ascending=True,by=[&quot;country&quot;, &quot;year&quot;, &quot;sex&quot;, &quot;age&quot;]) df.head(10) 8.5.5.4 Caso 4: Variables guardadas en filas y columnas # Variables guradads en filas y columnas weather = pd.read_csv(&quot;datos\\\\weather.csv&quot;) weather weather_melt = pd.melt( weather, id_vars = [&quot;id&quot;, &quot;year&quot;, &quot;month&quot;, &quot;element&quot;], var_name = &quot;day&quot;, value_name = &quot;temp&quot; ) weather_melt weather_tidy = weather_melt.pivot_table( index = [&quot;id&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;], columns = &quot;element&quot;, values = &quot;temp&quot; ) weather_tidy # Nos permite ver la jerarquí ad elos datos que tenemos weather_tidy.reset_index() Ejercicio 4.5.3: Reestructurar datos Cree una base de datos llamada productos con la siguiente información categoría tienda precio puntaje Limpieza Walmart 1132 4 Limpieza Dia 2350 3 Entretenimiento Walmart 1999 5 Entretenimiento Fnac 1595 7 Tecnología Dia 5575 5 Tecnología Walmart 11134 8 Transforme la base de datos de formato largo a formato ancho. productos = pd.DataFrame({&#39;categoria&#39;: [&#39;Cleaning&#39;, &#39;Cleaning&#39;, &#39;Entertainment&#39;, &#39;Entertainment&#39;, &#39;Tech&#39;, &#39;Tech&#39;], &#39;tienda&#39;: [&#39;Walmart&#39;, &#39;Dia&#39;, &#39;Walmart&#39;, &#39;Fnac&#39;, &#39;Dia&#39;,&#39;Walmart&#39;], &#39;precio&#39;:[1142, 2350, 1999, 1595, 5575, 11134], &#39;puntaje&#39;: [4, 3, 5, 7, 5, 8]}) productos productos_ancho = productos.pivot(index = &quot;categoria&quot;, columns = &quot;tienda&quot;, values = &quot;precio&quot;) productos_ancho 8.5.6 Missing values Datos del mundo real rara vez son homogeneos. Muchos valores no van a encontrarse por múltiples razones. Vamos a discutir un poco sobre este tipo de valores en la librería de pandas. NaN puede utilizarse como valor numérico en operaciones matemáticas, mientras que None no puede NaN es un valor numérico flotantes None es un tipo interno de Python ( NoneType ) y sería más como “inexistente” o “vacío” que “numéricamente inválido” en este contexto. 8.5.6.1 Realidad cuando importamos datos df = pd.read_csv(&#39;datos//survey_visited.csv&#39;) pd.read_csv(&#39;datos//survey_visited.csv&#39;, keep_default_na=False) pd.read_csv(&#39;datos//survey_visited.csv&#39;, na_values=[619, 622]) 8.5.6.2 Dos funciones claves df # Funciones is.na() y notna() pd.isna(df[&quot;dated&quot;]) # Resultado: valor booleano donde me indica que valores son na # Toda la base de datos pd.isna(df) pd.notna(df[&quot;dated&quot;]) # Resultado: inverso de lo anterior pd.notna(df) df.notna() df.isna() 8.5.6.3 Calculos con missing values a = pd.DataFrame({&quot;one&quot;: [None, None, 0.11, -2.1, -2.2], &quot;two&quot;:[32, 4, 0.11, -2.1, -2.2] }) a b = pd.DataFrame({&quot;one&quot;: [None, None, 0.11, -2.1, None], &quot;two&quot;:[32, 4, 0.11, None, -2.2], &quot;three&quot;:[322, 43, 1.3, 2.1, 2.2]}) b c = a + b c Entre objetos los missing values se traspasan. No se tratan como un cero. a # ¿Qué ocurre cuando queremos hacer estadistica descriptiva? a[&quot;one&quot;].sum() # Cuando sumamos todos los valores son tratados como ceros. c c[&quot;three&quot;].sum() # Si todos los valors son NA la suma será cero. # Otros tipos de funciones, por ejemplo, cumsum() ignoar NA por defecto, # pero los preservan en los resultados a a.cumsum() # Si quiero incorporar el hecho de que existen missing values, ocupo la opción skipna = False a.cumsum(skipna = False) a 8.5.6.4 ¿Qué hacer con los missing values? df = pd.read_csv(&#39;datos//banglore.csv&#39;) df.head(10) df.shape df.isnull().head(10) # Verdadero: significa NaN df.isnull? # Ahora vamos a sumar los missing values por columna df.isna().sum() 5502/13320 # Contamos el total df.isnull().sum().sum() 8.5.6.5 fillna() # Vamos a remplazar los valores missing por alguno arbitrario df2 = df.fillna(value = 0) df.head(10) df2.head(10) df2.isnull().sum().sum() df3 = df.fillna(value = 5) df3.isnull().sum().sum() df.head(10) # Llenar valores con alguno previo df4 = df.fillna(method = &quot;pad&quot;) df4.head(10) # Llenar valores con algun valor posterior df5 = df.fillna(method = &quot;bfill&quot;) df5.head(10) df.head(10) df6 = df.fillna(method = &quot;pad&quot;, axis = 1) df6.head(10) df7 = df.fillna(method = &quot;bfill&quot;, axis = 1) df7.head(10) df.head(10) # Llenar diferentes valores en diferentes columnas df8 = df.fillna({&quot;society&quot;:&quot;abc&quot;, &quot;balcony&quot;: &quot;defg&quot;}) df8.tail(10) # Ahora vamos a completarlo con el promedio de la columna df9 = df.fillna(value = df[&quot;balcony&quot;].mean()) df9.head(10) # alternativamente podemos ocupar max() o min() 8.5.6.6 Drop na () df df10 = df.dropna() df10 df11 = df.dropna(how = &quot;all&quot;) df11.shape df11a = df.dropna(how = &quot;any&quot;) df11a.shape 8.5.6.7 replace () # Remplaza cualquier valor df12 = df.replace(to_replace = np.nan, value = 343434) df12 df13 = df.replace(to_replace = 3.0, value = 343434) df13 8.5.6.8 interpolate() Cambia por el promedio de una lista ordenada df[&quot;balcony&quot;] = df[&quot;balcony&quot;].interpolate(method = &quot;linear&quot;) df.head(10) 8.5.7 Juntar bases de datos: merge y append 01figuras/image.png 01figuras/image.png 8.5.7.1 Ejemplo 1: Juntar por filas (append) df1 = pd.DataFrame({&quot;Nombre&quot;:[&quot;Carmen&quot;, &quot;Luis&quot;], &quot;Sexo&quot;:[&quot;Mujer&quot;, &quot;Hombre&quot;], &quot;Edad&quot;:[22, 18]}).set_index(&quot;Nombre&quot;) df1 df2 = pd.DataFrame({&quot;Nombre&quot;:[&quot;María&quot;, &quot;Pedro&quot;, &quot;Carmen&quot;], &quot;Sexo&quot;:[&quot;Mujer&quot;, &quot;Hombre&quot;, &quot;Mujer&quot;], &quot;Edad&quot;:[25, 30, 22], &quot;FT&quot;: [1,1,1]}).set_index(&quot;Nombre&quot;) df2 df3 = pd.DataFrame({&quot;Nombre&quot;:[&quot;María&quot;, &quot;Pedro&quot;, &quot;Carmen&quot;], &quot;Sexo&quot;:[&quot;Mujer&quot;, &quot;Hombre&quot;, &quot;Mujer&quot;], &quot;Edad&quot;:[25, 30, 22]}).set_index(&quot;Nombre&quot;) df3 df = pd.concat([df1, df3]) df 8.5.7.2 Ejemplo 2: Juntar por columnas (merge) Sintaxis básica df.merge(df1, df2, on = clave, how = tipo) Tipos inner (por defecto): El DataFrame resultante solo contiene las filas cuyos valores en la clave están en los dos DataFrames inner-join.gif outer: + El DataFrame resultante contiene todas las filas de los dos DataFrames. + Si una fila de un DataFrame no puede emparejarse con otra los mismos valores en la clave en el otro DataFrame, la fila se añade igualmente al DataFrame resultante rellenando las columnas del otro DataFrame con el valor NaN. + Es equivalente a la unión de conjuntos. full-join.gif left: El DataFrame resultante contiene todas las filas del primer DataFrame. Descarta las filas del segundo DataFrame que no pueden emparejarse con alguna fila del primero- left-join.gif right: El DataFrame resultante contiene todas las filas del segundo DataFrame Descarta las filas del primer DataFrame que no pueden emparejarse con alguna fila del segundo DataFrame a través de la clave. right-join.gif 8.5.7.3 Aplicaciones a bases de datos : append df1 = pd.read_csv(&#39;datos\\\\concat_1.csv&#39;) df2 = pd.read_csv(&#39;datos\\\\concat_2.csv&#39;) df3 = pd.read_csv(&#39;datos\\\\concat_3.csv&#39;) df1 df2 df3 row_concat = pd.concat([df1, df2, df3]) row_concat row_concat.loc[0] row_concat.iloc[0] # Si quiero resetear el indice utilizo opcion ignore_index = True row_concat_reset = pd.concat([df1, df2, df3], ignore_index = True) row_concat_reset row_concat_reset.loc[0] # ¿Qué ocurre si es que pego un data frame con una serie? new_row_series = pd.Series([&#39;n1&#39;, &#39;n2&#39;, &#39;n3&#39;, &#39;n4&#39;]) new_row_series df1 pd.concat([df1, new_row_series]) # Miren lo que ocurre si ahora defino columnas similares new_row_data = pd.DataFrame([[&#39;n1&#39;, &#39;n2&#39;, &#39;n3&#39;, &#39;n4&#39;]], columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) new_row_data pd.concat([df1, new_row_data]) # Tambien puedo modificar los ejes de forma tal de pegar hacia el lado col_concat = pd.concat([df1, df2, df3], axis=1) col_concat # Puedo hacer algunas selecciones que pueden generar tablas de interes col_concat[&#39;A&#39;] # Tambien puedo resetear los indices, en este caso el de las columnmas col_concat_ignore = pd.concat([df1, df2, df3], axis=1, ignore_index=True) col_concat_ignore # Miremos un ultimo ejemplo: junta lo común, genera nuevos en caso de que no lo sean df1.columns=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;] df2.columns=[&#39;E&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;] df3.columns=[&#39;A&#39;,&#39;H&#39;,&#39;F&#39;,&#39;C&#39;] df1 df2 df3 pd.concat([df1, df2, df3]) 8.5.7.4 Aplicaciones a bases de datos : merge person=pd.read_csv(&#39;datos\\\\survey_person.csv&#39;) site=pd.read_csv(&#39;datos\\\\survey_site.csv&#39;) survey=pd.read_csv(&#39;datos\\\\survey_survey.csv&#39;) visited=pd.read_csv(&#39;datos\\\\survey_visited.csv&#39;) person site survey visited visited_sub = visited.loc[[0, 2, 6]] visited_sub site # Por defecto va a ser inner, noten que le indico left_on y right_on. ¿Razón? o2o = site.merge(visited_sub, left_on=[&#39;name&#39;], right_on=&#39;site&#39;) o2o o2o.shape visited site m2o = site.merge(visited, left_on=&#39;name&#39;, right_on=&#39;site&#39;) m2o Ejercicio 4.5.4: Genere dos bases de datos (df1 y df2) con la siguiente información: df1 Nombre Género Carmen Walmart Luis Dia María Walmart df2 Nombre Género María Fnac Pedro Dia Luis Walmart Aplique los cuatro tipos de “join”: inner, outer, left y right. Compare sus resultados df1 = pd.DataFrame({&quot;Nombre&quot;:[&quot;Carmen&quot;, &quot;Luis&quot;, &quot;María&quot;], &quot;Sexo&quot;:[&quot;Mujer&quot;, &quot;Hombre&quot;, &quot;Mujer&quot;]}) df2 = pd.DataFrame({&quot;Nombre&quot;:[&quot;María&quot;, &quot;Pedro&quot;, &quot;Luis&quot;], &quot;Edad&quot;:[25, 30, 18]}) df1 df2 df_inner = pd.merge(df1, df2, on=&quot;Nombre&quot;) df_outer = pd.merge(df1, df2, on=&quot;Nombre&quot;, how=&quot;outer&quot;) df_left = pd.merge(df1, df2, on=&quot;Nombre&quot;, how=&quot;left&quot;) df_right = pd.merge(df1, df2, on=&quot;Nombre&quot;, how=&quot;right&quot;) df_inner df_outer df_left df_right 8.6 Análisis de datos (Parte I) La clase de hoy tiene como objetivo revisar algunos elementos releacionados con análisis estadístico. Veremos algunos concepts básicos y su aplicación en python. En la segunda parte de la clase revisaremos varios ejemplos. # Preambulo # Librerías: import pandas as pd # datos/dataframes import numpy as np # arreglos import datetime as dt # fechas import os # directorios import matplotlib.pyplot as plt # Visualización básicas y no tan básicas import scipy.stats as stats # Librería estadística import random # Números aleatorios import math # Operaciones matemáticas # Fecha de hoy print(dt.date.today()) # Carpetas main = os.getcwd() datos = main + &quot;\\\\datos&quot; resultados = main + &quot;\\\\resultados&quot; tablas = resultados + &quot;\\\\tablas&quot; figuras = resultados + &quot;\\\\figuras&quot; main 8.6.1 Estadistica Descriptiva Las estadísticas descriptivas son medidas que resumen características importantes de los datos. La elaboración de estadísticas descriptivas es un primer paso común después de limpiar y preparar un conjunto de datos para su análisis. Ya hemos visto varios ejemplos de estadísticas descriptivas en clases anteriores, en esta clase formalizaremos algunos temas y veremos muchos ejemplos. 8.6.1.1 Medidas de tendencia central : media, mediana y moda Las medidas de centralidad son estadísticas que nos dan una idea del “centro” de una variable numérica. En otras palabras, las medidas de centralidad dan una idea del valor típico que se espera ver en una distribución de datos. Las medidas de centralidad más comunes son la media, la mediana y la moda. La media es simplemente un promedio: la suma de los valores dividida por el número total de registros. Como hemos visto en lecciones anteriores, podemos utilizar df.mean() para obtener la media de cada columna de un DataFrame: # Importamos datos mtcars = pd.read_csv(&quot;datos\\\\mtcars.csv&quot;) mtcars del mtcars[&quot;model&quot;] mtcars mtcars.head(10) mtcars.tail() mtcars.columns list(mtcars.columns) sorted(mtcars.columns) # Promedio cada columna promedio = mtcars.mean() promedio t1 = pd.DataFrame(promedio) t1 t1 = t1.rename(columns = {0: &quot;Promedio&quot;}) t1 # Guardamos nuestra T1 os.chdir(tablas) t1.to_excel(&quot;t1.xlsx&quot;, sheet_name=&#39;t1&#39;, index=True) Recuerden que también podemos incluir el promedio por fila # Promedio por fila mtcars.mean(axis=1) La mediana de una distribución es el valor en el que el 50% de los datos se encuentra por debajo y el 50% por encima. En esencia, la mediana divide los datos por la mitad. La mediana también se conoce como el percentil del 50%, ya que el 50% de las observaciones se encuentran por debajo de ella. Se puede obtener la mediana utilizando la función df.median(): # Mediana mediana = mtcars.median() t2 = pd.DataFrame(mediana) t2 = t2.rename(columns = {0: &quot;Mediana&quot;}) t2 # Guardamos nuestra T2 os.chdir(tablas) t2.to_excel(&quot;t2.xlsx&quot;, sheet_name=&#39;t2&#39;, index=False) # ¿Si queremos juntar? t3 = pd.concat([t1, t2], axis = 1) t3 # Guardamos nuestra T3 os.chdir(tablas) t3.to_excel(&quot;t3.xlsx&quot;, sheet_name=&#39;t3&#39;, index=True) # Index = True/False me permite incluir el nombre de las filas. Aunque la media y la mediana nos dan una idea del centro de una distribución, no siempre son iguales. La mediana siempre nos da un valor que divide los datos en dos mitades, mientras que la media es un promedio numérico, por lo que los valores extremos pueden tener un impacto significativo en la media. En una distribución simétrica, la media y la mediana serán iguales. norm_data = pd.DataFrame(np.random.normal(size=100000)) norm_data.plot(kind=&quot;density&quot;, figsize=(10,10)); plt.vlines(norm_data.mean(), # Media ymin=0, ymax=0.4, linewidth=5.0); plt.vlines(norm_data.median(), # Mediana ymin=0, ymax=0.4, linewidth=2.0, color=&quot;red&quot;); En una distribución simetrica (como la normal) la media y la mediana son iguales skewed_data = pd.DataFrame(np.random.exponential(size=100000)) skewed_data.plot(kind=&quot;density&quot;, figsize=(10,10), xlim=(-1,5)); plt.vlines(skewed_data.mean(), # Media ymin=0, ymax=0.8, linewidth=5.0); plt.vlines(skewed_data.median(), # Mediana ymin=0, ymax=0.8, linewidth=2.0, color=&quot;red&quot;); En una distribución asimétrica, la media y la mediana difieren. La media también está muy influenciada por los valores atípicos, mientras que la mediana resiste la influencia de los mismos: norm_data = np.random.normal(size=50) outliers = np.random.normal(15, size=3) combined_data = pd.DataFrame(np.concatenate((norm_data, outliers), axis=0)) combined_data.plot(kind=&quot;density&quot;, figsize=(10,10), xlim=(-5,20)); plt.vlines(combined_data.mean(), # Media (azul) ymin=0, ymax=0.2, linewidth=5.0); plt.vlines(combined_data.median(), # Mediana (rojo) ymin=0, ymax=0.2, linewidth=2.0, color=&quot;red&quot;); Dado que la mediana tiende a resistir los efectos de la asimetría y los valores atípicos, se conoce como una estadística “robusta”. La mediana suele dar una mejor idea del valor típico en una distribución con asimetrías o valores atípicos significativos. Finalmente, la moda de una variable es simplemente el valor que aparece con más frecuencia. A diferencia de la media y la mediana, se puede tomar el modo de una variable categórica y es posible tener múltiples modas. moda = mtcars.mode() moda Las columnas con modas múltiples (valores múltiples con la misma cuenta) devuelven valores múltiples como el modo. Las columnas sin moda (ningún valor que aparezca más de una vez) devuelven NaN. moda.dropna(axis = 0, how = &#39;any&#39;, inplace = True) moda # Guardamos nuestra T4 os.chdir(tablas) moda.to_excel(&quot;t4.xlsx&quot;, sheet_name=&#39;t4&#39;, index=True) # Index = True/False me permite incluir el nombre de las filas. En resumen… 01figuras/image.png 01figuras/image.png 01figuras/image.png 8.6.1.2 Medidas de dispersión Las medidas de dispersión son estadísticas que describen cómo varían los datos. Mientras que las medidas de centro nos dan una idea del valor típico, las medidas de dispersión nos dan una idea de cuánto tienden a divergir los datos del valor típico. Una de las medidas de dispersión más sencillas es el rango. El rango es la distancia entre las observaciones máxima y mínima: # Rango rango = max(mtcars[&quot;mpg&quot;]) - min(mtcars[&quot;mpg&quot;]) rango Como se ha señalado anteriormente, la mediana representa el percentil 50 de un conjunto de datos. Se puede utilizar un resumen de varios percentiles para describir la dispersión de una variable. Podemos extraer el valor mínimo (percentil 0), el primer cuartil (percentil 25), la mediana, el tercer cuartil (percentil 75) y el valor máximo (percentil 100) utilizando el método quantile(): five_num = [mtcars[&quot;mpg&quot;].quantile(0), mtcars[&quot;mpg&quot;].quantile(0.25), mtcars[&quot;mpg&quot;].quantile(0.50), mtcars[&quot;mpg&quot;].quantile(0.75), mtcars[&quot;mpg&quot;].quantile(1)] five_num t5 = pd.DataFrame(mtcars.describe()) t5 # Guardamos nuestra T4 os.chdir(tablas) moda.to_excel(&quot;t5.xlsx&quot;, sheet_name=&#39;t5&#39;, index=True) # Index = True/False me permite incluir el nombre de las filas. El rango intercuartil (IQR) es otra medida común de dispersión. El IQR es la distancia entre el tercer cuartil y el primer cuartil: IQR = mtcars[&quot;mpg&quot;].quantile(0.75) - mtcars[&quot;mpg&quot;].quantile(0.25) IQR En un grafico de caja y bigote es posible ver estas medidas de dispersión de forma gráfica: mtcars.boxplot(column=&quot;mpg&quot;, return_type=&#39;axes&#39;, figsize=(8,8)) # Agregar texto plt.text(x=0.74, y=22.25, s=&quot;3er cuartil&quot;) plt.text(x=0.8, y=18.75, s=&quot;Mediana&quot;) plt.text(x=0.75, y=15.5, s=&quot;Primer cuartil&quot;) plt.text(x=0.9, y=10, s=&quot;Min&quot;) plt.text(x=0.9, y=33.5, s=&quot;Max&quot;) plt.text(x=0.7, y=19.5, s=&quot;IQR&quot;, rotation=360, size=15); La varianza y la desviación estándar son otras dos medidas comunes de la dispersión. La varianza de una distribución es la media de las desviaciones (diferencias) al cuadrado de la media. Utilice df.var() para comprobar la varianza: mtcars[&quot;mpg&quot;].var() La desviación estandar es la raíz cuadrada de la varianza. La desviación estándar puede ser más interpretable que la varianza, ya que la desviación estándar se expresa en términos de las mismas unidades que la variable en cuestión, mientras que la varianza se expresa en términos de unidades al cuadrado. Utilice df.std() para comprobar la desviación estándar: mtcars[&quot;mpg&quot;].std() Dado que tanto la varianza como la desviación estándar se derivan de la media, son susceptibles a la influencia de la asimetría de los datos y de los valores atípicos. La desviación absoluta de la mediana es una medida alternativa de la dispersión basada en la mediana, que hereda la solidez de la mediana frente a la influencia de la asimetría y los valores atípicos. Es la mediana del valor absoluto de las desviaciones de la mediana: abs_median_devs = abs(mtcars[&quot;mpg&quot;] - mtcars[&quot;mpg&quot;].median()) abs_median_devs.median() 8.6.1.3 Asimetría y curtosis Además de las medidas de centro y dispersión, las estadísticas descriptivas incluyen medidas que dan una idea de la forma de una distribución. La simetría mide el sesgo o la asimetría de una distribución, mientras que la curtosis mide la cantidad de datos que se encuentran en las colas de una distribución en comparación con el centro. No vamos a entrar en los cálculos exactos que hay detrás de la asimetría y la curtosis, pero son esencialmente estadísticas que llevan la idea de la varianza un paso más allá: mientras que la varianza implica elevar al cuadrado las desviaciones de la media, la asimetría implica elevar al cubo las desviaciones de la media y la curtosis implica elevar las desviaciones de la media a la cuarta potencia. Pandas ha incorporado funciones para comprobar la asimetría y la curtosis, df.skew() y df.kurt() respectivamente: mtcars[&quot;mpg&quot;].skew() # Simetría mtcars[&quot;mpg&quot;].kurt() # Kurtosis norm_data = np.random.normal(size=100000) skewed_data = np.concatenate((np.random.normal(size=35000)+2, np.random.exponential(size=65000)), axis=0) uniform_data = np.random.uniform(0,2, size=100000) peaked_data = np.concatenate((np.random.exponential(size=50000), np.random.exponential(size=50000)*(-1)), axis=0) data_df = pd.DataFrame({&quot;norm&quot;:norm_data, &quot;skewed&quot;:skewed_data, &quot;uniform&quot;:uniform_data, &quot;peaked&quot;:peaked_data}) data_df.plot(kind=&quot;density&quot;, figsize=(10,10), xlim=(-5,5)); Ahora vamos a comprobar la asimetría de cada una de las distribuciones. Dado que la simetría se mide con la asimetría, esperaríamos ver una asimetría baja para todas las distribuciones excepto la asimétrica, porque todas las demás son aproximadamente simétricas: data_df.skew() Veamos la curtosis: data_df.kurt() Como podemos ver en el resultado, los datos con distribución normal tienen una curtosis cercana a cero, la distribución plana tiene una curtosis negativa y las dos distribuciones con más datos en las colas frente al centro tienen mayor curtosis. 01figuras/image.png 8.6.1.4 Resumen sección 1 Las estadísticas descriptivas le ayudan a explorar las características de sus datos, como el centro, la dispersión y la forma, resumiéndolas con medidas numéricas. Los estadísticos descriptivos ayudan a informar sobre la dirección de un análisis y le permiten comunicar sus ideas a otros de forma rápida y sencilla. Además, ciertos valores, como la media y la varianza, se utilizan en todo tipo de pruebas estadísticas y modelos predictivos. Es importante hacer siempre una tabla con estos estadisticos al inicio de cualquier ejercicio de análisis de datos. 8.6.2 La distribución normal La normal o distribución gaussiana es una distribución de probabilidad continua caracterizada por una curva simétrica en forma de campana. Una distribución normal se define por su centro (media) y su dispersión (desviación estándar). La mayor parte de las observaciones generadas a partir de una distribución normal se sitúan cerca de la media, que se encuentra en el centro exacto de la distribución: como regla general, aproximadamente el 68% de los datos se sitúan dentro de una desviación estándar de la media, el 95% dentro de dos desviaciones estándar y el 99,8% dentro de tres desviaciones estándar. La distribución normal es quizá la más importante de toda la estadística. Resulta que muchos fenómenos del mundo real, como las puntuaciones de los tests de inteligencia y las alturas humanas, siguen aproximadamente una distribución normal, por lo que se utiliza a menudo para modelar variables aleatorias. Muchas pruebas estadísticas comunes asumen que las distribuciones son normales. El apodo de scipy para la distribución normal es norm. Vamos a investigar la distribución normal: # Estas caracteristicas matematicas nos permiten saber la posición de ciertos valores prob_under_minus1 = stats.norm.cdf(x= -1, loc = 0, scale = 1) prob_over_1 = 1 - stats.norm.cdf(x = 1, loc = 0, scale= 1) between_prob = 1-(prob_under_minus1 + prob_over_1) print(prob_under_minus1, prob_over_1, between_prob) El resultado muesta muestra que: Aproximadamente el 16% de los datos generados por una distribución normal con media 0 y desviación estándar 1 está por debajo de -1, El 16% está por encima de 1 y el 68% se encuentra entre -1 y 1. Lo anterior es coherente con la regla 68, 95, 99,7. Grafiquemos la distribución normal e inspeccionemos las áreas que hemos calculado: plt.rcParams[&quot;figure.figsize&quot;] = (9,9) plt.fill_between(x=np.arange(-4,-1,0.01), y1= stats.norm.pdf(np.arange(-4,-1,0.01)) , facecolor=&#39;red&#39;, alpha=0.35) plt.fill_between(x=np.arange(1,4,0.01), y1= stats.norm.pdf(np.arange(1,4,0.01)) , facecolor=&#39;red&#39;, alpha=0.35) plt.fill_between(x=np.arange(-1,1,0.01), y1= stats.norm.pdf(np.arange(-1,1,0.01)) , facecolor=&#39;blue&#39;, alpha=0.35) plt.text(x=-1.8, y=0.03, s= round(prob_under_minus1,3)) plt.text(x=-0.2, y=0.1, s= round(between_prob,3)) plt.text(x=1.4, y=0.03, s= round(prob_over_1,3)); 01figuras/image.png El gráfico anterior muestra la forma de campana de la distribución normal, el área por debajo y por encima de una desviación estándar y el área dentro de una desviación estándar de la media. Encontrar los cuantiles de la distribución normal es una tarea común cuando se realizan pruebas estadísticas. Puede comprobar los cuantiles de la distribución normal con stats.norm.ppf(): print(stats.norm.ppf(q=0.50)) print(stats.norm.ppf(q=0.975) ) El resultado del cuantil anterior confirma que aproximadamente el 5% de los datos se encuentra a más de 2 desviaciones estándar de la media. Esto es conocido como el valor z. Básicamente una medida de cuantas desviaciones estandar un punto esta alejado sobre la media. 8.6.2.1 De una muestra aleatoria a la población Hasta ahora, esta guía se ha centrado en las funciones y la sintaxis necesarias para manipular, explorar y describir los datos. La limpieza de datos y el análisis exploratorio son a menudo pasos preliminares hacia el objetivo final de extraer información de los datos a través de la inferencia estadística o el modelado predictivo. La inferencia estadística es el proceso de análisis de datos de muestra para obtener información sobre la población de la que se recogieron los datos y para investigar las diferencias entre las muestras de datos. En el análisis de datos, a menudo estamos interesados en las características de alguna población grande, pero recoger datos de toda la población puede ser inviable. 8.6.2.2 Estimaciones puntuales Las estimaciones puntuales son estimaciones de los parámetros de la población basadas en datos de la muestra. Por ejemplo, si quisiéramos saber la edad media de los votantes podríamos realizar una encuesta entre los votantes registrados y utilizar la edad media de los encuestados como una estimación puntual de la edad media de la población en su conjunto. La media de una muestra se conoce como media muestral. La media de la muestra no suele ser exactamente igual a la media de la población. Esta diferencia puede deberse a muchos factores, como un diseño deficiente de la encuesta, métodos de muestreo sesgados y la aleatoriedad inherente a la extracción de una muestra de una población. Investiguemos las estimaciones puntuales generando una población de datos de edad aleatorios y extrayendo luego una muestra de ella para estimar la media: np.random.seed(10) population_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000) population_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000) population_ages = np.concatenate((population_ages1, population_ages2)) population_ages population_ages.mean() np.random.seed(6) sample_ages = np.random.choice(a= population_ages, size=500) # Muestra aleatoria print ( sample_ages.mean() ) # Media muestral population_ages.mean() - sample_ages.mean() # Miremos la diferencia Nuestra estimación puntual basada en una muestra aleatoria de 500 individuos subestima la verdadera media de la población en 0,6 años, pero se aproxima. Esto ilustra un punto importante: podemos obtener una estimación bastante precisa de una gran población mediante el muestreo e un subconjunto relativamente pequeño de individuos. Otra estimación puntual que puede ser de interés es la proporción de la población que pertenece a alguna categoría o subgrupo. Por ejemplo, nos gustaría saber la ocupación de cada votante que encuestamos, para tener una idea de la demografía general de la base de votantes. Se puede hacer una estimación puntual de este tipo de proporción tomando una muestra y comprobando después la proporción en la muestra: random.seed(10) population_occ = ([&quot;occ1&quot;]*100000) + ([&quot;occ2&quot;]*50000) +\\ ([&quot;occ3&quot;]*50000) + ([&quot;occ4&quot;]*25000) +\\ ([&quot;occ5&quot;]*25000) demo_sample = random.sample(population_occ, 1000) # muestra aleatoria for occ in set(demo_sample): print( occ + &quot; proportion estimate:&quot; ) print( demo_sample.count(occ)/1000 ) Obsérvese que las estimaciones de las proporciones se acercan a las verdaderas proporciones de la población subyacente. 8.6.2.3 Distribuciones de muestreo y el teorema del límite central Muchos procedimientos estadísticos asumen que los datos siguen una distribución normal, porque la distribución normal tiene buenas propiedades como la simetría y que la mayoría de los datos se agrupan dentro de unas pocas desviaciones estándar de la media. Los datos del mundo real no suelen tener una distribución normal y la distribución de una muestra tiende a reflejar la distribución de la población. Esto significa que una muestra tomada de una población con una distribución sesgada también tenderá a ser sesgada. Vamos a investigarlo trazando los datos y la muestra que hemos creado antes y comprobando la inclinación: pd.DataFrame(population_ages).hist(bins=58, range=(17.5,75.5), figsize=(9,9)) print(stats.skew(population_ages)) La distribución tiene poca asimetría, pero el gráfico revela que los datos no son claramente normales: en lugar de una curva de campana simétrica, tiene una distribución bimodal con dos picos de alta densidad. La muestra que extrajimos de esta población debería tener aproximadamente la misma forma y asimetría: pd.DataFrame(sample_ages).hist(bins=58, range=(17.5,75.5), figsize=(9,9)); print(stats.skew(sample_ages)) La muestra tiene aproximadamente la misma forma que la población subyacente. Esto sugiere que no podemos aplicar las técnicas que suponen una distribución normal a este conjunto de datos, ya que no es normal. En realidad, sí podemos, gracias al teorema del límite central. El teorema del límite central es uno de los resultados más importantes de la teoría de la probabilidad y sirve de base a muchos métodos de análisis estadístico. El teorema afirma que la distribución de muchas medias muestrales, conocida como distribución muestral, se distribuirá normalmente. Esta regla es válida incluso si la propia distribución subyacente no está distribuida normalmente. En consecuencia, podemos tratar la media muestral como si se dibujara una distribución normal. Para ilustrarlo, creemos una distribución de muestreo tomando 200 muestras de nuestra población y haciendo luego 200 estimaciones puntuales de la media: np.random.seed(10) point_estimates = [] # Lista vacía for x in range(2000): # Generamos 200 muestras aleatorias sample = np.random.choice(a= population_ages, size=500) point_estimates.append(sample.mean()) pd.DataFrame(point_estimates).plot(kind=&quot;density&quot;, # Graficamos la media figsize=(9,9), xlim=(41,45)); La distribución del muestreo parece ser aproximadamente normal, a pesar de la distribución bimodal de la población de la que se extrajeron las muestras. Además, la media de la distribución muestral se aproxima a la media real de la población: population_ages.mean() - np.array(point_estimates).mean() Cuantas más muestras tomemos, mejor será nuestra estimación del parámetro poblacional. La media muestral converge a la media poblacional. population_ages.mean() 8.6.2.4 Intervalos de confianza Una estimación puntual puede dar una idea aproximada de un parámetro de la población, como la media, pero las estimaciones son propensas a errores y puede que no sea factible tomar múltiples muestras para obtener mejores estimaciones. Un intervalo de confianza es un rango de valores por encima y por debajo de una estimación puntual que capta el verdadero parámetro de la población con un nivel de confianza predeterminado. Por ejemplo, si quiere tener un 95% de posibilidades de capturar el verdadero parámetro de la población con una estimación puntual y un intervalo de confianza correspondiente, establecería su nivel de confianza en el 95%. Los niveles de confianza más altos dan lugar a intervalos de confianza más amplios. Calcule un intervalo de confianza tomando una estimación puntual y luego sumando y restando un margen de error para crear un rango. El margen de error se basa en el nivel de confianza deseado, la dispersión de los datos y el tamaño de la muestra. La forma de calcular el margen de error depende de si se conoce la desviación estándar de la población o no. El IC resultante nos dice que si repetimos la aleatorización múltiples veces, el 95% de los intervalos de confianza que generemos contendrán el valor real. Un IC de un 95% no significa que hay un 95% de probabilidades que el valor real se encuentre en un intervalo de confianza particular. Si conoce la desviación estándar de la población, el margen de error es igual a: \\[z * \\frac{\\sigma}{\\sqrt{n}}\\] Donde σ (sigma) es la desviación estándar de la población, n es el tamaño de la muestra, y z es un número conocido como el valor crítico z. El valor z-crítico es el número de desviaciones estándar que habría que alejar de la media de la distribución normal para capturar la proporción de los datos asociada al nivel de confianza deseado. Por ejemplo, sabemos que aproximadamente el 95% de los datos de una distribución normal se encuentra dentro de 2 desviaciones estándar de la media, por lo que podríamos utilizar 2 como valor z-crítico para un intervalo de confianza del 95% (aunque es más exacto obtener los valores z-críticos con stats.norm.ppf().). Calculemos un 95% de confianza para nuestra estimación puntual de la media: np.random.seed(10) sample_size = 1000 sample = np.random.choice(a= population_ages, size = sample_size) sample_mean = sample.mean() z_critical = stats.norm.ppf(q = 0.975) # Valor critico* print(&quot;z-critical value:&quot;) # Miramos el valor critico print(z_critical) pop_stdev = population_ages.std() # Get the population standard deviation margin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size)) confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error) print(&quot;Confidence interval:&quot;) print(confidence_interval) sample_mean Nota: Utilizamos stats.norm.ppf(q = 0,975) para obtener el valor z-crítico deseado en lugar de q = 0,95 porque la distribución tiene dos colas. Observa que el intervalo de confianza que hemos calculado captura la verdadera media poblacional de 43,0023. Vamos a crear varios intervalos de confianza y a trazarlos para tener una mejor idea de lo que significa “capturar” el verdadero valor np.random.seed(12) sample_size = 1000 intervals = [] sample_means = [] for sample in range(25): sample = np.random.choice(a= population_ages, size = sample_size) sample_mean = sample.mean() sample_means.append(sample_mean) z_critical = stats.norm.ppf(q = 0.975) # Get the z-critical value* pop_stdev = population_ages.std() # Get the population standard deviation stats.norm.ppf(q = 0.025) margin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size)) confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error) intervals.append(confidence_interval) plt.figure(figsize=(9,9)) plt.errorbar(x=np.arange(0.1, 25, 1), y=sample_means, yerr=[(top-bot)/2 for top,bot in intervals], fmt=&#39;o&#39;) plt.hlines(xmin=0, xmax=25, y=43.0023, linewidth=2.0, color=&quot;red&quot;); Observe que en el gráfico anterior, todos los intervalos de confianza del 95%, excepto uno, se superponen a la línea roja que marca la media real. Esto es de esperar: dado que un intervalo de confianza del 95% capta la media real el 95% de las veces, es de esperar que nuestro intervalo no capte la media real el 5% de las veces. Si no se conoce la desviación estandar de la población, hay que utilizar la desviación estandar de la muestra para crear intervalos de confianza. Dado que la desviación estándar de la muestra puede no coincidir con el parámetro de la población, el intervalo tendrá más error cuando no se conozca la desviación estándar de la población. Para tener en cuenta este error, utilizamos lo que se conoce como valor crítico t en lugar del valor crítico z. El valor crítico t se extrae de lo que se conoce como distribución t, una distribución que se asemeja mucho a la distribución normal pero que se hace cada vez más amplia a medida que disminuye el tamaño de la muestra. La distribución t está disponible en scipy.stats con el nombre de “t” para que podamos obtener los valores críticos de t con stats.t.ppf(). Tomemos una nueva muestra más pequeña y luego creemos un intervalo de confianza sin la desviación estándar de la población, utilizando la distribución t: np.random.seed(10) sample_size = 25 sample = np.random.choice(a= population_ages, size = sample_size) sample_mean = sample.mean() t_critical = stats.t.ppf(q = 0.975, df=24) # Valor t* print(&quot;t-critical value:&quot;) # Chequear valor t print(t_critical) sample_stdev = sample.std(ddof=1) # Obtenemos desv estandar muestral sigma = sample_stdev/math.sqrt(sample_size) # estimamos margin_of_error = t_critical * sigma confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error) print(&quot;Confidence interval:&quot;) print(confidence_interval) Nota: cuando se utiliza la distribución t, hay que proporcionar los grados de libertad (df). Para este tipo de prueba, los grados de libertad son iguales al tamaño de la muestra menos 1. Si el tamaño de la muestra es grande, la distribución t se aproxima a la distribución normal. Observe que el valor crítico de t es mayor que el valor crítico de z que utilizamos para el intervalo de confianza del 95%. Esto permite que el intervalo de confianza arroje una red más grande para compensar la variabilidad causada por el uso de la desviación estándar de la muestra en lugar de la desviación estándar de la población. El resultado final es un intervalo de confianza mucho más amplio (un intervalo con un mayor margen de error). Si tiene una muestra grande, el valor crítico t se acercará al valor crítico z, por lo que hay poca diferencia entre utilizar la distribución normal frente a la distribución t: # Chequeamos la diferencia entre valores criticos con un tamaño muestral de 1000 stats.t.ppf(q=0.975, df= 999) - stats.norm.ppf(0.975) &quot;&quot;&quot;En lugar de calcular un intervalo de confianza para una estimación puntual de la media a mano, puede calcularlo utilizando la función de Python stats.t.interval(): stats.t.interval(alpha = 0.95, # Nivel de confianza df= 24, # Grados de libertad loc = sample_mean, # Media muestral scale = sigma) # Desviación estandar muestral También podemos hacer un intervalo de confianza para una estimación puntual de una proporción poblacional. En este caso, el margen de error es igual a \\[z * \\sqrt{\\frac{p(1-p)}{n}}\\] Donde z es el valor crítico de z para nuestro nivel de confianza, p es la estimación puntual de la proporción poblacional y n es el tamaño de la muestra. Calculemos un intervalo de confianza del 95% para los de ocupación 3 según la proporción de la muestra que hemos calculado antes (0,192): z_critical = stats.norm.ppf(0.975) # Valor critico p = 0.192 # Estimación puntual de la proporción n = 1000 # Tamaño muestral margin_of_error = z_critical * math.sqrt((p*(1-p))/n) confidence_interval = (p - margin_of_error, # Intervalo de confianza p + margin_of_error) confidence_interval El resultado muestra que el intervalo de confianza capturó el verdadero parámetro poblacional de 0,2. De forma similar a nuestras estimaciones puntuales de la media de la población, podemos utilizar la función scipy stats.distribution.interval() para calcular un intervalo de confianza para una proporción de la población para nosotros. En este caso estamos trabajando con valores z-críticos por lo que queremos trabajar con la distribución normal en lugar de la distribución t: stats.norm.interval(alpha = 0.95, # Nivel de confianza loc = 0.192, # Estimación puntual de la proporción scale = math.sqrt((p*(1-p))/n)) # Escalando En resumen… La estimación de los parámetros de la población a través del muestreo es una forma de inferencia sencilla, pero potente. Las estimaciones puntuales combinadas con los márgenes de error nos permiten crear intervalos de confianza que capturan el verdadero parámetro de la población con una alta probabilidad. La próxima vez ampliaremos los conceptos de esta lección aprendiendo sobre las pruebas de hipótesis estadísticas. Ejercicio 4.6.1: Escriba una función que le permita calcular los intervalos de confianza de la media muestral. El input debe ser una columna de una data.frame. Utilice la función que acaba de generar junto con un iterador que le permita generar este intervalo de confianza para todas las columnas de una base de datos cualquiera. Pruebe su iterador con la base de datos mtcars. def mean_confidence_interval(data, confidence=0.05): mean = data.mean() sd = data.std() n = len(data) tcritico = stats.t.ppf(q = (1 - confidence/2), df= n-1) se = sd/np.sqrt(n) lcv = mean - tcritico * se ucv = mean + tcritico * se return mean, lcv, ucv list(mean_confidence_interval(mtcars[&quot;mpg&quot;], 0.05)) mtcars.columns lista = [] for col in mtcars.columns: a = mean_confidence_interval(mtcars[col], 0.05) a_list = list(a) lista.append(a_list) lista ic = pd.DataFrame(lista, index=mtcars.columns) ic ic = ic.rename(columns = {0: &quot;Promedio&quot;, 1: &quot;LIC&quot;, 2 : &quot;UIC&quot;}) ic 8.6.3 Test de hipotesis Las estimaciones puntuales y los intervalos de confianza son herramientas básicas de inferencia que sirven de base para otra técnica de inferencia: las pruebas de hipótesis estadísticas. La prueba de hipótesis estadística es un marco para determinar si los datos observados se desvían de lo esperado. La biblioteca scipy.stats de Python contiene una serie de funciones que facilitan la realización de pruebas de hipótesis. 8.6.3.1 Aspectos básicos Las pruebas de hipótesis estadísticas se basan en una afirmación llamada hipótesis nula que supone que no hay nada interesante entre las variables que se están probando (lo que no quiero que ocurra). La forma exacta de la hipótesis nula varía de un tipo de prueba a otro: si se está probando si los grupos difieren, la hipótesis nula afirma que los grupos son iguales. Por ejemplo, si quieres comprobar si la edad media de los votantes de tu comuna difiere de la media nacional, la hipótesis nula sería que no hay diferencia entre las edades medias. El propósito de una prueba de hipótesis es determinar si es probable que la hipótesis nula sea cierta dados los datos de la muestra. Si hay pocas pruebas en contra de la hipótesis nula teniendo en cuenta los datos, se acepta la hipótesis nula. Si la hipótesis nula es improbable teniendo en cuenta los datos, puede rechazar la hipótesis nula a favor de la hipótesis alternativa. La forma exacta de la hipótesis alternativa dependerá de la prueba específica que se realice. Siguiendo con el ejemplo anterior, la hipótesis alternativa sería que la edad media de los votantes de su estado difiere de hecho de la media nacional. Una vez que tenga la hipótesis nula y la alternativa en la mano, elija un nivel de significancia (a menudo denotado por la letra griega α.). El nivel de significacia es un umbral de probabilidad que determina cuándo se rechaza la hipótesis nula. Tras realizar una prueba, si la probabilidad de obtener un resultado tan extremo como el que se observa debido al azar es inferior al nivel, se rechaza la hipótesis nula a favor de la alternativa. Esta probabilidad de ver un resultado tan extremo o más extremo que el observado se conoce como valor p. La prueba T es una prueba estadística utilizada para determinar si una muestra de datos numéricos difiere significativamente de la población o si dos muestras difieren entre sí. 8.6.3.2 Test - T en una muestra Una prueba t de una muestra comprueba si la media de una muestra difiere de la media de la población. Creemos algunos datos de edad ficticia para la población de votantes de todo el país y una muestra de votantes de santiago. Comprobemos si la media de edad de los votantes de santiago difiere de la de la población: np.random.seed(6) population_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000) population_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000) population_ages = np.concatenate((population_ages1, population_ages2)) santiago_ages1 = stats.poisson.rvs(loc=18, mu=30, size=30) santiago_ages2 = stats.poisson.rvs(loc=18, mu=10, size=20) santiago_ages = np.concatenate((santiago_ages1, santiago_ages2)) print( population_ages.mean() ) print( santiago_ages.mean() ) Observa que hemos utilizado una combinación de distribuciones ligeramente diferente para generar los datos de la muestra de Santiago, por lo que sabemos que las dos medias son diferentes. Realicemos una prueba t con un nivel de confianza del 95% y veamos si rechaza correctamente la hipótesis nula de que la muestra procede de la misma distribución que la población. Para realizar una prueba t de una muestra, podemos la función stats.ttest_1samp(): stats.ttest_1samp(a = santiago_ages, # Datos muestrales popmean = population_ages.mean()) # Media poblacional El resultado de la prueba muestra que el estadístico de prueba “t” es igual a -2,574. Este estadístico de prueba nos indica cuánto se desvía la media de la muestra de la hipótesis nula. Si el estadístico t se encuentra fuera de los cuantiles de la distribución t correspondientes a nuestro nivel de confianza y grados de libertad, rechazamos la hipótesis nula. Podemos comprobar los cuantiles con stats.t.ppf(): stats.t.ppf(q=0.025, # Cuantile para chequear df=49) # Grados de libertad stats.t.ppf(q=0.975, # # Cuantile para chequear df=49) # Grados de libertad Podemos calcular las probabilidades de ver un resultado tan extremo como el que observamos (conocido como valor p) pasando el estadístico t como cuantil a la función stats.t.cdf(): stats.t.cdf(x = -2.5742, # Estadistico T df= 49) * 2 # Por dos (dos colas) Nota: La hipótesis alternativa que estamos comprobando es si la media de la muestra difiere (no es igual) a la media de la población. Dado que la muestra puede diferir en dirección positiva o negativa, multiplicamos el valor por dos. Observe que este valor es el mismo que el valor p que aparece en el resultado de la prueba t original. Un valor p de 0,01311 significa que esperaríamos ver datos tan extremos como nuestra muestra debido al azar alrededor del 1,3% de las veces si la hipótesis nula fuera cierta. En este caso, el valor p es inferior a nuestro nivel de significacia α (igual a 1-conf.nivel o 0,05), por lo que deberíamos rechazar la hipótesis nula. Veamos que ocurre con los intervalos de confianza: sigma = santiago_ages.std()/math.sqrt(50) # Desv. est. muestral stats.t.interval(0.95, # Intervalo de confianza df = 49, # Grados libertad loc = santiago_ages.mean(), # Media muestral scale= sigma) # Desviacion estandar muestral Por otra parte, dado que hay un 1,3% de posibilidades de ver un resultado tan extremo debido al azar, no es significativo al nivel de confianza del 99%. Esto significa que si construyéramos un intervalo de confianza del 99%, capturaría la media de la población: stats.t.interval(alpha = 0.99, # Intervalo de confianza df = 49, # Grados libertad loc = santiago_ages.mean(), # Media muestra scale= sigma) # Desviacion estandar muestral Con un nivel de confianza más alto, construimos un intervalo de confianza más amplio y aumentamos las probabilidades de que capte la verdadera media, con lo que es menos probable que rechacemos la hipótesis nula. En este caso, el valor p de 0,013 es mayor que nuestro nivel de significación de 0,01 y no rechazamos la hipótesis nula. 8.6.3.3 Test T en dos muestras Una prueba t de dos muestras investiga si las medias de dos muestras de datos independientes difieren entre sí. En una prueba de dos muestras, la hipótesis nula es que las medias de ambos grupos son iguales. A diferencia de la prueba de una muestra, en la que se compara con un parámetro poblacional conocido, la prueba de dos muestras sólo incluye las medias de las muestras. Puede realizar una prueba t de dos muestras pasando con la función stats.ttest_ind(). Vamos a generar una muestra de datos de la edad de los votantes de Valparaiso y a probarla contra la muestra que hicimos antes: np.random.seed(12) valparaiso_ages1 = stats.poisson.rvs(loc=18, mu=33, size=30) valparaiso_ages2 = stats.poisson.rvs(loc=18, mu=13, size=20) valparaiso_ages = np.concatenate((valparaiso_ages1, valparaiso_ages2)) print(valparaiso_ages.mean() ) stats.ttest_ind(a= santiago_ages, b= valparaiso_ages, equal_var=False) # asumimos que tienen igual varianza? La prueba arroja un valor p de 0,0907, lo que significa que hay un 9% de posibilidades de que veamos los datos de la muestra tan separados si los dos grupos analizados son realmente idénticos. Si utilizáramos un nivel de confianza del 95%, no podríamos rechazar la hipótesis nula, ya que el valor p es mayor que el nivel de significancia correspondiente del 5%. 8.6.3.4 T-Test pareados La prueba t básica de dos muestras está diseñada para probar las diferencias entre grupos independientes. En algunos casos, puede interesarle probar las diferencias entre muestras del mismo grupo en diferentes momentos. Por ejemplo, un hospital podría querer probar si un medicamento para perder peso funciona comprobando los pesos de los pacientes del mismo grupo antes y después del tratamiento. Una prueba t pareada permite comprobar si las medias de las muestras del mismo grupo difieren. Podemos realizar una prueba t emparejada utilizando la función scipy stats.ttest_rel(). Generemos algunos datos de peso de pacientes ficticios y hagamos una prueba t pareada: np.random.seed(11) before= stats.norm.rvs(scale=30, loc=250, size=100) after = before + stats.norm.rvs(scale=5, loc=-1.25, size=100) weight_df = pd.DataFrame({&quot;weight_before&quot;:before, &quot;weight_after&quot;:after, &quot;weight_change&quot;:after-before}) weight_df.describe() # Chequeamos El resumen muestra que los pacientes perdieron alrededor de 1,23 libras de media después del tratamiento. Realicemos una prueba t pareada para ver si esta diferencia es significativa con un nivel de confianza del 95%: stats.ttest_rel(a = before, b = after) Rechazamos la hipotesis nula de que ambos promedios son iguales. Ejercicio 4.6.2: Tome los siguientes valores de colesterol de los pacientes antes y despues de una dieta test_results_before_diet=[224, 235, 223, 253, 253, 224, 244, 225, 259, 220, 242, 240, 239, 229, 276, 254, 237, 227] test_results_after_diet=[198, 195, 213, 190, 246, 206, 225, 199, 214, 210, 188, 205, 200, 220, 190, 199, 191, 218] Realice un test de hipotesis pareado que le permita chequear si los niveles de colesterol han bajado posterior a la dieta utilizando un nivel de significancia del 0.05%. Asuma que efectivamente los datos se distribuyen normalmente. test_results_before_diet=[224, 235, 223, 253, 253, 224, 244, 225, 259, 220, 242, 240, 239, 229, 276, 254, 237, 227] test_results_after_diet=[198, 195, 213, 190, 246, 206, 225, 199, 214, 210, 188, 205, 200, 220, 190, 199, 191, 218] test_stat, p_value_paired = stats.ttest_rel(test_results_before_diet,test_results_after_diet) print(&quot;p value:%.6f&quot; % p_value_paired , &quot;one tailed p value:%.6f&quot; %(p_value_paired/2)) if p_value_paired &lt;0.05: print(&quot;Reject null hypothesis&quot;) else: print(&quot;Fail to reject null hypothesis&quot;) 8.6.3.5 Error tipo I y tipo II El resultado de una prueba de hipótesis estadística y la correspondiente decisión de rechazar o aceptar la hipótesis nula no son infalibles. Una prueba proporciona pruebas a favor o en contra de la hipótesis nula y luego se decide si se acepta o se rechaza en función de esas pruebas, pero éstas pueden carecer de la fuerza necesaria para llegar a la conclusión correcta. Las conclusiones incorrectas obtenidas a partir de las pruebas de hipótesis se clasifican en una de las dos categorías siguientes: error de tipo I y error de tipo II. El error de tipo I describe una situación en la que se rechaza la hipótesis nula cuando en realidad es verdadera. Este tipo de error también se conoce como “falso positivo” o “falso acierto”. La tasa de error de tipo 1 es igual al nivel de significación α, por lo que establecer un nivel de confianza más alto (y, por tanto, un alfa más bajo) reduce las posibilidades de obtener un falso positivo. El error de tipo II describe una situación en la que no se rechaza la hipótesis nula cuando en realidad es falsa. El error de tipo II también se conoce como “falso negativo” o “fallo”. Cuanto más alto sea el nivel de confianza, más probabilidades habrá de cometer un error de tipo II. 01figuras/image.png H0: No esta embarazada H1: Esta embarazada ETI: Rechazar la nula (rechazo “no estar embarazada”) cuando es verdadera (“no esta embarazado”). ETII: No rechazar la nula (“digo: no esta embarazada”), cuando es falsa (“esta embaradaza”). Investiguemos estos errores con un gráfico: plt.figure(figsize=(12,10)) plt.fill_between(x=np.arange(-4,-2,0.01), y1= stats.norm.pdf(np.arange(-4,-2,0.01)) , facecolor=&#39;red&#39;, alpha=0.35) plt.fill_between(x=np.arange(-2,2,0.01), y1= stats.norm.pdf(np.arange(-2,2,0.01)) , facecolor=&#39;grey&#39;, alpha=0.35) plt.fill_between(x=np.arange(2,4,0.01), y1= stats.norm.pdf(np.arange(2,4,0.01)) , facecolor=&#39;red&#39;, alpha=0.5) plt.fill_between(x=np.arange(-4,-2,0.01), y1= stats.norm.pdf(np.arange(-4,-2,0.01),loc=3, scale=2) , facecolor=&#39;grey&#39;, alpha=0.35) plt.fill_between(x=np.arange(-2,2,0.01), y1= stats.norm.pdf(np.arange(-2,2,0.01),loc=3, scale=2) , facecolor=&#39;blue&#39;, alpha=0.35) plt.fill_between(x=np.arange(2,10,0.01), y1= stats.norm.pdf(np.arange(2,10,0.01),loc=3, scale=2), facecolor=&#39;grey&#39;, alpha=0.35) plt.text(x=-0.8, y=0.15, s= &quot;Null Hypothesis&quot;) plt.text(x=2.5, y=0.13, s= &quot;Alternative&quot;) plt.text(x=2.1, y=0.01, s= &quot;Type 1 Error&quot;) plt.text(x=-3.2, y=0.01, s= &quot;Type 1 Error&quot;) plt.text(x=0, y=0.02, s= &quot;Type 2 Error&quot;); En el gráfico anterior, las áreas rojas indican los errores de tipo I que se producen cuando la hipótesis alternativa no es diferente de la nula para una prueba de dos caras con un nivel de confianza del 95%. El área azul representa los errores de tipo II que se producen cuando la hipótesis alternativa es diferente de la nula, como muestra la distribución de la derecha. Tenga en cuenta que la tasa de error de tipo II es el área bajo la distribución alternativa dentro de los cuantiles determinados por la distribución nula y el nivel de confianza. Podemos calcular la tasa de error de tipo II para las distribuciones anteriores de la siguiente manera: lower_quantile = stats.norm.ppf(0.025) # Lower cutoff value upper_quantile = stats.norm.ppf(0.975) # Upper cutoff value # Area bajo la alternativa, low = stats.norm.cdf(lower_quantile, loc=3, scale=2) # Area under alternative, to the left the upper cutoff value high = stats.norm.cdf(upper_quantile, loc=3, scale=2) # Area under the alternative, between the cutoffs (Type II error) high-low Con las distribuciones normales anteriores, no rechazaríamos la hipótesis nula en un 30% de las ocasiones porque las distribuciones están lo suficientemente cerca como para tener un solapamiento significativo. 8.6.3.6 Poder estadístico La potencia de una prueba estadística es la probabilidad de que la prueba rechace la hipótesis nula cuando la alternativa es realmente diferente de la nula. En otras palabras, la potencia es la probabilidad de que la prueba detecte que hay algo interesante cuando realmente hay algo interesante. La potencia es igual a uno menos la tasa de error de tipo II. La potencia de una prueba estadística está influida por El nivel de significancia elegido para la prueba. El tamaño de la muestra. El tamaño del efecto de la prueba. A la hora de elegir un nivel de significación para una prueba, existe un equilibrio entre el error de tipo I y el de tipo II. Un nivel de significación bajo, como 0,01, hace que sea poco probable que una prueba tenga errores de tipo I (falsos positivos), pero es más probable que tenga errores de tipo II (falsos negativos) que una prueba con un valor mayor del nivel de significancia α. Una convención común es que una prueba estadística debe tener una potencia de al menos 0,8. Un mayor tamaño de la muestra reduce la incertidumbre de la estimación puntual, haciendo que la distribución de la muestra se estreche, lo que da lugar a menores tasas de error de tipo II y a una mayor potencia. Tamaño del efecto es un término general que describe una medida numérica del tamaño de algún fenómeno. Hay muchas medidas diferentes del tamaño del efecto que surgen en diferentes contextos. En el contexto de la prueba T, un tamaño del efecto simple es la diferencia entre las medias de las muestras. Este número puede estandarizarse dividiéndolo por la desviación estándar de la población o la desviación estándar conjunta de las muestras. Esto pone el tamaño del efecto en términos de desviaciones estándar, por lo que un tamaño del efecto estandarizado de 0,5 se interpretaría como que la media de una muestra está a 0,5 desviaciones estándar de otra (en general, 0,5 se considera un tamaño del efecto “grande”). Dado que la potencia estadística, el nivel de significación, el tamaño del efecto y el tamaño de la muestra están relacionados, es posible calcular cualquiera de ellos para valores dados de los otros tres. Esto puede ser una parte importante del proceso de diseño de una prueba de hipótesis y del análisis de los resultados. Por ejemplo, si quiere realizar una prueba con un nivel de significancia determinado (digamos el estándar 0,05) y una potencia (digamos el estándar 0,8) y está interesado en un tamaño del efecto determinado (digamos 0,5 para la diferencia estandarizada entre las medias de la muestra), podría utilizar esa información para determinar el tamaño de la muestra que necesita. En python, la biblioteca statsmodels contiene funciones para resolver cualquier parámetro de la potencia de las pruebas T. Utilice statsmodels.stats.power.tt_solve_power para pruebas t de una muestra y statsmodels.stats.power.tt_ind_solve_power para una prueba t de dos muestras. Comprobemos el tamaño de la muestra que debemos utilizar dados los valores de los parámetros estándar anteriores para una prueba t de una muestra: from statsmodels.stats.power import tt_solve_power tt_solve_power(effect_size = 0.5, alpha = 0.05, power = 0.8) En este caso, querríamos un tamaño de muestra de al menos 34 para hacer un estudio con la potencia y el nivel de significación deseados capaz de detectar un tamaño de efecto grande. 8.6.4 Ejemplo 1: resumen exploratorio de datos Como sacar la primera impresión de tus datos No olvidar: http://www.danielmsullivan.com/pages/tutorial_stata_to_python.html # Basics import pandas as pd import numpy as np # Visualizations import seaborn as sns import matplotlib.pyplot as plt 8.6.4.1 Datos # Load the data df = sns.load_dataset(&#39;tips&#39;) # View first 5 rows df.head() # Miramos tipos de datos df.info() # Tamaño df.shape # Tipos de datos df.dtypes 8.6.4.2 Missing Values # Contamos missing values por variable missing_values = df.isnull().sum() missing_values missing_values_df = pd.DataFrame(missing_values) missing_values_df.rename(columns = {0 : &quot;Cantidad&quot;}) # Guardamos nuestra T6 missing_values_df.to_excel(&quot;t6.xlsx&quot;, sheet_name=&#39;t6&#39;, index=False) # Insertamos NA para ejemplificar dfna = pd.DataFrame([[10, 1.2, &#39;Female&#39;, np.nan, &#39;?&#39;, &#39;Dinner&#39;, 2]]*4, columns=df.columns) df2 = df.append(dfna).reset_index(drop=True) # tail of the df2, where NAs and nulls were inserted df2.tail() # Heatmap to see missing values sns.heatmap(df2.isnull(), cbar=False); # Chequiamos por otros posibles valores nilos - [?.,:*] solpom columnas de texto for col in df2.select_dtypes(exclude=&#39;number&#39;).columns: print(f&quot;{col}: {df2[col].str.contains(&#39;[?.,:*]&#39;).sum()} possible null(?.,:*) values&quot;) # Remplazamos ? df2.replace({&#39;?&#39;:np.nan}, inplace=True) df2.tail(2) # Ahora, vamos a los calores más comunes most_common_smoker = df.smoker.value_counts().index[0] most_common_day = df.day.value_counts().index[0] # Fill NA Values with the most common value df2.fillna(value={&#39;smoker&#39;:most_common_smoker, &#39;day&#39;:most_common_day}) # Drop NA values df2.dropna(inplace=True) df2 df2.describe() 8.6.4.3 Análisis univariado df.info() Bien, ahora sabemos que factura_total, punta y tamaño son variables numéricas, por lo que podemos analizar de la misma manera, con un boxplot o histograma. # Grafico caja y bigote fig, g = plt.subplots(1, 3, figsize=(20,9)) g1 = sns.boxplot(data=df, y=&#39;total_bill&#39;, color=&#39;royalblue&#39;, ax=g[0]) g1.set_title(&#39;Boxplot of Total_Bill&#39;, size=15) g2 = sns.boxplot(data=df, y=&#39;tip&#39;, color=&#39;coral&#39;, ax=g[1]) g2.set_title(&#39;Boxplot of Tip&#39;, size=15) g3 = sns.boxplot(data=df, y=&#39;size&#39;, color=&#39;gold&#39;, ax=g[2]) g3.set_title(&#39;Boxplot of Size&#39;, size=15); Los valores atípicos se confirman y tiran de la media hacia arriba. Ver los bigotes más largos para total_bill y propina. Para las variables categóricas, podemos trazar barras con recuentos. # Countplots of categorical variables fig, g = plt.subplots(1, 4, figsize=(28,9)) g1 = sns.countplot(data=df, x=&#39;sex&#39;, color=&#39;royalblue&#39;, ax=g[0]) g1.set_title(&#39;Countplot of Sex&#39;, size=15) g2 = sns.countplot(data=df, x=&#39;day&#39;, color=&#39;coral&#39;, ax=g[1]) g2.set_title(&#39;Countplot of Day&#39;, size=15) g3 = sns.countplot(data=df, x=&#39;time&#39;, color=&#39;gold&#39;, ax=g[2]) g3.set_title(&#39;Countplot of Time&#39;, size=15) g4 = sns.countplot(data=df, x=&#39;smoker&#39;, color=&#39;gold&#39;, ax=g[3]) g4.set_title(&#39;Countplot of Smoker&#39;, size=15); 8.6.4.4 Correlaciones La correlación es la medida estadística de la relación lineal entre dos variables numéricas. Así que, como vamos a hacer un análisis multivariante, creo que es interesante calcular las correlaciones, porque esto puede orientar tus esfuerzos. En caso de que tengas muchas variables, será mejor comprobar principalmente (si no sólo) las que tengan correlaciones más altas. import scipy.stats as stats stats.shapiro(mtcars[&quot;mpg&quot;]) # Test para evaluar normalidad # H0: normal # Función para testear normalidad con un alpha de 0.05 def test_normality(data): stat, p = stats.shapiro(data) if p &lt; 0.05: print(f&#39;p-Value: {p}. Not normaly distributed.&#39;) else: print(f&#39;p-Value: {p}. Normaly distributed.&#39;) # Tests for col in df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).columns: test_normality(df[col]) La prueba estándar es Pearson, pero si los datos no son normales, el método Spearman es más adecuado. # Data not normal, use Spearman correlation_calc = df.corr(method=&#39;spearman&#39;) sns.heatmap( correlation_calc , annot=True, cmap=&#39;RdBu&#39;); Si aplicamos el método de Pearson, los resultados no son tan diferentes. Esto tiene dos razones principales: 1. la muestra es pequeña; 2. lo que probablemente afecta más a la distribución son los valores atípicos. En el gráfico anterior, vemos que la factura total está más relacionada con la propina que con el tamaño de la fiesta. 8.6.5 Ejemplo 2: datos con precios de casas main datos df = pd.read_csv(&quot;datos\\\\MELBOURNE_HOUSE_PRICES_LESS.csv&quot;) df.head() df.info() average = df[&#39;Price&#39;].mean() print(average) med = df[&#39;Price&#39;].median() print(med) standard_deviation = df[&#39;Price&#39;].std() print(standard_deviation) # Commented out IPython magic to ensure Python compatibility. import matplotlib.pyplot as plt import seaborn as sns # %matplotlib inline sns.set(style=&quot;whitegrid&quot;) plt.figure(figsize=(10,8)) ax = sns.boxplot(x=&#39;Price&#39;, data=df, orient=&quot;v&quot;) os.chdir(figuras) plt.figure(figsize=(12,10)) ax = sns.boxplot(x=&#39;Type&#39;, y=&#39;Price&#39;, data=df, orient=&quot;v&quot;) plt.savefig(&quot;f1.pdf&quot;) filter_data = df.dropna(subset=[&#39;Price&#39;]) plt.figure(figsize=(14,8)) sns.distplot(filter_data[&#39;Price&#39;], kde=False) df[&#39;Type&#39;].unique() type_counts = df[&#39;Type&#39;].value_counts() df2 = pd.DataFrame( {&#39;house_type&#39;: type_counts}, index = [&#39;t&#39;, &#39;h&#39;, &#39;u&#39;] ) df2.plot.pie(y=&#39;house_type&#39;, figsize=(10,10), autopct=&#39;%1.1f%%&#39;) sns.set(style=&#39;darkgrid&#39;) plt.figure(figsize=(20,10)) ax = sns.countplot(x=&#39;Regionname&#39;, data=df) 8.6.6 Ejemplo 3 os.chdir(datos) df = pd.read_csv(&quot;house_prices.csv&quot;) df.head() df.head() df.shape df.info() # Histograma básico exploratorio saleprice = df[&#39;SalePrice&#39;] mean=saleprice.mean() median=saleprice.median() mode=saleprice.mode() print (&#39;Mean: &#39;,mean,&#39;\\nMedian: &#39;,median,&#39;\\nMode: &#39;,mode[0]) plt.figure(figsize=(10,5)) plt.hist(saleprice,bins=100,color=&#39;grey&#39;) plt.axvline(mean,color=&#39;red&#39;,label=&#39;Mean&#39;) plt.axvline(median,color=&#39;yellow&#39;,label=&#39;Median&#39;) plt.axvline(mode[0],color=&#39;green&#39;,label=&#39;Mode&#39;) plt.xlabel(&#39;SalePrice&#39;) plt.ylabel(&#39;Frequency&#39;) plt.legend() plt.show() saleprice.cumsum().head() Dispersión saleprice.min() saleprice.max() saleprice.max()-saleprice.min() #varianza saleprice.var() from math import sqrt std = sqrt(saleprice.var()) std saleprice.skew() saleprice.kurt() # Pasamos a numpy para ocupar funciones de numpy h = np.array(df[&#39;SalePrice&#39;]) h = sorted(h) # Generamos una distribución normal con la misma media y sd fit = stats.norm.pdf(h, np.mean(h), np.std(h)) #plot both series on the histogram plt.plot(h, fit,&#39;-&#39;,linewidth = 4,label=&quot;Normal distribution with same mean and var&quot;) plt.hist(h, bins = 100,label=&quot;Actual distribution&quot;, density = True) plt.legend() plt.show() fit # Chequeamos correlaciones corelation=df[[&#39;LotArea&#39;,&#39;GrLivArea&#39;,&#39;GarageArea&#39;,&#39;SalePrice&#39;]].corr() print (corelation) sns.heatmap(corelation) # Chequeamos covarianza df[[&#39;LotArea&#39;,&#39;GrLivArea&#39;,&#39;GarageArea&#39;,&#39;SalePrice&#39;]].cov().head() # #50 percentile i.e median # np.percentile(df[&#39;salary&#39;], 50) saleprice.quantile(0.5) # q75 = np.percentile(df[&#39;salary&#39;], 75) # q75 q3 = saleprice.quantile(0.75) q3 #25th percentile # q25 = np.percentile(df[&#39;salary&#39;], 25) q1 = saleprice.quantile(0.25) q1 #interquartile range IQR = q3 - q1 IQR plt.boxplot(saleprice) plt.show() 8.6.7 Ejemplo 4 df = pd.read_csv(&#39;insurance.csv&#39;) df df.head() df.info() df.shape df.columns df.describe() df.describe(include=&#39;O&#39;) list(df.sex.unique()) df.isnull().sum() df[df.duplicated(keep=&#39;first&#39;)] df.drop_duplicates(keep=&#39;first&#39;,inplace=True) plt.figure(figsize=(10,6)) sns.distplot(df.expenses,color=&#39;r&#39;) plt.title(&#39;Expenses Distribution&#39;,size=18) plt.xlabel(&#39;Expenses&#39;,size=14) plt.ylabel(&#39;Density&#39;,size=14) plt.show() plt.figure(figsize=(10,6)) sns.histplot(df.age) plt.title(&#39;Age Distribution&#39;,size=18) plt.xlabel(&#39;Age&#39;,size=14) plt.ylabel(&#39;Count&#39;,size=14) plt.show() plt.figure(figsize=(10,6)) plt.hist(df.bmi,color=&#39;y&#39;) plt.title(&#39;BMI Distribution&#39;,size=18) plt.show() plt.figure(figsize = (10,6)) sns.boxplot(df.expenses) plt.title(&#39;Distribution Charges&#39;,size=18) plt.show() Q1 = df[&#39;expenses&#39;].quantile(0.25) Q3 = df[&#39;expenses&#39;].quantile(0.75) IQR = Q3 - Q1 print(IQR) df[(df[&#39;expenses&#39;]&lt; Q1-1.5* IQR) | (df[&#39;expenses&#39;]&gt; Q3+1.5* IQR)] plt.figure(figsize=(10,6)) sns.countplot(x = &#39;sex&#39;, data = df) plt.title(&#39;Total Number of Male and Female&#39;,size=18) plt.xlabel(&#39;Sex&#39;,size=14) plt.show() plt.figure(figsize = (10,6)) sns.countplot(df.children) plt.title(&#39;Children Distribution&#39;,size=18) plt.xlabel(&#39;Children&#39;,size=14) plt.ylabel(&#39;Count&#39;,size=14) plt.show() plt.figure(figsize = (10,6)) sns.countplot(df.smoker) plt.title(&#39;Smoker Distribution&#39;,size=18) plt.xlabel(&#39;Smoker&#39;,size=14) plt.ylabel(&#39;Count&#39;,size=14) plt.show() df.smoker.value_counts() plt.figure(figsize = (10,6)) sns.countplot(df.region,palette=&#39;Blues&#39;) plt.title(&#39;Region Distribution&#39;,size=18) plt.xlabel(&#39;Region&#39;,size=14) plt.ylabel(&#39;Count&#39;,size=14) plt.show() plt.figure(figsize = (10,6)) sns.scatterplot(x=&#39;age&#39;,y=&#39;expenses&#39;,color=&#39;r&#39;,data=df) plt.title(&#39;Age vs Charges&#39;,size=18) plt.xlabel(&#39;Age&#39;,size=14) plt.ylabel(&#39;Charges&#39;,size=14) plt.show() print(&#39;Correlation between age and charges is : {}&#39;.format(round(df.corr()[&#39;age&#39;][&#39;expenses&#39;],3))) plt.figure(figsize = (10,6)) sns.set_style(&#39;darkgrid&#39;) sns.boxplot(x=&#39;smoker&#39;,y=&#39;expenses&#39;,data=df) plt.title(&#39;Smoker vs Expenses&#39;,size=18); sns.pairplot(df, markers=&quot;+&quot;, diag_kind=&quot;kde&quot;, kind=&#39;reg&#39;, plot_kws={&#39;line_kws&#39;:{&#39;color&#39;:&#39;#aec6cf&#39;}, &#39;scatter_kws&#39;: {&#39;alpha&#39;: 0.7, &#39;color&#39;: &#39;red&#39;}}, corner=True); plt.figure(figsize = (10,6)) sns.heatmap(df.corr(),annot=True,square=True, cmap=&#39;RdBu&#39;, vmax=1, vmin=-1) plt.title(&#39;Correlations Between Variables&#39;,size=18); plt.xticks(size=13) plt.yticks(size=13) plt.show() 8.6.8 Ejemplo 5 : Desigualdad de ingreso usando Gapminder Table of Contents Introducción Data Wrangling Exploratory Data Analysis Conclusions 8.6.8.1 Introducción Este análisis se centra en la desigualdad de ingresos medida por el Índice de Gini y su asociación con métricas económicas como el PIB per cápita, las inversiones como % del PIB y los ingresos fiscales como % del PIB. También se incluye una métrica política, el índice de democracia de la EIU. Esta investigación puede considerarse un punto de partida para cuestiones complejas como ¿Está asociado un mayor ingreso fiscal como % del PIB con una menor desigualdad de ingresos? ¿Se asocia un mayor índice de democracia de la EIU con una menor desigualdad de ingresos? ¿Se asocia un mayor PIB per cápita con una menor desigualdad de ingresos? ¿Se asocian las inversiones más altas como porcentaje del PIB con una menor desigualdad de ingresos? Este análisis utiliza el conjunto de datos gapminder de la Fundación Gapminder. La Fundación Gapminder es una empresa sin ánimo de lucro registrada en Estocolmo, Suecia, que promueve el desarrollo global sostenible y la consecución de los Objetivos de Desarrollo del Milenio de las Naciones Unidas mediante un mayor uso y comprensión de las estadísticas y otra información sobre el desarrollo social, económico y medioambiental a nivel local, nacional y global. *El Índice de Gini es una medida de dispersión estadística que pretende representar la distribución de la renta o la riqueza de los residentes de una nación, y es la medida de desigualdad más utilizada. Fue desarrollado por el estadístico y sociólogo italiano Corrado Gini y publicado en su documento de 1912 Variabilidad y Mutabilidad. import pandas as pd import numpy as np import matplotlib.pyplot as plt #import plotly #import plotly.express as px #%matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 10) pd.options.display.max_columns = 100 pd.set_option(&quot;display.precision&quot;, 2) 8.6.8.2 Data Wrangling En esta sección del informe se han cargado los datos, se ha comprobado su limpieza y se han comunicado los resultados. ##### Propiedades generales El conjunto de datos contiene datos de los siguientes conjuntos de datos de GapMinder: Índice de democracia de la EIU: “Este índice de democracia utiliza los datos de Economist Inteligence Unit para expresar la calidad de las democracias como un número entre 0 y 100. Se basa en 60 aspectos diferentes de las sociedades que son relevantes para la democracia: sufragio universal para todos los adultos, participación de los votantes, percepción de la protección de los derechos humanos y libertad para formar organizaciones y partidos. El índice de democracia se calcula a partir de los 60 indicadores, divididos en cinco”“subíndices”“, que son Índice de pluralismo electoral; Índice de gobierno; Índice de participación políticam; Índice de cultura política; Índice de libertad civil. Los subíndices se basan en la suma de las puntuaciones de aproximadamente 12 indicadores por subíndice, convertidos en una puntuación entre 0 y 100. (The Economist publica el índice con una escala de 0 a 10, pero Gapminder lo ha convertido en 0 a 100 para facilitar su comunicación en forma de porcentaje)“. https://docs.google.com/spreadsheets/d/1d0noZrwAWxNBTDSfDgG06_aLGWUz4R6fgDhRaUZbDzE/edit#gid=935776888 Ingresos: PIB per cápita, dólares constantes de la PPA: El PIB per cápita mide el valor de todo lo producido en un país durante un año, dividido por el número de personas. La unidad está en dólares internacionales, a precios fijos de 2011. Los datos se ajustan para tener en cuenta la inflación y las diferencias en el coste de la vida entre los países, los llamados dólares PPA. El final de la serie temporal, entre 1990 y 2016, utiliza los últimos datos del PIB per cápita del Banco Mundial, procedentes de sus Indicadores de Desarrollo Mundial. Para retroceder en el tiempo antes de que la serie del Banco Mundial comience en 1990, hemos utilizado varias fuentes, como Angus Maddison. https://www.gapminder.org/data/documentation/gd001/ Inversiones (% del PIB) La formación de capital es un término utilizado para describir la acumulación neta de capital durante un período contable para un país determinado. El término se refiere a las adiciones de bienes de capital, como equipos, herramientas, activos de transporte y electricidad. Los países necesitan bienes de capital para sustituir los más antiguos que se utilizan para producir bienes y servicios. Si un país no puede sustituir los bienes de capital cuando llegan al final de su vida útil, la producción disminuye. En general, cuanto mayor sea la formación de capital de una economía, más rápido podrá crecer su renta agregada. Ingresos fiscales (% del PIB) Se refiere a las transferencias obligatorias al gobierno central para fines públicos. No incluye la seguridad social. https://data.worldbank.org/indicator/GC.TAX.TOTL.GD.ZS 8.6.8.2.1 Initial Analysis of the Datasets Ingresos fiscales en porcentaje del PIB A continuación se presentan los resultados de un primer análisis: Los archivos csv para este análisis se descargaron del sitio web de GapMinder. Pueden encontrarse aquí: https://github.com/psterk1/data_analytics/tree/master/intro/final_project El primer archivo es ‘tax_revenue_percent_of_gdp.csv’ tax = pd.read_csv(&quot;datos\\\\tax_revenue_percent_of_gdp.csv&quot;) tax print(&quot;number of rows: &quot;, tax.shape[0]) print(&quot;number of columns: {}&quot;.format(tax.shape[1])) print(&quot;number of duplicates: {}&quot;.format(tax.duplicated().sum())) print(&quot;datatypes:\\n&quot;) print(tax.dtypes) print(&quot;\\nSample:&quot;) tax.head(3) Un primer análisis reveló que aproximadamente la mitad de los años con &gt; 0,50 valores perdidos. Véase más abajo: Debido a los resultados que se presentan a continuación, los años con el menor porcentaje de valores nulos se encontraban en el rango de 10 años de 2006 a 2016. En consecuencia, se seleccionó este intervalo anual para el resto de los conjuntos de datos tax_null = tax.isnull().sum()/tax.shape[0] tax_null.to_frame().transpose() Renta por persona - PIB per cápita A continuación se presentan los resultados de un primer análisis: income = pd.read_csv(&#39;datos\\\\income_per_person_gdppercapita_ppp_inflation_adjusted.csv&#39;) print(&quot;number of rows: &quot;, income.shape[0]) print(&quot;number of columns: {}&quot;.format(income.shape[1])) print(&quot;number of duplicates: {}&quot;.format(income.duplicated().sum())) print(&quot;datatypes:\\n&quot;) print(income.dtypes) income.head(3) Los resultados siguientes muestran que no hay nulos. income_null = income.isnull().sum()/income.shape[0] income_null.to_frame().transpose() Inversión Porcentaje del PIB A continuación se presentan los resultados de un primer análisis: invest = pd.read_csv(&#39;datos\\\\investments_percent_of_gdp.csv&#39;) print(&quot;number of rows: &quot;, invest.shape[0]) print(&quot;number of columns: {}&quot;.format(invest.shape[1])) print(&quot;number of duplicates: {}&quot;.format(invest.duplicated().sum())) print(&quot;datatypes:\\n&quot;) print(invest.dtypes) print(&quot;\\nSample:&quot;) invest.head(3) Los resultados que aparecen a continuación muestran que los años con el menor porcentaje de valores nulos se encontraban en el intervalo de 10 años comprendido entre 2006 y 2016. En consecuencia, se seleccionó este intervalo anual para el resto de los conjuntos de datos. invest_null = invest.isnull().sum()/invest.shape[0] invest_null.to_frame().transpose() Índice de Democracia de la EIU A continuación se presentan los resultados del análisis inicial: demo = pd.read_csv(&#39;datos\\\\demox_eiu.csv&#39;) print(&quot;number of rows: &quot;, demo.shape[0]) print(&quot;number of columns: {}&quot;.format(demo.shape[1])) print(&quot;number of duplicates: {}&quot;.format(demo.duplicated().sum())) print(&quot;datatypes:\\n&quot;) print(demo.dtypes) print(&quot;\\nSample:&quot;) demo.head(3) Los resultados que aparecen a continuación muestran que no hay nulos en el conjunto de datos. demo_null = demo.isnull().sum()/demo.shape[0] demo_null.to_frame().transpose() Conjunto de datos de Gini A continuación se muestran los resultados del análisis inicial: gini = pd.read_csv(&#39;datos\\\\gini.csv&#39;) print(&quot;number of rows: &quot;, gini.shape[0]) print(&quot;number of columns: {}&quot;.format(gini.shape[1])) print(&quot;number of duplicates: {}&quot;.format(gini.duplicated().sum())) print(&quot;datatypes:\\n&quot;) print(gini.dtypes) print(&quot;\\nSample:&quot;) gini.head(3) Los resultados que aparecen a continuación muestran que no hay nulos en el conjunto de datos. gini_null = gini.isnull().sum()/gini.shape[0] gini_null.to_frame().transpose() 8.6.8.2.2 Conclusiones y próximos pasos A continuación se exponen las conclusiones del análisis inicial: Dado que el Índice de Democracia de la EIU solo tiene datos para los años 2006 - 2018, se seleccionará un rango de años similar para los otros conjuntos de datos. El porcentaje de nulos en los ingresos fiscales como porcentaje del PIB fue menor entre los años 2006 y 2016. La mayoría de los valores fueron nulos para 2017. Como resultado de esto, más el resultado en 1. anterior, el período de años 2006 - 2016 será seleccionado para este conjunto de datos. Próximos pasos Los conjuntos de datos anteriores tendrán la columna del país más los años 2006 - 2016. Los conjuntos de datos serán pivotados para tener el continente, el país, el año y los conjuntos de datos anteriores. A continuación se muestra un ejemplo de cómo será el conjunto de datos combinado final: continent country year demox_eiu income_per_person invest_%_gdp tax_%_gdp gini_index Asia Afghanistan 2006 30.6 1120 23.40 6.88 36.8 Asia Afghanistan 2007 30.4 1250 19.90 5.23 36.8 Asia Afghanistan 2008 30.2 1270 18.90 6.04 36.8 8.6.8.2.3 Reorganizamos los datos EIU Democracy Index demo_last_10 = demo.iloc[:, :-2] demo_last_10.head(3) demo_last_10 = demo_last_10.melt(id_vars=[&#39;country&#39;], var_name=&#39;year&#39;, value_name=&#39;demox_eiu&#39;) demo_last_10.sort_values([&#39;country&#39;,&#39;year&#39;], inplace=True) demo_last_10.head(3) Income Per Person (GDP per Capita) income_last_10 = income.iloc[:, np.r_[:1, 207:218]] income_last_10 income_last_10 = income_last_10.melt(id_vars=[&#39;country&#39;], var_name=&#39;year&#39;, value_name=&#39;income_per_person&#39;) income_last_10.sort_values([&#39;country&#39;, &#39;year&#39;], inplace=True) income_last_10.head(3) Investment Percent of GDP invest_last_10 = invest.iloc[:, np.r_[:1, 47:58]] invest_last_10 = invest_last_10.melt(id_vars=[&#39;country&#39;], var_name=&#39;year&#39;, value_name=&#39;invest_%_gdp&#39;) invest_last_10.sort_values([&#39;country&#39;, &#39;year&#39;], inplace=True) invest_last_10.head(3) Tax Revenue Percent of GDP tax_last_10 = tax.iloc[:, np.r_[:1, 35:46]] tax_last_10 = tax_last_10.melt(id_vars=[&#39;country&#39;], var_name=&#39;year&#39;, value_name=&#39;tax_%_gdp&#39;) tax_last_10.sort_values([&#39;country&#39;, &#39;year&#39;], inplace=True) tax_last_10.head(3) Gini Index gini_last_10 = gini.iloc[:, np.r_[:1, 207:218]] gini_last_10 = gini_last_10.melt(id_vars=[&#39;country&#39;], var_name=&#39;year&#39;, value_name=&#39;gini_index&#39;) gini_last_10.sort_values(by=[&#39;country&#39;, &#39;year&#39;], inplace=True) gini_last_10.head(3) Juntamos las bases de datos combined = demo_last_10.merge(income_last_10, left_on=[&#39;country&#39;, &#39;year&#39;], right_on=[&#39;country&#39;, &#39;year&#39;]) combined = combined.merge(invest_last_10, left_on=[&#39;country&#39;, &#39;year&#39;], right_on=[&#39;country&#39;, &#39;year&#39;]) combined = combined.merge(tax_last_10, left_on=[&#39;country&#39;, &#39;year&#39;], right_on=[&#39;country&#39;, &#39;year&#39;]) combined = combined.merge(gini_last_10, left_on=[&#39;country&#39;, &#39;year&#39;], right_on=[&#39;country&#39;, &#39;year&#39;]) combined cont = pd.read_csv(&#39;datos\\\\continent_country.csv&#39;) cont Juntamos continente con país. En este paso, emparejamos cada país con su continente. Esto permitirá el análisis a nivel de continente para la detección de tendencias más amplias. combined_final = cont.merge(combined, left_on=[&#39;country&#39;], right_on=[&#39;country&#39;]) combined_final 8.6.8.2.4 Limpieza de datos A continuación se detallan las medidas adoptadas para garantizar la calidad del conjunto de datos: Missing Values: A continuación se muestra un resumen de los valores que faltan (nulos) en el conjunto de datos: combined_final.isna().sum() Una opción para tratar los valores “tax_%_gdp” que faltan sería sustituirlos por la media del país. Sin embargo, algunos de los países tienen todos nulos y otros tienen la mayoría de nulos para esta columna. Una segunda opción es eliminar las filas con nulos. En aras de la simplicidad, utilizaremos esta opción. combined_final.dropna(inplace=True) combined_final.isna().sum() Duplicates. No hay duplicados en el conjunto de datos: combined_final.duplicated().sum() 8.6.8.2.5 Estadísticas descriptivas A continuación se presentan las estadísticas descriptivas del conjunto de datos. Un examen de los valores indica que los valores mínimo, máximo y medio parecen razonables. combined_final.describe() 8.6.8.2.6 Resumen Dado que los resultados del análisis inicial indican que el conjunto de datos está limpio, no es necesario realizar más pasos de limpieza. print(&quot;number of rows: &quot;, combined_final.shape[0]) print(&quot;number of columns: {}&quot;.format(combined_final.shape[1])) print(&quot;datatypes:\\n&quot;) print(combined_final.dtypes) # Estos son los continentes incluidos en el conjunto de datos. Todos los valores parecen razonables. combined_final.continent.unique() combined_final.country.unique() 8.6.8.2.7 Guardar el conjunto de datos limpiado combined_final.to_csv(&#39;datos\\\\combined_final_last_10_years.csv&#39;, index=False) combined_final.to_excel(&#39;datos\\\\combined_final_last_10_years.xlsx&#39;, index=False) 8.6.8.3 Análisis exploratorio Pregunta de investigación 1 - ¿La desigualdad de ingresos está empeorando o mejorando en los últimos 10 años? Mejor significa que el índice de Gini está bajando. Media global de Gini por año: columns = [&#39;year&#39;, &#39;gini_index&#39;] gini = combined_final[columns] gini gini_annual_average = gini.groupby(&#39;year&#39;)[&#39;gini_index&#39;].mean() gini_annual_average As the plot below shows, the mean global gini index has been going down over the last 10 years, meaning global income inequality is improving. plt.plot(gini_annual_average.index, gini_annual_average) plt.title(&#39;Mean Global Gini Index by Year&#39;) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;Mean Global Gini Index&#39;); Índice de Gini global medio por continente: columns = [&#39;year&#39;, &#39;continent&#39;, &#39;gini_index&#39;] gini = combined_final[columns] gini gini_cont_average = gini.groupby([&#39;year&#39;, &#39;continent&#39;])[&#39;gini_index&#39;].mean() gini_cont_average El gráfico siguiente revela que, por continentes, todos disminuyeron o se mantuvieron prácticamente estables, excepto África. gini_cont_average.unstack(level=1).plot(kind=&#39;line&#39;, subplots=False, \\ title=&#39;Mean Global Gini Index by Continent by Year&#39;).\\ set_ylabel(&quot;Gini Index&quot;); columns = [&#39;year&#39;, &#39;continent&#39;, &#39;country&#39;, &#39;gini_index&#39;] gini = combined_final[columns] gini Pregunta de investigación 2 - ¿Qué 10 países tienen la menor y la mayor desigualdad de ingresos? La más baja. En general, la mayoría de los países con menor desigualdad de ingresos se encuentran en Europa. gini.groupby([&#39;country&#39;, &#39;continent&#39;])[&#39;gini_index&#39;].mean().to_frame().sort_values(by=[&#39;gini_index&#39;]).head(10) Más alto: En general, la mayoría de los países con menor desigualdad de ingresos están en África y en América. gini.groupby([&#39;country&#39;, &#39;continent&#39;])[&#39;gini_index&#39;].mean().to_frame().sort_values(by=[&#39;gini_index&#39;], ascending=False).head(10) Pregunta de investigación 3 - ¿Se asocia una mayor recaudación fiscal como % del PIB con una menor desigualdad de ingresos? La hipótesis es que los países con mayores ingresos fiscales en % del PIB están asociados a una menor desigualdad de ingresos. La hipótesis es que los mayores ingresos fiscales se distribuyen a los estratos económicos más bajos en forma de prestaciones sociales. Veamos qué muestran los datos. columns = [&#39;continent&#39;, &#39;country&#39;, &#39;year&#39;, &#39;tax_%_gdp&#39;, &#39;gini_index&#39;] tax = combined_final[columns] tax Es difícil ver una tendencia en el gráfico de dispersión de abajo: tax.plot(x=&#39;tax_%_gdp&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, title=&#39;Mean Global Gini Index by Tax % of GDP&#39;); Si se observa el logaritmo de ambos valores, se observa que la correlación entre las dos variables es esencialmente plana: no hay pruebas convincentes de que un mayor porcentaje de impuestos sobre el PIB conduzca a una menor desigualdad de ingresos. tax_plot = tax.plot(x=&#39;tax_%_gdp&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, loglog=True, \\ title=&#39;log Mean Global Gini Index by log Tax % of GDP&#39;) tax_plot.set_xlabel(&#39;log(tax % gdp)&#39;) tax_plot.set_ylabel(&#39;log(gini index)&#39;); La correlación de Pearson es ligeramente negativa: -0,08: tax_log = np.log(tax[&#39;tax_%_gdp&#39;]).to_frame() tax_log[&#39;log_gini_index&#39;] = np.log(tax[&#39;gini_index&#39;]) tax_log.corr() Pregunta de investigación 4 - ¿Se asocia una mayor renta por persona -PIB per cápita- con una menor desigualdad de ingresos? La hipótesis es que una mayor renta por persona indica que una mayor parte del PIB del país se distribuye de forma equitativa entre su población. columns = [&#39;continent&#39;, &#39;country&#39;, &#39;year&#39;, &#39;income_per_person&#39;, &#39;gini_index&#39;] income = combined_final[columns] income income.plot(x=&#39;income_per_person&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, title=&#39;Mean Gini Index by Income Per Person&#39;); income_plot = income.plot(x=&#39;income_per_person&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, loglog=True, \\ title=&#39;log Mean Gini Index by Income Per Person&#39;) income_plot.set_xlabel(&#39;log(income_per_person)&#39;) income_plot.set_ylabel(&#39;log(gini index)&#39;); En este caso, el coeficiente de correlación de Persona es -0,34, lo que indica que existe una débil correlación entre log(renta_por_persona) y log(índice_gini): income_log = np.log(income[&#39;income_per_person&#39;]).to_frame() income_log[&#39;log_gini_index&#39;] = np.log(tax[&#39;gini_index&#39;]) income_log.corr() Pregunta de investigación 5 - ¿Se asocia una mayor inversión como porcentaje del PIB con una menor desigualdad de ingresos? La hipótesis es que una mayor inversión como porcentaje del PIB indica que una mayor parte del PIB del país se invierte en mejoras de capital, lo que distribuye los beneficios de los ingresos entre un amplio segmento de la población, lo que conduce a una mayor igualdad entre su población. columns = [&#39;continent&#39;, &#39;country&#39;, &#39;year&#39;, &#39;invest_%_gdp&#39;, &#39;gini_index&#39;] invest = combined_final[columns] invest invest = invest[invest[&#39;invest_%_gdp&#39;] &gt; 0] invest.plot(x=&#39;invest_%_gdp&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, title=&#39;Mean Gini Index by Investment % GDP&#39;); invest_plot = invest.plot(x=&#39;invest_%_gdp&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, loglog=True, \\ title=&#39;log Mean Global Gini Index by log Invest % of GDP&#39;) invest_plot.set_xlabel(&#39;log(invest % gdp)&#39;) invest_plot.set_ylabel(&#39;log(gini index)&#39;); El coeficiente de Pearson de -0,03 indica que no hay correlación entre estas dos variables. invest_log = np.log(invest[&#39;invest_%_gdp&#39;]).to_frame() invest_log[&#39;log_gini_index&#39;] = np.log(tax[&#39;gini_index&#39;]) invest_log.corr() Pregunta de investigación 6 - ¿Un mayor Índice de Democracia de la EIU está asociado a una menor desigualdad de ingresos? La hipótesis es que los países con un mayor Índice de Democracia de la EIU atienden las necesidades de un segmento más amplio de la población, lo que conduce a una menor desigualdad de ingresos. columns = [&#39;continent&#39;, &#39;country&#39;, &#39;year&#39;, &#39;demox_eiu&#39;, &#39;gini_index&#39;] demo = combined_final[columns] demo demo.plot(x=&#39;demox_eiu&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, title=&#39;Mean Gini Index by EIU Democracy Index&#39;); demo_plot = demo.plot(x=&#39;demox_eiu&#39;, y=&#39;gini_index&#39;, kind=&#39;scatter&#39;, loglog=True, \\ title=&#39;log Mean Global Gini Index by log EIU Democracy Index&#39;) demo_plot.set_xlabel(&#39;log(demox_eiu)&#39;) demo_plot.set_ylabel(&#39;log(gini index)&#39;); En este caso, el coeficiente de correlación de Person es -0,2, lo que indica que existe una débil correlación entre log(demox_eiu) y log(gini_index): demo_log = np.log(demo[&#39;demox_eiu&#39;]).to_frame() demo_log[&#39;log_gini_index&#39;] = np.log(tax[&#39;gini_index&#39;]) demo_log.corr() 8.6.8.4 Conclusiones Las conclusiones de este análisis son las siguientes: Pregunta de investigación 1: ¿Ha empeorado o mejorado la desigualdad de ingresos en los últimos 10 años? Respuesta: Sí, está mejorando, pasando del 38,7 al 37,3 En cuanto a los continentes, todos han disminuido o se han mantenido estables, excepto África. Pregunta de investigación 2: ¿Qué 10 países tienen la menor y la mayor desigualdad de ingresos? Respuesta: La más baja: Eslovenia, Ucrania, República Checa, Noruega, República Eslovaca, Dinamarca, Kazajstán, Finlandia, Bielorrusia, República Kirguisa La más alta: Colombia, Lesoto, Honduras, Bolivia, República Centroafricana, Zambia, Surinam, Namibia, Botsuana, Sudáfrica Pregunta de investigación 3: ¿Se asocia una mayor recaudación fiscal como porcentaje del PIB con una menor desigualdad de ingresos? Respuesta: No Pregunta de investigación 4: - ¿Se asocia una mayor renta por persona - PIB per cápita con una menor desigualdad de ingresos? Respuesta: No, pero sí una débil correlación negativa. *Pregunta de investigación 5: - ¿Se asocia una mayor inversión como porcentaje del PIB con una menor desigualdad de ingresos? Respuesta: No *Pregunta de investigación 6: - ¿Se asocia un mayor índice de democracia de la EIU con una menor desigualdad de ingresos? Respuesta: No, pero sí una débil correlación negativa. Los resultados anteriores sugieren que hay otros factores que impulsan la reducción general de la desigualdad de ingresos. Deberían realizarse más análisis de los factores adicionales. Firm Level Innovation and CEO Compensation Utilizando los datos de panel sobre las patentes de Estados Unidos de 1992 a 2006 y los datos sobre el valor de las opciones sobre acciones concedidas a los directores ejecutivos (CEO) de las empresas estadounidenses emparejadas, este estudio tiene como objetivo investigar la relación entre el valor de las opciones sobre acciones concedidas a los CEO y la actividad de innovación a nivel de empresa, medida por el número de patentes concedidas cada año. Este estudio tiene como objetivo proporcionar evidencia empírica para la teoría de Manso(2011), que predice que el esquema de incentivos óptimos presenta una tolerancia sustancial al fracaso temprano la recompensa por el éxito a largo plazo y el compromiso con un plan de compensación a largo plazo para directores generales por parte del consejo de administración # Commented out IPython magic to ensure Python compatibility. import matplotlib.pyplot as plt import numpy as np import pandas as pd #import qeds from sklearn.linear_model import LinearRegression import seaborn as sns # %matplotlib inline Data Cleaning: Los datos primarios de este estudio proceden de la base de datos de patentes de la Oficina Nacional de Investigación Económica (NBER) y de la base de datos de compensación de ejecutivos de Wharton Research Data Service (WRDS) Compustat. Datos de la patente: Los datos de patentes patsic06_mar09_ip.csv contienen información sobre el año de solicitud y de concesión, la clase de patente estadounidense, junto con todos los IPC originales de la patente, el número de cesionario original y todos los números pdpass que resultan de la división de patentes de propiedad conjunta, la estandarización de nombres, etc. datos pats = pd.read_stata(&quot;datos\\\\pat76_06_ipc.dta&quot;) pats.head() Descripción de la variable Datos de patentes appyear registra el año de presentación y solicitud de la patente; gyear es el año de concesión de la patente. Cada patente concedida tiene un número de patente único como patent. Los datos cat, icl_class, icl_maingroup, nclass, subcat, subclass son la clase de patente o la clase de tecnología de cada patente, que son menos relevantes en mi estudio Identificador del cesionario. El assignee es el número que se asigna a una empresa al solicitar o conceder una patente, creado originalmente por la Oficina de Patentes de los Estados Unidos (U.S.P.O). El pdpass es también un número único de cesionario y se utiliza para cotejar los datos con los de Compustat. Un pdpass puede ser asignado por varios números de asignatario. Tratamiento del cesionario no emparejado Hay patentes que no coinciden con el cesionario mediante el algoritmo de coincidencia de nombres desarrollado por la base de datos de patentes NBER, estas observaciones se muestran como cesionario = 0 o pdpass = NaN. Estas observaciones se eliminan automáticamente ““” # observations with pdpass not missing pats = pats[pats[&#39;pdpass&#39;].notnull()] # count number of unique patent numbers to have patent count for a assignee in a year pat_yc1 = pats.groupby([&quot;pdpass&quot;,&quot;gyear&quot;])[&quot;patent&quot;].nunique().to_frame().reset_index()\\ .rename(columns={&quot;patent&quot;:&quot;patent_count&quot;}) pat_yc1.head() pat_yc1.info() Añadir gvkey La gvkey es el identificador de empresas del Wharton Rearch Data Service (WRDS) El archivo dynasscsv.csv mantiene un registro de todos los cambios dinámicos en la propiedad de las patentes. Los cambios en la titularidad de las patentes pueden deberse a la fusión y adquisición de empresas, o a la venta de patentes. En este estudio, sólo me centro en el cesionario original (como pdpco1 y gvkey1) de la patente y no tengo en cuenta ningún cambio dinámico en la propiedad de la patente. Esto se debe a que estoy más interesado en la relación entre la patente y la compensación a través de la decisión de los ejecutivos sobre el propio proyecto de investigación de la empresa y el proceso de exploración de nuevas ideas, no a través de la compra de patentes o la adquisición de patentes de otras empresas, ya que la compra de patentes es una decisión con recompensas conocidas dynass = pd.read_stata(&quot;datos\\\\dynass.dta&quot;) dynass = dynass[[&quot;pdpass&quot;,&quot;pdpco1&quot;,&quot;gvkey1&quot;]] dynass = dynass.rename(columns = {&quot;pdpco1&quot;:&quot;pdpco&quot;, &quot;gvkey1&quot;:&quot;gvkey&quot;}) dynass = dynass.astype({&quot;pdpass&quot;: float}) dynass.head() En muchos casos, pdpass es igual a gvkey para una empresa. Mi objetivo es que la “clave de acceso” sea el identificador único de las empresas, por lo que elimino cualquier “paso de acceso” que no coincida con una “clave de acceso” y realizo la fusión interna. Además, nos gustaría investigar cualquier cambio en el recuento de patentes después de que los ejecutivos hayan concedido alguna opción de compra de acciones; dado que el rango de datos para la opción de compra de acciones de los ejecutivos es de 1992 a 2006, sólo nos centraríamos en el subconjunto del recuento de patentes que es posterior a 1992 pgvkey = pd.merge(pat_yc1, dynass, on=&quot;pdpass&quot;,how=&quot;inner&quot;) # align with executive compensation data (1992 onwards) pgvkey1 = pgvkey.loc[pgvkey[&quot;gyear&quot;]&gt;=1992] pgvkey1.head() Tratamiento del recuento de patentes del año que falta Para cada empresa (identificada por gvkey), la razón del valor que falta en el recuento de patentes puede ser 1) que haya patentes concedidas en ese año, pero que no coincidan con el pdpass y, por tanto, no con la empresa en la base de datos de patentes del NBER; 2) que simplemente no haya ninguna patente concedida en ese año. En consecuencia, la regla general para tratar el valor faltante del recuento de patentes cada año es simplemente sustituirlo por 0 en este estudio pgvkey2 = pgvkey1[[&quot;gvkey&quot;,&quot;gyear&quot;,&quot;patent_count&quot;]] pgvkey3 = pgvkey2.pivot_table(index=&quot;gvkey&quot;, columns =&quot;gyear&quot;, values=&quot;patent_count&quot;, aggfunc= &quot;sum&quot;) pgvkey3.head() pgvkey3 = pgvkey3.stack(&quot;gyear&quot;,dropna=False).to_frame().reset_index()\\ .rename(columns={0: &#39;patent_count&#39;}) pgvkey3.head() pgvkey3[&quot;patent_count&quot;] = pgvkey3[&quot;patent_count&quot;].fillna(0) pgvkey3.head() pgvkey3.info() import numpy as np import pandas as pd #import geopandas as gpd import seaborn as sns import matplotlib.pyplot as plt from scipy.stats import pearsonr from sklearn.model_selection import LeaveOneOut import statsmodels.api as sm import sklearn.metrics as sklm pd.options.mode.chained_assignment = None pd.options.display.float_format = &#39;{:.4f}&#39;.format data = pd.read_csv(&#39;datos\\\\final_demographics_data.csv&#39;) Preprocessing data[&#39;log GDP per worker&#39;] = np.log(data[&#39;GDP (constant 2010 US$)&#39;]/data[&#39;Labor force, total&#39;]) data[&#39;Urban population (%)&#39;] = (data[&#39;Urban population&#39;]/data[&#39;Population, total&#39;]) * 100 data[&#39;log population&#39;] = np.log(data[&#39;Population, total&#39;]) data = data[data[&#39;log GDP per worker&#39;].notna()] gdp_per_worker = data[&#39;GDP (constant 2010 US$)&#39;]/data[&#39;Labor force, total&#39;] fig = plt.subplots(figsize=(15,15)) plt.title(&#39;Heatmap of Missing Values&#39;) sns.heatmap(data.isna()) plt.show() Sólo consideré las características con menos de 10 valores perdidos. Rellené el resto de los valores que faltaban con los últimos datos disponibles en https://data.worldbank.org/. Los valores de Canadá y Etiopía para la característica “Crédito nacional al sector privado por parte de los bancos (% del PIB)” son, con mucho, los más antiguos. Son de 2008. # only considerng features with less than 10 missing values and a sensible relationship with the producitivity features = [&#39;country&#39;, &#39;log GDP per worker&#39;, &#39;Urban population (%)&#39;, &#39;Domestic credit to private sector by banks (% of GDP)&#39;, &#39;Fixed broadband subscriptions (per 100 people)&#39;, &#39;Age dependency ratio, old (% of working-age population)&#39;, &#39;Age dependency ratio, young (% of working-age population)&#39;, &#39;Unemployment, total (% of total labor force) (modeled ILO estimate)&#39;, &#39;Final consumption expenditure (% of GDP)&#39;] # Filling nan with last available data from https://data.worldbank.org/ # Domestic credit to private sector by banks (% of GDP) data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Argentina&#39;] = 15.4 # 2017 data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Ethiopia&#39;] = 17.6 # 2008 data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Canada&#39;] = 124.1 # 2008 data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Iraq&#39;] = 9.2 # 2018 data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Saudi Arabia&#39;] = 54.0 # 2018 data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Switzerland&#39;] = 168.5 # 2016 data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Viet Nam&#39;] = 137.9 #2019 # fixed broadband data[&#39;Fixed broadband subscriptions (per 100 people)&#39;].iloc[data[&#39;country&#39;] == &#39;Peru&#39;] = 7.93 # 2018 data[&#39;Fixed broadband subscriptions (per 100 people)&#39;].iloc[data[&#39;country&#39;] == &#39;Philippines&#39;] = 5.48 # 2019 data[&#39;Fixed broadband subscriptions (per 100 people)&#39;].iloc[data[&#39;country&#39;] == &#39;Ethiopia&#39;] = 0.06 # 2017 data[&#39;Fixed broadband subscriptions (per 100 people)&#39;].iloc[data[&#39;country&#39;] == &#39;Nepal&#39;] = 2.82 # 2018 # final consumption data[&#39;Final consumption expenditure (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;China&#39;] = 56.0 # 2019 data[&#39;Final consumption expenditure (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Ghana&#39;] = 77.6 # 2019 data[&#39;Final consumption expenditure (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Iraq&#39;] = 65.4 # 2019 data[&#39;Final consumption expenditure (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Kazakhstan&#39;] = 61.4 # 2019 data[&#39;Final consumption expenditure (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Kenya&#39;] = 95.5 # 2019 data[&#39;Final consumption expenditure (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;Tunisia&#39;] = 92.9 # 2019 data[&#39;Final consumption expenditure (% of GDP)&#39;].iloc[data[&#39;country&#39;] == &#39;United States of America&#39;] = 81.8 # 2020 data = data[features] data = data.dropna() data Pib por trabajador: El PIB per cápita es una medida útil cuando queremos evaluar la riqueza de un país. Sin embargo, yo quería identificar predictores de la productividad de un país. Una medida mejor sería en este caso el PIB por trabajador, ya que sólo tiene en cuenta a la población activa. La medida ideal sería el PIB por horas trabajadas, pero este conjunto de datos no incluye la información necesaria para construir esta característica y el PIB por trabajador también es ya suficiente. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.histplot(gdp_per_worker, ax=ax1, color=&#39;salmon&#39;) ax1.set_title(&#39;GDP per worker&#39;) sns.histplot(data[&#39;log GDP per worker&#39;], ax=ax2, color=&#39;salmon&#39;) ax2.set_title(&#39;Log GDP per worker&#39;) plt.show() Urban Population: La característica “Población urbana (%)” describe qué porcentaje de la población vive en las ciudades. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.regplot(ax=ax1, x=&#39;Urban population (%)&#39;, y=&#39;log GDP per worker&#39;, data=data, color = &#39;salmon&#39;) sns.histplot(data[&#39;Urban population (%)&#39;], ax=ax2, color=&#39;salmon&#39;) plt.show() print(&#39;Pearson-r and P-Value: &#39; + str(pearsonr(data[&#39;Urban population (%)&#39;], data[&#39;log GDP per worker&#39;]))) Domestic credit to private sector by banks (% of GDP): Esta característica describe la cantidad de recursos financieros proporcionados al sector privado por los bancos en relación con el PIB. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.regplot(ax=ax1, x=&#39;Domestic credit to private sector by banks (% of GDP)&#39;, y=&#39;log GDP per worker&#39;, data=data, color = &#39;salmon&#39;) sns.histplot(data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;], ax=ax2, color=&#39;salmon&#39;) plt.show() print(&#39;Pearson-r and P-Value: &#39; + str(pearsonr(data[&#39;Domestic credit to private sector by banks (% of GDP)&#39;], data[&#39;log GDP per worker&#39;]))) Fixed broadband subscriptions (per 100 people): Suscripciones para el acceso de alta velocidad a la Internet pública. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.regplot(ax=ax1, x=&#39;Fixed broadband subscriptions (per 100 people)&#39;, y=&#39;log GDP per worker&#39;, data=data, color = &#39;salmon&#39;) sns.histplot(data[&#39;Fixed broadband subscriptions (per 100 people)&#39;], ax=ax2, color=&#39;salmon&#39;) plt.show() print(&#39;Pearson-r and P-Value: &#39; + str(pearsonr(data[&#39;Fixed broadband subscriptions (per 100 people)&#39;], data[&#39;log GDP per worker&#39;]))) Age dependency ratio, old (% of working-age population): Número de personas mayores de 64 años en relación con el número de personas entre 15 y 64 años. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.regplot(ax=ax1, x=&#39;Age dependency ratio, old (% of working-age population)&#39;, y=&#39;log GDP per worker&#39;, data=data, color = &#39;salmon&#39;) sns.histplot(data[&#39;Age dependency ratio, old (% of working-age population)&#39;], ax=ax2, color=&#39;salmon&#39;) plt.show() print(&#39;Pearson-r and P-Value: &#39; + str(pearsonr(data[&#39;Age dependency ratio, old (% of working-age population)&#39;], data[&#39;log GDP per worker&#39;]))) Age dependency ratio, young (% of working-age population): Number of people who are younger than 15 in relation to the number of people between 15 and 64. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.regplot(ax=ax1, x=&#39;Age dependency ratio, young (% of working-age population)&#39;, y=&#39;log GDP per worker&#39;, data=data, color = &#39;salmon&#39;) sns.histplot(data[&#39;Age dependency ratio, young (% of working-age population)&#39;], ax=ax2, color=&#39;salmon&#39;) plt.show() print(&#39;Pearson-r and P-Value: &#39; + str(pearsonr(data[&#39;Age dependency ratio, young (% of working-age population)&#39;], data[&#39;log GDP per worker&#39;]))) Unemployment, total (% of total labor force) (modeled ILO estimate): Unemployment estimate modeled by the Iternational Labor Organization. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.regplot(ax=ax1, x=&#39;Unemployment, total (% of total labor force) (modeled ILO estimate)&#39;, y=&#39;log GDP per worker&#39;, data=data, color = &#39;salmon&#39;) sns.histplot(data[&#39;Unemployment, total (% of total labor force) (modeled ILO estimate)&#39;], ax=ax2, color=&#39;salmon&#39;) plt.show() print(&#39;Pearson-r and P-Value: &#39; + str(pearsonr(data[&#39;Unemployment, total (% of total labor force) (modeled ILO estimate)&#39;], data[&#39;log GDP per worker&#39;]))) Final consumption expenditure (% of GDP): Final household and government consumption expenditure relative to the GDP. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6)) sns.regplot(ax=ax1, x=&#39;Final consumption expenditure (% of GDP)&#39;, y=&#39;log GDP per worker&#39;, data=data, color = &#39;salmon&#39;) sns.histplot(data[&#39;Final consumption expenditure (% of GDP)&#39;], ax=ax2, color=&#39;salmon&#39;) plt.show() print(&#39;Pearson-r and P-Value: &#39; + str(pearsonr(data[&#39;Final consumption expenditure (% of GDP)&#39;], data[&#39;log GDP per worker&#39;]))) Análisis de regresión (intro) data.corr().style.background_gradient(cmap=&#39;coolwarm&#39;) Existe una fuerte correlación entre “Tasa de dependencia de la edad, ancianos (% de la población en edad de trabajar)” y “Abonados a la banda ancha fija (por cada 100 personas)”, ya que el coeficiente de correlación de Pearson es de 0,897830. Esto puede causar problemas a la hora de interpretar el modelo de regresión. Intento resolver este problema construyendo tres modelos. El primero tendrá en cuenta todas las características, para el segundo modelo dejaré fuera la “Tasa de dependencia de la edad, ancianos (% de la población en edad de trabajar)” y el tercer modelo no incluirá las “Suscripciones a la banda ancha fija (por cada 100 personas)”. X = data.drop([&#39;country&#39;], axis = 1) y = X.pop(&#39;log GDP per worker&#39;) # columns for model 2 model2_l = [&#39;const&#39;,&#39;Urban population (%)&#39;, &#39;Domestic credit to private sector by banks (% of GDP)&#39;, &#39;Fixed broadband subscriptions (per 100 people)&#39;, &#39;Final consumption expenditure (% of GDP)&#39;, &#39;Age dependency ratio, young (% of working-age population)&#39;, &#39;Unemployment, total (% of total labor force) (modeled ILO estimate)&#39;] # columns for model 3 model3_l = [&#39;const&#39;, &#39;Urban population (%)&#39;, &#39;Domestic credit to private sector by banks (% of GDP)&#39;, &#39;Age dependency ratio, old (% of working-age population)&#39;, &#39;Final consumption expenditure (% of GDP)&#39;, &#39;Age dependency ratio, young (% of working-age population)&#39;, &#39;Unemployment, total (% of total labor force) (modeled ILO estimate)&#39;] X_lin = sm.tools.add_constant(X) model1 = sm.OLS(y, X_lin.astype(&#39;float&#39;)).fit() model2 = sm.OLS(y, X_lin[model2_l].astype(&#39;float&#39;)).fit() model3 = sm.OLS(y, X_lin[model3_l].astype(&#39;float&#39;)).fit() def return_results(model, model_list): return [model.pvalues[i] for i in model_list] results = pd.DataFrame(index=X_lin.columns) results[&#39;Model 1 Coeff.&#39;] = pd.Series(model1.params) results[&#39;Model 1 P-Values&#39;] = pd.Series(return_results(model1, X_lin.columns), index = X_lin.columns) results[&#39;Model 2 Coeff.&#39;] = pd.Series(model2.params) results[&#39;Model 2 P-Values&#39;] = pd.Series(return_results(model2, model2_l), index = model2_l) results[&#39;Model 3 Coeff.&#39;] = pd.Series(model3.params) results[&#39;Model 3 P-Values&#39;] = pd.Series(return_results(model3, model3_l), index = model3_l) results = results.fillna(&#39;---&#39;) results El coeficiente de “Población urbana (%)” es positivo y el efecto es significativo en los tres modelos. El efecto de “Crédito interno al sector privado por parte de los bancos (% del PIB)” no es significativo en ninguno de los modelos. “Abonados a la banda ancha fija (por cada 100 personas)” sólo se incluyó en los modelos 1 y 2 y el efecto es positivo y significativo en ambos. “Tasa de dependencia de la edad, mayores (% de la población en edad de trabajar)” tiene un efecto positivo y significativo en el modelo 1 y el modelo 3, mientras que “Tasa de dependencia de la edad, jóvenes (% de la población en edad de trabajar)” no tiene un efecto significativo en ningún modelo. Lo mismo puede decirse de “Desempleo, total (% de la población activa total) (estimación modelada de la OIT)”. Sin embargo, el “Gasto en consumo final (% del PIB)” muestra un efecto negativo y altamente significativo en todos los modelos. La falta de significación de algunas de las variables puede deberse a la multicolinealidad, especialmente en lo que respecta a “Dependencia de la edad, jóvenes”. Por último, examinaremos la eficacia de cada modelo para predecir el PIB por trabajador de un país. El pequeño tamaño de la muestra no es ideal para construir un modelo de predicción, pero permite la validación cruzada “Leave-one-out”, que llevaría mucho tiempo en el caso de grandes conjuntos de datos. loo = LeaveOneOut() m1_score = [] m2_score = [] m3_score = [] for train_index, test_index in loo.split(X_lin, gdp_per_worker[X_lin.index]): X_train, X_test = X_lin.iloc[train_index], X_lin.iloc[test_index] y_train, y_test = gdp_per_worker.iloc[train_index], gdp_per_worker.iloc[test_index] model1 = sm.OLS(list(y_train), X_train.astype(&#39;float&#39;)).fit() model2 = sm.OLS(list(y_train), X_train[model2_l].astype(&#39;float&#39;)).fit() model3 = sm.OLS(list(y_train), X_train[model3_l].astype(&#39;float&#39;)).fit() m1_score.append(np.sqrt(sklm.mean_squared_error(y_test, model1.predict(X_test.astype(&#39;float&#39;))))) m2_score.append(np.sqrt(sklm.mean_squared_error(y_test, model2.predict(X_test[model2_l].astype(&#39;float&#39;))))) m3_score.append(np.sqrt(sklm.mean_squared_error(y_test, model3.predict(X_test[model3_l].astype(&#39;float&#39;))))) print(&#39;GDP per worker std.: &#39; + str(np.std(gdp_per_worker[X_lin.index]))) print(&#39;Model 1 LOOCV Error (RMSE): &#39; + str(np.mean(m1_score))) print(&#39;Model 2 LOOCV Error (RMSE): &#39; + str(np.mean(m2_score))) print(&#39;Model 3 LOOCV Error (RMSE): &#39; + str(np.mean(m3_score))) 8.7 Análisis de datos (Parte II) 8.7.1 Optimización con Scipy import numpy as np from scipy import linalg arr = np.array([[1, 2], [3, 4]]) linalg. arr arr = np.array([[3, 2], [6, 4]]) linalg.det(arr) #2. Inversa de una matriz linalg.inv(arr) # calcular la inversa de una matriz singular (su determinante es cero) generará un error arr = np.array([[1, 2], [3, 4]]) linalg.inv(arr) from scipy import optimize import matplotlib.pyplot as plt def f(x): return x**2 + 10*np.sin(x) x = np.arange(-10, 10, 0.1) plt.plot(x, f(x)) plt.show() Mínimo global: corresponde al valor mínimo en el dominio de la función. Mínimo local: corresponde al valor mínimo en una región particular de la curva. En el ejemplo tenemos un mínimo local y un mínimo global. Mediante un algoritmo de scipy podemos encontrar estos valores, por ejemplo podemos usar la función fmin_bfgs (método BFGS) para encontrar el mínimo. # Usamos el código optmize, la función fmin_bfgs # La sintaxis es fmin_bfgs(función a optimizar, valor inicial) opt = optimize.fmin_bfgs(f, 0) print(opt) #El resultado nos entrega: #1) Valor de la función evaluada en el punto mínimo #2) Número de iteraciones que se demoró en encontrar el mínimo #3) Número de evaluaciones de la función #4) Número de evaluaciones del gradiente #5) Parámetro que minimiza la función Por la naturaleza del método podemos quedar en el mínimo local si no partimos del “lado correcto” optimize.fmin_bfgs(f, 3) Podemos encontrar un mínimo local (dentro de un intervalo), usando fminbound xmin_local = optimize.fminbound(f, 0, 10) print(xmin_local) root = optimize.fsolve(f, 1) # our initial guess is 1 root Si la función tiene más de una raíz, tenemos que cambiar el valor inicial para partir de un punto que nos lleve a la segunda (tercera…) solución root2 = optimize.fsolve(f, -2.5) root2 fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x, f(x), &#39;b-&#39;, label=&quot;f(x)&quot;) xmins = np.array([xmin_global[0], xmin_local]) ax.plot(xmins, f(xmins), &#39;go&#39;, label=&quot;Minima&quot;) roots = np.array([root, root2]) ax.plot(roots, f(roots), &#39;kv&#39;, label=&quot;Roots&quot;) ax.legend() ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;f(x)&#39;) Supongamos que f tiene un poco de ruido xdata = np.linspace(-10, 10, num=20) ydata = f(xdata) + np.random.randn(xdata.size) plt.plot(x, f(x), &#39;b-&#39;, label=&quot;f(x)&quot;) plt.plot(xdata, ydata, color=&quot;red&quot;) La forma funcional original es \\(f(x)=x2+10∗sin(x)\\) Entonces podemos generalizar mediante \\[f(x,a,b)=a∗x^2+b∗sin(x)\\] Mediante curve_fit() podemos encontrar los valores de a y b. def f2(x,a ,b): return a*x**2 + b**np.sin(x) #Valor inicial guess = [2,2] # curve_fit(f, xdata, ydata, valor inicial) params, params_covariance = optimize.curve_fit(f2, xdata, ydata, guess) # retorna: #1) Valores óptimos que minimizan f(xdata) - ydata #2) Covarianza estimada de valores óptimos params fig = plt.figure() ax = fig.add_subplot(111) ax.plot(x, f(x), &#39;b-&#39;, label=&quot;f(x)&quot;) ax.plot(x, f2(x, *params), &#39;r--&#39;, label=&quot;Curve fit result&quot;) xmins = np.array([xmin_global[0], xmin_local]) ax.plot(xmins, f(xmins), &#39;go&#39;, label=&quot;Minima&quot;) roots = np.array([root, root2]) ax.plot(roots, f(roots), &#39;kv&#39;, label=&quot;Roots&quot;) ax.legend() ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;f(x)&#39;) 8.7.1.1 Aplicación de predicción A = [[1, 2], [3, 4]] B = [[11, 12], [13, 14]] print(A) print(B) A_np = np.array(A) B_np = np.array(B) A_np * B_np A_np@B_np import numpy as np import matplotlib.pyplot as plt np.random.seed(0) \\(y_i = 0.5 + x_i - 0.1x^2 + \\epsilon_i\\) T = 2000 x = np.random.normal(5,2,T) ## media, sd, largo de serie ϵ = np.random.normal(0,1,T) y = 0.5 + x - 0.1*x**2+ϵ y plt.scatter(x,y,s=5,color=&#39;blue&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) 8.7.1.2 Polonomio de orden 1 X_1 = np.column_stack([np.ones_like(x),x]) X_1[0:3,:] np.linalg.lstq -&gt; Return the least-squares solution to a linear matrix equation. https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html β_1 = np.linalg.lstsq(X_1,y,rcond=None)[0] print(β_1) plt.scatter(x,y,s=5,color=&#39;blue&#39;) plt.scatter(x,X_1@β_1,s=5,color=&#39;red&#39;) 8.7.1.3 Polinomio de orden 2 X_2 = np.column_stack([np.ones_like(x),x,x**2]) X_2[0:3,:] β_2 = np.linalg.lstsq(X_2,y,rcond=None)[0] print(β_2) plt.scatter(x,y,s=5,color=&#39;blue&#39;) plt.scatter(x,X_2@β_2,s=5,color=&#39;black&#39;) K =10 X_K = np.column_stack([x**k for k in range(K+1)]) #X_K[0:3,:] β_K = np.linalg.lstsq(X_K,y,rcond=None)[0] print(β_K) plt.scatter(x,y,s=5,color=&#39;blue&#39;) plt.scatter(x,X_K@β_K,s=5,color=&#39;orange&#39;) ECM_1 = np.mean((y-X_1@β_1)**2) ECM_2 = np.mean((y-X_2@β_2)**2) ECM_K = np.mean((y-X_K@β_K)**2) print(ECM_1,ECM_2,ECM_K) 8.7.2 Modelo de regresión lineal 8.7.2.1 Introducción import numpy as np import pandas as pd import os import matplotlib import matplotlib.pyplot as plt import scipy.stats as stats matplotlib.style.use(&#39;ggplot&#39;) mtcars.head() # Load mtcars data set mtcars = pd.read_csv(&quot;mtcars.csv&quot;) mtcars.plot(kind=&quot;scatter&quot;, x=&quot;wt&quot;, y=&quot;mpg&quot;, figsize=(9,9), color=&quot;black&quot;); La biblioteca scikit-learn de Python contiene una amplia gama de funciones para el modelado predictivo. Vamos a cargar su función de entrenamiento de regresión lineal y a ajustar una línea a los datos de mtcars: from sklearn import linear_model # Modelo inicial regression_model = linear_model.LinearRegression() # Entrenar al modelo regression_model.fit(X = pd.DataFrame(mtcars[&quot;wt&quot;]), y = mtcars[&quot;mpg&quot;]) # Chequear coeficientes print(regression_model.intercept_) print(regression_model.coef_) Podemos tener una idea de la parte de la varianza de la variable de respuesta que explica el modelo utilizando la función model.score(): regression_model.score(X = pd.DataFrame(mtcars[&quot;wt&quot;]), y = mtcars[&quot;mpg&quot;]) El resultado de la función de puntuación para la regresión lineal es el “R-cuadrado”, un valor que va de 0 a 1 y que describe la proporción de la varianza de la variable de respuesta que explica el modelo. En este caso, el peso del coche explica aproximadamente el 75% de la varianza en mpg. La medida R-cuadrado se basa en los residuos: las diferencias entre lo que el modelo predice para cada punto de datos y el valor real de cada punto de datos. Podemos extraer los residuos del modelo haciendo una predicción con el modelo sobre los datos y luego restando el valor real de cada predicción: train_prediction = regression_model.predict(X = pd.DataFrame(mtcars[&quot;wt&quot;])) # Actual - prediction = residuals residuals = mtcars[&quot;mpg&quot;] - train_prediction residuals.describe() SSResiduals = (residuals**2).sum() SSTotal = ((mtcars[&quot;mpg&quot;] - mtcars[&quot;mpg&quot;].mean())**2).sum() # R-squared 1 - (SSResiduals/SSTotal) Ahora que tenemos un modelo lineal, vamos a trazar la línea a la que se ajusta en nuestro gráfico de dispersión para tener una idea de lo bien que se ajusta a los datos: mtcars.plot(kind=&quot;scatter&quot;, x=&quot;wt&quot;, y=&quot;mpg&quot;, figsize=(9,9), color=&quot;black&quot;, xlim = (0,7)) # Graficamos plt.plot(mtcars[&quot;wt&quot;], # Variables explicativas train_prediction, # Variables predichas color=&quot;blue&quot;); La línea de regresión parece un ajuste razonable y se ajusta a nuestra intuición: a medida que aumenta el peso del coche, cabe esperar que el consumo de combustible disminuya. Los valores atípicos pueden tener una gran influencia en los modelos de regresión lineal: como la regresión consiste en minimizar los residuos al cuadrado, los residuos grandes tienen una influencia desproporcionada en el modelo. Trazar el resultado nos ayuda a detectar los valores atípicos influyentes. En este caso no parece haber ningún valor atípico influyente. Añadamos un valor atípico -un coche superpesado de bajo consumo- y tracemos un nuevo modelo de regresión: mtcars_subset = mtcars[[&quot;mpg&quot;,&quot;wt&quot;]] super_car = pd.DataFrame({&quot;mpg&quot;:50,&quot;wt&quot;:10}, index=[&quot;super&quot;]) new_cars = mtcars_subset.append(super_car) # Initialize model regression_model = linear_model.LinearRegression() # Train the model using the new_cars data regression_model.fit(X = pd.DataFrame(new_cars[&quot;wt&quot;]), y = new_cars[&quot;mpg&quot;]) train_prediction2 = regression_model.predict(X = pd.DataFrame(new_cars[&quot;wt&quot;])) # Plot the new model new_cars.plot(kind=&quot;scatter&quot;, x=&quot;wt&quot;, y=&quot;mpg&quot;, figsize=(9,9), color=&quot;black&quot;, xlim=(1,11), ylim=(10,52)) # Plot regression line plt.plot(new_cars[&quot;wt&quot;], # Explanatory variable train_prediction2, # Predicted values color=&quot;blue&quot;); Aunque se trata de un caso extremo y artificial, el gráfico anterior ilustra la influencia que puede tener un solo valor atípico en un modelo de regresión lineal. En un modelo de regresión lineal que se comporte bien, nos gustaría que los residuos se distribuyeran de forma aproximadamente normal. Es decir, nos gustaría que el error se distribuyera de forma más o menos uniforme por encima y por debajo de la línea de regresión. Podemos investigar la normalidad de los residuos con un gráfico Q-Q (cuantil-cuantil). Haga un qqplot pasando los residuos a la función stats.probplot() en la biblioteca scipy.stats: plt.figure(figsize=(9,9)) stats.probplot(residuals, dist=&quot;norm&quot;, plot=plt); Cuando los residuos se distribuyen normalmente, tienden a situarse a lo largo de la línea recta en el gráfico Q-Q. En este caso, los residuos parecen seguir un patrón ligeramente no lineal: los residuos se inclinan un poco lejos de la línea de normalidad en cada extremo. Esto es una indicación de que una simple línea recta podría no ser suficiente para describir completamente la relación entre el peso y el mpg. Después de hacer las predicciones del modelo, es útil tener algún tipo de métrica para evaluar lo bien que funcionó el modelo. El R-cuadrado ajustado es una medida útil, pero sólo se aplica al modelo de regresión en sí: nos gustaría tener alguna métrica de evaluación universal que nos permita comparar el rendimiento de diferentes tipos de modelos. El error medio cuadrático (RMSE) es una métrica de evaluación común para las predicciones con números reales. El error medio cuadrático es la raíz cuadrada de la media del error cuadrático (residuos). Si recuerda, escribimos una función para calcular el RMSE en la lección 12: def rmse(predicted, targets): &quot;&quot;&quot; Computes root mean squared error of two numpy ndarrays Args: predicted: an ndarray of predictions targets: an ndarray of target values Returns: The root mean squared error as a float &quot;&quot;&quot; return (np.sqrt(np.mean((targets-predicted)**2))) rmse(train_prediction2, new_cars[&quot;mpg&quot;]) rmse(train_prediction, mtcars[&quot;mpg&quot;]) rmse(train_prediction2, new_cars[&quot;mpg&quot;]) En lugar de definir su propia función RMSE, puede utilizar la función de error cuadrático medio de la biblioteca scikit-learn y tomar la raíz cuadrada del resultado: from sklearn.metrics import mean_squared_error RMSE = mean_squared_error(train_prediction, mtcars[&quot;mpg&quot;])**0.5 RMSE 8.7.2.2 Agregando polinomios # Iniciamos el modelo poly_model = linear_model.LinearRegression() # Data frmame predictors = pd.DataFrame([mtcars[&quot;wt&quot;], # Include weight mtcars[&quot;wt&quot;]**2]).T # Include weight squared # &quot;Entrenar&quot; el modelo poly_model.fit(X = predictors, y = mtcars[&quot;mpg&quot;]) # Chequear print(&quot;Model intercept&quot;) print(poly_model.intercept_) print(&quot;Model Coefficients&quot;) print(poly_model.coef_) print(&quot;Model Accuracy:&quot;) print(poly_model.score(X = predictors, y = mtcars[&quot;mpg&quot;])) Vamos a trazar la línea curva definida por el nuevo modelo para ver si el ajuste es mejor que el anterior. Para empezar, vamos a crear una función que toma una matriz de valores x, coeficientes del modelo y un término de intercepción y devuelve los valores x y los valores y ajustados correspondientes a esos valores x. # Plot the curve from 1.5 to 5.5 poly_line_range = np.arange(1.5, 5.5, 0.1) # Get first and second order predictors from range poly_predictors = pd.DataFrame([poly_line_range, poly_line_range**2]).T # Get corresponding y values from the model y_values = poly_model.predict(X = poly_predictors) mtcars.plot(kind=&quot;scatter&quot;, x=&quot;wt&quot;, y=&quot;mpg&quot;, figsize=(9,9), color=&quot;black&quot;, xlim = (0,7)) # Plot curve line plt.plot(poly_line_range, # X-axis range y_values, # Predicted values color=&quot;blue&quot;); La función cuadrática parece ajustarse a los datos un poco mejor que la lineal. Investiguemos más a fondo utilizando el nuevo modelo para hacer predicciones sobre los datos originales y comprobemos el error medio cuadrático: preds = poly_model.predict(X=predictors) rmse(preds , mtcars[&quot;mpg&quot;]) Como el RMSE del modelo cuadrático es menor que el anterior y el R-cuadrado ajustado es mayor, probablemente sea un modelo mejor. Sin embargo, tenemos que tener cuidado con el sobreajuste de los datos de entrenamiento (“overfitting”). La sobreadaptación describe una situación en la que nuestro modelo se ajusta demasiado a los datos que utilizamos para crearlo (datos de entrenamiento), lo que provoca una mala generalización a los nuevos datos. Por eso, generalmente no queremos utilizar los datos de entrenamiento para evaluar un modelo: nos da una evaluación sesgada, normalmente demasiado optimista. Uno de los puntos fuertes de la regresión lineal de primer y segundo orden es que son tan simples que es poco probable que sobreajusten los datos. Cuanto más complejo sea el modelo que creemos y más libertad tenga para ajustarse a los datos de entrenamiento, mayor será el riesgo de sobreajuste. Por ejemplo, podríamos seguir incluyendo más términos polinómicos en nuestro modelo de regresión para ajustarnos más a los datos de entrenamiento y conseguir puntuaciones de RMSE más bajas con respecto al conjunto de entrenamiento, pero es casi seguro que esto no se generalizaría bien a los nuevos datos. Ilustremos este punto ajustando un modelo de 10º orden a los datos de mtcars: # Initialize model poly_model = linear_model.LinearRegression() # Make a DataFrame of predictor variables predictors = pd.DataFrame([mtcars[&quot;wt&quot;], mtcars[&quot;wt&quot;]**2, mtcars[&quot;wt&quot;]**3, mtcars[&quot;wt&quot;]**4, mtcars[&quot;wt&quot;]**5, mtcars[&quot;wt&quot;]**6, mtcars[&quot;wt&quot;]**7, mtcars[&quot;wt&quot;]**8, mtcars[&quot;wt&quot;]**9, mtcars[&quot;wt&quot;]**10]).T # Train the model using the new_cars data poly_model.fit(X = predictors, y = mtcars[&quot;mpg&quot;]) # Check trained model y-intercept print(&quot;Model intercept&quot;) print(poly_model.intercept_) # Check trained model coefficients (scaling factor given to &quot;wt&quot;) print(&quot;Model Coefficients&quot;) print(poly_model.coef_) # Check R-squared poly_model.score(X = predictors, y = mtcars[&quot;mpg&quot;]) Obsérvese que la puntuación R-cuadrado ha aumentado sustancialmente con respecto a nuestro modelo cuadrático. Vamos a trazar la línea de mejor ajuste para investigar lo que hace el modelo: p_range = np.arange(1.5, 5.45, 0.01) poly_predictors = pd.DataFrame([p_range, p_range**2, p_range**3, p_range**4, p_range**5, p_range**6, p_range**7, p_range**8, p_range**9, p_range**10]).T # Get corresponding y values from the model y_values = poly_model.predict(X = poly_predictors) mtcars.plot(kind=&quot;scatter&quot;, x=&quot;wt&quot;, y=&quot;mpg&quot;, figsize=(9,9), color=&quot;black&quot;, xlim = (0,7)) # Plot curve line plt.plot(p_range, # X-axis range y_values, # Predicted values color=&quot;blue&quot;); Observe cómo el modelo polinómico de 10º orden se curva de forma salvaje en algunos lugares para ajustarse a los datos de entrenamiento. Aunque este modelo se ajusta mejor a los datos de entrenamiento, es casi seguro que no se generalizará bien a los nuevos datos, ya que conduce a predicciones absurdas, como que un coche tiene menos de 0 mpg si pesa 5.000 libras. 8.7.2.3 Agregar más variables Cuando se enfrenta a una tarea de modelado predictivo, a menudo tendrá varias variables en sus datos que pueden ayudar a explicar la variación en la variable de respuesta. Puede incluir más variables explicativas en un modelo de regresión lineal incluyendo más columnas en el marco de datos que pasa a la función de entrenamiento del modelo. Hagamos un nuevo modelo que añada la variable de potencia a nuestro modelo original:. # Initialize model multi_reg_model = linear_model.LinearRegression() # Train the model using the mtcars data multi_reg_model.fit(X = mtcars.loc[:,[&quot;wt&quot;,&quot;hp&quot;]], y = mtcars[&quot;mpg&quot;]) # Check trained model y-intercept print(multi_reg_model.intercept_) # Check trained model coefficients (scaling factor given to &quot;wt&quot;) print(multi_reg_model.coef_) # Check R-squared multi_reg_model.score(X = mtcars.loc[:,[&quot;wt&quot;,&quot;hp&quot;]], y = mtcars[&quot;mpg&quot;]) La mejora de la puntuación R-cuadrado sugiere que la potencia tiene una relación lineal con el mpg. Vamos a investigar con un gráfico: mtcars.plot(kind=&quot;scatter&quot;, x=&quot;hp&quot;, y=&quot;mpg&quot;, figsize=(9,9), color=&quot;black&quot;); Aunque las millas por galón tienden a disminuir con los caballos, la relación parece más curvada que lineal, por lo que añadir términos polinómicos a nuestro modelo de regresión múltiple podría producir un mejor ajuste: # Initialize model multi_reg_model = linear_model.LinearRegression() # Include squared terms poly_predictors = pd.DataFrame([mtcars[&quot;wt&quot;], mtcars[&quot;hp&quot;], mtcars[&quot;wt&quot;]**2, mtcars[&quot;hp&quot;]**2]).T # Train the model using the mtcars data multi_reg_model.fit(X = poly_predictors, y = mtcars[&quot;mpg&quot;]) # Check R-squared print(&quot;R-Squared&quot;) print( multi_reg_model.score(X = poly_predictors , y = mtcars[&quot;mpg&quot;]) ) # Check RMSE print(&quot;RMSE&quot;) print(rmse(multi_reg_model.predict(poly_predictors),mtcars[&quot;mpg&quot;])) El nuevo R-cuadrado y el menor RMSE sugieren que éste es un modelo mejor que cualquiera de los que hicimos anteriormente y no nos preocuparía demasiado el sobreajuste, ya que sólo incluye 2 variables y 2 términos al cuadrado. Hay que tener en cuenta que cuando se trabaja con modelos multidimensionales, resulta difícil visualizar los resultados, por lo que se depende en gran medida de los resultados numéricos. Podríamos seguir añadiendo más variables explicativas para intentar mejorar el modelo. Añadir variables que tienen poca relación con la respuesta o incluir variables demasiado relacionadas entre sí puede perjudicar los resultados cuando se utiliza la regresión lineal. También debe tener cuidado con las variables numéricas que toman pocos valores únicos, ya que pueden actuar más como variables categóricas que numéricas. 8.7.2.4 Desde una perspectiva económetrica import statsmodels.api as sm import statsmodels.formula.api as smf #import plotnine as p import ssl ssl._create_default_https_context = ssl._create_unverified_context # read data def read_data(file): return pd.read_csv(&quot;https://raw.github.com/scunning1975/mixtape/master/&quot; + file) np.random.seed(1) tb = pd.DataFrame({ &#39;x&#39;: np.random.normal(size=10000), &#39;u&#39;: np.random.normal(size=10000)}) tb[&#39;y&#39;] = 5.5*tb[&#39;x&#39;].values + 12*tb[&#39;u&#39;].values tb reg_tb = sm.OLS.from_formula(&#39;y ~ x&#39;, data=tb).fit() reg_tb reg_tb.summary() tb[&#39;yhat1&#39;] = reg_tb.predict(tb) tb[&#39;yhat2&#39;] = 0.1114 + 5.6887*tb[&#39;x&#39;] tb[&#39;uhat1&#39;] = reg_tb.resid tb[&#39;uhat2&#39;] = tb[&#39;y&#39;] - tb[&#39;yhat2&#39;] tb.describe() tb = pd.DataFrame({ &#39;x&#39;: 9*np.random.normal(size=10), &#39;u&#39;: 36*np.random.normal(size=10)}) tb[&#39;y&#39;] = 3*tb[&#39;x&#39;].values + 2*tb[&#39;u&#39;].values reg_tb = sm.OLS.from_formula(&#39;y ~ x&#39;, data=tb).fit() tb[&#39;yhat1&#39;] = reg_tb.predict(tb) tb[&#39;uhat1&#39;] = reg_tb.resid tb.describe() 8.7.2.5 Ejemplo: estimacion de regresion lineal import numpy as np import matplotlib.pyplot as plt import pandas as pd data = pd.read_csv(&#39;real_estate.csv&#39;) data.head(5) data.shape data.describe() data.precio.describe() data.hist(figsize=(6,6)) data.superficie.hist() data.plot(kind = &#39;kde&#39;, subplots=True, layout = (2,2), sharex=False,figsize=(10,8)) data.plot(kind=&#39;box&#39;,subplots=True,layout=(2,2),sharex=False,sharey=False,figsize=(10,8)) plt.scatter(data.superficie,data.precio,s=10,color=&#39;purple&#39;) plt.xlabel(&#39;superficie&#39;) plt.ylabel(&#39;precio&#39;) from pandas.plotting import scatter_matrix scatter_matrix(data,figsize=(10,10)) 8.7.3 Una simulación de montecarlo \\[y = 3 + 2x + u \\] \\[x ∼ N(0,9)\\] \\[u ∼ N(0,36)\\] coefs = np.zeros(1000) for i in range(1000): tb = pd.DataFrame({ &#39;x&#39;: 9*np.random.normal(size=10000), &#39;u&#39;: 36*np.random.normal(size=10000)}) tb[&#39;y&#39;] = 3 + 2*tb[&#39;x&#39;].values + tb[&#39;u&#39;].values reg_tb = sm.OLS.from_formula(&#39;y ~ x&#39;, data=tb).fit() coefs[i] = reg_tb.params[&#39;x&#39;] plt.hist(coefs, bins = 100) plt.show() len(coefs) coefs.mean() coefs.std() 8.7.3.1 Ejemplo 1: Tirar dados # Importing Packages import matplotlib.pyplot as plt import random En este juego, la casa tiene más oportunidades de ganar (30 resultados frente a los 6 del jugador), lo que significa que la casa tiene bastante ventaja. Digamos que nuestro jugador comienza con un saldo de 1.000 dólares y está dispuesto a perderlo todo, por lo que apuesta 1 dólar en cada tirada (lo que significa que se lanzan ambos dados) y decide jugar 1.000 tiradas. Como la casa es tan generosa, ofrece pagar 4 veces la apuesta del jugador cuando éste gana. Por ejemplo, si el jugador gana la primera tirada, su saldo aumenta en 4 dólares, y termina la ronda con un saldo de 1.004 dólares. Si milagrosamente se produce una racha de 1.000 tiradas ganadas, podría irse a casa con 5.000 dólares. Si perdieran todas las rondas, podrían irse a casa sin nada. No es una mala relación riesgo-recompensa… o quizás sí. # Creating Roll Dice Function def roll_dice(): die_1 = random.randint(1, 6) die_2 = random.randint(1, 6) # Determining if the dice are the same number if die_1 == die_2: same_num = True else: same_num = False return same_num roll_dice() # Inputs num_simulations = 10000 max_num_rolls = 1000 bet = 1 # Tracking win_probability = [] end_balance = [] # Creating Figure for Simulation Balances fig = plt.figure() plt.title(&quot;Monte Carlo Dice Game [&quot; + str(num_simulations) + &quot;simulations]&quot;) plt.xlabel(&quot;Roll Number&quot;) plt.ylabel(&quot;Balance [$]&quot;) plt.xlim([0, max_num_rolls]) # For loop to run for the number of simulations desired for i in range(num_simulations): balance = [1000] num_rolls = [0] num_wins = 0 # Run until the player has rolled 1,000 times while num_rolls[-1] &lt; max_num_rolls: same = roll_dice() # Result if the dice are the same number if same: balance.append(balance[-1] + 4 * bet) num_wins += 1 # Result if the dice are different numbers else: balance.append(balance[-1] - bet) num_rolls.append(num_rolls[-1] + 1) # Store tracking variables and add line to figure win_probability.append(num_wins/num_rolls[-1]) end_balance.append(balance[-1]) plt.plot(num_rolls, balance) # Showing the plot after the simulations are finished plt.show() # Averaging win probability and end balance overall_win_probability = sum(win_probability)/len(win_probability) overall_end_balance = sum(end_balance)/len(end_balance) # Displaying the averages print(&quot;Average win probability after &quot; + str(num_simulations) + &quot;runs: &quot; + str(overall_win_probability)) print(&quot;Average ending balance after &quot; + str(num_simulations) + &quot;runs: $&quot; + str(overall_end_balance)) 8.7.3.2 Ejemplo 2: Modelo import numpy as np import statsmodels.api as sm import matplotlib as plt np.random.seed(2021) mu = 0 sigma = 1 n = 100 # assumed population parameters alpha = np.repeat(0.5, n) beta = 1.5 def MC_estimation_slope(M): MC_betas = [] MC_samples = {} for i in range(M): # randomly sampling from normal distribution as error terms e = np.random.normal(mu, sigma, n) # generating independent variable by making sure the variance in X is larger than the variance in error terms X = 9 * np.random.normal(mu, sigma, n) # population distribution using the assumd parameter values alpha/beta Y = (alpha + beta * X + e) # running OLS regression for getting slope parameters model = sm.OLS(Y.reshape((-1, 1)), X.reshape((-1, 1))) ols_result = model.fit() coeff = ols_result.params MC_samples[i] = Y MC_betas.append(coeff) MC_beta_hats = np.array(MC_betas).flatten() return(MC_samples, MC_beta_hats) MC_samples, MC_beta_hats = MC_estimation_slope(M = 10000) beta_hat_MC = np.mean(MC_beta_hats) MC_samples, MC_beta_hats plt.hist(MC_beta_hats) beta_hat_MC 8.7.4 Bootstraping # Import necessary libraries import pandas as pd import numpy as np from scipy import stats import seaborn as sns import matplotlib.pyplot as plt Data # Import the iris dataset data = pd.read_csv(&#39;weight-height.csv&#39;) # Looking at information print(&quot;\\nInfo:&quot;) display(data.info()) # Summary Statistics print(&quot;\\nSummary Statistics:&quot;) display(data.describe()) # Display first 5 rows display(data.head()) Convertimos datos en centimetros # Convert inches to centimeters data[&quot;Height(cm)&quot;] = data[&quot;Height&quot;]*2.54 # Get summary statistics of Heights in centimeters display(data[&#39;Height(cm)&#39;].describe()) Como podemos ver, la altura máxima y mínima del conjunto de datos son 137,8 cm y 200,6 cm respectivamente Tome 500 alturas seleccionadas al azar # Extraemos 500 random heights = data[&#39;Height(cm)&#39;].sample(500).reset_index(drop=True) # Eastadisticas display(heights.describe()) Si los datos se distribuyen normalmente, ¿qué pasaría? Para ver esto, vamos a trazar la función de distribución acumulativa con la media y la desviación estándar de nuestros datos empíricos. # Obtenga la desviación estándar y la media heights_std = np.std(heights) heights_mean = np.mean(heights) # Plot Normal CDF def plot_normal_CDF(data,mean,std,label,title): sns.set() # CDF of the data x = np.linspace(min(data),max(data),len(data)) cdf = stats.norm.cdf(x,mean,std) plt.plot(x,cdf) plt.xlabel(label) plt.ylabel(&#39;percentage&#39;) plt.title(title) plt.show() return x,cdf # CDF of Versicolor x_cdf, y_cdf = plot_normal_CDF(heights, heights_mean, heights_std, &quot;Height (cm)&quot;,&quot;CDF&quot;) Además de la FCD, trace la función de distribución acumulativa empírica y la función de densidad de probabilidad. # Create a function to get x, y for of ecdf def get_ecdf(data): # Get lenght of the data into n n = len(data) # We need to sort the data x = np.sort(data) # the function will show us cumulative percentages of corresponding data points y = np.arange(1,n+1)/n return x,y # Create a function to plot ecdf def plot_ecdf(data,labelx,labely,title,color): &quot;&quot;&quot;Plot ecdf&quot;&quot;&quot; # Call get_ecdf function and assign the returning values x, y = get_ecdf(data) plt.plot(x,y,marker=&#39;.&#39;,linestyle=&#39;none&#39;,c=color) plt.xlabel(labelx) plt.ylabel(labely) plt.title(title) # Create a function overlay ECDF on CDF def plot_overlay_ecdf(data,labelx,labely,title,color,x_cdf, y_cdf): x, y = get_ecdf(data) plt.plot(x,y,marker=&#39;.&#39;,linestyle=&#39;none&#39;,c=color,alpha=0.5) plt.plot(x_cdf, y_cdf) plt.xlabel(labelx) plt.ylabel(labely) plt.title(title) # Plotting Empirical CDF plot_ecdf(heights,&quot;Height (cm)&quot;,&quot;percentage&quot;,&quot;Empirical CDF&quot;,&quot;r&quot;) plt.show() # Overlap Empirical CDF on CDF plot_overlay_ecdf(heights,&quot;Height (cm)&quot;,&quot;percentage&quot;,&quot;Empirical CDF on CDF&quot;,&quot;y&quot;,x_cdf, y_cdf) plt.show() # Plotting PDF sns.distplot(heights,hist=False) plt.xlabel(&quot;Petal length(cm)&quot;) plt.ylabel(&quot;PDF&quot;) plt.title(&quot;Probability Density Function&quot;) plt.show() Si hubieramos hecho el mismo experimento muchas veces hubiesemos obtenido lo mismo? # Plot our original sample ecdf plot_ecdf(heights,&quot;Height (cm)&quot;,&quot;percentage&quot;,&quot;ECDF&#39;s of bootstrap samples&quot;,&quot;y&quot;) for i in range(1000): # Generate a bootstrap sample bs_sample_heights = np.random.choice(heights,size=len(heights)) # Plot ecdf for bootstrap sample x, y = get_ecdf(bs_sample_heights) plt.scatter(x, y,s=1,c=&#39;b&#39;,alpha=0.3) plt.show() Necesitaremos una función que genere muestras bootstrap y extraiga réplicas bootstrap de esas muestras tanto como queramos. La función tomará tres parámetros data, func y size. El parámetro ‘func’ corresponde a los estadísticos de resumen que querremos utilizar al crear una réplica bootstrap. El parámetro ‘size’ nos indicará cuántas réplicas necesitamos. def draw_bs_replicates(data,func,size): &quot;&quot;&quot;creates a bootstrap sample, computes replicates and returns replicates array&quot;&quot;&quot; # Create an empty array to store replicates bs_replicates = np.empty(size) # Create bootstrap replicates as much as size for i in range(size): # Create a bootstrap sample bs_sample = np.random.choice(data,size=len(data)) # Get bootstrap replicate and append to bs_replicates bs_replicates[i] = func(bs_sample) return bs_replicates Ahora utilizaremos nuestra función para obtener 10.000 réplicas bootstrap. Después veremos cómo son su PDF y su ECDF. # Draw 10000 bootstrap replicates bs_replicates_heights = draw_bs_replicates(heights,np.mean,15000) # Plot probability density function plt.hist(bs_replicates_heights,bins=30,density=True) plt.axvline(x=np.percentile(bs_replicates_heights,[2.5]), ymin=0, ymax=1,label=&#39;2.5th percentile&#39;,c=&#39;y&#39;) plt.axvline(x=np.percentile(bs_replicates_heights,[97.5]), ymin=0, ymax=1,label=&#39;97.5th percentile&#39;,c=&#39;r&#39;) plt.xlabel(&quot;Height(cm)&quot;) plt.ylabel(&quot;PDF&quot;) plt.title(&quot;Probability Density Function&quot;) plt.legend() plt.show() # Plot the ECDF of replicates xsbs_ecdf, ysbs_ecdf = get_ecdf(bs_replicates_heights) plt.scatter(xsbs_ecdf, ysbs_ecdf,s=5,c=&#39;b&#39;,alpha=0.5) plt.xlabel(&quot;Height(cm)&quot;) plt.ylabel(&quot;ECDF&quot;) plt.title(&quot;ECDF of Bootstrap Replicates&quot;) plt.show() Comparemos la media empírica y la media de nuestras réplicas print(&quot;Empirical mean: &quot; + str(heights_mean)) # Print the mean of bootstrap replicates print(&quot;Bootstrap replicates mean: &quot; + str(np.mean(bs_replicates_heights))) Intervalo de confianza? Medias muy iguales Ecnontremos el intervalo de confianza Necesitamos conocer el area entre los cuantiles # Get the corresponding values of 2.5th and 97.5th percentiles conf_interval = np.percentile(bs_replicates_heights,[2.5,97.5]) # Print the interval print(&quot;The confidence interval: &quot;,conf_interval) Encontramos un intervalo de confianza entre 167,2cm y 162,5cm. Podemos decir que si elegimos a alguien al azar, su altura estará entre este intervalo con un %95 de probabilidad. 8.7.5 Regresión Logistica import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import scipy.stats as stats plt.figure(figsize=(9,9)) def sigmoid(t): # Define the sigmoid function return (1/(1 + np.e**(-t))) plot_range = np.arange(-6, 6, 0.1) y_values = sigmoid(plot_range) # Plot curve plt.plot(plot_range, # X-axis range y_values, # Predicted values color=&quot;red&quot;); La función sigmoide está limitada por debajo por 0 y por encima por 1. En la regresión logística, el resultado se interpreta como una probabilidad: la probabilidad de que una observación pertenezca a la segunda de las dos categorías modeladas. Cuando la combinación lineal de variables produce números positivos, la probabilidad resultante es mayor que 0,5 y cuando produce números negativos, la probabilidad es menor que 0,5. No profundizaremos en los detalles de cómo funciona la regresión logística, sino que nos centraremos en cómo utilizarla en Python. Lo más importante es saber que la reg&lt;resión logística produce probabilidades que podemos utilizar para clasificar las observaciones. 8.7.5.1 Titanic Para el resto de la lección trabajaremos con los datos de entrenamiento de supervivencia del Titanic de Kaggle que vimos en la lección 14. Empezaremos cargando los datos y realizando algunas de las mismas tareas de preprocesamiento que hicimos en la lección 14: import os os.chdir(&quot;datos&quot;) titanic_train = pd.read_csv(&quot;train.csv&quot;) # Read the data char_cabin = titanic_train[&quot;Cabin&quot;].astype(str) # Convert cabin to str new_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter titanic_train[&quot;Cabin&quot;] = pd.Categorical(new_Cabin) # Save the new cabin var # Impute median Age for NA Age values new_age_var = np.where(titanic_train[&quot;Age&quot;].isnull(), # Logical check 28, # Value if check is true titanic_train[&quot;Age&quot;]) # Value if check is false titanic_train[&quot;Age&quot;] = new_age_var new_fare_var = np.where(titanic_train[&quot;Fare&quot;].isnull(), # Logical check 50, # Value if check is true titanic_train[&quot;Fare&quot;]) # Value if check is false titanic_train[&quot;Fare&quot;] = new_fare_var Ahora estamos listos para utilizar un modelo de regresión logística para predecir la supervivencia. La biblioteca scikit-learn tiene una función de regresión logística en la subcarpeta learn model. Hagamos un modelo de regresión logística que sólo utilice la variable Sexo como predictor. Antes de crear un modelo con la variable sexo, necesitamos convertirla a un número real porque las funciones de aprendizaje automático de sklearn sólo tratan con números reales. Podemos convertir una variable categórica como en un número utilizando la función de preprocesamiento de sklearn LabelEncoder(): from sklearn import linear_model from sklearn import preprocessing # Initialize label encoder label_encoder = preprocessing.LabelEncoder() # Convert Sex variable to numeric encoded_sex = label_encoder.fit_transform(titanic_train[&quot;Sex&quot;]) # Initialize logistic regression model log_model = linear_model.LogisticRegression(solver = &#39;lbfgs&#39;) # Train the model log_model.fit(X = pd.DataFrame(encoded_sex), y = titanic_train[&quot;Survived&quot;]) # Check trained model intercept print(log_model.intercept_) # Check trained model coefficients print(log_model.coef_) Los coeficientes del modelo de regresión logística son similares a los resultados de la regresión lineal. Podemos ver que el modelo produjo un valor de intercepción positivo y un peso de -2,421 en el género. Utilicemos el modelo para hacer predicciones en el conjunto de pruebas: # Make predictions preds = log_model.predict_proba(X= pd.DataFrame(encoded_sex)) preds = pd.DataFrame(preds) preds.columns = [&quot;Death_prob&quot;, &quot;Survival_prob&quot;] # Generate table of predictions vs Sex pd.crosstab(titanic_train[&quot;Sex&quot;], preds.loc[:, &quot;Survival_prob&quot;]) Nota: Utilice model.predict_proba() para obtener las probabilidades de clase predichas. Utilice model.predict() para obtener las clases predichas. La tabla muestra que el modelo predijo una probabilidad de supervivencia de aproximadamente el 19% para los hombres y el 73% para las mujeres. Si utilizamos este modelo simple para predecir la supervivencia, acabaríamos prediciendo que todas las mujeres sobrevivieron y que todos los hombres murieron. Hagamos un modelo más complicado que incluya algunas variables más del conjunto de entrenamiento del titán: # Convert more variables to numeric encoded_class = label_encoder.fit_transform(titanic_train[&quot;Pclass&quot;]) encoded_cabin = label_encoder.fit_transform(titanic_train[&quot;Cabin&quot;]) train_features = pd.DataFrame([encoded_class, encoded_cabin, encoded_sex, titanic_train[&quot;Age&quot;]]).T # Initialize logistic regression model log_model = linear_model.LogisticRegression(solver = &#39;lbfgs&#39;) # Train the model log_model.fit(X = train_features , y = titanic_train[&quot;Survived&quot;]) # Check trained model intercept print(log_model.intercept_) # Check trained model coefficients print(log_model.coef_) A continuación, vamos a hacer predicciones de clase utilizando este modelo y luego compararemos las predicciones con los valores reales: # Make predictions preds = log_model.predict(X= train_features) # Generate table of predictions vs actual pd.crosstab(preds,titanic_train[&quot;Survived&quot;]) La tabla anterior muestra las clases que nuestro modelo predijo frente a los valores reales de la variable Supervivencia. Esta tabla de valores predichos frente a los reales se conoce como matriz de confusión. 8.7.6 Matching To compare results, let’s first look at the treatment effect identified by a true experiment. import pandas as pd import numpy as np import statsmodels.api as sm import statsmodels.formula.api as smf import ssl ssl._create_default_https_context = ssl._create_unverified_context def read_data(file): full_path = &quot;https://raw.github.com/scunning1975/mixtape/master/&quot; + file return pd.read_stata(full_path) Before using the propensity score methods for estimating treatment effects, let’s calculate the average treatment effect from the actual experiment. nsw_dw = read_data(&#39;nsw_mixtape.dta&#39;) mean1 = nsw_dw[nsw_dw.treat==1].re78.mean() mean0 = nsw_dw[nsw_dw.treat==0].re78.mean() ate = np.unique(mean1 - mean0)[0] print(&quot;The experimental ATE estimate is {:.2f}&quot;.format(ate)) Using the code above, we calculate that the NSW job-training program caused real earnings in 1978 to increase by $1,794.343. A continuación vamos a ver varios ejemplos en los que estimamos el efecto medio del tratamiento o algunas de sus variantes, como el efecto medio del tratamiento en el grupo tratado o el efecto medio del tratamiento en el grupo no tratado. Pero aquí, en lugar de utilizar el grupo de control experimental del experimento aleatorio original, utilizaremos el grupo de control no experimental de la Current Population Survey. Es muy importante subrayar que, mientras que el grupo de tratamiento es un grupo experimental, el grupo de control consiste ahora en una muestra aleatoria de estadounidenses de ese periodo de tiempo. Por lo tanto, el grupo de control sufre un sesgo de selección extremo, ya que la mayoría de los estadounidenses no funcionarían como contrafactuales para el grupo de trabajadores en apuros que se seleccionó en el programa NSW. A continuación, añadiremos los datos de la CPS a los datos experimentales y estimaremos la puntuación de propensión utilizando logit para ser coherentes con Dehejia y Wahba (2002). # Prepare data for logit nsw_dw_cpscontrol = read_data(&#39;cps_mixtape.dta&#39;) nsw_dw_cpscontrol = pd.concat((nsw_dw_cpscontrol, nsw_dw)) nsw_dw_cpscontrol[[&#39;u74&#39;, &#39;u75&#39;]] = 0 nsw_dw_cpscontrol.loc[nsw_dw_cpscontrol.re74==0, &#39;u74&#39;] = 1 nsw_dw_cpscontrol.loc[nsw_dw_cpscontrol.re75==0, &#39;u75&#39;] = 1 # estimating propensity score logit_nsw = smf.glm(formula=&quot;&quot;&quot;treat ~ age + age**2 + age**3 + educ + educ**2 + marr + nodegree + black + hisp + re74 + re75 + u74 + u75 + educ*re74&quot;&quot;&quot;, family=sm.families.Binomial(), data=nsw_dw_cpscontrol).fit() nsw_dw_cpscontrol[&#39;pscore&#39;] = logit_nsw.predict(nsw_dw_cpscontrol) nsw_dw_cpscontrol nsw_dw_cpscontrol.groupby(&#39;treat&#39;)[&#39;pscore&#39;].mean() nsw_dw_cpscontrol[&quot;pscore&quot;][nsw_dw_cpscontrol[&quot;treat&quot;] == 1] # histogram plt.hist(nsw_dw_cpscontrol[&quot;pscore&quot;][nsw_dw_cpscontrol[&quot;treat&quot;] == 1]) plt.hist(nsw_dw_cpscontrol[&quot;pscore&quot;][nsw_dw_cpscontrol[&quot;treat&quot;] == 0]) Recordemos lo que hace la ponderación probabilística inversa. Está ponderando las unidades de tratamiento y control según la puntuación p, lo que hace que las unidades con valores muy pequeños de la puntuación de propensión se disparen y se vuelvan inusualmente influyentes en el cálculo del TCA. # continuation N = nsw_dw_cpscontrol.shape[0] # Manual with non-normalized weights using all data nsw_dw_cpscontrol = nsw_dw_cpscontrol nsw_dw_cpscontrol[&#39;d1&#39;] = nsw_dw_cpscontrol.treat/nsw_dw_cpscontrol.pscore nsw_dw_cpscontrol[&#39;d0&#39;] = (1-nsw_dw_cpscontrol.treat)/(1-nsw_dw_cpscontrol.pscore) s1 = nsw_dw_cpscontrol.d1.sum() s0 = nsw_dw_cpscontrol.d0.sum() nsw_dw_cpscontrol[&#39;y1&#39;] = nsw_dw_cpscontrol.treat * nsw_dw_cpscontrol.re78 / nsw_dw_cpscontrol.pscore nsw_dw_cpscontrol[&#39;y0&#39;] = (1 - nsw_dw_cpscontrol.treat) * nsw_dw_cpscontrol.re78 / (1 - nsw_dw_cpscontrol.pscore) nsw_dw_cpscontrol[&#39;ht&#39;] = nsw_dw_cpscontrol[&#39;y1&#39;] - nsw_dw_cpscontrol[&#39;y0&#39;] te_1 = nsw_dw_cpscontrol.ht.mean() print(&quot;Treatment Effect (non-normalized, all data): {:.2f}&quot;.format(te_1)) nsw_dw_cpscontrol[&#39;y1&#39;] = nsw_dw_cpscontrol.treat * nsw_dw_cpscontrol.re78 / nsw_dw_cpscontrol.pscore nsw_dw_cpscontrol[&#39;y1&#39;] /= s1/N nsw_dw_cpscontrol[&#39;y0&#39;] = (1 - nsw_dw_cpscontrol.treat) * nsw_dw_cpscontrol.re78 / (1 - nsw_dw_cpscontrol.pscore) nsw_dw_cpscontrol[&#39;y0&#39;] /= s0/N nsw_dw_cpscontrol[&#39;ht&#39;] = nsw_dw_cpscontrol[&#39;y1&#39;] - nsw_dw_cpscontrol[&#39;y0&#39;] te_2 = nsw_dw_cpscontrol.ht.mean() print(&quot;Treatment Effect (normalized, all data): {:.2f}&quot;.format(te_2)) nsw_dw_trimmed = nsw_dw_cpscontrol.drop([&#39;d1&#39;, &#39;d0&#39;, &#39;y1&#39;, &#39;y0&#39;], axis=1) nsw_dw_trimmed = nsw_dw_trimmed[nsw_dw_trimmed.pscore.between(.1, .9)] N = nsw_dw_trimmed.shape[0] nsw_dw_trimmed[&#39;y1&#39;] = nsw_dw_trimmed.treat * nsw_dw_trimmed.re78 / nsw_dw_trimmed.pscore nsw_dw_trimmed[&#39;y0&#39;] = (1 - nsw_dw_trimmed.treat) * nsw_dw_trimmed.re78 / (1 - nsw_dw_trimmed.pscore) nsw_dw_trimmed[&#39;ht&#39;] = nsw_dw_trimmed[&#39;y1&#39;] - nsw_dw_trimmed[&#39;y0&#39;] te_3 = nsw_dw_trimmed.ht.mean() print(&quot;Treatment Effect (non-normalized, trimmed data): {:.2f}&quot;.format(te_3)) nsw_dw_trimmed[&#39;y1&#39;] = nsw_dw_trimmed.treat * nsw_dw_trimmed.re78 / nsw_dw_trimmed.pscore nsw_dw_trimmed[&#39;y1&#39;] /= s1/N nsw_dw_trimmed[&#39;y0&#39;] = (1 - nsw_dw_trimmed.treat) * nsw_dw_trimmed.re78 / (1 - nsw_dw_trimmed.pscore) nsw_dw_trimmed[&#39;y0&#39;] /= s0/N nsw_dw_trimmed[&#39;ht&#39;] = nsw_dw_trimmed[&#39;y1&#39;] - nsw_dw_trimmed[&#39;y0&#39;] te_4 = nsw_dw_trimmed.ht.mean() print(&quot;Treatment Effect (normalized, trimmed data): {:.2f}&quot;.format(te_4)) 8.8 Análisis de Datos (Parte III) 8.8.1 Bootstraping en el contexto de MCO import numpy as np import matplotlib.pyplot as plt import pandas as pd import statsmodels.formula.api as sm # synthetic sample data n_points = 25 x = np.linspace(0, 10, n_points) y = x + (np.random.rand(len(x)) * 5) data_df = pd.DataFrame({&quot;x&quot;: x, &quot;y&quot;: y}) ols_model = sm.ols(formula = &quot;y ~ x&quot;, data=data_df) results = ols_model.fit() # coefficients print(&quot;Intercept, x-Slope : {}&quot;.format(results.params)) y_pred = ols_model.fit().predict(data_df[&quot;x&quot;]) # plot results plt.scatter(x, y) plt.plot(x, y_pred, linewidth=2) plt.grid(True) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.title(&quot;x vs y&quot;) plt.show() # resample with replacement each row boot_slopes = [] boot_interc = [] n_boots = 100 plt.figure() for _ in range(n_boots): # sample the rows, same size, with replacement sample_df = data_df.sample(n=n_points, replace=True) # fit a linear regression ols_model_temp = sm.ols(formula = &quot;y ~ x&quot;, data=sample_df) results_temp = ols_model_temp.fit() # append coefficients boot_interc.append(results_temp.params[0]) boot_slopes.append(results_temp.params[1]) # plot a greyed out line y_pred_temp = ols_model_temp.fit().predict(sample_df[&quot;x&quot;]) plt.plot(sample_df[&quot;x&quot;], y_pred_temp, color=&quot;grey&quot;, alpha=0.2) # add data points plt.scatter(x, y) plt.plot(x, y_pred, linewidth=2) plt.grid(True) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y&quot;) plt.title(&quot;x vs y&quot;) plt.show() sns.distplot(boot_slopes) plt.show() sns.distplot(boot_interc) plt.show() Los datos anteriores pintan una bonita imagen del bootstrapping paramétrico. Sin embargo, si por casualidad, y esto es lo más probable, tuviéramos datos dispersos, podría existir la posibilidad de que nuestra selección aleatoria de puntos se encontrara totalmente en una zona y no en otra - recordemos la mención de que el valor atípico puede ser muestreado varias veces a pesar de ser un único punto atípico. Para luchar contra esto, podemos aplicar un tipo diferente de bootstrapping, llamado “bootstrapping no paramétrico”, por el que aplicamos el bootstrapping en los residuos, y no en el propio parámetro. En resumen… Bootstrap y la simulación de Montecarlo de una estadística: ambos se basan en la repetición del muestreo y en el examen directo de los resultados. Sin embargo, una gran diferencia entre los métodos es que el bootstrap utiliza la muestra inicial original como población de la que se vuelve a tomar la muestra, mientras que la simulación de Montecarlo se basa en establecer un proceso de generación de datos (con valores conocidos de los parámetros). Mientras que Monte Carlo se utiliza para probar los estimadores de la unidad, los métodos de bootstrap pueden utilizarse para estimar la variabilidad de un estadístico y la forma de su distribución muestral. import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import scipy.stats as stats La función sigmoide está limitada por debajo por 0 y por encima por 1. En la regresión logística, el resultado se interpreta como una probabilidad: la probabilidad de que una observación pertenezca a la segunda de las dos categorías modeladas. Cuando la combinación lineal de variables produce números positivos, la probabilidad resultante es mayor que 0,5 y cuando produce números negativos, la probabilidad es menor que 0,5. No profundizaremos en los detalles de cómo funciona la regresión logística, sino que nos centraremos en cómo utilizarla en Python. Lo más importante es saber que la reg&lt;resión logística produce probabilidades que podemos utilizar para clasificar las observaciones. 8.8.2 Logit: Probabilidad de sobrevivir al hundimiento del Titanic import os os.chdir(&quot;datos&quot;) titanic_train = pd.read_csv(&quot;train.csv&quot;) # Read the data char_cabin = titanic_train[&quot;Cabin&quot;].astype(str) # Convert cabin to str new_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter titanic_train[&quot;Cabin&quot;] = pd.Categorical(new_Cabin) # Save the new cabin var # Impute median Age for NA Age values new_age_var = np.where(titanic_train[&quot;Age&quot;].isnull(), # Logical check 28, # Value if check is true titanic_train[&quot;Age&quot;]) # Value if check is false titanic_train[&quot;Age&quot;] = new_age_var new_fare_var = np.where(titanic_train[&quot;Fare&quot;].isnull(), # Logical check 50, # Value if check is true titanic_train[&quot;Fare&quot;]) # Value if check is false titanic_train[&quot;Fare&quot;] = new_fare_var titanic_train Ahora estamos listos para utilizar un modelo de regresión logística para predecir la supervivencia. La biblioteca scikit-learn tiene una función de regresión logística en la subcarpeta linear model. Hagamos un modelo de regresión logística que sólo utilice la variable Sexo como predictor. Antes de crear un modelo con la variable sexo, necesitamos convertirla a un número real porque las funciones de aprendizaje automático de sklearn sólo tratan con números reales. Podemos convertir una variable categórica como en un número utilizando la función de preprocesamiento de sklearn LabelEncoder(): from sklearn import linear_model from sklearn import preprocessing # Initialize label encoder label_encoder = preprocessing.LabelEncoder() # Convert Sex variable to numeric encoded_sex = label_encoder.fit_transform(titanic_train[&quot;Sex&quot;]) # Initialize logistic regression model log_model = linear_model.LogisticRegression(solver = &#39;lbfgs&#39;) # Train the model log_model.fit(X = pd.DataFrame(encoded_sex), y = titanic_train[&quot;Survived&quot;]) # Check trained model intercept print(log_model.intercept_) # Check trained model coefficients print(log_model.coef_) Los coeficientes del modelo de regresión logística son similares a los resultados de la regresión lineal. Podemos ver que el modelo produjo un valor de intercepción positivo y un peso de -2,421 en el género. Utilicemos el modelo para hacer predicciones en el conjunto de pruebas: encoded_sex # Make predictions preds = log_model.predict_proba(X= pd.DataFrame(encoded_sex)) preds = pd.DataFrame(preds) preds.columns = [&quot;Death_prob&quot;, &quot;Survival_prob&quot;] # Generate table of predictions vs Sex pd.crosstab(titanic_train[&quot;Sex&quot;], preds.loc[:, &quot;Survival_prob&quot;]) preds Nota: Utilice model.predict_proba() para obtener las probabilidades de clase predichas. Utilice model.predict() para obtener las clases predichas. La tabla muestra que el modelo predijo una probabilidad de supervivencia de aproximadamente el 19% para los hombres y el 73% para las mujeres. Si utilizamos este modelo simple para predecir la supervivencia, acabaríamos prediciendo que todas las mujeres sobrevivieron y que todos los hombres murieron. Hagamos un modelo más complicado que incluya algunas variables más del conjunto de entrenamiento del titán: # Convert more variables to numeric encoded_class = label_encoder.fit_transform(titanic_train[&quot;Pclass&quot;]) encoded_cabin = label_encoder.fit_transform(titanic_train[&quot;Cabin&quot;]) train_features = pd.DataFrame([encoded_class, encoded_cabin, encoded_sex, titanic_train[&quot;Age&quot;]]).T # Initialize logistic regression model log_model = linear_model.LogisticRegression(solver = &#39;lbfgs&#39;) # Train the model log_model.fit(X = train_features , y = titanic_train[&quot;Survived&quot;]) # Check trained model intercept print(log_model.intercept_) # Check trained model coefficients print(log_model.coef_) A continuación, vamos a hacer predicciones de clase utilizando este modelo y luego compararemos las predicciones con los valores reales: # Make predictions preds = log_model.predict(X= train_features) # Generate table of predictions vs actual pd.crosstab(preds,titanic_train[&quot;Survived&quot;]) La tabla anterior muestra las clases que nuestro modelo predijo frente a los valores reales de la variable Supervivencia. Esta tabla de valores predichos frente a los reales se conoce como matriz de confusión. 8.8.3 GMM El método generalizado de momentos (GMM) es un método para construir estimadores, análogo al de máxima verosimilitud (ML). El GMM utiliza suposiciones sobre momentos específicos de las variables aleatorias en lugar de suposiciones sobre toda la distribución, lo que hace que el GMM sea más robusto que el ML, a costa de cierta eficiencia. Los supuestos se denominan condiciones de momento. # GMM from numpy import hstack, ones, array, mat, tile, reshape, squeeze, eye, asmatrix from numpy.linalg import inv from pandas import read_csv, Series from scipy.linalg import kron from scipy.optimize import fmin_bfgs import numpy as np import statsmodels.api as sm A continuación se utiliza una función invocable para producir la salida iteración por iteración cuando se utiliza el optimizador no lineal. def gmm_objective(params, pRets, fRets, Winv, out=False): global lastValue, functionCount T,N = pRets.shape T,K = fRets.shape beta = squeeze(array(params[:(N*K)])) lam = squeeze(array(params[(N*K):])) beta = reshape(beta,(N,K)) lam = reshape(lam,(K,1)) betalam = beta @ lam expectedRet = fRets @ beta.T e = pRets - expectedRet instr = tile(fRets,N) moments1 = kron(e,ones((1,K))) moments1 = moments1 * instr moments2 = pRets - betalam.T moments = hstack((moments1,moments2)) avgMoment = moments.mean(axis=0) J = T * mat(avgMoment) * mat(Winv) * mat(avgMoment).T J = J[0,0] lastValue = J functionCount += 1 if not out: return J else: return J, moments Se define la matriz G, que es la derivada de los momentos del GMM con respecto a los parámetros. def gmm_G(params, pRets, fRets): T,N = pRets.shape T,K = fRets.shape beta = squeeze(array(params[:(N*K)])) lam = squeeze(array(params[(N*K):])) beta = reshape(beta,(N,K)) lam = reshape(lam,(K,1)) G = np.zeros((N*K+K,N*K+N)) ffp = (fRets.T @ fRets) / T G[:(N*K),:(N*K)]=kron(eye(N),ffp) G[:(N*K),(N*K):] = kron(eye(N),-lam) G[(N*K):,(N*K):] = -beta.T return G A continuación, se importan los datos y se selecciona un subconjunto de las carteras de prueba para que la estimación sea más rápida. os.chdir(&quot;datos&quot;) data = read_csv(&#39;FamaFrench.csv&#39;) # Split using both named colums and ix for larger blocks dates = data[&#39;date&#39;].values factors = data[[&#39;VWMe&#39;,&#39;SMB&#39;,&#39;HML&#39;]].values riskfree = data[&#39;RF&#39;].values portfolios = data.iloc[:,5:].values T,N = portfolios.shape portfolios = portfolios[:,np.arange(0,N,2)] T,N = portfolios.shape excessRet = portfolios - np.reshape(riskfree,(T,1)) K = np.size(factors,1) portfolios.shape data Los valores iniciales de las cargas de los factores y las primas de riesgo se estiman mediante MCO y medias simples. betas = [] for i in range(N): res = sm.OLS(excessRet[:,i],sm.add_constant(factors)).fit() betas.append(res.params[1:]) avgReturn = excessRet.mean(axis=0) avgReturn.shape = N,1 betas = array(betas) res = sm.OLS(avgReturn, betas).fit() riskPremia = res.params riskPremia Los valores de partida se calculan las estimaciones del primer paso se encuentran utilizando el optimizador no lineal. La matriz de ponderación inicial es sólo la matriz de identificación. riskPremia.shape = 3 startingVals = np.concatenate((betas.flatten(),riskPremia)) Winv = np.eye(N*(K+1)) args = (excessRet, factors, Winv) iteration = 0 functionCount = 0 step1opt = fmin_bfgs(gmm_objective, startingVals, args=args) step1opt premia = step1opt[-3:] premia premia = step1opt[-3:] premia = Series(premia,index=[&#39;VWMe&#39;, &#39;SMB&#39;, &#39;HML&#39;]) print(&#39;Annualized Risk Premia (First step)&#39;) print(12 * premia) out = gmm_objective(step1opt, excessRet, factors, Winv, out=True) S = np.cov(out[1].T) Winv2 = inv(S) args = (excessRet, factors, Winv2) iteration = 0 functionCount = 0 step2opt = fmin_bfgs(gmm_objective, step1opt, args=args) Por último, se calcula el VCV de las estimaciones de los parámetros. out = gmm_objective(step2opt, excessRet, factors, Winv2, out=True) G = gmm_G(step2opt, excessRet, factors) S = np.cov(out[1].T) vcv = inv(G @ inv(S) @ G.T)/T Premio al riesgo con sus test - t premia = step2opt[-3:] premia = Series(premia,index=[&#39;VWMe&#39;, &#39;SMB&#39;, &#39;HML&#39;]) premia_vcv = vcv[-3:,-3:] print(&#39;Annualized Risk Premia&#39;) print(12 * premia) premia_stderr = np.diag(premia_vcv) premia_stderr = Series(premia_stderr,index=[&#39;VWMe&#39;, &#39;SMB&#39;, &#39;HML&#39;]) print(&#39;T-stats&#39;) print(premia / premia_stderr) 8.8.4 Diferencias en Diferencias Pregunta clave: ¿cómo saber si el aumento o disminución de algún fenomeno que observamos no es una tendencia natural? En otras palabras, ¿cómo se puede saber el contrafactual \\(Y_0\\) de lo que habría sucedido si no se hubiera pasado algo? Una técnica para responder a este tipo de preguntas es la Diferencia en Diferencia. La diferencia en la diferencia se utiliza habitualmente para evaluar el efecto de las macrointervenciones, como el efecto de la inmigración en el desempleo, el efecto de los cambios en la ley de armas en los índices de delincuencia o simplemente la diferencia en el compromiso de los usuarios debido a una campaña de marketing. En todos estos casos, se tiene un periodo antes y después de la intervención y se desea desentrañar el impacto de la intervención de una tendencia general. Como ejemplo motivador, veamos una pregunta similar a la que tuve que responder. Vamos a ver un ejemplo del impacto de una publicidad. En este ejemplo se colocaron Vamos a colocar 3 vallas en la ciudad de Porto Alegre, la capital del estado de Rio Grande do Sul. Queremos ver si eso aumentaba los depósitos en nuestra cuenta de ahorros. Como nota para los que no estén muy familiarizados con la geografía brasileña, Rio Grande do Sul forma parte del sur del país, una de las regiones más desarrolladas. Teniendo esto en cuenta, decidimos también mirar los datos de otra capital del sur. Colocamos la valla publicitaria en Porto Alegre durante todo el mes de junio. Los datos que tenemos son los siguientes: # Commented out IPython magic to ensure Python compatibility. import warnings warnings.filterwarnings(&#39;ignore&#39;) import pandas as pd import numpy as np from matplotlib import style from matplotlib import pyplot as plt import seaborn as sns import statsmodels.formula.api as smf # %matplotlib inline style.use(&quot;fivethirtyeight&quot;) data = pd.read_csv(&quot;billboard_impact.csv&quot;) data.head() Recordemos que los depósitos son nuestra variable de resultado, la que deseamos aumentar con las vallas publicitarias. POA es un indicador ficticio de la ciudad de Porto Alegre. Cuando es cero, significa que las muestras son de Florianópolis. Jul es una variable ficticia para el mes de julio, o para el período posterior a la intervención. Cuando es cero, se refiere a las muestras de mayo, el período anterior a la intervención. 8.8.4.1 Estimador DID Para evitar la confusión entre Tiempo y Tratamiento, a partir de ahora, utilizaré D para denotar el tratamiento y T para denotar el tiempo. Sea \\(Y_D(T)\\) el resultado potencial del tratamiento D en el período T. En un mundo ideal en el que tenemos la capacidad de observar el contrafactual, estimaríamos el efecto del tratamiento de una intervención de la siguiente manera: $ = E[Y_1(1) - Y_0(1)|D=1] $ El efecto causal es el resultado en el periodo posterior a la intervención en el caso de un tratamiento menos el resultado también en el periodo posterior a la intervención, pero en el caso de no tratamiento. Por supuesto, no podemos medir esto porque \\(Y_0(1)\\) es contrafactual. Una forma de evitarlo es comparar el antes y el después. $ = E[Y(1)|D=1] - E[Y(0)|D=1] $ En nuestro ejemplo, compararíamos los depósitos medios de POA antes y después de la colocación del cartel publicitario. poa_before = data.query(&quot;poa==1 &amp; jul==0&quot;)[&quot;deposits&quot;].mean() poa_after = data.query(&quot;poa==1 &amp; jul==1&quot;)[&quot;deposits&quot;].mean() poa_after - poa_before Este estimador nos dice que debemos esperar que los depósitos aumenten 41,04 R después de la intervención. ¿Pero podemos confiar en esto? Obsérvese que \\(E[Y(0)|D=1]=E[Y_0(0)|D=1]\\), es decir, el resultado observado para la unidad tratada antes de la intervención es igual al resultado contrafactual para la unidad tratada también antes de la intervención. Como estamos utilizando esto para estimar el resultado contrafactual después de la intervención \\(E[Y_0(1)|D=1]\\), esta estimación anterior supone que \\(E[Y_0(1)|D=1] = E[Y_0(0)|D=1]\\). Está diciendo que en el caso de que no haya intervención, el resultado en el último período sería el mismo que el resultado del período inicial. Obviamente, esto sería falso si la variable de resultado sigue algún tipo de tendencia. Por ejemplo, si los depósitos están subiendo en POA, \\(E[Y_0(1)|D=1] &gt; E[Y_0(0)|D=1]\\), es decir, el resultado del último período sería mayor que el del período inicial incluso en ausencia de la intervención. Con un argumento similar, si la tendencia de Y es descendente, \\(E[Y_0(1)|D=1] &lt; E[Y_0(0)|D=1]\\). Esto es para mostrar que esto del antes y el después no es un gran estimador. Otra idea es comparar el grupo tratado con un grupo no tratado que no recibió la intervención: $ = E[Y(1)|D=1] - E[Y(1)|D=0] $ En nuestro ejemplo, sería comparar los depósitos de POA con los de Florianópolis en el período posterior a la intervención. fl_after = data.query(&quot;poa==0 &amp; jul==1&quot;)[&quot;deposits&quot;].mean() poa_after - fl_after Este estimador nos dice que la campaña es perjudicial y que los clientes disminuirán los depósitos en R$ 119,10. Observe que \\(E[Y(1)|D=0]=E[Y_0(1)|D=0]\\). Y como estamos utilizando \\(E[Y(1)|D=0]\\) para estimar el contrafactual para los tratados después de la intervención, estamos asumiendo que podemos reemplazar el contrafactual faltante así: \\(E[Y_0(1)|D=0] = E[Y_0(1)|D=1]\\). Pero nótese que esto sólo sería cierto si ambos grupos tienen un nivel de base muy similar. Por ejemplo, si Florianópolis tiene muchos más depósitos que Porto Alegre, esto no sería cierto porque \\(E[Y_0(1)|D=0] &gt; E[Y_0(1)|D=1]\\). Por otro lado, si el nivel de depósitos es menor en Florianópolis, tendríamos \\(E[Y_0(1)|D=0] &lt; E[Y_0(1)|D=1]\\). De nuevo, esto no es una gran idea. Para resolver esto, podemos utilizar tanto la comparación espacial como la temporal. Esta es la idea del enfoque de la diferencia en la diferencia. Funciona sustituyendo el contrafactual que falta de la siguiente manera: $ E[Y_0(1)|D=1] = E[Y_1(0)|D=1] + (E[Y_0(1)|D=0] - E[Y_0(0)|D=0]) $ Lo que esto hace es tomar la unidad tratada antes de la intervención y añadirle un componente de tendencia, que se estima utilizando el control \\(E[Y_0(1)|D=0] - E[Y_0(0)|D=0]\\). En palabras, está diciendo que el tratado después de la intervención, si no hubiera sido tratado, se parecería al tratado antes del tratamiento más un factor de crecimiento que es el mismo que el del control. Es importante observar que esto supo ne que las tendencias del tratamiento y del control son las mismas: $ E[Y_0(1) - Y_0(0)|D=1] = E[Y_0(1) - Y_0(0)|D=0] $ donde el lado izquierdo es la tendencia contrafactual. Ahora, podemos reemplazar el contrafactual estimado en la definición del efecto del tratamiento \\(E[Y_1(1)|D=1] - E[Y_0(1)|D=1]\\) $ = E[Y(1)|D=1] - (E[Y(0)|D=1] + (E[Y(1)|D=0] - E[Y(0)|D=0]) $ Si reordenamos los términos, obtenemos el estimador clásico Diff-in-Diff. $ = (E[Y(1)|D=1] - E[Y(1)|D=0]) - (E[Y(0)|D=1] - E[Y(0)|D=0]) $ Recibe ese nombre porque obtiene la diferencia entre el tratamiento y el control después y antes del tratamiento. Esto es lo que se ve en el código. fl_before = data.query(&quot;poa==0 &amp; jul==0&quot;)[&quot;deposits&quot;].mean() diff_in_diff = (poa_after-poa_before)-(fl_after-fl_before) diff_in_diff Diff-in-Diff nos dice que deberíamos esperar que los depósitos aumenten en 6,52 R$ por cliente. Obsérvese que el supuesto que hace diff-in-diff es mucho más plausible que los otros 2 estimadores. Simplemente asume que el patrón de crecimiento entre las 2 ciudades es el mismo. Pero no requiere que tengan el mismo nivel de base ni que la tendencia sea cero. Para visualizar lo que hace diff-in-diff, podemos proyectar la tendencia de crecimiento de la ciudad no tratada a la tratada para ver el contrafactual, es decir, el número de depósitos que deberíamos esperar si no hubiera intervención. plt.figure(figsize=(10,5)) plt.plot([&quot;May&quot;, &quot;Jul&quot;], [fl_before, fl_after], label=&quot;FL&quot;, lw=2) plt.plot([&quot;May&quot;, &quot;Jul&quot;], [poa_before, poa_after], label=&quot;POA&quot;, lw=2) plt.plot([&quot;May&quot;, &quot;Jul&quot;], [poa_before, poa_before+(fl_after-fl_before)], label=&quot;Counterfactual&quot;, lw=2, color=&quot;C2&quot;, ls=&quot;-.&quot;) plt.legend(); ¿Ves esa pequeña diferencia entre las líneas discontinuas rojas y amarillas? Si te concentras bien, puedes ver el pequeño efecto del tratamiento en Porto Alegre. Ahora bien, lo que se puede preguntar es “¿hasta qué punto puedo confiar en este estimador? Tengo derecho a que me informen de los errores estándar”. Lo cual tiene sentido, ya que los estimadores sin ellos parecen tontos. Para ello, utilizaremos un ingenioso truco que utiliza la regresión. En concreto, estimaremos el siguiente modelo lineal $ Y_i = _0 + _1 POA_i + _2 Jul_i + _3 POA_i*Jul_i + e_i $ Obsérvese que \\(\\beta_0\\) es la línea de base del control. En nuestro caso, es el nivel de depósitos en Florianópolis en el mes de mayo. Si activamos la variable ficticia de la ciudad tratada, obtenemos \\(\\beta_1\\). Así, \\(\\beta_0 + \\beta_1\\) es la línea de base de Porto Alegre en mayo, antes de la intervención, y \\(\\beta_1\\) es el aumento de la línea de base de Porto Alegre sobre Florianópolis. Si desactivamos la variable ficticia del POA y activamos la de julio, obtenemos \\(\\beta_0 + \\beta_2\\), que es el nivel de Florianópolis en julio, después del período de intervención. \\(\\beta_2\\) es entonces la tendencia del control, ya que lo añadimos a la línea de base para obtener el nivel del control en el periodo posterior a la intervención. Recapitulando, \\(\\beta_1\\) es el incremento que obtenemos al pasar del control al tratamiento, \\(\\beta_2\\) es el incremento que obtenemos al pasar del periodo anterior al periodo posterior a la intervención. Por último, si activamos ambas variables ficticias, obtenemos \\(\\beta_3\\). \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\\) es el nivel en Porto Alegre después de la intervención. Así que \\(\\beta_3\\) es el impacto incremental cuando se pasa de mayo a julio y de Florianópolis a POA. En otras palabras, es el estimador de la diferencia en la diferencia. Si no me cree, compruébelo usted mismo. Deberías obtener exactamente el mismo número que obtuvimos arriba. Y también observe cómo obtenemos nuestros tan deseados errores estándar. smf.ols(&#39;deposits ~ poa*jul&#39;, data=data).fit().summary().tables[1] 8.8.5 Plotly 8.8.5.1 Introducción: tablas y gráficos interactivos plotly es una librería de python gráficos interactivos, en el ámbito estadístico, financiero, geográfico, científico. Es muy útil para hacer gráficos que puedan interactuar con el usuario/a import plotly.graph_objects as go fig = go.Figure(data=go.Bar(y=[2, 3, 1])) fig.show() También podemos hacer tablas interactivas import plotly.graph_objects as go # Valores a mostrar encabezado = [&#39;A Scores&#39;, &#39;B Scores&#39;] valores = [[100, 90, 80, 90], [95, 85, 75, 95]] # Crear figura # 1. llamar librería y función: go.Figure() # 2. Definir datos: data=[] # 3. Usar go.Table() para definir encabezado (header) y valores(cells) # 4. Guardamos el resultado en una variable -&gt; alpicamos fig.show() fig = go.Figure(data=[go.Table(header=dict(values=encabezado), cells=dict(values=valores)) ]) fig.show() # Podemos hacer modificaciones fig = go.Figure(data=[go.Table( #Encabezado header=dict(values=encabezado, line_color=&#39;red&#39;, #Color borde: cambiar rojo fill_color=&#39;lightskyblue&#39;, #Color fondo align=&#39;left&#39;), #Alineación #Valores cells=dict(values=valores, line_color=&#39;darkslategray&#39;,#Color borde fill_color=&#39;lightcyan&#39;, #Color rojo align=&#39;left&#39;)) #Alineación ]) #Una vez creado el fig podemos editar su tamaño fig.update_layout(width=300, height=300) fig.show() &quot;&quot;&quot;+ Para modificar el formato de los datos, ejemplo número de decimales, podemos usar format encabezado = [&#39;A Scores&#39;, &#39;B Scores&#39;, &#39;C Scores&#39;] valores = [[100, 90, 80, 90], [95, 85, 75, 95], [95.12345, 85.12345, 75.12345, 95.12345]] fig = go.Figure(data=[go.Table( #Encabezado header=dict(values=encabezado, line_color=&#39;darkslategray&#39;, #Color borde: cambiar rojo fill_color=&#39;lightskyblue&#39;, #Color fondo align=&#39;left&#39;), #Alineación #Valores cells=dict(values=valores, line_color=&#39;darkslategray&#39;,#Color borde fill_color=&#39;lightcyan&#39;, #Color rojo align=&#39;left&#39;, #Alineación format = [None, &quot;.1f&quot;, &quot;.2f&quot;])) #Formato: en una lista va según el número de columnas ]) #Una vez creado el fig podemos editar su tamaño fig.update_layout(width=400, height=300) fig.show() 8.8.5.2 Graficos de lineas Para este ejemplo vamos a usar la librería plotly.express. Dentro de esta tenemos el DataFrame gapminder datos de expectativa de vida, población, PIB por país. #Importamos la librería import plotly.express as px #Usamos un subconjunto de los datos, seleccionamos América df = px.data.gapminder().query(&quot;continent==&#39;Americas&#39;&quot;) df.head() fig = px.line(df, x=&quot;year&quot;, y=&quot;lifeExp&quot;, color=&#39;country&#39;) fig.show() df_sur = df[(df.country==&quot;Argentina&quot; )| (df.country==&quot;Chile&quot;) | (df.country==&quot;Uruguay&quot;)] fig = px.line(df_sur, x=&quot;year&quot;, y=&quot;gdpPercap&quot;, color=&#39;country&#39;) fig.show() Usamos plotly.express cuando hacemos un ejercicio sencillo. Para comenzar a ejemplos más genéricos o en algunos casos, más complejos, vamos a usar go.Scatter Como primer ejemplo, vamos a usar una función de los DataFrame llamada pivot_table que nos va a permitir modificar nuestra serie del tipo [Fecha, PIB, Pais] a [Fecha, PIB, Argentina, Chile, Uruguay] df.head() import pandas as pd df_sur2 = pd.pivot_table(df_sur, values=[&quot;gdpPercap&quot;], index=[&quot;year&quot;], columns=&quot;country&quot; ) df_sur2.head() #Importamos la librería import plotly.graph_objects as go #Creamos figura fig = go.Figure() #Primer plot #go.Scatter(x=variable_x, y=variable_y) fig.add_trace(go.Scatter(x=df_sur2.index, y=df_sur2.gdpPercap.Argentina, mode=&#39;lines&#39;, name=&#39;Argentina&#39;)) #Segundo plot fig.add_trace(go.Scatter(x=df_sur2.index, y=df_sur2.gdpPercap.Chile, mode=&#39;lines+markers&#39;, name=&#39;Chile&#39;)) # #Tercer plot fig.add_trace(go.Scatter(x=df_sur2.index, y=df_sur2.gdpPercap.Uruguay, mode=&#39;markers&#39;, name=&#39;Uruguay&#39;)) fig.show() 8.8.5.3 Grafico de puntos #Definimos el tipo gráfico fig = px.scatter(df_sur, x=&quot;gdpPercap&quot;, y=&quot;lifeExp&quot;, color = &quot;country&quot;) fig.show() Podemos agregar una tercera dimensión, representada en el tamaño de la burbuja. Para esto usamos el argumento size y podemos transformlo en un gráfico de burbujas. #Definimos el tipo gráfico fig = px.scatter(df_sur, x=&quot;gdpPercap&quot;, y=&quot;lifeExp&quot;, color = &quot;country&quot;, size=&quot;pop&quot;) fig.show() Otro ejemplo es agregar una escala de colores para resaltar una categoría import plotly.graph_objects as go import numpy as np df_chile = df[(df.country==&quot;Chile&quot;)] fig = go.Figure(data=go.Scatter( x = df_chile.gdpPercap, y = df_chile.lifeExp, mode=&#39;markers&#39;, marker=dict( size=16, color=df_chile.lifeExp, #set color equal to a variable colorscale=&#39;Viridis&#39;, # one of plotly colorscales showscale=True ) )) fig.show() import plotly.express as px df = px.data.gapminder().query(&quot;year==2007 and continent==&#39;Americas&#39;&quot;) fig = px.scatter(df, x=&quot;gdpPercap&quot;, y=&quot;lifeExp&quot;, text=&quot;country&quot;, log_x=True, size_max=60) fig.update_traces(textposition=&#39;top center&#39;) fig.update_layout( height=800, title_text=&#39;GDP and Life Expectancy (Americas, 2007)&#39; ) fig.show() 8.8.5.4 Grafico de barras fig = px.bar(df_chile, x=&#39;year&#39;, y=&#39;pop&#39;) fig.show() df_total = px.data.gapminder() df_2007 = df_total[df_total.year==2007] df_2007.head() fig = px.bar(df_2007, x=&quot;continent&quot;, y=&quot;pop&quot;, title=&quot;Wide-Form Input&quot;) fig.show() 8.8.5.5 Grafico de torta df.loc[df[&#39;pop&#39;] &lt; 8.e6, &#39;country&#39;] = &#39;Other countries&#39; # Represent only large countries fig = px.pie(df, values=&#39;pop&#39;, names=&#39;country&#39;, title=&#39;Población en América&#39;) fig.show() 8.8.5.6 Animación df.head() import plotly.express as px df = px.data.gapminder() fig = px.scatter(df, x=&quot;gdpPercap&quot;, y=&quot;lifeExp&quot;, animation_frame=&quot;year&quot;, animation_group=&quot;country&quot;, size=&quot;pop&quot;, color=&quot;continent&quot;, hover_name=&quot;country&quot;, facet_col=&quot;continent&quot;, log_x=True, size_max=45, range_x=[100,100000], range_y=[25,90] ) fig.show() fig = px.bar(df, x=&quot;continent&quot;, y=&quot;pop&quot;, color=&quot;continent&quot;, animation_frame=&quot;year&quot;, animation_group=&quot;country&quot;, range_y=[0,4000000000]) fig.show() 8.8.5.7 Mapas de calor df = px.data.gapminder().query(&quot;continent==&#39;Americas&#39;&quot;) df.head() fig = go.Figure(data=go.Heatmap( z=df.lifeExp, x=df.year, y=df.country, colorscale=&#39;Viridis&#39; )) fig.update_layout( title=&#39;Expectativa de vida&#39;, xaxis_nticks=13) fig.update_layout(width=750, height=750) fig.show() 8.8.5.8 Sunburst charts import plotly.express as px df = px.data.gapminder().query(&quot;year == 2007&quot;) fig = px.sunburst(df, path=[&#39;continent&#39;, &#39;country&#39;], values=&#39;pop&#39;, color=&#39;lifeExp&#39;, hover_data=[&#39;iso_alpha&#39;]) fig.show() 8.8.5.9 Treemap df.head() import plotly.express as px import numpy as np df = px.data.gapminder().query(&quot;year == 2007&quot;) fig = px.treemap(df, path=[px.Constant(&#39;world&#39;), &#39;continent&#39;, &#39;country&#39;], values=&#39;pop&#39;, hover_data=[&#39;iso_alpha&#39;]) fig.show() 8.8.5.10 Histograma import plotly.express as px df = px.data.tips() fig = px.histogram(df, x=&quot;total_bill&quot;, y=&quot;tip&quot;, color=&quot;sex&quot;, marginal=&quot;rug&quot;, hover_data=df.columns) fig.show() 8.8.5.11 Caja y bigote import plotly.express as px df = px.data.tips() fig = px.box(df, x=&quot;day&quot;, y=&quot;total_bill&quot;, color=&quot;smoker&quot;, notched=True) fig.show() 8.8.5.12 Mapas Un mapa Choropleth es un mapa compuesto por polígonos de colores. Se utiliza para representar las variaciones espaciales de una cantidad. Vamos a utilizar la función px.choropleth_mapbox Los principales parámetros de una mapa son: GeoJson- formatted: información geometrica donde cada caracteristica tiene un id o identificador que es posible mapear Lista de valores indexados al ID. Vamos a cargar un archivo GeoJSON que contiene información geometrica para los condados en Estados Unidos. from urllib.request import urlopen import json with urlopen(&#39;https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json&#39;) as response: counties = json.load(response) counties[&quot;features&quot;][0] import pandas as pd df = pd.read_csv(&quot;https://raw.githubusercontent.com/plotly/datasets/master/fips-unemp-16.csv&quot;, dtype={&quot;fips&quot;: str}) df.head() import plotly.express as px fig = px.choropleth_mapbox(df, geojson=counties, locations=&#39;fips&#39;, color=&#39;unemp&#39;, color_continuous_scale=&quot;Viridis&quot;, range_color=(0, 12), mapbox_style=&quot;carto-positron&quot;, zoom=3, center = {&quot;lat&quot;: 37.0902, &quot;lon&quot;: -95.7129}, opacity=0.5, labels={&#39;unemp&#39;:&#39;unemployment rate&#39;} ) fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() 8.8.5.13 Mapas de puntos import pandas as pd us_cities = pd.read_csv(&quot;https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv&quot;) import plotly.express as px fig = px.scatter_mapbox(us_cities, lat=&quot;lat&quot;, lon=&quot;lon&quot;, hover_name=&quot;City&quot;, hover_data=[&quot;State&quot;, &quot;Population&quot;], color_discrete_sequence=[&quot;fuchsia&quot;], zoom=3, height=300) fig.update_layout(mapbox_style=&quot;open-street-map&quot;) fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() 8.8.5.14 Mapas de burbujas df = px.data.gapminder().query(&quot;year==2007&quot;) fig = px.scatter_geo(df, locations=&quot;iso_alpha&quot;, color=&quot;continent&quot;, hover_name=&quot;country&quot;, size=&quot;pop&quot;, projection=&quot;natural earth&quot;) fig.show() import plotly.express as px df = px.data.gapminder() fig = px.scatter_geo(df, locations=&quot;iso_alpha&quot;, color=&quot;continent&quot;, hover_name=&quot;country&quot;, size=&quot;pop&quot;, animation_frame=&quot;year&quot;, projection=&quot;natural earth&quot;) fig.show() import plotly.graph_objects as go df = pd.read_csv(&#39;https://raw.githubusercontent.com/plotly/datasets/master/2014_us_cities.csv&#39;) df.head() df[&#39;text&#39;] = df[&#39;name&#39;] + &#39;&lt;br&gt;Population &#39; + (df[&#39;pop&#39;]/1e6).astype(str)+&#39; million&#39; limits = [(0,2),(3,10),(11,20),(21,50),(50,3000)] colors = [&quot;royalblue&quot;,&quot;crimson&quot;,&quot;lightseagreen&quot;,&quot;orange&quot;,&quot;lightgrey&quot;] cities = [] scale = 5000 fig = go.Figure() for i in range(len(limits)): lim = limits[i] df_sub = df[lim[0]:lim[1]] fig.add_trace(go.Scattergeo( locationmode = &#39;USA-states&#39;, lon = df_sub[&#39;lon&#39;], lat = df_sub[&#39;lat&#39;], text = df_sub[&#39;text&#39;], marker = dict( size = df_sub[&#39;pop&#39;]/scale, color = colors[i], line_color=&#39;rgb(40,40,40)&#39;, line_width=0.5, sizemode = &#39;area&#39; ), name = &#39;{0} - {1}&#39;.format(lim[0],lim[1]))) fig.update_layout( title_text = &#39;2014 US city populations&lt;br&gt;(Click legend to toggle traces)&#39;, showlegend = True, geo = dict( scope = &#39;usa&#39;, landcolor = &#39;rgb(217, 217, 217)&#39;, ) ) fig.show() 8.8.6 Automatización import pandas as pd import openpyxl from openpyxl import load_workbook from openpyxl.styles import Font from openpyxl.chart import BarChart, Reference import string 8.8.6.1 Tablas ancha excel_file = pd.read_excel(&#39;datasets\\\\supermarket_sales.xlsx&#39;) excel_file[[&#39;Gender&#39;, &#39;Product line&#39;, &#39;Total&#39;]] # Confeccionamos tabla report_table = excel_file.pivot_table(index=&#39;Gender&#39;, columns=&#39;Product line&#39;, values=&#39;Total&#39;, aggfunc=&#39;sum&#39;).round(0) report_table # Exportamos tabla report_table.to_excel(&#39;report_2021.xlsx&#39;, sheet_name=&#39;Report&#39;, startrow=4) 8.8.6.2 Hacer el reporte con Openpyxl Crear filas y columnas de referencia: wb = load_workbook(&#39;report_2021.xlsx&#39;) sheet = wb[&#39;Report&#39;] # Celdas de referencia min_column = wb.active.min_column max_column = wb.active.max_column min_row = wb.active.min_row max_row = wb.active.max_row print(f&#39;Min Columns: {min_column}&#39;) print(f&#39;Max Columns: {max_column}&#39;) print(f&#39;Min Rows: {min_row}&#39;) print(f&#39;Max Rows: {max_row}&#39;) Añadir graficos: wb = load_workbook(&#39;report_2021.xlsx&#39;) sheet = wb[&#39;Report&#39;] # Grafico de barras barchart = BarChart() # Localizar datos y categorias data = Reference(sheet, min_col=min_column+1, max_col=max_column, min_row=min_row, max_row=max_row) categories = Reference(sheet, min_col=min_column, max_col=min_column, min_row=min_row+1, max_row=max_row) # Agregar datos y categorias barchart.add_data(data, titles_from_data=True) barchart.set_categories(categories) #location chart sheet.add_chart(barchart, &quot;B12&quot;) barchart.title = &#39;Sales by Product line&#39; barchart.style = 5 #choose the chart style wb.save(&#39;report_2021.xlsx&#39;) Aplicar formulas manualmente: wb = load_workbook(&#39;report_2021.xlsx&#39;) sheet = wb[&#39;Report&#39;] sheet[&#39;B7&#39;] = &#39;=SUM(B5:B6)&#39; sheet[&#39;B7&#39;].style = &#39;Currency&#39; wb.save(&#39;report_2021.xlsx&#39;) Aplicar formulas con celdas de referencias: import string alphabet = list(string.ascii_uppercase) excel_alphabet = alphabet[0:max_column] #note: Python lists start on 0 -&gt; A=0, B=1, C=2. #note2 the [a:b] takes b-a elements excel_alphabet wb = load_workbook(&#39;report_2021.xlsx&#39;) sheet = wb[&#39;Report&#39;] # Suma en columnas B - G: for i in excel_alphabet: if i!=&#39;A&#39;: sheet[f&#39;{i}{max_row+1}&#39;] = f&#39;=SUM({i}{min_row+1}:{i}{max_row})&#39; sheet[f&#39;{i}{max_row+1}&#39;].style = &#39;Currency&#39; # adding total label sheet[f&#39;{excel_alphabet[0]}{max_row+1}&#39;] = &#39;Total&#39; wb.save(&#39;report_2021.xlsx&#39;) # Formatear la tabla wb = load_workbook(&#39;report_2021.xlsx&#39;) sheet = wb[&#39;Report&#39;] sheet[&#39;A1&#39;] = &#39;Sales Report&#39; sheet[&#39;A2&#39;] = &#39;2021&#39; sheet[&#39;A1&#39;].font = Font(&#39;Arial&#39;, bold=True, size=20) sheet[&#39;A2&#39;].font = Font(&#39;Arial&#39;, bold=True, size=10) wb.save(&#39;report_2021.xlsx&#39;) 8.8.6.3 Automatizar el reporte con Python import pandas as pd import openpyxl from openpyxl import load_workbook from openpyxl.styles import Font from openpyxl.chart import BarChart, Reference import string def automate_excel(file_name): &quot;&quot;&quot;The file name should have the following structure: sales_month.xlsx&quot;&quot;&quot; # read excel file excel_file = pd.read_excel(file_name) # make pivot table report_table = excel_file.pivot_table(index=&#39;Gender&#39;, columns=&#39;Product line&#39;, values=&#39;Total&#39;, aggfunc=&#39;sum&#39;).round(0) # splitting the month and extension from the file name month_and_extension = file_name.split(&#39;_&#39;)[1] # send the report table to excel file report_table.to_excel(f&#39;report_{month_and_extension}&#39;, sheet_name=&#39;Report&#39;, startrow=4) # loading workbook and selecting sheet wb = load_workbook(f&#39;report_{month_and_extension}&#39;) sheet = wb[&#39;Report&#39;] # cell references (original spreadsheet) min_column = wb.active.min_column max_column = wb.active.max_column min_row = wb.active.min_row max_row = wb.active.max_row # adding a chart barchart = BarChart() data = Reference(sheet, min_col=min_column+1, max_col=max_column, min_row=min_row, max_row=max_row) #including headers categories = Reference(sheet, min_col=min_column, max_col=min_column, min_row=min_row+1, max_row=max_row) #not including headers barchart.add_data(data, titles_from_data=True) barchart.set_categories(categories) sheet.add_chart(barchart, &quot;B12&quot;) #location chart barchart.title = &#39;Sales by Product line&#39; barchart.style = 2 #choose the chart style # applying formulas # first create alphabet list as references for cells alphabet = list(string.ascii_uppercase) excel_alphabet = alphabet[0:max_column] #note: Python lists start on 0 -&gt; A=0, B=1, C=2. #note2 the [a:b] takes b-a elements # sum in columns B-G for i in excel_alphabet: if i!=&#39;A&#39;: sheet[f&#39;{i}{max_row+1}&#39;] = f&#39;=SUM({i}{min_row+1}:{i}{max_row})&#39; sheet[f&#39;{i}{max_row+1}&#39;].style = &#39;Currency&#39; sheet[f&#39;{excel_alphabet[0]}{max_row+1}&#39;] = &#39;Total&#39; # getting month name month_name = month_and_extension.split(&#39;.&#39;)[0] # formatting the report sheet[&#39;A1&#39;] = &#39;Sales Report&#39; sheet[&#39;A2&#39;] = month_name.title() sheet[&#39;A1&#39;].font = Font(&#39;Arial&#39;, bold=True, size=20) sheet[&#39;A2&#39;].font = Font(&#39;Arial&#39;, bold=True, size=10) wb.save(f&#39;report_{month_and_extension}&#39;) return # Aplicar la función a un archivo automate_excel(&#39;datasets\\\\sales_2021.xlsx&#39;) # Aplicar la función a múltiples archivos # cada uno por separado automate_excel(&#39;datasets\\\\sales_january.xlsx&#39;) automate_excel(&#39;datasets\\\\sales_february.xlsx&#39;) automate_excel(&#39;datasets\\\\sales_march.xlsx&#39;) # option 2: todos juntos excel_file_1 = pd.read_excel(&#39;datasets\\\\sales_january.xlsx&#39;) excel_file_2 = pd.read_excel(&#39;datasets\\\\sales_february.xlsx&#39;) excel_file_3 = pd.read_excel(&#39;datasets\\\\sales_march.xlsx&#39;) new_file = pd.concat([excel_file_1, excel_file_2, excel_file_3], ignore_index=True) new_file.to_excel(&#39;sales_2021.xlsx&#39;) automate_excel(&#39;sales_2021.xlsx&#39;) 8.9 Web Scraping en Python 8.9.1 Introducción Web scrapping o escrapeo de datos es un conjunto de herramientas que permiten obtener datos que esten disponibles en sitios web. Estos datos pueden estar estructurados (datos enfocados en el analisis de datos) o no estructurados (datos previos a un formato de analisis de datos). Vamos a ver algunos conceptos básicos, solo con el fin de que los conozcan, y luego vamos a ver varios ejemplos. 8.9.1.1 HTML Hypertext Markup Language Es el lenguaje utilizado para diseñar sitios web. Contiene varias etiquetas que almacenan todo tipo de información contenido en una página web. Cada etiqueta tiene un rol especifico. Para ver detalles: w3schools.com/tags/default.asp Esta organizada jerarquicamente (parecen ramas de arboles). Es importante entender este concepto Recuerden que nuestro objetivo es entender esta estructura para poder extraer la información. 8.9.1.2 Estructura 01figuras/image.png html: inicio/fin archivo html body: cuerpo div: secciones p: parrafos. 01figuras/image.png 01figuras/image.png # Crear un HTML html = &#39;&#39;&#39; &lt;html&gt; &lt;head&gt; &lt;title&gt;Intro HTML&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Chao Chao!&lt;/p&gt; &lt;p&gt;Hola Hola!&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &#39;&#39;&#39; html 8.9.1.3 CSS selectors Cascading Style Sheets Al igual que HTML sirve para generar sitios web. En CSS hay algunos selectores adicionales, tres principales: class, id y tag. Estos atributos son subconjuntos de información especifica contenida en cada sección # HTML/CSS con atributos html = &#39;&#39;&#39; &lt;html&gt; &lt;body&gt; &lt;div class=&quot;class1&quot; id=&quot;div1&quot;&gt; &lt;p class=&quot;class2&quot;&gt;atributos&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;holaholabola&quot;&gt; &lt;p class=&quot;class2&quot;&gt;holaholahola!&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; &#39;&#39;&#39; 8.9.1.4 XPATH Es un lenguaje que permite construir expresiones que recorren y procesan un documento. Es parecido a la idea de expresiones regulares al trabajar con texto.- Tiene en cuenta la estructura jerarquica. xpath = &quot;/html/body/div[2]&quot; /: sirve para moverse de una generación a otra etiqueta: indica donde buscar Con doble // llamo a todas las generaciones image.png Más detalles en: https://www.geeksforgeeks.org/introduction-to-xpath/ Ejercicio 4.9.1: Para el siguiente html escriba el xpath que le permita ir a la frase “aprendo python haciendo” html = &#39;&#39;&#39; &lt;html&gt; &lt;body&gt; &lt;div&gt; &lt;p&gt;Me gusta el chocolate&lt;/p&gt; &lt;p&gt;Aquí no&lt;/p&gt; &lt;/div&gt; &lt;div&gt; &lt;p&gt;Aprendo python haciendo&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; &#39;&#39;&#39; xpath = &#39;/html/body/div[2]/p[1]&#39; 8.9.2 Muchos ejemplos practicos de Web - scraping Todos los conceptos anteriores son muy útiles cuando se trabajan con sitios web complejos. Es probable que si profundizan en el uso de web-scraping los vayan a necesitar. Sin embargo, en la practica vamos a utilziar programas auxiliares que nos permiten conocer las rutas Ahora, vamos a mirar como utilizar la siguiente extensión: https://selectorgadget.com/ WS va a utilizar los selectores de CSS y algunas librerías para obtener información guardada jerarquicamente en algún sitio web. Imaginemos queremos obtener información de los precios de productos en Amazon, los pasos que necesitamos son: Primero, vamos a hacer una conexión con la página web desde el código de Python. Una vez conectada, vamos a acceder a la información neta de la págica utilizando alguna librearía. Vamos a rastrear los selectores que contienen la información que deseamos obtener (en este caso el precio) Una vez reconocido, utilizaremos distintas funciones para extraer la información. Una ves onbtenida la información la ordenamos con funciones que ya conocemos (ej. utilizando Pandas). Vamos a revisar tres librerías: (i) Beatiful Soap; (ii) Selenium. Van avanzando en nivel de dificultad. 8.9.2.1 Beautiful Soap Beautiful Soup es un paquete de python que se utiliza para analizar documentos HTML y XML. Es una de las bibliotecas más simples y amigables de python. Beautiful soup crea un árbol de análisis para diferentes páginas analizadas que se utilizan para extraer datos de la fuente (HTML). Sintaxis Básica: from bs4 import BeautifulSoup import requests ##A Python HTTP library #url = &#39;WEBSITE_URL&#39; (link) #res = requests.get(url) ## HTTP Requests #print(res) ## if o/p is &lt;200&gt; then connection established #soup = BeautifulSoup(res.text,&#39;html.parser&#39;) #print(soup) ## source code html.parser se utiliza para analizar documentos HTML. soup es un nombre de variable general que se utiliza para la definición del código fuente. # Find, Find All y Select Find es una función incorporada en la biblioteca que se utiliza para obtener el contenido de la fuente. Encuentra la primera ocurrencia del elemento o selector. find se utiliza principalmente para extraer encabezados, títulos, nombres de productos, etc. de la página web. Ejemplos. ## Retorna el título print(soup.find(&#39;title&#39;).text) ## Retorna el texto del primer encabezado print(soup.find(&#39;h1&#39;).text) ## Retorna el contenido de la primera etiqueta div con los atributos tags print(soup.find(&#39;div&#39;, {&#39;class&#39;: &#39;tags&#39;})) ## Retorna el texto de etiqueta &quot;a&quot;, pero con atributo &quot;tag&quot;. print(soup.find(&#39;a&#39;,{&#39;class&#39;:&#39;tag&#39;}).text) Find_all encuentra todas las ocurrencias del elemento o selector. Se utiliza principalmente para rastrear los datos de las tablas, reseñas de productos, detalles y productos listados en una página web. Devuelve la salida en forma de lista. ## Retorna todos los div con el atributo/clase product name print(soup.find_all(&#39;div&#39;, {&#39;class&#39;: &#39;productname&#39;})) productnames = soup.find_all(&#39;div&#39;, {&#39;class&#39;: &#39;productname&#39;}) for name in productnames: print(name.text) Select es una de las funciones más potente. Selecciona el elemento o selector directamente en forma de lista. Utiliza selectores CSS y selectores de etiquetas. También devuelve la salida en forma de lista. ## Retorna todas las etiquetas con la clase &quot;productname&quot; en una lista # print(soup.select(.productname)) ## Retorna el primer div con el id product en formato texto print(soup.select(div #productkey)[0].get_text()) 8.9.2.2 Ejemplo 1: Scraping para Wikipedia - conociendo Villasana de Mena # Cargamos librerías from bs4 import BeautifulSoup import requests # Establecemos conexión res = requests.get(&#39;https://es.wikipedia.org/wiki/Villasana_de_Mena&#39;) res # Obtenemos la codificación de la página web soup = BeautifulSoup(res.text,&#39;html.parser&#39;) #print(soup) nombre = soup.select(&quot;#firstHeading&quot;)[0].getText().strip() print(nombre) primerparrafo = soup.select(&#39;.ambox-content+ p&#39;)[0].get_text().strip() primerparrafo todoslosparrafos = [soup.select(&#39;p&#39;)[i].get_text().strip() for i in range(1,len(soup.select(&#39;p&#39;)))] todoslosparrafos mapa = soup.select(&quot;tr:nth-child(5) img&quot;)[0] mapa image_url = mapa.attrs[&quot;src&quot;] image_url url_base = &quot;https:&quot; image_url = mapa.attrs[&quot;src&quot;] url_final = url_base + image_url url_final img_data = requests.get(url_final).content #img_data img_data = requests.get(url_final).content with open(&#39;m1.jpg&#39;, &#39;wb&#39;) as handler: handler.write(img_data) import matplotlib.pyplot as plt import matplotlib.image as mpimg img = mpimg.imread(&#39;m1.jpg&#39;) imgplot = plt.imshow(img) plt.show() # Importemos todas las imagenes en un iterador imagenes = [soup.select(&quot;td :nth-child(1) .image img&quot;)[i] for i in range(0,len(soup.select(&quot;td :nth-child(1) .image img&quot;)))] imagenes url_1 = &quot;https:&quot; + imagenes[0].attrs[&quot;src&quot;] url_2= &quot;https:&quot; + imagenes[1].attrs[&quot;src&quot;] url_3 = &quot;https:&quot; +imagenes[2].attrs[&quot;src&quot;] urls = [url_1, url_2, url_3] urls for i in range(0,3): img_data = requests.get(urls[i]).content with open(&quot;imagen&quot; + str(i) + &quot;.jpg&quot;, &quot;wb&quot;) as handler: handler.write(img_data) 8.9.2.3 Ejemplo 2: Datos de libros # importing the libraries from bs4 import BeautifulSoup import requests import pandas as pd # Request r = requests.get(&#39;http://books.toscrape.com/&#39;) # Obtenemos los datos soup = BeautifulSoup(r.text, &#39;html.parser&#39;) #print(soup) # Generamos una variable que tenga toda la info con el atributo produc_pod results = soup.find_all(attrs={&#39;class&#39;:&#39;product_pod&#39;}) # let&#39;s check how many results we got len(results) #print(results) # Exploremos el primer libro first_book = results[0] first_book # Obtenemos el titulo first_book.h3.a.get(&#39;title&#39;) # su precio first_book.find(&#39;p&#39;, class_=&quot;price_color&quot;).text[2:] # selecting from the 3rd charcater on to get just the price number # Ahora hagamoslos para todos los libros names= [] prices= [] # Rango de páginmas para trabajar pages = [str(i) for i in range(1,51)] for page in pages: response = requests.get(&#39;http://books.toscrape.com/catalogue/page-&#39; + page + &#39;.html&#39;) page_html = BeautifulSoup(response.text, &#39;html.parser&#39;) book_containers = page_html.find_all(attrs={&#39;class&#39;:&#39;product_pod&#39;}) for book in book_containers: names.append(book.h3.a.get(&#39;title&#39;)) prices.append(book.find(&#39;p&#39;, class_=&quot;price_color&quot;).text[2:]) # Ahora vamos a crear una base de datos con todos los libros books_df = pd.DataFrame(list(zip(names, prices)), columns=[&#39;titles&#39;,&#39;prices (£)&#39;]) books_df # Ahora hagamoslos para todos los links links = [] # Rango de páginmas para trabajar pages = [str(i) for i in range(1,51)] for page in pages: response = requests.get(&#39;http://books.toscrape.com/catalogue/page-&#39; + page + &#39;.html&#39;) page_html = BeautifulSoup(response.text, &#39;html.parser&#39;) book_containers = page_html.find_all(attrs={&#39;class&#39;:&#39;product_pod&#39;}) for book in book_containers: links.append(book.h3.a.get(&quot;href&quot;)) links[100] enlace_final = [] for i in range(0,len(links)): enlace_final.append(&quot;http://books.toscrape.com/catalogue/&quot; + links[i]) #print(enlace_final) Ejercicio 4.9.2: Arme un código que le permita extraer todas las portadas. Utilice la lista enlace_final que acaba de crear enlace_final[0] portadas = [] response = requests.get(enlace_final[0]) page_html = BeautifulSoup(response.text, &#39;html.parser&#39;) book_containers = page_html.find_all(&quot;img&quot;) book_containers[0].get(&quot;src&quot;) enlace_book = &quot;http://books.toscrape.com/&quot; + book_containers[0].get(&quot;src&quot;) enlace_book #enlace_final # Ahora hagamoslos para todos las portadas portadas = [] # Rango a trabajar for i in range(0,100): response = requests.get(enlace_final[i]) page_html = BeautifulSoup(response.text, &#39;html.parser&#39;) book_containers = page_html.find_all(&quot;img&quot;) enlace_book = &quot;http://books.toscrape.com/&quot; + book_containers[0].get(&quot;src&quot;) portadas.append(enlace_book) # Exportemos for i in range(0,10): img_data = requests.get(portadas[i]).content with open(&quot;imagen&quot; + str(i) + &quot;.jpg&quot;, &quot;wb&quot;) as handler: handler.write(img_data) 8.9.2.4 Ejemplo 3: Datos de supermercado Vamos a operar en dos pasos para que vean una forma alternativa de hacer un scrapping Vamos a scrapiar la página de frutas del lider # importing the libraries from bs4 import BeautifulSoup import requests import pandas as pd # importing the libraries from bs4 import BeautifulSoup import requests import pandas as pd # Request r = requests.get(&#39;https://www.lider.cl/supermercado/category/Frutas-y-Verduras/Frutas/_/N-1hd567c&#39;) # Obtenemos los datos soup = BeautifulSoup(r.text, &#39;html.parser&#39;) #print(soup) # Generamos una variable que tenga toda la info con el atributo produc_pod contenedor = soup.find_all(attrs={&#39;class&#39;:&#39;product-image&#39;}) pproducto = contenedor[0] pproducto # Obtenemos el nombre del producto nombre = pproducto.find(&#39;img&#39;).get(&quot;alt&quot;) nombre # Vamos a obtener los links enlace = &quot;https://www.lider.cl&quot; + pproducto.find(&#39;a&#39;, class_=&quot;product-link&quot;).get(&quot;href&quot;) enlace # Ahora para todos los productos de la pagina nombre = [] enlaces = [] response = requests.get(&quot;https://www.lider.cl/supermercado/category/Frutas-y-Verduras/Frutas/_/N-1hd567c&quot;) page_html = BeautifulSoup(response.text, &#39;html.parser&#39;) product_containers = page_html.find_all(attrs={&#39;class&#39;:&#39;product-image&#39;}) for product in product_containers: nombre.append(product.find(&#39;img&#39;).get(&quot;alt&quot;)) enlaces.append(&quot;https://www.lider.cl&quot; + product.find(&#39;a&#39;, class_=&quot;product-link&quot;).get(&quot;href&quot;)) nombre enlaces productos_df = pd.DataFrame(list(zip(nombre, enlaces)), columns=[&#39;Nombre Producto&#39;,&#39;enlace&#39;]) productos_df.head() Ahora vamos a operar en el segundo paso. Para cada enlace vamos a crear una base de datos con la información que contiene. En este caso vamos a iterar sobre los enlaces de los productos #enlaces # Nombre producto response = requests.get(&quot;https://www.lider.cl/supermercado/product/Palta-Palta-Hass-Malla/4545&quot;) soup = BeautifulSoup(response.text, &#39;html.parser&#39;) contenedor = soup.find_all(attrs={&#39;class&#39;:&#39;product-info&#39;}) contenedor # Nombre del producto nombre = soup.find(&quot;span&quot;, class_=&quot;product-descript&quot;).get_text().strip() nombre precio = soup.find(&quot;p&quot;, class_=&quot;price&quot;).get_text().strip() precio puntos = soup.find(class_=&quot;miclub&quot;).get_text().strip().replace(u&#39;\\xa0&#39;, u&#39; &#39;) puntos #enlaces # Ahora escalemos... nombres = [] precios = [] puntos = [] for i in range(0,10): response = requests.get(enlaces[i]) soup = BeautifulSoup(response.text, &#39;html.parser&#39;) nombres.append(soup.find(&quot;span&quot;, class_=&quot;product-descript&quot;).get_text().strip()) precios.append(soup.find(&quot;p&quot;, class_=&quot;price&quot;).get_text().strip()) puntos.append(soup.find(class_=&quot;miclub&quot;).get_text().strip().replace(u&#39;\\xa0&#39;, u&#39; &#39;)) puntos productos_final_df = pd.DataFrame(list(zip(nombres, precios, puntos)), columns=[&#39;Nombre Producto&#39;,&#39;Precio&#39;, &quot;Puntos&quot;]) productos_final_df # Hacemos un merge df = productos_df.merge(productos_final_df, on = &quot;Nombre Producto&quot;) df 8.9.2.5 Ejemplo 4: Datos de transparencia # importing the libraries from bs4 import BeautifulSoup import requests import pandas as pd # Request r = requests.get(&#39;http://transparencia.mineduc.cl/2018/dotacion/enero/per_planta.html&#39;) # Obtenemos los datos soup = BeautifulSoup(r.text, &#39;html.parser&#39;) #print(soup) print(&#39;Classes of each table:&#39;) for table in soup.find_all(&#39;table&#39;): print(table.get(&#39;class&#39;)) # Creating list with all tables tables = soup.find_all(&#39;table&#39;) # Definimos dataset df = pd.DataFrame(columns=[&#39;Estamento&#39;, &#39;Profesion&#39;, &#39;Region&#39;,&#39;Sueldo&#39;]) df import warnings warnings.filterwarnings(&quot;ignore&quot;) for row in table.tbody.find_all(&#39;tr&#39;): columns = row.find_all(&#39;td&#39;) if(columns != []): estamento= columns[0].text.strip() calif = columns[5].text.strip() region = columns[7].text.strip() plata = columns[10].text.strip() df = df.append({&#39;Estamento&#39;: estamento, &#39;Profesion&#39;: calif, &#39;Region&#39;: region, &#39;Sueldo&#39;: plata}, ignore_index=True) df 8.9.3 Selenium Nos permite hacer web-scraping más sofisticados. Por ejemplo, si deseo hacer un click en algo. Veremos en detalle un ejemplo que muestra la utilidad de Selenium para automatizar procesos de extracción de datos 8.9.3.1 Ejemplo 1: Obteniendo información de TripAdvisor from selenium import webdriver import csv import time import warnings warnings.filterwarnings(&quot;ignore&quot;) # Ejemplo: obtener los reviews desde Trip Advisor # Importamos librearía principal from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # Definimos link que vamos a mirar URL = &quot;https://www.tripadvisor.com/Restaurant_Review-g190328-d8867662-Reviews-Storie_Sapori_La_Valletta-Valletta_Island_of_Malta.html&quot; # Abrimos el navegador que vamos a utilizar driver = webdriver.Chrome() # Abrimos la página web que vamos a utilizar driver.get(URL) # Hacemos que la página web espere 10 segundos, la idea es que pueda cargar el cookie wait = WebDriverWait(driver, 10) wait.until(EC.element_to_be_clickable((By.XPATH, &quot;//button[text()=&#39;I Accept&#39;]&quot;))).click() # Vamos a abrir un archivo csv csvFile = open(&quot;reviews.csv&quot;, &quot;w&quot;, newline=&#39;&#39;, encoding=&quot;utf-8&quot;) csvWriter = csv.writer(csvFile) # Vamos a incluir las columnas para las cuales queremos obtener información csvWriter.writerow([&#39;Score&#39;,&#39;Date&#39;,&#39;Title&#39;,&#39;Review&#39;]) # Ahora, tengo que ir a las recomendaciones y apretar en &quot;more reviews&quot; para # cargar todas las revisiones driver.find_element_by_xpath(&#39;//*[@id=&quot;taplc_location_reviews_list_resp_rr_resp_0&quot;]/div/div[17]&#39;).click() time.sleep(5) # Wait for reviews to load # Vamos a revisar paso a paso como logramos esto # Ahora vamos a buscar los elementos contenidos en las reviews reviews = driver.find_elements_by_xpath(&quot;//div[@class=&#39;ui_column is-9&#39;]&quot;) num_page_items = min(len(reviews), 10) # Vemos la cantidad de reviews de esta pagina num_page_items # Iteramos sobre la cantidad de reviews. Para cada review vamos a ir obteniendo la información for i in range(num_page_items): # Obtenemos el puntaje, la fecha, el titulo y la review score_class = reviews[i].find_element_by_xpath(&quot;.//span[contains(@class, &#39;ui_bubble_rating bubble_&#39;)]&quot;).get_attribute(&quot;class&quot;) score = score_class.split(&quot;_&quot;)[3] date = reviews[i].find_element_by_xpath(&quot;.//span[@class=&#39;ratingDate&#39;]&quot;).get_attribute(&quot;title&quot;) title = reviews[i].find_element_by_xpath(&quot;.//span[@class=&#39;noQuotes&#39;]&quot;).text review = reviews[i].find_element_by_xpath(&quot;.//p[@class=&#39;partial_entry&#39;]&quot;).text.replace(&quot;\\n&quot;, &quot;&quot;) # Escribimos sobre el CSV csvWriter.writerow((score, date, title, review)) # Cerramos CSV csvFile.close() # Ahora esperamos unos segundos # Apretamos la pagina siguiente driver.find_element_by_xpath(&quot;/html/body/div[2]/div[2]/div[2]/div[6]/div/div[1]/div[4]/div/div[5]/div/div[18]/div/div/div/a[2]&quot;).click() time.sleep(3) # Repitamos el proceso csvFile = open(&quot;reviews2.csv&quot;, &quot;w&quot;, newline=&#39;&#39;, encoding=&quot;utf-8&quot;) csvWriter = csv.writer(csvFile) # Vamos a incluir las columnas para las cuales queremos obtener información csvWriter.writerow([&#39;Score&#39;,&#39;Date&#39;,&#39;Title&#39;,&#39;Review&#39;]) driver.find_element_by_xpath(&#39;//*[@id=&quot;taplc_location_reviews_list_resp_rr_resp_0&quot;]/div/div[17]&#39;).click() time.sleep(5) # Wait for reviews to load reviews = driver.find_elements_by_xpath(&quot;//div[@class=&#39;ui_column is-9&#39;]&quot;) num_page_items = min(len(reviews), 10) for i in range(num_page_items): # Obtenemos el puntaje, la fecha, el titulo y la review score_class = reviews[i].find_element_by_xpath(&quot;.//span[contains(@class, &#39;ui_bubble_rating bubble_&#39;)]&quot;).get_attribute(&quot;class&quot;) score = score_class.split(&quot;_&quot;)[3] date = reviews[i].find_element_by_xpath(&quot;.//span[@class=&#39;ratingDate&#39;]&quot;).get_attribute(&quot;title&quot;) title = reviews[i].find_element_by_xpath(&quot;.//span[@class=&#39;noQuotes&#39;]&quot;).text review = reviews[i].find_element_by_xpath(&quot;.//p[@class=&#39;partial_entry&#39;]&quot;).text.replace(&quot;\\n&quot;, &quot;&quot;) # Escribimos sobre el CSV csvWriter.writerow((score, date, title, review)) # Cerramos CSV csvFile.close() # Cerramos navegador driver.close() De esta forma podemos ir automatizando el scraping de diversas formas Para cualquier proceso de obtención de datos que requiera manipulación más complejas: cambio de páginas, ingresar contraseñas, entre otras. Selenium es una buena opción 8.9.3.2 Ejemplo 2: Ingresar una contraseña import time from selenium import webdriver driver = webdriver.Chrome() driver.get(&quot;https://mail.rediff.com/cgi-bin/login.cgi&quot;) # Identificamos la posición del nombre de usuario, la contrase{a y señalamos que poner driver.find_element_by_name(&quot;login&quot;).send_keys(&quot;ncamposb&quot;) time.sleep(0.2) driver.find_element_by_name(&quot;passwd&quot;).send_keys(&quot;contraeñaultrasecreta23&quot;) time.sleep(0.4) # Le indicamos que aprete sobre sign in driver.find_element_by_class_name(&quot;signinbtn&quot;).click() # Cerramos el navegador "],["análisis-de-datos-con-r.html", "9 Análisis de datos con R 9.1 Introducción a R y RStudio 9.2 Herramientas de programación 9.3 Manipulación de bases de datos (Parte I) 9.4 Manipulación de bases de datos (Parte II) 9.5 Manipulación de bases de datos (Parte III) 9.6 Análisis de datos 9.7 Visualización 9.8 Análisis de datos y generación de informes con R Markdown", " 9 Análisis de datos con R 9.1 Introducción a R y RStudio 9.1.1 ¿Por qué utilizar R como lenguaje de programación? Código abierto: Licencia no tiene costo. Versátil: análisis estadístico y econométrico + tareas de programación útiles (ej. web - scraping). Flexible y reproducible: garantiza reproducibilidad, es flexible ante cambios y detección de errores. Demandado: Lenguajes de programación de código abierto son solicitados cada vez más en el mercado laboral. 9.1.2 Términos clave y recurrentes a lo largo del curso RStudio: Interfaz Gráfica para el usuario. Esta diseñada para un uso más sencillo de R. Objetos: Cualquier cosa que guardes en R. Por ejemplo: bases de datos, variables, listas de nombres, gráficos. Funciones: Una operación producida por un código tal que acepta insumos y retorna productos. Paquetes: Es un conjunto de funciones agrupadas según su objetivo. Scripts/Códigos: Documento que contiene todos los comandos utilizados en un proceso de análisis de datos. 9.1.3 Recursos de Aprendizaje Hojas de resumen: material disponible con un resumen de los principales comandos/paquetes/funciones para distintos temas. Disponibles en: https://www.rstudio.com/resources/cheatsheets/ Foros y sitios web: Stackoverflow Medium El Blog de RStudio Libros: R for Data Science Recursos sobre R en español. Bookdown 9.1.4 Interfaz RStudio 9.1.4.1 Cuatro secciones en Rstudio Las fundamentales son: Scripts, Console, Environment/Files/Help. Scripts: Donde desarrollaremos los códigos. Es un cuarderno. Console: Donde veremos los resultados. Environment: Donde veremos los datos y la información que vamos generando. Files: Donde veremos los archivos asociados a nuestros proyectos. Help: Donde desarrollaremos los códigos. Es un cuarderno. Para personalizar RStudio hay que ir a: Tools -&gt; Global Options. Allí pueden cambiar el color del ambiente, tipo de letra, orden. La idea es utilizar RStudio de la forma mas intuitiva posible. 9.1.4.2 Escribir comentarios Es muy importante escribir comentarios a lo largo de los códigos. Tambíen es importante saber escribir buenos comentarios. Una buena guía que habla un poco de eso y de muchas cosas más es: Code and Data for the Social Sciences: A Practitioner’s Guide. (#) para escribir comentarios en el script. Si quiero marcar varias lineas como comentario: ctrl + shift + c 9.1.4.3 Índices Es importante escribir índices para documentar sus códigos. Para observarlo debo escribir: ctrl + shift + o Los títulos pueden estar jerarquizados: # El mas importante ## Este es un poco menos importante ### Este es un poco menos importante que el anterior #### El menos importante Es importante actualizarlo constantemente para no perder funcionalidad. 9.1.5 Instalación de paquetes y ayuda Los paquetes son basicamente conjuntos de funciones. Los paquetes se instalan una vez, pero se llaman siempre que se vayan a utilizar. 9.1.5.1 Librerías Esto se puede hacer de dos maneras: Desde la pestaña packages en la esquina inferior derecha. Con comandos install.packages(&quot;dplyr&quot;) Varios paquetes a la vez: install.packages(c(&quot;dplyr&quot;,&quot;ggplot2&quot;,&quot;rio&quot;)) Error común: install.packages(&quot;dplyr&quot;, &quot;ggplot2&quot;) Por lo general es bueno instalarlas desde el inicio del trabajo dado que comúnmente usamos las mismas librerías al realizar análisis de datos. Con esta función le decimos a R que instale el paquete si este no está instalado (algo típico cuando cambiamos de computador): if(!require(dplyr)) {install.packages(&quot;dplyr&quot;)} Cargar librerías: library(dplyr) Si queremos ver que hay dentro de cada paquete: ls(&quot;package:dplyr&quot;, all = TRUE) #ls = list objects Importante: el paquete debe ser instalado una vez, pero cargado cada vez que se utilice. Muchas veces hay actualizaciones. Para revisar e instalarlas. update.packages() Alternativamente puedo conectar paquete con función utilizando ::. Si hago esto no es necesario llamar a la librería para utilizar esa función en particular. No obstante, lo recomendable es cargar todas las librerías de los paquetes que voy a utilizar al inicio. 9.1.5.2 Función de ayuda Sobre una función en particular: help(mean) ?mean mean #pulsar la tecla F1 sd Sobre un paquete en particular: help(&quot;dplyr&quot;) library(help=&quot;dplyr&quot;) 9.1.5.3 Shortcuts útiles Esc: interrumpir el comando actual Ctrl + s: guardar tab: autocompletar Ctrl + Enter: ejecutar línea Ctrl + Shift + C: comentar &lt;-: Alt + - / option + - %&gt;%: ctrl + shift + m (pipe) Ctrl + l: limpiar Ctrl + alt + b: ejecutar todo hasta aquí (flechas en la consola me permiten ver los últimos comandos utilizados). Shift + lineas: seleccionar varias lineas Ctrl + f: buscar/remplazar Ctrl + \"flecha arriba\" en la consola: ver comandos utilizados. 9.1.5.4 Limpiar “environment” en R Eliminar todos los objetos: rm(list=ls()) Eliminar sólo un objeto: rm(data1) Si quiero limpiar la consola tengo que apretar Ctrl + L. 9.1.5.5 Identificar el paquete de una función Hay ocasiones en que queremos saber de que paquete es una función determinada. Para ello, revisar: https://sebastiansauer.github.io/finds_funs/ 9.1.5.6 Error común Por ejemplo: x &lt;- &quot;hola&quot; Noten que en la consola aparece un signo +. En estos casos RStudio se detiene porque probablemente se les olvido un ) o bien un #. En estos casos hay que corregir el error para ejecutar nuevamente y luego apretar esc en la consola para seguir ejecutando los comandos. 9.1.6 Manipulación de objetos 9.1.6.1 Usar R como calculadora/ejecutar comandos De manera separada (seleccionar las ordenes+ctrl+Enter): 2+2 3*5^(1/2) Ejecutar todas las instrucciones: 2+2 ; 3*5^(1/2) 3+4 5*4 8/4 6^7 6^77 log(10) log(1) sqrt(91) # raiz cuadrada round(7.3) # redondear Incluso grandes operaciones: sqrt(91) + 4892788*673 - (log(4)*round(67893.9181, digits = 2)) Incluso uso de números imaginarios: 2i+5i+sqrt(25i) 9.1.6.2 Creacion de objetos: asignaciones y funciones con el signo &lt;- asignamos valores. También se puede utilizar =, pero no es recomendable, ya que confunde. y &lt;- 2 + 4 y Las asignaciones son MUDAS. Si no las llamo, no aparecen en la consola. Lo anterior es una operación sencilla, pero lo que queremos es generar asignaciones con funciones. Podemos utilizar funciones. Las funciones son la parte central del uso de R. Algunas funciones vienen instaladas en R. Otras funciones hay que obtenerlas desde paquetes. También es posible escribir tus propias funciones (). Las funciones estan por lo general escritas en paréntesis, por ejemplo filter(). Hay ocasiones en que las funciones estan relacionadas con un paquete específico dplyr::filter(). Ejemplo 1: función simple sqrt(49) Ejemplo 2: sobre una base de datos summary(mtcars$mpg) Nota: mtcars viene incluidad en R. Para ver mas: data() Otra función: x &lt;- 2 y &lt;- 3 z &lt;-c(x,y) z Uso de funciones aritméticas: mean(z) median(z) Relaciones entre objetos: w &lt;- mean(z) Creación de objetos por asignación: a &lt;- 3+10 b &lt;- 2*4 Comparar objetos: a &gt; b Notar que las asignaciones son silenciosas: a b # o altenativamente utilizar print print(a) print(b) Creación de objetos usando funciones: valores &lt;- c(a,b) promedio &lt;- mean(valores) print(promedio) promedio Podemos escribir un promedio aquí usando función mean(): a &lt;- 2 b &lt;- 5 valores1 &lt;- c(a,b) promedio1 &lt;-mean(valores1) print(promedio1) Limpiamos datos nuevamente: rm(list = ls()) rm(promedio) 9.1.6.3 Creacion de objetos y asignaciones Es importante espaciar codigos. Definimos dos vectores utilizando la función c() educ &lt;- c(8,12,8,11,16,14,8,10,14,12) ingreso &lt;- c(325,415,360,380,670,545,350,420,680,465) Calculamos promedio, desviación estándar y correlación mean(ingreso) promedioingreso &lt;- mean(ingreso) sd(ingreso) sdingreso &lt;- sd(ingreso) cor(educ, ingreso) coreduing &lt;- cor(educ,ingreso) Graficamos plot(educ, ingreso) Estimar una regresión lineal lm(ingreso ~ educ) Ejercicio 2.1.1: Nombrar objetos De los siguientes ejemplos, ¿Cuáles son nombres de variables válidas en R? # min_height # max.height # _age # .mass # MaxLength # Min-length # 2widths # Calsius2kelvin 9.1.7 Tipos de objetos 9.1.7.1 Vectores R opera componente por componente, por lo que es muy sencillo poder trabajar con vectores y matrices. Para crear un vector utilizamos la funcion c() x &lt;- c(1,2,3,4,5) #o bien y &lt;- c(6:8) Veamos los vectores z &lt;- x + y z Supongamos los siguientes vectores: x&lt;-c(1:4) y&lt;-c(1:3) ¿Cuál es su longitud? length(x) length(y) Si no tienen la misma longitud, ¿cual sería el resultado de x + y? z &lt;- x + y z IMPORTANTE: En este caso R realiza la operación de todos modos, pero nos indica que hay una advertencia de que sus dimensiones difieren. Lo relevante de los vectores es que sólo se puede concatenar elementos del mismo tipo, de lo contrario R nos arroja error. x &lt;- rep(1.5:9.5,4) #genera repeticiones de los valores definidos y &lt;- c(20:30) x1 &lt;- c(1,2) x2 &lt;- c(3,4) x3 &lt;- c(x1,x2) x4 &lt;- c(c(1,2), c(3,4)) Subconjunto de un vector y[3] # obtener el tercer elemento y[2:4] y[4:2] y[c(2,6)] y[c(2,16)] 9.1.7.2 Matrices 9.1.7.2.1 Definir matrices Sintaxis general mi.matriz &lt;- matrix(vector, ncol = num_columnas, nrow = num_filas, byrow = valor_logico, dimnames = list(vector_nombres_filas, vector_nombres_columnas) ) Para crear matrices utilizamos la función matrix() x &lt;- matrix (data = c(1,2,3,4), nrow = 2, ncol = 2) x1 &lt;- matrix(c(1,2,3,4), 2, 2) No es necesario poner data=, pero por orden mental es mejor hacerlo. x x1 Notar que por DEFECTO rellena columna por columna. Podemos explicitar que queremos realizar la matriz fila por fila y &lt;- matrix(data = c(1:4), nrow = 3, ncol = 2, byrow = TRUE) y Podemos saber cual es la dimensión de x dim(y) dim(y)[1] # cantidad de filas dim(y)[2] # cantidad de columnas y&lt;- matrix(c(1,2,3,4), nrow=2, ncol=2, byrow=2) y Los va a repetir! y &lt;- matrix(c(1,2,3,4), nrow=2, ncol=3, byrow= 2) y Notar que el orden en cualquier matrix es filas x columnas. Podemos también omitir la cantidad de filas o columnas en la matriz y obtenemos el mismo resultado y &lt;- matrix(c(1:4), 2, byrow=T) y En el caso de crear matrices vacías hay que definir las dimensiones y &lt;- matrix(nrow=3, ncol=3) y #útil para los loops Darle nombre a las filas y columnas y &lt;- matrix (c(1:4), 2, byrow = FALSE, dimnames=list(c(&quot;X1&quot;,&quot;X2&quot;),c(&quot;Y1&quot;, &quot;Y2&quot;))) y Se puede realizar desde las funciones colnames y rownames colnames(x) &lt;- c(&quot;Variable 1&quot;, &quot;Variable 2&quot;) rownames(x) &lt;- c(&quot;a1&quot;, &quot;a2&quot;) x Añadir filas o columnas a una matriz w &lt;- c(5,6) 9.1.7.2.2 Unir matrices Unir mediante filas (queda con el nombre del vector la observación) z &lt;- rbind(x,w) z Unir mediante columnas z &lt;- cbind(x,w) z ¿Y si tienen diferente cantidad de filas y/o columnas? repite el vector o observaci?n con menor longitud x &lt;- matrix(c(1:9),3) x y &lt;- c(5,6) y z&lt;-rbind(x,y) z Podemos pasar un vector a una matriz x&lt;-1:10 x dim(x)&lt;-c(2,5) x Trasponer matrices: x &lt;- matrix(c(1:9),3) xtraspuesta &lt;- t(x) Potencialmente se pueden hacer muchas más operaciones que involucren matrices. Por ejemplo, subconjuntos de una matriz: segundo y cuarto elemento de la segunda fila: M &lt;- matrix(1:8, nrow=2) M M[1,1] M[1,] M[,2] M[2,c(2,4)] 9.2 Herramientas de programación 9.2.1 Objetos adicionales 9.2.1.1 Arreglos Crear arreglos: La única diferencia con matrices es que acepta mas de dos dimensiones. Para generarlos: mi.arreglo &lt;-array(vector, dimensiones, dimnames = etiquetas_dim) Para nombrarlos definimos etiquetas y luego las agregamos. Es mucho mejor y mas ordenado hacerlo así: dim1 &lt;- c(&quot;A1&quot;, &quot;A2&quot;) dim2 &lt;- c(&quot;B1&quot;, &quot;B2&quot;, &quot;B3&quot;, &quot;B4&quot;) dim3 &lt;- c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;) Ejemplo de arreglo: Defino un arreglo de 3 matrices de 2 (filas) x 4 (columnas) x &lt;- array(1:24, c(2,4,3), dimnames = list(dim1, dim2, dim3)) x Notar la tercera dimensión! Subconjuntos de un arreglo: x[Fila,Columna, Matriz] x[1,2,3] # 1 fila, 2da columna, 3ra matriz x[,,3] # 3era matriz completa x[,4,] # 4ta columna de todas las matrices x[,-1,2] # Todas las filas, sin la primera columna, de la matriz 2. x[-1,c(1,2),3] # Todas las columnas, sin la fila 1, de la matriz 3. x[-1,1:2,3] # Columnas 1 y 2, sin la fila 1, de la matriz 3. x[,,1]*2 9.2.1.2 Listas Las listas contienen elementos de diferente tipo: matrices, objetos, dataframes, vectores, etc. x &lt;- list(c(1:8), &quot;R&quot;, TRUE, 2+3i, 5L) x #separa todo componente Para acceder a un objeto dentro de la lista se debe utilizar DOBLE CORCHETE: x[[5]] x[5] Veamos un ejemplo de cómo trabaja: x &lt;- list(Titulacion = c(&quot;Economia&quot;, &quot;Administracion&quot;, &quot;Politica&quot;), Edad =c(25,23,27)) x Al nombrar los componentes dentro de la lista, podemos llamarlos sin necesidad de los corchetes: x$Titulacion x[[1]] x[[&quot;Titulacion&quot;]] x$Edad x[[2]] Como los componentes dentro de las listas tienen definida una naturaleza, podemos hacer operaciones con ellas también: x[[&quot;Edad&quot;]]*3 x$Edad[c(1,3)]*3 Finalmente, tambien podemos crear listas vacias (útil para rellenar utilizando iteradores): x &lt;-vector(&quot;list&quot;, length = 10) Ejemplo de una lista: Definimos una lista milista &lt;- list( # Primer elemento sera un vector hospitales = c(&quot;Van Buren&quot;, &quot;Gustavo Fricke&quot;, &quot;Salvador&quot;), # Segundo elemento sera un dataframe direccion = data.frame( calle = c(&quot;San Ignacio&quot;, &quot;Av. Alvarez&quot;, &quot;Av. Salvador&quot;), ciudad = c(&quot;Valparaíso&quot;, &quot;Viña del Mar&quot;, &quot;Santiago&quot;) ) ) milista Llamar a subconjuntos # Retorna el elemento de la lista milista$hospitales milista[2] # Retorna el elemento, pero sin nombre y como vector milista[[1]] milista[[&quot;hospitales&quot;]] # Elementos particulares milista[[1]][3] milista[[2]][,1] milista[[2]][1,] milista[[2]][1,2] Notar que es importante saber el tipo de dato. 9.2.1.3 Dataframes Crear un dataframe: Es básicamente una matriz donde filas y columnas tienen significado. Contiene tanto valores numericos, carácteres, variables categóricas, etc. datos &lt;- data.frame(Titulacion = c(&quot;Economia&quot;, &quot;Administracion&quot;, &quot;Administracion&quot;), Edad =c(25,23,27), ocupacion = c(1,0,1)) datos Esto es lo mas cercano a una base de datos. Es uno de los objetos básicos que veremos muchas veces a lo largo del curso. Dimensiones de un dataframe: Para ver las dimensiones hay que utilizar nrow(), ncol() o bien dim(). nrow(datos) ncol(datos) dim(datos) Notar que nrow() y ncol() tambien sirven para matrices. Subconjuntos de un dataframe: Al igual que en las matrices, utilizamos [ ] para acceder a elementos dentro de la base. datos[1,1] datos[,1] datos[,-2] datos[, c(1:2)] datos[, c(&quot;Titulacion&quot;)] # Tambien puedo ocupar $ para llamar. datos$Titulacion # Ojo que esto lo puedo hacer con todo summary(datos[,2]) summary(datos[,2])[3] summary(datos[,2])[&quot;Max.&quot;] Tibbles: Una version mas moderna de dataframes. Es la misma idea: lista de vectores con nombres. Altura &lt;- c(168, 177, 177, 177, 178, 172, 165, 171, 178, 170) Peso &lt;- c(88, 72, 85, 95, 71, 69, 61, 61, 51, 75) M &lt;- cbind(Altura, Peso) Paquete para trabajar con bases de datos (más detalles en secciones 2.3 y 2.4) install.packages(&quot;tidyverse&quot;) library(tibble) # Una libreria dentro de tidyverse. misdatos &lt;- as_tibble(M) misdatos # Podemos ver los nombres o bien estadistica básica de cada variable. names(misdatos) summary(misdatos) Agregar columnas a un dataframe: Podemos agregar variables a la base. Dos opciones: # Opción 1: utilizando $nombre_columna&lt;-vector datos$id &lt;- c(1:3) datos # Opción 2: utilizando el cbind (al igual que con matrices) id2&lt;-c(1:3) datos1 &lt;- cbind(id2,datos) datos1 Attach: Notación muy inconveniente misdatos$Altura misdatos$Peso Para evitar escribir datos cada vez que quiera llamar a una variable voy a utilizar la funcion attach(): attach(datos) # para comenzar # Ahora puedo llamar a sus variables sin utilizar datos nueva &lt;- Edad * ocupacion nueva &lt;- datos$Edad * datos$ocupacion nueva detach(datos) # para terminar Esta es una forma mucho mas conveniente de trabajar con dataframes o tibble en R. Ejercicio 2.2.1: Crear un dataframe Cree su primera base de datos. Debe armar una tabla igual a la figura. Tiene 5 minutos. Respuesta: ejer &lt;- data.frame(Tipo_animal = c(&quot;Perro&quot;, &quot;Perro&quot;,&quot;Gato&quot;,&quot;Perro&quot;, &quot;Gato&quot;,&quot;Gato&quot;,&quot;Gato&quot;), Color =c(&quot;Café&quot;,&quot;Blanco&quot;,&quot;Negro&quot;,&quot;Manchas&quot;,&quot;Café&quot;,&quot;Tricolor&quot;,&quot;Negro&quot;), Peso = c(7,5,3,4,2,5,4), Pasea=c(&quot;Sí&quot;,&quot;Sí&quot;,&quot;Sí&quot;,&quot;No&quot;,&quot;No&quot;,&quot;No&quot;,&quot;Sí&quot;)) 9.2.2 Acceder a elementos de un objeto 9.2.2.1 Vectores Para acceder a los elementos de un objeto debemos utilizar los corchetes [ ] # Veamos en un vector x&lt;-c(&quot;T&quot;, &quot;FALSE&quot;, 1:9,1+2i,&quot;t&quot;, &quot;c&quot;,&quot;a&quot;,6) class(x) # Podemos llamar a un objeto de un vector x[1] x[3] De esta manera podemos extraerlo o utilizarlo en diferentes operaciones. Pero sólo debe ser entre observaciones numericas: x[1]+x[6] #de lo contrario R nos arroja un error y&lt;-c(1:6, 1+2i) y[3]+y[7] #O podemos visualizar a x, pero sin el primer objeto x[-1] #O eliminar el primer objeto x&lt;-x[-1] x1 &lt;- x[-1] 9.2.2.2 Matrices Si analizamos los objetos de una matriz: w&lt;-matrix(1:9,3) w # Elemento [1,1] w[1,1] # Toda la primera columna w[,1] # Toda la segunda fila w[2,] # Dos columnas w[,1:2] # w[,-3] # Todas las filas menos la primera w[-1,] 9.2.2.3 Listas Si queremos llamar a un objeto dentro de una lista: z &lt;- list(c(1:8), &quot;R&quot;, T, 2+3i, 5L) z #separa todo componente Para acceder a un objeto dentro de la lista se debe utilizar doble corchete: z[[1]] #Y algo dentro de ese objeto z[[1]][5] #Tambien podemos analizar su clasificacion class(z[[1]]) #numerico #Si es numerico, entonces tambien podemos hacer operaciones z[[1]][3]*z[[1]][5] 9.2.3 Tipos de objetos y datos 9.2.3.1 Identificar tipos de objetos Veamos en un vector cualquiera x&lt;-c(&quot;T&quot;, FALSE, 1:9,1+2i,&quot;t&quot;, &quot;c&quot;,&quot;a&quot;,6) class(x) Como bien sabemos todo en R es un objeto. Para saber que tipo de objeto es puedo utilizar la función class(): class(datos) class (x) Note que class() identifico que es un “data.frame” y una lista. También puedo ver los tipos de elementos dentro de un determinado objeto. Para datos, existen 5 tipos principales, llamados: carácteres: texto. Se escriben con comillas (ej: “3”, “swc”). numéricos: numeros reales (ej. 2, 15.5). enteros: numeros enteros (“L” le dice a R que guarde esto como un entero). logical: valores logicos (ej. TRUE, FALSE). complejos: 1 + 4i (Numero complejos). 9.2.3.2 Identificar tipos de datos # 1. Carácteres z&lt;-c(&quot;a&quot;,&quot;b&quot;) class(z) # 2. Números enteros w&lt;-c(1L,2L,3L) #la L es para obligar que sea entero class(w) # 3. Numéricos w1 &lt;- c(1,2,3) class(w1) # Notar que la L lo obliga a ser entero ¿Qué ocurre si no coloco la L? # 4. Valores logicos v1 &lt;- c(TRUE, FALSE) class(v1) # Notar que si estan entre comillas son caracteres, no logical!. v&lt;-c(&quot;TRUE&quot;, &quot;FALSE&quot;) class(v) # 5. Números complejos t&lt;-c(1+2i,1+3i) class(t) Hay más clases: Date(fechas), Factor(variables categóricas), data.frame, tibble, list. 9.2.3.3 ¿Importan los tipos de los objetos? R intenta mantener dentro de un vector el tipo de objeto. Si es que tratamos de juntar distintos tipos de datos en un vector. Igualar a la clase. Por ejemplo, si intentamos generar un vector con texto y número/logical todo será texto: h &lt;- c(TRUE,&quot;a&quot;, TRUE, 2) h class(h) Si intentamos juntar un “Logical” con un número todo será número: h1 &lt;- c(TRUE,2,3) h1 class(h1) Aquí carácter todo se va a caracter: x &lt;- c(&quot;T&quot;, &quot;FALSE&quot;, 1:3, 1+2i, &quot;t&quot;, &quot;c&quot;, &quot;a&quot;) class(x) y &lt;- c(1:4) class(y) y &lt;- c(1,2,3,4) class(y) Si utilizo el operador : para crear vectores lo interpreta como un integer. 9.2.3.4 Forzar a R para que utilice un tipo de dato Forzar a R a llevar el vector solo a numérico: z &lt;- as.numeric(x) class(z) z as.numeric() fuerza al vector a solo tener números. Noten que remplaza los que no son numérico por NA. Forzar a R a llevar el vector solo a carácteres: z1 &lt;- as.character(y) class(z1) z1 Noten que ahora se agregaron las dobles comillas. Forzar a R a llevar el vector sólo a carácter. x z2 &lt;- as.logical(x) class(z2) z2 Noten que “T” fue aceptado como TRUE! 9.2.3.5 ¿Cómo saber que tipo de datos tengo? # Puedo preguntar a R sobre el tipo. is.numeric(y) # Es numerico? is.character(y) # Es caracter? is.logical(y) # Es logico? # La respuesta a esta pregunta va a ser un valor lógico class(is.numeric(y)) # También se puede utilizar la opcion `typeof` typeof(y) 9.2.3.6 Mayúsculas importan Como vimos en lasección anterior, R es sensible a las mayúsculas. Esto aplica para nombres de funciones, paquetes, comando y también para los datos. Por ejemplo: x &lt;- c(&quot;A&quot;, &quot;a&quot;,&quot;a&quot;,&quot;a&quot;, &quot;B&quot;, &quot;B&quot;,&quot;b&quot;, &quot;A&quot;) length(x) sum(x == &#39;a&#39;) sum(x == &#39;b&#39;) Para R las minúsculas y mayúsculas SI IMPORTAN!. 9.2.3.7 Carácteres especiales Vimos que existen nombres de variables no validos en R. Tambien existe un conjunto de caracteres invalidos. NA: Not Available (missing values) NaN: Not a Numbers (ej. 0/0) Inf: Infinito (1/0) -Inf: Menos infinito 0/0 -1/0 9.2.3.8 Missing values En R, los valores perdidos (“missing values”) se representan con el valor especial NA (letras mayusculas N y A - sin comillas). Para saber si tengo valores NA en un objeto, puedo ocupar la funcion is.na(): ejemplo &lt;- c(1,3,NA,4) is.na(ejemplo) ejemplo[!is.na(ejemplo)] # El resultado es un vector. Noten que es importante para utilizar algunas funciones: mean(ejemplo) mean(ejemplo, na.rm = TRUE) No funciona en el primer caso, si en el segundo. Veremos mas detalles sobre como tratar missing values (ej. recodificar) en las clases de manipulacion y analisis de datos. 9.2.3.9 Funciones útiles # Redondear round(x,digits = n) round(c(2.53, 3.52), 1) # Estadística simple x &lt;- c(1,2,3,4,5,6) mean(x) # promedio median(x) # mediana sd(x) # desviacion estandar sum(x) # suma del vector min(x) # Valor minimo max(x) # Valor maximo range(x) # rango summary(x) # resumen # Notar que tienen que ser vectores! mean(10, 6, 12, 10, 5, 0) mean(c(10, 6, 12, 10, 5, 0)) # En el primer caso sólo toma el primer valor! Ojo, siempre un vector. # Otras funciones seq(1,10,2 ) # Crear secuencias rep(c(1,2,3),10) # Repetir cut(x,2) # subdividir sample(x, size = 3, replace = TRUE) # Generar un aleatorio # El analisis de datos es... Altura = c(168, 177, 177, 177, 178, 172, 165, 171, 178, 170) Peso = c(88, 72, 85, 95, 71, 69, 61, 61, 51, 75) M = cbind(Altura, Peso) # Paquete para trabajar con bases de datos (más detalles en secciones 2.3 y 2.4) install.packages(&quot;tidyverse&quot;) library(tibble) # Una libreria dentro de tidyverse. misdatos &lt;- as_tibble(M) misdatos attach(misdatos) max(Altura) min(misdatos$Peso) detach(misdatos) 9.2.4 Herramientas en R 9.2.4.1 Escribir Códigos Algunos comentarios con respecto a la escritura de codigos: Siempre escribir comentarios autocontenidos. Siempre utilizar índice. Me permite reducir el código. Me permite ver donde estoy también en un código largo. Puedo verlo con ctrl + shift + O o bien en la esquina inferior izquierda del script. Separar códigos largos en varios códigos. Cada código debe tener un objetivo claro que debe explicar en una oración. Escribir códigos en bloques. Tres consejos básicos de estilo: utilizar _ para generar variables o bien otra convención. separar entre objetos y operaciones. 9.2.4.2 Condicionales y controladores de flujo Operadores básicos: rm(list=ls()) # Asignador a &lt;- 2 * 3 a a1 = 2 * 3 a1 # Nota: Nunca ocupar igual. # Igualdad TRUE == TRUE TRUE == FALSE # Nota: Para comparar elementos se utiliza doble igual &quot;==&quot;. # Desigualdad (!=) TRUE != FALSE &quot;Hola&quot; != &quot;Chao&quot; # Otros comparadores: &lt;, &gt; (&gt;=), (&lt;=) 3 &lt; 5 5 &gt; 8 5 &gt;= 5 # Nota: No confundir con &lt;- que es para asignar. TRUE &gt; FALSE # Noten que la respuesta en un logical. # En vectores y matrices vector1 &lt;- c(16,9,13) vector2 &lt;- c(10,12,15) # Comparar contra un escalar vector1 &gt; 10 vector2 &lt; 10 # Compararlos entre ellos vector1 &lt; vector2 # Veamos en una matriz matrix &lt;- matrix(c(vector1, vector2), byrow = TRUE, nrow = 2) matrix matrix &gt; 10 Operador %in%: Un operador muy útil para comparar valores y para evaluar rápidamente si un valor está dentro de un vector o marco de datos. rm(list=ls()) v1 &lt;- 3 v2 &lt;- 101 t &lt;- c(1,2,3,4,5,6,7,8) # El valor v1 ¿se encuentra dentro de t? v1 %in% t # Otro ejemplo.... mivector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) &quot;a&quot; %in% mivector # Si no está en el vector da un FALSE &quot;h&quot; %in% mivector # Si le quiero preguntar si NO está, coloco un signo de exclamación al frente. !&quot;a&quot; %in% mivector # Un vector en otro vector a &lt;- seq(12, 19, 1) b &lt;- seq(1, 16, 1) # Veamos si los elementos de un vector &quot;largo&quot; están en uno &quot;corto&quot; b %in% a # Esto va a ser muy útil cuando trabajemos con bases de datos. Operadores para seleccionar subconjuntos de datos: y &lt;- c(2,3,3,4,NA,8) y # Vamos a seleccionar solo los que no son NA y1 &lt;- y[!is.na(y)] y1 # Noten que utilizamos la funcion is.NA y un operador (!). Más detalles en parte de análisis de datos, pero noten que la idea principal está aquí. Operadores lógicos: # Operador (&amp;): operador &quot;y&quot; TRUE &amp; TRUE TRUE &amp; FALSE FALSE &amp; TRUE FALSE &amp; FALSE # Ejemplo x &lt;- 12 x &gt; 5 &amp; x &lt; 11 # Operador (|): operador &quot;o&quot;. Condiciones no excluyentes. TRUE | TRUE TRUE | FALSE FALSE | TRUE FALSE | FALSE # Ejemplo y &lt;- 4 y &lt; 5 | y &gt; 15 # Operador de negación (!) !TRUE !FALSE # Combinarlo con funciones !is.numeric(5) !is.numeric(&quot;Hello&quot;) # Operadores con vectores c(TRUE, TRUE, FALSE) &amp; c(TRUE, FALSE, FALSE) c(TRUE, TRUE, FALSE) | c(TRUE, FALSE, FALSE) !c(TRUE,TRUE,FALSE) # Si utilizo dos &quot;&amp;&amp;&quot; sólo compara el primer elemento del vector c(TRUE,FALSE,FALSE) || c(TRUE,FALSE,FALSE) c(TRUE,FALSE,FALSE) &amp;&amp; c(TRUE,TRUE, TRUE) 9.2.4.3 Condicionales If: x &lt;- 3 #if (condicion){ # cualquier cosa que quiero que se haga si la condición se cumple #} if (x &gt; 0) { print(&quot;x es número mayor que cero&quot;) } if (x &gt; 0){ print(&quot;x es un número mayor que cero &quot;) } if (x &lt; 0){ print(&quot;x es un número menor que cero&quot;) } Else: x &lt;- 0 if (x &gt; 0){ print(&quot;x es un número positivo&quot;) } else{ print(&quot;x es un número negativo&quot;) } Elseif: x &lt;- 0 if (x &gt; 0){ print(&quot;x es mayor que cero&quot;) } else if (x == 0){ print(&quot;x es igual a cero&quot;) } else{ print(&quot;x es menor que cero&quot;) } # Importante: `(%%) corresponde al resto de una división` x &lt;- 6 if (x %% 2 == 0){ print(&quot;x es divisible por 2&quot;) } else if (x %% 3 == 0){ print(&quot;x es divisible por 3&quot;) } else { print(&quot;x no es divisible ni por 2 ni por 3&quot;) } # Notar que si la primera condición se cumple, la segunda no se ejecuta aunque sea cierta. 9.2.4.4 Funciones Idea principal: \\(f(x) = 2x + 1\\), \\(x \\in R\\) \\(f(x) = &#39;hola&#39; + x\\), \\(x \\in (&#39;pepe&#39;, &#39;pepa&#39;, &#39;marta&#39;)\\) valores &lt;- c(1,2,3,4) mean(valores) Algunas cosas adicionales sobre funciones: n me permite ver los argumentos de una función, sin necesidad de ver la documentación. Útil a veces. # Escribir funciones con un argumento triple &lt;- function(x){ y &lt;- 3 * x return(y) } triple &lt;- function(x){ x/2 } triple(500) # Escribir funciones con mas de un argumento operacion &lt;- function(a,b){ a*b + a/b } operacion(4,2) # Escribir funciones fijando opción por defecto. operacion_defecto &lt;- function(a,b = 1){ a*b + a/b } operacion_defecto(4) operacion_defecto(4,0) # Escribir funciones utilizando if y return. operacion_condicionales &lt;- function(a,b = 1){ if (b == 0){ return(0) } a*b + a/b } operacion_condicionales(4,0) # Funciones con texto texto &lt;- function(){ print(&quot;Hola mundo!&quot;) return(TRUE) } texto() # Funciones por defecto en ambos casos operacion_dosdefectos &lt;- function(a = 1,b = 1){ if (b == 0){ return(0) } a*b + a/b } operacion_dosdefectos() Ejercicio 2.2.2: Funciones Genere una función que sea igual a la división de dos elementos. Coloque un mensaje que indique cuando la división es indeterminada. Respuesta: operacion_indeterminada &lt;- function(a,b){ a/b if (b == 0){ print(&quot;Es indeterminado&quot;) } } operacion_indeterminada(1,0) 9.3 Manipulación de bases de datos (Parte I) 9.3.1 Principios de programación 9.3.1.1 Iteradores For loop: # Imaginen que queremos mostrar los nombres de un vector de forma reiterada ciudades &lt;- c(&quot;Nueva York&quot;, &quot;Paris&quot;, &quot;Santiago&quot;, &quot;Rancagua&quot;) print(ciudades[1]) print(ciudades[2]) print(ciudades[3]) print(ciudades[4]) # Lo anterior se puede hacer utilizando un iterador. for (i in 1:4){ print(ciudades[i]) } # Utilizando variables ocultas... for (.j in 1:4){ print(ciudades[.j]) } for (.j in 1:length(ciudades)){ print(ciudades[.j]) } # o escrito señalando el nombre de cada elemento de un vector. for (ciudad in ciudades){ print(ciudad) } # Otro ejemplo: semana &lt;- c(&quot;Domingo&quot;, &quot;Lunes&quot;, &quot;Martes&quot;, &quot;Miercoles&quot;, &quot;Jueves&quot;, &quot;Viernes&quot;, &quot;Sabado&quot;) for (dia in semana) { print(dia) } For loop con opciones: ciudades &lt;- c(&quot;Nueva York&quot;, &quot;Paris&quot;, &quot;Santiago&quot;, &quot;Tokio&quot;) # Agregar opcion break: quiebra el loop for (ciudad in ciudades){ if(nchar(ciudad) == 8){ break } print(ciudad) } # Agregar Opción next: se salta ese elemento for (ciudad in ciudades){ if (nchar(ciudad) == 8){ next } print(ciudad) } Flexibilizar el iterador: ciudades &lt;- c(&quot;Nueva York&quot;, &quot;Paris&quot;, &quot;Santiago&quot;, &quot;Tokio&quot;, &quot;Rancagua&quot;, &quot;Roma&quot;) # Ahora el tamaño del loop es flexible. Esto es muy inportante para cuando trabajemos con bases de datos. for (i in 1:length(ciudades)){ print(ciudades[i]) } # Noten que ahora llamo a los elementos dentro de un loop como subconjuntos de un vector. # En resumen, dos versiones de lo mismo for (i in 1:length(ciudades)){ print(ciudades[i]) } for (ciudad in ciudades){ print(ciudad) } # Ejemplo: para dejar mensajes for (i in 1:length(ciudades)){ print(paste(ciudades[i], &quot;esta en la posicion&quot;,i, &quot;en el vector ciudades&quot;)) } # Lo anterior es aplicable para inspecciones de bases de datos, por ejemplo, podemos dejar un mensaje for (i in 1:length(mtcars)){ print(paste(&quot;el promedio de la variable&quot;, names(mtcars[i]), &quot;es&quot;, mean(mtcars[,i]))) } While: mientras que… x &lt;- 1 while (x &lt;= 7){ print(paste(&quot;x es&quot;, x)) # Actualización x &lt;- x + 1 } while (x &lt;= 700){ print(paste(&quot;x es&quot;, x)) # Actualizacion x &lt;- x + 1 } # Muy importante la actualización. 9.3.1.2 Lapply, Sapply y Vapply Lapply: nyc &lt;- list(poblacion = 8405837, barrios = c(&quot;Manhattan&quot;, &quot;Bronx&quot;, &quot;Brooklyn&quot;, &quot;Queens&quot;, &quot;Staten Island&quot;)) nyc Equivalencia loop y lapply. Quiero saber todas las clases de la lista: # Puedo hacer esto con un loop for (objeto in nyc){ print(class(objeto)) } for (i in 1:length(nyc)){ print(class(nyc[[i]])) } # Pero, si utilizo `lapply()` puedo hacerlo mucho más eficiente. # Lapply: que aplica esto como si fuese un `for`. lapply(nyc,class) Resultado de lapply con vector: # Lapply ejecuta la función, en este caso class, para todo elemento del objeto, en este caso nyc. # Si quiero saber el número de carácteres cities &lt;- c(&quot;New york&quot;,&quot;Paris&quot;, &quot;Tokyo&quot;, &quot; Rio de Janeiro&quot;) lapply(cities,nchar) class(lapply(cities,nchar)) # Noten que el resultado aquí es una lista. Si quiero que sea un vector, puedo utilizar la función `unlist()`. unlist(lapply(cities,nchar)) class(unlist(lapply(cities,nchar))) Lapply como función: precios &lt;- list(2.25, 2.18, 2.89, 2.84, 2.89) # Creamos una función multiplicar &lt;- function(x,factor){ x * factor } # Ahora, podemos aplicar lapply y agregar opciones de la función tresveces &lt;- lapply(precios, multiplicar, factor = 3) tresveces &lt;- unlist(tresveces) # Noten que la sintaxis es: objeto, función, opciones. Es decir, igual a lo anterior, pero pudiendo agregar opcionales. Sapply: Es una variacion de lapply que sirve para simplificar lapply. Ahora el resultado es un vector, no una lista. cities &lt;- c(&quot;New york&quot;,&quot;Paris&quot;, &quot;Tokyo&quot;, &quot; Rio de Janeiro&quot;) lapply(cities,nchar) sapply(cities,nchar) # Noten que es bastante ordenado. Sin embargo, falla cuando no es fácil ordenar el resultado. Vapply: Es una variación que sirve para definir explícitamente el tipo de objeto del resultado. vapply(cities, nchar, numeric(1)) 9.3.2 Manipulacion de bases de datos en R Analizar datos es parte importante de las labores que uno desea realizar al utilizar R. Vamos a revisar cuatro aspectos iniciales de cualquier trabajo con datos: Importar datos en distintos formatos. Inspeccionar y limpiar los datos que tenemos. Transformar los datos con el fin de crear nuevas variables. Juntar bases de datos de distinto tipo. 9.3.2.1 Importar bases de datos Para poder utilizar, cargar, renovar datos es importante tenerlos todos en un solo lugar. Esto también aplica para los resultados y codigos. Para que esto efectivamente ocurra necesitamos decirle a R cual va a ser nuestro directorio de trabajo. Es decir, el lugar donde guardaremos los datos que queremos trabajar, los resultados de nuestros análisis y nuestros códigos. Directorios de trabajo en R: # Me dice donde estoy getwd() # Si quiero indicarle otra ruta tengo dos opciones: # Opcion 1: Indicar a R la ruta/carpeta donde tengo mis datos setwd(&quot;C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3&quot;) # Opcion 2: Lo mismo que opcion 1, pero más ordenado. ruta &lt;-&quot;C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3&quot; setwd(ruta) # Noten que es necesario ocupar `/`. Por defecto las rutas al copiarlas vienen con otro tipo de &quot;slash&quot;. Para cambiarlas fácilmente, y no una por una, vamos a utilizar `ctrl + f`. # Con este atajo podemos remplazar varios elementos a la vez. Cuidado!! Es importante decirle si queremos que modifique todo el documento o bien solo una parte. dir(ruta) Ejemplo de ordenar carpetas: ruta &lt;- &quot;C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3&quot; # Codigos codigos &lt;- &quot;C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3/codigos&quot; codigos &lt;- paste(ruta,&quot;/&quot;,&quot;codigos&quot;, sep = &quot;&quot;) setwd(codigos) dir(codigos) # Datos datos &lt;- &quot;C:/Users/catab/Dropbox/Curso R - Agosto 2022/Clase 3/datos&quot; datos &lt;- paste(ruta,&quot;/&quot;,&quot;datos&quot;,sep=&quot;&quot;) dir(datos) # Datos raw datosraw &lt;- &quot;C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3/datos/raw&quot; dir(datosraw) # Todo lo anterior no es muy recomendado. Proyectos de R: Proyecto: piensen en muchos proyectos con muchos códigos. Es una herramienta incorporada en RStudio que permite manejar un proyecto de análisis de datos. Permite dividir el trabajo en múltiples contextos cada uno con su propio directorio de trabajo, espacio de trabajo e historial. ¿Por qué utilizarlo? Mantiene códigos y datos en la misma carpeta. Mantiene códigos y datos separados de otros proyectos: evita confusiones o errores. Identifica automáticamente el directorio de trabajo, facilitando cooperación. Pasos para crear un proyecto: Dos opciones: R crea una carpeta de trabajo. Crear una carpeta primero, y luego decirle a R que la identifique. Todos los nuevos archivos serán automáticamente guardados en la carpeta del proyecto. Una vez hecho el proyecto se genera un archivo con extensión R.proj. Aquí R guarda información del proyecto: historial, datos, etc. Lo más importante es que fija el directorio de trabajo de forma tal de que siempre sea el mismo una vez que se abre un proyecto. Evita cambiar directorios de trabajo al colaborar. Package here: install.packages(&quot;pacman&quot;) pacman::p_load(&quot;here&quot;) here() here(&quot;datos&quot;) Package rio: R tiene bases de datos propias. Sin embargo, muchas veces van a querer trabajar con datos propios. o bien de datos públicos que muchas veces estan en distintos formatos. Rio package es una forma flexible de importar datos en distintos. Rio debe su nombre por R input/output. Dos funciones principales: import() y export(). Además, cuando se le indica la extensión a rio este leerá y utilizará la herramienta indicada para leer esos datos. library(pacman) p_load(here, # Modificar carpetas rio) # Importar/exportar datos Hay otras opciones tambien para importar datos. Por ejemplo, read.csv() (\"base R\"); read.xlsx (\"openxlsx\"). El problema es que son difíciles de recordar. Mejor utilizar una. Importar datos en distintos formatos: Utilizar import() para importar un conjunto de datos es bastante sencillo. Basta con proporcionar la ruta del archivo (incluyendo el nombre y la extensión del archivo) entre comillas. # Ocupamos R project, here e import para importar datos fácilmente datos_xlsx &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;)) datos_dta &lt;- import(here(&quot;datos&quot;, &quot;Data.dta&quot;)) datos_txt &lt;- import(here(&quot;datos&quot;, &quot;Data.txt&quot;)) datos_csv &lt;- import(here(&quot;datos&quot;, &quot;Data.csv&quot;)) Opciones: # 1. Importar distintas hojas: Por defecto se importa la primer hoja de una base de datos, con &quot;which&quot; puedo elegir la hoja. datos_xlsx_h1 &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) datos_xlsx_h2 &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja2&quot;) # 2. Puedo decirle a priori que valores son missings. # Especificar un missing datos_xlsx &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), na = &quot;2018&quot;) # Especificar varios a la vez.. datos_xlsx &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), na = c(&quot;Missing&quot;, &quot;&quot;, &quot; &quot;)) # 3. Saltar filas. datos_xlsx &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), skip = 1) # Noten que puedo hacerlo con cualquier tipo de datos, ya que son opciones de la función `rio`. datos_dta &lt;- import(here(&quot;datos&quot;, &quot;Data.dta&quot;) , skip = 1) # En la tabla de este link pueden ver los distintos paquetes que soporta `rio`, junto a ejemplo adicionales. browseURL(&quot;https://cran.r-project.org/web/packages/rio/vignettes/rio.html&quot;) 9.4 Manipulación de bases de datos (Parte II) Ya sabemos cargar datos. Ahora vamos a hacer el primer proceso para analizar cualquier base de datos. 9.4.1 Funciones clave Tidyverse es una colección de paquetes de R. Tidyverse contiene múltiples paquetes que iremos utilizando. Incluye: dplyr, ggplot2, tidyr, stringr, tibble, purrr, magrittr y forcats. Una paquete clave es dplyr que contiene muchas funciones para trabajar con bases de datos. Instalamos tidyverse: install.packages(&quot;tidyverse&quot;) library(tidyverse) library(pacman) p_load(tidyverse) 9.4.2 Operador piping Piping %&gt;%: crtrl + shift + m. Paquete asociado: magrittr p_load(magrittr) Operador que permite encadenar las funciones para realizar de manera sencilla transformaciones complejas en las bases de datos. Lo que dice es pasar el elemento que esta a su izquierda como un argumento de la función que tiene a la derecha. Coloca el énfasis en las acciones. Pasa un output intermedio de una función a la siguiente. magrittr es el paquete que permite ocupar piping. piping se utiliza mucho con las librerias de tidyverse y dplyr enfocadas en análisis de datos. Excelente cuando existe una secuencia de acciones/operaciones que queremos realizar. data(iris) # Ejemplo 1: usar pipe como encadenador head(iris, n = 4) # Con piping iris %&gt;% head(n = 4) # Ejemplo 2: obtener número total de observaciones y un promedio. summarize(mtcars, media = mean(disp)) mtcars %&gt;% summarize(promedio = mean(disp)) # Con piping mtcars %&gt;% filter(mpg &gt; 20) %&gt;% summarise(promedio = mean(disp)) promedio_mpg_20 &lt;- mtcars %&gt;% filter(mpg &gt; 20) %&gt;% summarise(promedio = mean(disp)) promedio_mpg_20 9.4.3 Proceso de análisis de datos (Parte I): cargar, inspeccionar y limpiar Preámbulo: # Limpiar rm(list = ls()) # Cargamos paquetes que vamos a utilizar library(pacman) p_load( rio, # Importar/Exportar datos here, # Determinar las rutas de mi carpeta tidyverse, # Analisis de datos y visualización magrittr, # Para utilizar operador %&gt;% janitor # Para análisis de datos ) # Importamos datos datos &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) Inspeccionar: # Visión general datos view(datos) # Para una inspección detallada p_load(skimr) skim(datos) str(datos) # Mirar algunas filas, columnas, etc. head(datos) # muestra las 6 primeras filas tail(datos) # últimas 6 filas head(datos, 11) # podemos pedir m?s de 6 tail(datos, 3) # o menos de 6 # Mirar nombre variables/columnas names(datos) datos %&gt;% names() # `names()` también puede ser names(datos) &lt;- c(&quot;YEAR&quot;, &quot;GDP&quot;, &quot;GROSS_EXPORTS&quot;, &quot;GROSS_IMPORTS&quot;, &quot;NET_EXPORTS&quot; ) names(datos) # Mirar filas row.names(datos) row.names(mtcars) # Mirar la cantidad de variables de la base de datos length(datos) # columnas o variables dim(datos) ncol(datos) nrow(datos) # ¿Qué tipo de objeto es? class(datos) # También podemos inspeccionar elementos específicos datos&lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) # Si queremos ver la columna de GDP datos$gdp datos[2] datos[,2] # Para llamarlos solo por su nombre recordar ocupar `attach()` attach(datos) gdp detach(datos) # Si queremos un objeto dentro de una variable datos$gdp[7] datos[7,2] #[fila, columna] datos[7,&quot;gdp&quot;] # Si queremos seleccionar parte de la columna datos$gdp[5:10] # o bien (solo por `attach()`) attach(datos) gdp[1:2] # o bien datos[1:2,2] # mostrar las filas 1:2, de la columna 2 # `table()` me permite hacer una tabla sencilla de frecuencias table(datos$year) attach(datos) table(year) detach(datos) Ejercicio 2.4.1: Utilizando el operador pipping, mostrar las últimas 2 filas de las primeras 11 filas. Limpiar: names(datos) &lt;- c(&quot;YEAR&quot;, &quot;GDP&quot;, &quot;GROSS_EXPORTS&quot;, &quot;GROSS_IMPORTS&quot;, &quot;NET_EXPORTS&quot; ) names(datos) # La función `clean_names()` del paquete `janitor` estandariza nombres: datos_nuevos &lt;- clean_names(datos) names(datos) names(datos_nuevos) Renombrar variables: rm(list = ls()) datos&lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) # Función para renombrar variables. Sintaxis: rename(nuevonombre = viejonombre) # Escribamos esto, pero con `piping` datos_renombrados &lt;- datos %&gt;% rename(tiempo = year, pib = gdp, exportaciones = gross_exports) names(datos_renombrados) rm(datos_renombrados) Seleccionar variables o columnas: rm(list= ls()) datos&lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) # select() de `dplyr` permite seleccionar variables datos_select &lt;- datos %&gt;% select(year, gdp) datos_select1 &lt;- datos %&gt;% select(c(1:4)) datos_select1 &lt;- datos %&gt;% select(c(1:ncol(datos)-1)) # También puedo seleccionar en base a un criterio (Sólo sirve para string) datos_select2 &lt;- datos %&gt;% select(year, contains(&quot;Gross&quot;)) # Con `select` también se puede renombrar datos_select_renombrados &lt;- datos %&gt;% select(tiempo = year, pib = gdp) # Una opción es ocupar select con `everything()` para ordeanar mis columnas. datos_select_ordenados &lt;- datos %&gt;% select(gdp, year, everything()) Remover columnas: # La idea aqui es decir: todas se quedan, menos las que pongo aquí. datos_select_remover &lt;- datos %&gt;% select(-c(gdp, year)) Mirar si hay duplicados. En ocasiones es importante revisar si hay duplicados. El paquete dplyr contiene distinct(). Esta función examina cada fila y reduce los datos solo a las que sean valores diferentes: datos1 &lt;- datos %&gt;% distinct() # ¿Cuantos duplicados? nrow(datos) nrow(datos1) dif &lt;- nrow(datos) - nrow(datos1) dif 9.4.4 Proceso de análisis de datos (Parte II): cargar, inspeccionar y limpiar Cargar: # Limpiamos consola rm(list = ls()) # Cargamos paquetes que vamos a utilizar pacman::p_load( rio, # importar/exportar datos. here, # escribir rutas de las carpetas. janitor, # limpiar datos y tablas. tidyverse, # Manejo de bases de datos y visualización. magrittr, # Permite utilizar operador %&gt;% (piping). skimr, # Inspeccionar datos inspectdf, # Inspeccionar datos gapminder # Base de datos con información de países. ) data(&quot;gapminder&quot;) gapminder %&gt;% view() # Cargamos datos que vienen incluidos en R. data(starwars) starwars Inspeccionar datos: # Inspect_cat() retorna una base de datos que resume características de un data.frame. inspeccion &lt;- inspect_cat(starwars) inspeccion class(inspeccion) # Las columnas son: # col_names: nombre de cada columna # cnt: número de valores únicos por nivel # common: el nivel más común # common_pcnt: el porcentaje de ocurrencia del nivel más común. # levels: una lista de dataframes (tibbles) cada uno con tablas de frecuencia para todos los niveles. # Notar que una de las columnas de un data frame pueden ser listas: inspeccion[1,5] inspeccion[2,5] inspeccion[3,5] inspeccion[4,5] inspeccion[5,5] # Ahora, si quiero ver el contenido, debo utilizar doble paréntesis cuadrado inspeccion[1,5] # Con el nombre inspeccion[[1,5]] # El contenido inspeccion[[5]] # El contenido de todas las filas, en este caso, listas inspeccion[[5]][[1]] inspeccion[[5]][[2]] # Una forma más simple de mirar esta información es: inspeccion$levels$eye_color inspeccion[[5]][[1]] # Otra cosa interesante de este paquete es la función `show_plot()` starwars %&gt;% inspect_cat() %&gt;% show_plot() # Esta función permite ver las categorías de cada variable categórica. Noten que las zonas en gris son los NA. Limpiar bases de datos (continuación): rm(list= ls()) datos&lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) Seleccionar columnas: # Vimos que `select()` es parte importante. `select()` puede ser utilizado con varias funciones adicionales: # 1. everything () - todas las otras columnas no mencionadas. datos_everything &lt;- datos %&gt;% select (year, gdp, everything()) # 2. last_col () - la última columna. datos_last_col &lt;- datos %&gt;% select(year, last_col()) # 3. where () - aplicar una función a todas las columnas y # selecciona solo las que cumple esta condición # (es decir, cuando es verdadera). datos_where &lt;- datos %&gt;% select(where(is.logical)) # 4. contains () - columns containing a character string datos_contains &lt;- datos %&gt;% select(contains(&quot;exports&quot;)) # 5. starts_with () - selecciona la variable si se tiene un prefijo determinado datos_starts_with &lt;- datos %&gt;% select(starts_with(&quot;gross_&quot;)) # 6. ends_with () - selecciona la variable si se tiene un sufijo determinado datos_ends_with &lt;- datos %&gt;% select(ends_with(&quot;_imports&quot;)) # 7. matches () - aplicar una expresión regular datos_matches &lt;- datos %&gt;% select(matches(&quot;gross|gdp&quot;)) # 8. any_of () - la selecciona si la columna existe pero NO retorna error # si no la encuentra. datos_any_of &lt;- datos %&gt;% select(any_of(c(&quot;year&quot;, &quot;gdp&quot;, &quot;cualquiercosa&quot;))) Manipulación de NAs: Con vectores. Los NA son parte importante de la limpieza de los datos. Recordemos que la función is.na() nos permite identificarlos. x &lt;- c(1,2,NA,NA,5) malos &lt;- is.na(x) malos class(malos) # Si queremos eliminar los NA es cosa de colocar un vector sobre otro x[!malos] x[!is.na(x)] x &lt;- x[!malos] x Manipulación de NAs: con complete cases() # Miremos un caso más práctico con iris summary(iris) skim(iris) data(&quot;iris&quot;) # Generar NA en la base de datos (después veremos esto en detalle ahora concéntrense solo en los NA) iris$Sepal.Length&lt;-ifelse(iris$Sepal.Length&lt;5, NA,iris$Sepal.Length) # Dos opciones: # (i) como antes malos &lt;- is.na(iris$Sepal.Length) iris2 &lt;- iris[!malos,] iris2 # (ii) utilizando `complete.cases()` completos &lt;- complete.cases(iris$Sepal.Length) head(completos, 100) # Creamos una base nueva solo con casos completos. iris3 &lt;- iris[completos,] head(iris3) summary(iris3) 9.5 Manipulación de bases de datos (Parte III) 9.5.1 Crear Variables 9.5.1.1 Crear variables binarias Ahora vamos a ver como crear variables dicótomas o binarias. Estas variables son muy importantes para hacer análisis de datos. Vamos a utilizar la función ifelse(). rm(list= ls()) data &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) # Asignar valor 1 si estamos en democracia, 0 caso contrario data &lt;- data %&gt;% mutate(demo = ifelse(year&gt;=1990,1,0)) table(data$demo) # Tabla 1: ¿Cual fue el pib promedio en democracia y en dictadura? tabla1 &lt;- data %&gt;% group_by(demo) %&gt;% summarize(promediopib = mean(gdp, na.rm = TRUE)) tabla1 # Asignar valor 1 si estamos en democracia y el pib es mayor a su media data &lt;- data %&gt;% mutate(avance = ifelse(demo == 1 &amp; gdp &gt; mean(gdp), 1,0)) mean(data$gdp) # Asignar valor 1 si estamos en democracia y si dentro de esos periodos # las exportaciones netas son mayores a la mediana de las importaciones data$expo &lt;-ifelse(data$demo==1, ifelse(data$gross_exports&gt; median(data$gross_imports),1,0),0) table(data$expo) Ejercicio 2.5.1: Escriba lo anterior utilizando mutate. Respuesta: data &lt;- data %&gt;% mutate(expo = ifelse(demo==1, ifelse(gross_exports&gt; median(gross_imports),1,0),0)) 9.5.1.2 Crear variables categóricas También podemos crear variables categóricas utilizando ifelse(). De manera muy sencilla podemos generar variables según ciertas condiciones: # De manera muy sencilla podemos generar variables según ciertas condiciones data$tipo&lt;-ifelse(data$year&lt;1980 &amp; data$year&gt;1959,1, ifelse(data$year&lt;2000 &amp; data$year&gt;1979,2, ifelse(data$year&lt;2010 &amp; data$year&gt;1999,3,4) ) ) table(data$tipo) # Nota: no es recomendable utilizar o escribir tantos `ifelse()` juntos. # Es mejor utilizar `case_when()` que veremos un más adelante. 9.5.2 Factores R puede codificar automáticamente una variable categorica (factor) con un número entero. Sirven para hacer estadisticas o estimar regresiones. Los factores, pueden ser ordenados o no ordenados, se utilizan para representar variables que se agrupan en categorías. Veamos un ejemplo: x &lt;- rep(c(&quot;Ford&quot;,&quot;BMW&quot;,&quot;Peugeot&quot;),10) x class(x) # `factor()` factor_nominal &lt;- factor(x) factor_nominal class(factor_nominal) levels(factor_nominal) #para analizar que niveles tiene el objeto table(factor_nominal) # resume la cantidad de observaciones por nivel 9.5.3 Funciones útiles para transformar bases de datos across(): Algunas veces queremos aplicar una función a múltiples variables, para ello vamos a utilizar la función across() y especificar la función con fns. Por ejemplo: datos2 &lt;- data %&gt;% mutate(across(cols = everything(), fns = as.numeric)) # Aquí `across()`, que es una función de &quot;dplyr&quot;, permite ser utilizada con `mutate()`, `select()`, `filter()`, `summarise()`, etc. datos3 &lt;- data %&gt;% mutate(across(.cols = contains(&quot;gross&quot;), .fns = as.numeric)) cumsum(): Suma acumulada # Para hacer operaciones acumuladas sum(c(2,4,15,10)) # Retorna la suma del vector cumsum(c(2,4,15,10)) # Retorna la suma acumulada del vector # Puedo utilizarla con mutate () datos_acumulados &lt;- data %&gt;% arrange(year) %&gt;% count(gdp) %&gt;% mutate(gdp_acumulado = cumsum(n)) datos_acumulados Recodificar variables: A continuación se presentan algunos escenarios en los que es necesario recodificar (cambiar) los valores: Editar un valor específico (por ejemplo, una fecha con un año o formato incorrecto) Para conciliar valores que no se escriben igual Crear una nueva columna de valores categóricas Crear una nueva columna de categorías numéricas (por ejemplo, categorías de edad) recode (): Cambiar valores rm(list= ls()) data &lt;- import(here(&quot;datos&quot;, &quot;Data.xlsx&quot;), which = &quot;hoja1&quot;) data &lt;- data %&gt;% mutate(demo = ifelse(year&gt;=1990,1,0)) # Seguimos con data names(data) # Creamos una nueva variable datos &lt;- data %&gt;% mutate(regimen = ifelse(demo == 1, &quot;Democracia&quot;, &quot;Dictadura&quot;)) datos # Recodificamos (cambiar nombres) datos &lt;- datos %&gt;% mutate(regimen = recode(regimen, &quot;Democracia&quot; = &quot;Demo&quot;, &quot;Dictadura&quot; = &quot;Dict&quot;)) datos # Notar que es vieja variable por nueva variable replace(): Reemplazar valores datos &lt;- datos %&gt;% mutate(avance = ifelse(demo == 1 &amp; gdp &gt; mean(gdp), 1,0)) # Similar sintaxis a la de recode datos &lt;- datos %&gt;% mutate(regimen = replace(regimen, avance == 0, &quot;No cambio en el pib&quot;)) # Sintaxis # mutate(columna a cambiar = replace(columna a cambiar, # criterio para las filas, # nuevo valor)). # Un equivalente a `replace()` es utilizar []. datos$regimen[datos$avance == 0] &lt;- &quot;No cambio en el pib&quot; replace_na(): Para cambiar los valores perdidos (NA) por un valor específico, como “Missing”, utilice la función dplyr replace_na() dentro de mutate(): datos_ficticios &lt;- data.frame(var1 = c(seq(1,10), NA), var2 = c(rep(NA,11))) datos_ficticios datos_ficticios1 &lt;- datos_ficticios %&gt;% mutate(var1 = replace_na(var1, 0)) datos_ficticios1 Recordatorio: Esta función cambia los missing values solo por valores de la misma clase de la variable/columna, por lo tanto, si trabajamos con valores de clase numérica los reemplazos deben ser también numéricos. case_when(): una función de dplyr. Es útil para asignar múltiples valores. Sirve si se necesita recodificar muchos grupos: rm(list = ls()) datosedad &lt;- data.frame(edad = c(2,3,4,1,500,2330,8,10,12), unidad = c(&quot;años&quot;,&quot;años&quot;, &quot;años&quot;, NA, &quot;meses&quot;, &quot;meses&quot;, &quot;años&quot;, &quot;años&quot;, &quot;semanas&quot;)) datosedad # Imaginemos que queremos tener una medida comparable de edad, para ello # podemos utilizar `case_when()`. datos_case_when &lt;- datosedad %&gt;% mutate(edad_años = case_when( unidad == &quot;años&quot; ~ edad, unidad == &quot;meses&quot; ~ edad/12, unidad == &quot;semanas&quot; ~ edad/52, is.na(unidad) ~ edad)) datos_case_when 9.5.4 Cambio de formato de los datos 9.5.4.1 De ancho a largo En en siguiente ejemplo, los datos están guardaos en “wide” para las columnas que tienen el número de casos de malaria por tramos de edad. Para un trabajo de análisis de datos es importante transformar los datos a “long”. rm(list = ls()) count_data &lt;- import(here(&quot;datos&quot;,&quot;malaria_facility_count_data.rds&quot;)) head(count_data) pivot_longer(): función del paquete tidyr. Paquete incluido en tidyverse(). Transforma los datos de wide a long. df_long &lt;- count_data %&gt;% pivot_longer( cols = c(&quot;malaria_rdt_0-4&quot;, &quot;malaria_rdt_5-14&quot;, &quot;malaria_rdt_15&quot;, &quot;malaria_tot&quot;) ) df_long # Una mejor opción es con la función starts_with(): count_data %&gt;% pivot_longer( cols = starts_with(&quot;malaria_&quot;) ) # Para agregar nombres a las nuevas variable creadas df_long1 &lt;- count_data %&gt;% pivot_longer( cols = starts_with(&quot;malaria_&quot;), names_to = &quot;grupo_edad&quot;, values_to = &quot;casos_malaria&quot; ) df_long1 9.5.4.2 De largo a ancho pivot_wider(): Transforma los datos de long a wide. Útil si quiero hacer una tabla mas amigable para el o la lectora. rm(list = ls()) linelist &lt;- import(here(&quot;datos&quot;,&quot;linelist_cleaned.rds&quot;)) df_wide &lt;- linelist %&gt;% count(age_cat, gender) df_wide # En un mejor formato table_wide &lt;- df_wide %&gt;% pivot_wider( id_cols = age_cat, names_from = gender, values_from = n ) table_wide Otra opción para hacer esto mismo es gather() y spread(): rm(list = ls()) datos &lt;- import(here(&quot;datos&quot;,&quot;Cuadro_1.xls&quot;), skip = 1) # Cargamos datos que estan en formato ancho y los preparamos un poco... datos &lt;- datos[-c(1:2)] colnames(datos) &lt;- c(1960:2018) colnames(datos) # 1. Como vemos, los años estan en las columnas, y queremos pasar a long data, # de forma tal que los años esten en una columna y el pib en otra # 2. La funciom &quot;gather()&quot; transforma los datos de formato ancho (wide) a # formato largo (long) # De ancho a largo: utilizando la función gather() # La primera es la variable &quot;clave&quot; y la segunda es la del valor data_long &lt;- datos %&gt;% gather(año, pib, 1:59) data_long # De largo a ancho: utilizando la función spread() data_wide &lt;- data_long %&gt;% spread(año,pib) data_wide 9.5.5 Juntar bases de datos 9.5.5.1 Pegar hacia el lado (por columnas) En el siguiente ejemplo, tenemos datos de hospitales. rm(list = ls()) hosp_info &lt;- import(here(&quot;datos&quot;,&quot;hosp_info_final.xlsx&quot;)) linelist_mini &lt;- import(here(&quot;datos&quot;,&quot;linelist_mini_final.xlsx&quot;)) Sintaxis: Imaginemos que tenemos dos bases de datos “df1”, “df2”. df1 tienen una columna llamada “ID”. df2 tiene una columna que se llama identificador. # Caso 1: nombres de identificadores distintos. data_junta &lt;- join(df1, df2, by = c(&quot;ID&quot; = &quot;identificador&quot;)) # Caso 2: imaginemos ambas bases de datos (df1, df2) tienen un # identificador llamado &quot;ID&quot; data_junta &lt;- join(df1, df2, by = &quot;ID&quot;) # Caso 3: imaginemos queremos pegar bases de datos considerando más # de un identificador. data_junta &lt;- join(df1, df2, by = c(&quot;nombre&quot; = &quot;primernombre&quot;, &quot;apellido&quot; = &quot;primerapellido&quot;, &quot;Edad&quot; = &quot;edad&quot;)) Left y Right join: # Left join: la primera base de datos que aparece es la referencia. # Right join: la segunda base de datos que aparece es la referencia. # Left_join left_join_ex1 &lt;- left_join(linelist_mini, hosp_info, by = c(&quot;hospital&quot; = &quot;hosp_name&quot;)) right_join_ex1 &lt;- right_join(hosp_info, linelist_mini, by = c(&quot;hosp_name&quot; = &quot;hospital&quot;)) # Ambos son equivalentes. right_join_ex2 &lt;- right_join(linelist_mini, hosp_info, by = c(&quot;hospital&quot; = &quot;hosp_name&quot;)) left_join_ex2 &lt;- linelist_mini %&gt;% left_join(hosp_info, by = c(&quot;hospital&quot; = &quot;hosp_name&quot;)) # Notas: # 1. Todas las filas/observaciones de la base de datos de referencia # se mantienen. # 2. Si hay más de un match, se duplican las observaciones. # 3. Los identificadores se combinan. Según el nombre # de la columna de la base de datos de referencia. # 4. Cuando no hay un &quot;match&quot; las columnas se llenan con un NA # para las observaciones de la base de referencia. # 5. No &quot;match&quot; por la base que no es de referencia se borran. Full Join: El más inclusivo de los “joins”. Retorna todas las observaciones/filas: full_join_ex3 &lt;- full_join(linelist_mini, hosp_info, by = c(&quot;hospital&quot; = &quot;hosp_name&quot;)) full_join_ex3 &lt;- linelist_mini %&gt;% full_join(hosp_info, by = c(&quot;hospital&quot; = &quot;hosp_name&quot;)) Inner join: El más restrictivo de los “join”. Retorna solo las filas que hicieron match entre ambas bases de datos. # Su análogo en stata es: merge 1:1, keep _merge==3 # Los que se pegaron perfectamente entre ambas bases. inner_join_example &lt;- linelist_mini %&gt;% inner_join(hosp_info, by = c(&quot;hospital&quot; = &quot;hosp_name&quot;)) Semi - join: Mantiene todas las observaciones de la base de referencia que tengan un “match” en la base secundaria, pero NO agrega nuevas columnas ni duplicados en casos de multiples “match”. semi_join_example1 &lt;- semi_join(hosp_info, linelist_mini, by = c(&quot;hosp_name&quot; =&quot;hospital&quot;)) semi_join_example &lt;- hosp_info %&gt;% semi_join(linelist_mini, by = c(&quot;hosp_name&quot; = &quot;hospital&quot;)) Anti- join: Se le llama, al igual que semi-join “join de filtros”. Retorna las observaciones/filas en la base de datos de referencia que no hacen “match” en base de datos secundaria. anti_join_example &lt;- hosp_info %&gt;% anti_join(linelist_mini, by = c(&quot;hosp_name&quot; = &quot;hospital&quot;)) La función merge: rm(list = ls()) authors &lt;- data.frame( surname = (c(&quot;Tukey&quot;, &quot;Venables&quot;, &quot;Tierney&quot;, &quot;Ripley&quot;, &quot;McNeil&quot;)), nationality = c(&quot;US&quot;, &quot;Australia&quot;, &quot;US&quot;, &quot;UK&quot;, &quot;Australia&quot;), deceased = c(&quot;yes&quot;, rep(&quot;no&quot;, 4))) authors books &lt;- data.frame( name = (c(&quot;Tukey&quot;, &quot;Venables&quot;, &quot;Tierney&quot;, &quot;Ripley&quot;, &quot;Ripley&quot;, &quot;McNeil&quot;, &quot;R Core&quot;)), title = c(&quot;Exploratory Data Analysis&quot;, &quot;Modern Applied Statistics ...&quot;, &quot;LISP-STAT&quot;, &quot;Spatial Statistics&quot;, &quot;Stochastic Simulation&quot;, &quot;Interactive Data Analysis&quot;, &quot;An Introduction to R&quot;), other.author = c(NA, &quot;Ripley&quot;, NA, NA, NA, NA, &quot;Venables &amp; Smith&quot;)) books m0 &lt;- merge(authors, books, by.x = &quot;surname&quot;, by.y = &quot;name&quot;) m0 # Por defecto solo mantiene las que hicieron match. m1 &lt;- merge(authors, books, by.x = &quot;surname&quot;, by.y = &quot;name&quot;, all = TRUE) m1 9.5.5.2 Pegar hacia abajo (por filas) Otra forma de unir bases de datos es agregar filas. Similar a lo que se hace con append() en Stata. Vamos a utilizar la función bind_rows() desde el paquete “dplyr”. bind_rows() es bastante inclusivo. Cualquier columna presente en las bases de datos se incluye en el output. Si ambas columnas se llaman igual, se alinearan correctamente. Adicionalmente, podemos agregar el argumento .id=. Este argumento genera una nueva columna que sirve para identificar de donde proviene la informacion. Ejemplo 1: caso sencillo continente_resumen &lt;- gapminder %&gt;% group_by(continent) %&gt;% summarise( cases = n(), gdpPercapmedian = median(gdpPercap, na.rm = TRUE)) continente_resumen # Crear Tabla 2: sin agrupar totales &lt;- gapminder %&gt;% summarise( cases = n(), gdpPercapmedian = median(gdpPercap, na.rm=T) ) totales # Ahora las podemos pegar combinadas &lt;- bind_rows(continente_resumen, totales) combinadas # ¿Como cambiar ese NA? combinadas &lt;- combinadas %&gt;% mutate(continent = replace_na(&quot;total&quot;)) combinadas # Muy util colocar &quot;id&quot; combinadas_id &lt;- bind_rows(continente_resumen, totales, .id = &quot;id&quot;) combinadas_id Ejemplo 2: ¿Qué ocurre si hay más de un archivo? # Base de datos maestra trial &lt;- data.frame( year = c(2016, 2017, 2018, 2019), n = c(501, 499, 498, 502), outcome = c(51, 52, 49, 50) ) %&gt;% print() # Base de datos 1 trial_2020 &lt;- data.frame( year = 2020, n = 500, outcome = 48 ) %&gt;% print() # Base de datos 2 trial_2021 &lt;- data.frame( year = 2021, n = 598, outcome = 57 ) %&gt;% print() # Para combinar mas de una base de datos trial1&lt;- bind_rows(trial,trial_2020, trial_2021) Ejemplo 3: ¿Qué ocurre si tengo muchos archivos? rm(list=ls()) library(pacman) p_load(plyr) # Recomendable ocupar paquete plyr. allfiles &lt;- list.files(path = &quot;datos&quot;, pattern = &quot;.csv&quot;, full.names = TRUE) allfiles # Append data combined_data&lt;- ldply(allfiles, read_csv) combined_data # Transformar a un data.frame. combined_data_sep &lt;- separate(data = combined_data, col = &quot;year;n;outcome&quot;, into = c(&quot;year&quot;, &quot;n&quot;, &quot;outcome&quot;), sep = &quot;;&quot;) combined_data_sep 9.6 Análisis de datos 9.6.1 Inspección de datos Preámbulo: # Limpiamos consola rm(list = ls()) # Cargamos paquetes que vamos a utilizar pacman::p_load( rio, # importar/exportar datos. here, # escribir rutas de las carpetas. janitor, # limpiar datos y tablas. tidyverse, # Manejo de bases de datos y visualización. magrittr, # Permite utilizar operador %&gt;% (piping). skimr, # Inspeccionar datos inspectdf, # Inspeccionar datos gapminder # Base de datos con información de países. ) Ahora vamos a crear tablas de estadística descriptiva que nos interesen para el análisis de datos. # Importamos datos datos &lt;- import(here(&quot;datos&quot;,&quot;linelist_cleaned.rds&quot;)) #Inspeccionamos los datos # a. Visión general de la base de datos skim(datos) # b. Información sobre cada columna summary(datos) # c. Información sobre cada variable categórica. insp &lt;- inspect_cat(datos) insp insp_figura &lt;- insp %&gt;% show_plot() insp_figura # d. Miramos nombre de las variables names(datos) # Nota 1: Tenemos 29 variables y el id es igual a case_id. Transformamos variables de interés: # a. Miramos la clase de cada variable unlist(lapply(datos,class)) # Nota 2: gender esta como texto, podría estar como factor. Lo mismo ocurre con outcome. # b. Transformamos gender/outcome en factores datos$gender &lt;- as.factor(datos$gender) levels(datos$gender) datos$outcome&lt;- as.factor(datos$outcome) levels(datos$outcome) # Miramos como esta codificado ahora unlist(lapply(datos,class))[&quot;gender&quot;] skim(datos) # c. Seleccionamos solo variables que nos interesa ocupar: reducir el problema! datos_trabajo &lt;- datos %&gt;% select(case_id, outcome, gender, age, age_years, age_cat, hospital, wt_kg:temp, days_onset_hosp) # d. Vemos que hay variables que están codificadas como &quot;yes&quot; y &quot;no&quot;. # Vamos a crearlas como variables binarias. datos_trabajo &lt;- datos_trabajo %&gt;% mutate(chills = ifelse(chills == &quot;yes&quot;,1,0), cough = ifelse(cough == &quot;yes&quot;,1,0), aches = ifelse(aches == &quot;yes&quot;,1,0), vomit = ifelse(vomit == &quot;yes&quot;,1,0)) summary(datos_trabajo) # Limpiamos para quedarnos solo con los datos que nos interesan. rm(insp, insp_figura) # Inspeccionamos nuevamente skim(datos_trabajo) # e. Ahora vamos a dejar una base de datos unicamente con valores completos completos &lt;- complete.cases(datos_trabajo) completos datos_trabajo &lt;- datos_trabajo[completos,] skim(datos_trabajo) # f. Finalmente, vamos a renombrar la base de datos con la que vamos a trabajar. rm(datos, completos) datos &lt;- datos_trabajo rm(datos_trabajo) 9.6.2 Estadística descriptiva Describiendo los datos: pacman::p_load(rstatix) # Opcion 1: utilizar get_summary_stats() del paquete &quot;rstatix&quot;. # El resultado se guarda en un dataframe. tabla1 &lt;- datos %&gt;% get_summary_stats( everything(), type = &quot;full&quot;) tabla1 # Puedo exportar esta tabla en excel utilizando `rio` y `here`. export(tabla1, here(&quot;resultados&quot;, &quot;ejercicio1&quot;, &quot;tabla1.xlsx&quot;)) 9.6.3 Tablas de frecuencia El paquete “janitor” ofrece la función tabyl() para producir tabulaciones simples y tabulaciones cruzadas, que pueden ser “adornadas” o modificadas con funciones de ayuda para mostrar porcentajes, proporciones, recuentos, etc. El uso por defecto de tabyl() en una columna específica produce los valores únicos, los recuentos y los “porcentajes” de la columna (en realidad proporciones). Las proporciones pueden tener muchos dígitos. Puede ajustar el número de décimales con la función adorn_rounding() como se describe a continuación. 9.6.3.1 Tablas de frecuencias con una entrada # Frecuencia para age_cat tabla2a &lt;- datos %&gt;% tabyl(outcome) tabla2a tabla2b &lt;- datos %&gt;% tabyl(gender) tabla2b tabla2c &lt;- datos %&gt;% tabyl(age_cat) tabla2c rm(tabla2a, tabla2b, tabla2c) # ¿Qué pasa si queremos hacer una tabla de frecuencias para # todas las variables que son factores? -&gt; iteradores! # Nombre de todas las variables de la base de datos nombres &lt;- names(datos) nombres # Selecciono solo a las que son categóricas (factores) select &lt;- unlist(lapply(datos, is.factor)) select nombres &lt;- nombres[select == TRUE] nombres Ahora voy a generar tantas tablas como variables categóricas para ello vamos a hacer uso de iteraciones. Creo una lista vacía, donde voy a guardar las tablas (data frames) que se generen. tabla2 &lt;- list() tabla2a &lt;- datos %&gt;% tabyl(outcome) tabla2a tabla2b &lt;- datos %&gt;% tabyl(gender) tabla2b tabla2c &lt;- datos %&gt;% tabyl(age_cat) tabla2c for (i in 1:length(nombres)){ tabla2[[i]] &lt;- datos %&gt;% tabyl(nombres[i]) } Ahora tengo una lista de tablas. Cada tabla habla de un variable categórica. Si vemos son equivalente a generarlas de otra forma. Si tengo pocas variables categóricas, esta forma puede no ser relevante, sin embargo, si tengo muchas, puede ser una buena forma de generar estadística descriptiva. # Comparamos datos %&gt;% tabyl(outcome,show_na = FALSE) tabla2[[1]] datos %&gt;% tabyl(gender,show_na = FALSE) tabla2[[2]] # Exportamos utilizando `rio` y `here` de nuevo export(tabla2[[1]],here(&quot;resultados&quot;, &quot;ejercicio1&quot;, &quot;tabla2-1-freq-outcomes.xlsx&quot;)) export(tabla2[[2]],here(&quot;resultados&quot;, &quot;ejercicio1&quot;, &quot;tabla2-2-freq-gender.xlsx&quot;)) export(tabla2[[3]],here(&quot;resultados&quot;, &quot;ejercicio1&quot;, &quot;tabla2-3-freq-age_cat.xlsx&quot;)) 9.6.3.2 Tablas de frecuencias con más de una entrada Imaginen que estamos interesados o interesadas en saber cuantas observaciones por categoría de edad por genero. Para ello nos gustaría hacer una tabla de doble entrada. Podemos ocupar tabyl() para hacer eso fácilmente: tabla3 &lt;- datos %&gt;% tabyl(age_cat, gender, show_na = FALSE) tabla3 rm(tabla3) # Hacemos una lista tabla3 &lt;- list() tabla3[[1]] &lt;- datos %&gt;% tabyl(outcome, gender) tabla3[[2]] &lt;- datos %&gt;% tabyl(outcome, age_cat) tabla3[[3]] &lt;- datos %&gt;% tabyl(age_cat, gender) tabla3[[1]] tabla3[[2]] tabla3[[3]] # Exportamos pero a diferentes hojas export(list(&quot;tabla 3.1&quot; = tabla3[[1]],&quot;tabla 3.2&quot; = tabla3[[2]], &quot;tabla 3.3&quot; = tabla3[[3]]), #Podemos asignarles nombres here(&quot;resultados&quot;, &quot;ejercicio1&quot;, &quot;tabla3_cruzadas.xlsx&quot;)) 9.6.3.3 Tablas mejoradas con opciones # Tabular los recuentos y las proporciones por categoría de edad # Con porcentajes para una de una entrada tabla4 &lt;- datos %&gt;% tabyl(age_cat) %&gt;% adorn_pct_formatting() # convertir proporciones en porcentajes tabla4 # Con porcentajes por fila tabla5 &lt;- datos %&gt;% tabyl(age_cat, gender) %&gt;% # contar para edad y género adorn_totals(where = &quot;row&quot;) %&gt;% # agregar totales adorn_percentages(denominator = &quot;row&quot;) %&gt;% # cambiar a porcentaje adorn_pct_formatting(digits = 2) # numero de digitos tabla5 # Nota: ¿Qué ocurre si cambio col/row en adorn_percentages? # Con porcentajes por columna tabla6 &lt;- datos %&gt;% tabyl(age_cat, gender) %&gt;% # contar para edad y genero adorn_totals(where = &quot;row&quot;) %&gt;% # agregar totales adorn_percentages(denominator = &quot;col&quot;) %&gt;% # cambiar a porcentaje adorn_pct_formatting(digits = 2) # nùmero de digitos tabla6 # Con total por columna y fila tabla7 &lt;- datos %&gt;% tabyl(age_cat, gender) %&gt;% # contar para edad y genero adorn_totals(where = list(&quot;row&quot;,&quot;col&quot;)) %&gt;% # agregar totales adorn_percentages(denominator = &quot;col&quot;) %&gt;% # cambiar a porcentaje adorn_pct_formatting(digits = 2) # nùmero de digitos tabla7 # ¿Como se diferencian estas tablas? tabla4 tabla5 # por fila tabla6 # por la columna tabla7 # agrega dos totales # Agregamos titulos tabla8 &lt;- datos %&gt;% tabyl(age_cat, gender) %&gt;% adorn_totals(where = &quot;row&quot;) %&gt;% adorn_percentages(denominator = &quot;row&quot;) %&gt;% # adorn_pct_formatting(digits = 2) %&gt;% adorn_ns(position = &quot;rear&quot;) %&gt;% # front para colocar parentesis en % adorn_title( # Agregar titulos row_name = &quot;Cat. edad&quot;, col_name = &quot;Genero&quot;) tabla8 # Exportamos tablas utilizando `rio` y `here`. lista &lt;- list(tabla4,tabla5,tabla6,tabla7,tabla8) lista for (i in 1:5){ export(lista[i],here(&quot;resultados&quot;, &quot;ejercicio1&quot;, paste(&quot;tabla&quot;,i + 3,&quot;.xlsx&quot;,sep=&quot;&quot;))) } pacman::p_load(flextable) # Exportamos la tabla directamente desde flextable tabla8_imagen_word &lt;- datos %&gt;% tabyl(age_cat, gender) %&gt;% adorn_totals(where = &quot;col&quot;) %&gt;% adorn_percentages(denominator = &quot;col&quot;) %&gt;% adorn_pct_formatting(digits = 2) %&gt;% adorn_ns(position = &quot;front&quot;) %&gt;% adorn_title( row_name = &quot;Categoria Edad&quot;, col_name = &quot;Genero&quot;, placement = &quot;combined&quot;) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::save_as_docx(path = &quot;tabla8_imagen.docx&quot;) 9.6.4 Estadística descriptiva con “dplyr” Ahora vamos a volver a utilizar los verbos del paquete “dplyr” que vimos la sección anterior. Vamos a generar las mismas cosas, pero de otra forma. Frecuencias: # Removemos todo, menos datos rm(list=ls()[! ls() %in% c(&quot;datos&quot;)]) # 1. ¿Cuál es el total de datos? tabla1 &lt;- datos %&gt;% summarise(n_rows = n()) tabla1 # 2. ¿Cuál es el total por categorías de edad? (con group_by y con summarize)? tabla2 &lt;- datos %&gt;% group_by(age_cat) %&gt;% summarise(n_rows = n()) tabla2 # 3. ¿Cuál es el total por categorías de edad (con count)? tabla3 &lt;- datos %&gt;% count(age_cat) tabla3 # 4. ¿Cuál es el total por categorías de edad y outcome (con count)? tabla4 &lt;- datos %&gt;% count(age_cat, outcome) tabla4 # 5. Equivalente con group_by() tabla4a &lt;- datos %&gt;% group_by(age_cat, outcome) %&gt;% summarise(n_rows = n()) tabla4a Porcentajes: tabla5 &lt;- datos %&gt;% count(outcome, age_cat) %&gt;% mutate( percent = (n / sum(n))*100) tabla5 tabla6 &lt;- datos %&gt;% group_by(outcome) %&gt;% count(age_cat) %&gt;% mutate(percent = (n / sum(n)*100)) tabla6 Resumen: tabla7 &lt;- datos %&gt;% # armamos la tabla como un nuevo objeto group_by(hospital) %&gt;% # agrupamos todos los cálculos por hospital summarise( # generamos la estadística que nos interesa cases = n(), # número de observaciones por grupo delay_max = max(days_onset_hosp, na.rm = TRUE), # maximo retraso delay_mean = round(mean(days_onset_hosp, na.rm = TRUE), digits = 1), # retraso promedio delay_sd = round(sd(days_onset_hosp, na.rm = TRUE), digits = 1), # desviacion estandar retraso delay_3 = sum(days_onset_hosp &gt;= 3, na.rm = TRUE), # número de hospitales on retrasos mayores a tres dias pct_delay_3 = (delay_3 / cases)) # porcentaje de lo anterior tabla7 Estadísticas condicionales: tabla8 &lt;- datos %&gt;% group_by(hospital) %&gt;% summarise( max_temp_fvr = max(temp[fever == &quot;yes&quot;], na.rm = TRUE), max_temp_no = max(temp[fever == &quot;no&quot;], na.rm = TRUE) ) tabla8 # ¿Cuál es la temperatura promedio para los pacientes que tuvieron fiebre y para los que no? tabla9 &lt;- datos %&gt;% group_by(hospital) %&gt;% summarise( promedio_temp_fvr = mean(temp[fever == &quot;yes&quot;], na.rm = TRUE), promedio_temp_no = mean(temp[fever == &quot;no&quot;], na.rm = TRUE) ) tabla9 Percentiles: # Percentiles por defecto de la variable edad (0%, 25%, 50%, 75%, 100%). # Usamos la función `quantile()`. tabla10 &lt;- datos %&gt;% summarise(percentiles_edad = quantile(age_years, na.rm = TRUE)) tabla10 # Agregarlos manualmente tabla11 &lt;- datos %&gt;% summarise( percentiles_edad = quantile( age_years, probs = c(.05, 0.5, 0.75, 0.98), na.rm=TRUE) ) tabla11 # También los podemos calcular combinando con otro verbos y mas especifico tabla12 &lt;- datos %&gt;% group_by(hospital,outcome) %&gt;% summarise( p05 = quantile(age_years, probs = 0.05, na.rm=TRUE), p50 = quantile(age_years, probs = 0.5, na.rm=TRUE), p75 = quantile(age_years, probs = 0.75, na.rm=TRUE), p98 = quantile(age_years, probs = 0.98, na.rm=TRUE) ) tabla12 # También podemos hacerlo utilizando `get_summary_stats()` tabla13 &lt;- datos %&gt;% group_by(hospital) %&gt;% rstatix::get_summary_stats(age, type = &quot;quantile&quot;) tabla13 Resumir datos agregados: # Contamos y quitamos los NA tabla14 &lt;- datos %&gt;% drop_na(gender, outcome) %&gt;% count(outcome, gender) tabla14 # Contamos por grupos y con condiciones. tabla15 &lt;- tabla14 %&gt;% group_by(outcome) %&gt;% summarise( total_cases = sum(n, na.rm=T), male_cases = sum(n[gender == &quot;m&quot;], na.rm=T), female_cases = sum(n[gender == &quot;f&quot;], na.rm=T)) tabla15 # Hacer lo mismo, pero para mas grupos y variables tabla16 &lt;- datos %&gt;% group_by(outcome) %&gt;% summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columnas .fns = mean, # funcion na.rm = TRUE)) # opciones tabla16 # Para todas las variables numéricas tabla17 &lt;- datos %&gt;% group_by(outcome) %&gt;% summarise(across( .cols = where(is.numeric), .fns = mean, na.rm=T)) tabla17 # De long a wide pacman::p_load(scales) tabla18 &lt;- datos %&gt;% group_by(outcome) %&gt;% count(age_cat) %&gt;% mutate(percent = scales::percent(n / sum(n))) tabla18 # La tabla esta en formato long! tabla19 &lt;- tabla18 %&gt;% select(-percent) %&gt;% # no quiero porcentaje pivot_wider(names_from = age_cat, values_from = n) tabla19 # Tabla con totales tabla20 &lt;- datos %&gt;% group_by(gender) %&gt;% summarise( known_outcome = sum(!is.na(outcome)), # Número de filas del grupo en las que no falta el resultado n_death = sum(outcome == &quot;Death&quot;, na.rm=T), # Número de filas en el grupo donde el resultado es Muerte n_recover = sum(outcome == &quot;Recover&quot;, na.rm=T), # Número de filas del grupo cuyo resultado es Recuperado ) %&gt;% adorn_totals() %&gt;% # Adornar la fila total (suma de cada columna numérica) adorn_percentages(&quot;row&quot;) %&gt;% # Proporciones adorn_pct_formatting() %&gt;% # porcetnaje adorn_ns(position = &quot;rear&quot;) # () tabla20 # Exportamos todas las tablas lista_tablas &lt;- list(tabla1,tabla2,tabla3,tabla4,tabla5, tabla6,tabla7,tabla8,tabla9,tabla10, tabla11,tabla12,tabla13,tabla14,tabla15, tabla16,tabla17,tabla18,tabla19,tabla20) lista_tablas for (i in 1:20){ export(lista_tablas[i],here(&quot;resultados&quot;, &quot;ejercicio2&quot;, paste(&quot;tabla&quot;,i,&quot;.xlsx&quot;,sep=&quot;&quot;))) } Juntar tablas: # Algunas estadísticas por hospital y outcome tabla21 &lt;-datos %&gt;% filter(!is.na(outcome) &amp; hospital != &quot;Missing&quot;) %&gt;% # Dejo todo lo que no sea missing group_by(hospital, outcome) %&gt;% # agrupo por hospital y outcome summarise( N = n(), ct_value = median(ct_blood, na.rm=T)) tabla21 # Algunas estadísticas solo por outcome tabla22 &lt;- datos %&gt;% filter(!is.na(outcome) &amp; hospital != &quot;Missing&quot;) %&gt;% group_by(outcome) %&gt;% # Ahora agrupo solo por outcome, no hospital summarise( N = n(), # Estadisticas solo por outcome ct_value = median(ct_blood, na.rm=T)) tabla22 # Juntando table_long &lt;- bind_rows(tabla21, tabla22) %&gt;% mutate(hospital = replace_na(hospital, &quot;Total&quot;)) table_long # A formato long y a exportar table_long %&gt;% mutate(hospital = replace_na(hospital, &quot;Total&quot;)) %&gt;% pivot_wider( # de largo a ancho values_from = c(ct_value, N), # cambio valores names_from = outcome) %&gt;% # cambio columnas mutate( # agrego nuevas columnas N_Known = N_Death + N_Recover, # numero total Pct_Death = scales::percent(N_Death / N_Known, 0.1), # % casos que murieron Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # % casos recuperados select( # reordeno columnas hospital, N_Known, # totales N_Recover, Pct_Recover, ct_value_Recover, # recuperado N_Death, Pct_Death, ct_value_Death) %&gt;% # muertes arrange(N_Known) %&gt;% # Ordeno flextable::flextable() %&gt;% # a imagen flextable::autofit() %&gt;% # una linea por fila flextable::save_as_docx(path = here(&quot;resultados&quot;,&quot;ejercicio2&quot;,&quot;tablefinal.docx&quot;)) # exporto 9.7 Visualización 9.7.1 Visualización: R base Vamos a hacer graficos unicamente utilizando R básico. 9.7.1.1 Plot # Para visualizar datos altura &lt;- c(168, 177, 177, 177, 178, 172, 165, 171, 178, 170) peso &lt;- c(88, 72, 85, 52, 71, 69, 61, 61, 51, 75) # Gráfico simple plot(altura, peso, ylab = &quot;Peso (kg)&quot;, xlab = &quot;Altura (cm)&quot;) # Ahora lo vamos a exportar setwd(here(&quot;figuras&quot;)) pdf(&quot;g1.pdf&quot;) plot(altura,peso) dev.off() rm(altura,peso) 9.7.1.2 Histograma # Buscar ayuda: ?hist() # Implementarla g2 &lt;- hist(wage, breaks = 20, # número de intervalos main = &quot;Distribución del Salario (dólares por hora)&quot;, # título del gráfico xlab = &quot;Salario&quot;, # título del eje x ylab = &quot;Número de personas&quot; # título del eje y ) g2 # Podemos agregar colores g2 &lt;- hist(wage, breaks = 20, # n?mero de intervalos main = &quot;Distribución del Salario (dólares por hora)&quot;, # título del gráfico xlab = &quot;Salario&quot;, # título del eje x ylab = &quot;Numero de personas&quot;, # título del eje y col = &quot;pink&quot; ) g2 # También podemos agregar límites en los ejes g2 &lt;- hist(wage, breaks = 20, # número de intervalos main = &quot;Distribución del Salario (dólares por hora)&quot;, xlab = &quot;Salario&quot;, ylab = &quot;Número de personas&quot;, col=&quot;pink&quot;, xlim = c(0,50),# Límites del histograma freq = TRUE #TRUE = freq. absoluta, FALSE: relativa. ) g2 # Podemos hacer lo mismo, pero agregar la distribución empirica pdf(&quot;g3.pdf&quot;) g3 &lt;- hist(wage, breaks = 20, # número de intervalos main = &quot;Distribución del Salario (dólares por hora)&quot;, xlab = &quot;Salario&quot;, ylab = &quot;Número de personas&quot;, col=&quot;pink&quot;, xlim = c(0,30), ylim = c(0, .12), # Límites del histograma freq = FALSE, #TRUE = freq. absoluta, FALSE: relativa. ) # Agrego la distribución empirica de los datos lines(density(wage), col=&quot;blue&quot;, lwd=2) Opciones: # lwd = line width # col = color # lty = line type # opciones de lty (0 = blank, 1 = solid (default), # 2 = dashed, 3 = dotted, # 4 = dotdash, 5 = longdash, # 6 = twodash) # Agregar más de una distribución lines(density(wage, adjust=2), col=&quot;red&quot;, lwd=2, lty=2) # adjust=2 lo que hace es suavizar un poco la curva # Agregar el promedio y la mediana # Promedio abline(v = mean(wage), lwd = 2, lty = 3, col=&quot;darkgreen&quot;) # Mediana abline(v = median(wage), lwd = 2, lty = 3, col=&quot;darkblue&quot;) dev.off() # Notar que solo hemos utilizado paquetes básicos de R. # Ahora, podemos exportar el gráfico a la carpeta. 9.7.1.3 Gráfico de dispersión # Básico dev.off() plot(experience, wage) plot(wage~experience) # Mejorando la presentación plot (wage ~ experience, main = &quot;Salario en función de la experiencia&quot;, xlab = &quot;Experiencia (en años)&quot;, ylab = &quot;Salario&quot; ) # Podemos agrupar CPS1985 &lt;- CPS1985 %&gt;% mutate(sexo = as.numeric(gender)) pdf(&quot;g4.pdf&quot;) g4 &lt;- plot (wage ~ experience, data = CPS1985, pch = sexo, #Diferente simbolo dependiendo del género col = sexo, #Color diferenciado por género main = &quot;Salario en función de la experiencia&quot;, xlab = &quot;Experiencia (en años)&quot;, ylab = &quot;Salario&quot; ) # Agregamos Leyenda legend(&quot;topright&quot;, legend=c(&quot;Hombres&quot;,&quot;Mujeres&quot;), pch=1:2, col=1:2, bty=&quot;n&quot;) #Caja de la leyenda legend(&quot;topleft&quot;, legend=c(&quot;Hombres&quot;,&quot;Mujeres&quot;), pch=1:2, col=1:2) # Agregamos una regresión with(CPS1985[gender==&quot;male&quot;,], abline(lm(wage~experience), col=&quot;black&quot;)) with(CPS1985[gender==&quot;female&quot;,], abline(lm(wage~experience), col=&quot;red&quot;)) dev.off() 9.7.1.4 Gráfico de barras # Primero tenemos que crear las frecuencias frecuencias &lt;- table(occupation) # con variable categórica frecuencias # Básico barplot(frecuencias, col = c(1:6) ) # Ahora con educación educacion &lt;- table(education) pdf(&quot;g5.pdf&quot;) g5 &lt;- barplot(educacion, col = 1:dim(educacion), # para establecer el número de diferentes colores (=diferentes valores) horiz = FALSE, # orientación de las barras ylim = c(0,250) ) # Agregamos un título title(&quot;Distribución de la educación&quot;, xlab= &quot;Educación (en años)&quot;, ylab = &quot;Número de personas&quot;) dev.off() 9.7.1.5 Gráfico de torta datos &lt;- table(ethnicity) datos pie(datos, labels=c(&quot;Caucasicos&quot;, &quot;Hispanos&quot;, &quot;Otros&quot;), col = heat.colors(3), main = &quot;Frecuencia de etnias&quot;, clockwise = FALSE) # Podemos cambiar los colores colores&lt;-c(&quot;darkred&quot;,&quot;pink&quot;,&quot;red&quot;) pdf(&quot;g6.pdf&quot;) g6 &lt;- pie(datos, labels = levels(ethnicity), col = colores, main = &quot;Frecuencia etnias&quot;) dev.off() 9.7.1.6 Boxplot Algunas cosas que tener en cuenta sobre un boxplot: Se le llama grafico de caja y bigote. Primer quartil (1): borde inferior de la caja. Mediana: Linea de al medio de la caja. Tercer quartil: Borde superior de la caja. Rango intercuartil: Diferencia entre el tercer cuartil y el primero. Sobre el gráfico: Caja más grande implica que los datos están mas dispersos. Tamaño de la caja es el rango intercuartil. Si la mediana esta al centro, la distribución es simétrica. ¿Que pasa si la parte superior (sobre la linea) es más grande? Los datos se concentran en la parte baja de la distribución. Los bigotes determinan el límite para valores atípicos.Su longitud máxima es de un 150% del Rango intercuartil (RIC). dev.off() boxplot(wage, main = &quot;Salario (dólares por hora)&quot;, ylab = &quot;Salario&quot;, col = &quot;pink&quot;, border = &quot;red&quot;) # Separando por sexo boxplot(wage ~ gender, main = &quot;Salario (dólares por hora) según sexo&quot;, ylab = &quot;Salario&quot;, col = c(&quot;pink&quot;,&quot;darkgreen&quot;), border = &quot;black&quot;) #Horizontal boxplot(wage ~ gender, main = &quot;Salario (dólares por hora) según sexo&quot;, ylab = &quot;Salario&quot;, names = c(&quot;Hombres&quot;,&quot;Mujeres&quot;), #Cambiar nombres horizontal = T, #Posición horizontal col = rainbow(2, alpha=0.8), #Paleta rainbow (aleatoria) y transparencia (alpha) border = &quot;black&quot;) 9.7.2 Uso paquete ggplot Ahora vamos a utilizar un paquete avanzado de visualización. Primero, algunos aspectos relacionados con la lógica de su uso. Utilizar ggplot2 es básicamente ir agregando capas. Entre cada cosa que quiero agregar al gráfico debo colocar un signo +. La sintaxis básica incluye: Comience con el comando ggplot() de la línea de base, esto “abre” el ggplot y permite que las funciones subsecuentes sean agregadas con +. Normalmente el conjunto de datos también se especifica en este comando. Añadir capas “geom”, estas funciones visualizan los datos como geometrías (formas), por ejemplo, como un gráfico de barras, un gráfico de líneas, un gráfico de dispersión, un histograma (¡o una combinación!). Todas estas funciones comienzan con geom_ como prefijo. Añade elementos de diseño al gráfico, como etiquetas de ejes, títulos, fuentes, tamaños, esquemas de color, leyendas o rotación de ejes. Ejemplo: ggplot(datos, aes(x = experience, y = wage)) + geom_point( color = &quot;blue&quot;) + labs()+ theme() Ahora ya entendemos la sintaxis general. Para cambiar de tipo de gráficos tenemos distintas geometrías. # geom_point (para puntos) # geom_line (para lineas) # geom_histogram (para histograma) # geom_boxplot (para boxplot) # geom_bar o geom_col() (para barras) # geom_smooth (lineas suavizadas) 9.7.2.1 Geometrías Otros tienen más sentido con una variable ggplot(datos, aes(x = experience))+ geom_histogram() + labs()+ theme() ggplot(datos, mapping = aes(x = experience, y = wage))+ geom_smooth(color = &quot;red&quot;) + labs()+ theme() 9.7.2.2 Aesthetics Otro aspecto importante es lo que se denomina “aesthetics”. Estas son las características visuales de los datos. En ggplot estos son modificados dentro de la opción theme(). No todas las geometrías tienen las mismas opciones. Sin embargo, algunas comunes son: shape: Mostrar un punto con geom_point() como punto, estrella, triángulo o cuadrado. fill: El color interior (por ejemplo, de una barra o boxplot) color: La línea exterior de una barra, boxplot, etc., o el color del punto si se utiliza geom_point() size: Tamaño (por ejemplo, grosor de la línea, tamaño del punto) alpha: Transparencia (1 = opaco, 0 = invisible) binwidtho: Ancho de los bins o cajas del histograma width: Anchura de las columnas del “bar plot”. linetype: Tipo de línea (por ejemplo, sólida, discontinua, punteada) Los “aesthetics” pueden ser asignados a valores o a vectores. Ejemplo: # Visualización de puntos ggplot(datos, aes(x = experience, y = wage))+ geom_point(color = &quot;darkgreen&quot;, size = 2.8, alpha = 0.2) + theme() # Histograma ggplot(datos, mapping = aes(x = wage))+ geom_histogram(color = &quot;white&quot;, fill = &quot;pink&quot;, binwidth = 5 , alpha = 1) 9.7.2.3 Escalar los valores o agrupar # Si queremos diferenciar alguna variable categorica por color (agrupar) ggplot(datos, aes(experience, wage, color = gender)) + geom_point() # Si queremos diferenciar alguna variable continua por tamaño (escalar) ggplot(datos, aes(x = experience, y = wage, size = age)) + geom_point(shape = &quot;circle&quot;, alpha = 0.3) # Ambos (agrupar y escalar) ggplot(data = datos, mapping = aes(x = experience, y = wage, size = age, color = gender)) + geom_point(shape = &quot;circle&quot;, alpha = 0.3) 9.7.2.4 Equivalencia sintaxis ggplot # Notar que estas tres sintaxis son equivalentes ggplot(data = datos, mapping = aes(x = age))+ geom_histogram() ggplot(data = datos)+ geom_histogram(mapping = aes(x = age)) ggplot()+ geom_histogram(data = datos, mapping = aes(x = age)) 9.7.2.5 Combinar gráficos facet_wrap(): Mostrar un panel diferente para cada nivel de una sola variable. Un ejemplo de esto podría ser mostrar una figura diferente para cada region de un país. Las facetas se ordenan alfabéticamente, a menos que la variable sea un factor con otro ordenamiento definido. Por ejemplo: # Datos malaria_data &lt;- import(here(&quot;datos&quot;, &quot;malaria_facility_count_data.rds&quot;)) %&gt;% select(-submitted_date, -Province, -newid) # A plot with facets by district ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) + geom_col(width = 1, fill = &quot;darkred&quot;) + # graficamos datos como columnas theme_minimal() + # simplificamos la parte de atras labs(x = &quot;Fecha&quot;, #Etiquetas/labels y = &quot;Casos de Malaria&quot;, title = &quot;Casos de Malaria por distrito&quot;) + facet_wrap(~District) # creamos las facetas facet_grid(): Se utiliza cuando se quiere introducir una segunda variable en la disposición de las facetas. Aquí cada panel de una cuadrícula muestra la intersección entre los valores de dos columnas. Por ejemplo: malaria_age &lt;- malaria_data %&gt;% select(-malaria_tot) %&gt;% pivot_longer( cols = c(starts_with(&quot;malaria_rdt_&quot;)), names_to = &quot;grupo_edad&quot;, values_to = &quot;num_casos&quot; ) %&gt;% mutate( age_group = str_replace(grupo_edad, &quot;malaria_rdt_&quot;, &quot;&quot;)) ggplot(malaria_age, aes(x = data_date, y = num_casos)) + geom_col(fill = &quot;darkred&quot;, width = 1) + theme_minimal()+ labs( x = &quot;Fechas&quot;, y = &quot;Casos&quot;, title = &quot;Casos de Malaria por grupos de edad y distrito&quot;) + facet_grid(District ~ age_group) 9.7.2.6 Modificar y guardar Modificar graficos: Una cosa muy positiva de ggplot() es que uno puede definirlo con un nombre y luego ir sobre-escribiendolo. Eso es muy útil para la sintaxis. # Gráfico original histograma &lt;- ggplot(data = datos, mapping = aes(x = age))+ geom_histogram() histograma # Gráfico modificado. histograma_modificado &lt;- histograma + geom_vline(xintercept = 50) histograma_modificado # Noten que solamente hemos agregado la nueva opción sobre el objeto asignado. # Muy útil y recomendable para hacer visualizaciones en R. Exportar gráficos: Para los gráfico que se generen con ggplot() es posible utilizar la opción ggsave(). ggsave(here(&quot;figuras&quot;, &quot;histograma.pdf&quot;), histograma_modificado) 9.7.2.7 Labels distribucion_salarios &lt;- ggplot( data = datos, # datos mapping = aes( # ejes x = age, y = wage, color = occupation))+ # agrupo geom_point()+ # geometria labs( title = &quot;Salarios por edad&quot;, subtitle = &quot;Estados Unidos, 1985&quot;, x = &quot;Edad en años&quot;, y = &quot;Salario en dólares por hora&quot;, color = &quot;Ocupación&quot;, caption = stringr::str_glue(&quot;Maxima edad es: {max(datos$age, na.rm=TRUE)}&quot;)) distribucion_salarios 9.7.2.8 Temas # Clasico distribucion_salarios + theme_classic() # Mínimo distribucion_salarios + theme_minimal() # Oscuro distribucion_salarios + theme_dark() # Claro distribucion_salarios + theme_light() # Gris distribucion_salarios + theme_grey() # Blanco y negro distribucion_salarios + theme_bw() 9.7.2.9 Piping y ggplot rm(list = ls()) datos &lt;- import(here(&quot;datos&quot;, &quot;linelist_cleaned.rds&quot;)) datos %&gt;% select(c(case_id, fever, chills, cough, aches, vomit)) %&gt;% pivot_longer( cols = -case_id, names_to = &quot;symptom_name&quot;, values_to = &quot;symptom_is_present&quot;)%&gt;% mutate( symptom_is_present = replace_na(symptom_is_present, &quot;unknown&quot;)) %&gt;% ggplot( mapping = aes(x = symptom_name, fill = symptom_is_present))+ geom_bar(position = &quot;fill&quot;, col = &quot;black&quot;) + theme_minimal() + labs( x = &quot;Sintoma&quot;, y = &quot;Sintoma (status)&quot; ) 9.7.2.10 Gráficos para variables continuas Histograma: rm(list = ls()) data(&quot;CPS1985&quot;) data &lt;- CPS1985 rm(CPS1985) attach(data) #Gráfico absoluto ggplot(data, aes(x=wage)) + geom_histogram() #Gráfico relativo ggplot(data, aes(x=wage)) + geom_histogram(aes(y=..density..)) # Histograma con 20 intervalos ggplot(data, aes(x=wage)) + geom_histogram(bins=100, color=&quot;black&quot;, fill=&quot;blue&quot;, alpha = 0.8) #Agregando títulos y limites ggplot(data, aes(x=wage)) + geom_histogram(bins=20, color=&quot;white&quot;, fill=&quot;blue&quot;) + labs(title = &quot;Distribución del salario (dólares por hora)&quot;, x = &quot;Salario&quot;, y = &quot;Número de empleados&quot;) + xlim(0,30) #Gráficos de subconjuntos ggplot(data, aes(x=wage)) + geom_histogram(bins=20, color=&quot;white&quot;, fill=&quot;pink&quot;) + facet_grid(gender~.) # U horizontal ggplot(data, aes(x=wage)) + geom_histogram(bins=20, color=&quot;white&quot;, fill=&quot;pink&quot;) + facet_wrap(~gender)+ labs(title = &quot;Distribución del salario (dólares por hora)&quot;, x = &quot;Salario&quot;, y = &quot;Número de empleados&quot;) + xlim(0,30) + theme_minimal() # O ambos juntos ggplot(data, aes(x=wage)) + geom_histogram(bins=20, aes(fill=gender), position=&quot;fill&quot;, alpha=0.6) + labs(title = &quot;Distribución del salario (dólares por hora)&quot;, x= &quot;Salario&quot;, y=&quot;Empleados&quot;, fill=&quot;Género&quot;) + # títulos de ejes y leyenda scale_fill_discrete(labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) + # títulos claves leyenda xlim(0,30) Gráfico de densidad: ggplot(data, aes(x=wage, color=gender)) + geom_freqpoly(bins=20, aes(y=..density..)) + labs(color=&quot;Género&quot;) # Uniendo a y b ggplot(data, aes(x=wage)) + geom_histogram(aes(y=..density..), bins=20, color=&quot;white&quot;, fill=&quot;pink&quot;) + #stat_function(fun = dnorm, colour = &quot;red&quot;, # args = list(mean = mean(wage, na.rm = TRUE), # sd = sd(wage, na.rm = TRUE))) + geom_density(color=&quot;blue&quot;)+ labs(title=&quot;Distribución del salario (dólares por hora)&quot;, x= &quot;Salario&quot;, y=&quot;Empleados&quot;) Diagrama de dispersión (scatterplot): ggplot(data, aes(experience, log(wage))) + geom_point() + labs(title=&quot;Diagrama de dispersión&quot;, subtitle= &quot;ScatterPlot&quot;, caption=&quot;Fuente: CPS1985 (paquete AER)&quot;, x=&quot;Experiencia (en años)&quot;, y=&quot;Salario (en logaritmo)&quot;) # Formas de darle color al gráfico # (1) Desde geom_() ggplot(data, aes(experience, log(wage))) + geom_point(aes(color=gender)) # (2) Desde ggplot() ggplot(data, aes(experience, log(wage), color=&quot;red&quot;)) + geom_point() # Función jitter (shortcut geom_point(position = &quot;jitter&quot;) ggplot(data, aes(experience,log(wage))) + geom_jitter(width=.2, alpha=0.5) + #Suavizar la visualización de solapamiento labs(title=&quot;Diagrama de dispersión&quot;, subtitle= &quot;ScatterPlot&quot;, caption=&quot;Fuente: CPS1985 (paquete AER)&quot;, x=&quot;Experiencia (en años)&quot;, y=&quot;Salario (en logaritmo)&quot;) # Diviendo por categorías ggplot(data, aes(experience,log(wage), color=gender)) + geom_jitter() + geom_smooth(method=&quot;lm&quot;) + labs(title=&quot;Diagrama de dispersión&quot;, x=&quot;Experiencia (en años)&quot;, y=&quot;Salario (en logaritmo)&quot;) + scale_color_discrete(name=&quot;Género&quot;, labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) # Nota: Ojo con realizar de otra manera, cambia la recta de regresión ggplot(data, aes(experience,log(wage))) + geom_point(aes(color=gender)) + geom_smooth(method=&quot;lm&quot;) + labs(title=&quot;Diagrama de dispersión&quot;, x=&quot;Experiencia (en años)&quot;, y=&quot;Salario (en logaritmo)&quot;) + scale_color_discrete(&quot;Género&quot;, labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) Boxplot: # Básico ggplot(data, aes(x=gender, y=wage, fill=gender)) + geom_boxplot() + labs(title=&quot;Boxplot&quot;, x=&quot;Género&quot;, y=&quot;Salario&quot;, fill=&quot;Género&quot;) + # titulo ejes y leyenda scale_x_discrete(labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) + # etiquetas del eje x scale_fill_discrete(labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) # etiquetas claves leyenda #Visualizando todos los datos con paletas creadas (armamos una función) ts4_colors &lt;- c( `pink` = &quot;#B40586&quot;, `light blue` = &quot;#00A2E9&quot;, `green` = &quot;#00AC8B&quot;, `orange` = &quot;#f37735&quot;, `red` = &quot;#FF1B6B&quot;, `purple` = &quot;#805FAA&quot;, `blue` = &quot;#0059ff&quot;, `light green` = &quot;#7EAF64&quot;) ts4_cols &lt;- function(...) { cols &lt;- c(...) if (is.null(cols)){ return (ts4_colors) } ts4_colors[cols] } ts4_cols(&quot;red&quot;) ts4_cols() ggplot(data, aes(x=gender, y=wage, fill=gender) ) + geom_boxplot(alpha=0.3, fill=ts4_cols(&quot;pink&quot;,&quot;light green&quot;), outlier.colour = ts4_cols(&quot;blue&quot;)) + labs(title=&quot;Boxplot&quot;,x=&quot;Género&quot;, y=&quot;Salario&quot;) + scale_x_discrete(labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) + guides(fill=FALSE) + coord_flip() + geom_point(stat= &quot;summary&quot;, shape=16, size=4, colour=ts4_cols(&quot;purple&quot;)) + geom_jitter(width = 0.1, alpha = 0.2, colour=ts4_cols(&quot;orange&quot;)) Gráfico de barras: ggplot(data, aes(ethnicity)) + geom_bar() + labs(title=&quot;Diagrama de barras&quot;, x= &quot;Raza&quot;, y=&quot;Empleados&quot;) + scale_x_discrete(labels=c(&quot;Caucásico&quot;, &quot;Hispano&quot;, &quot;Otros&quot;)) # Distintos colores ggplot(data, aes(ethnicity, fill=ethnicity)) + geom_bar() + labs(title=&quot;Distribución de empleadores según etnia&quot;, x= &quot;Raza&quot;, y=&quot;Empleados&quot;) + scale_x_discrete(labels=c(&quot;Caucásico&quot;, &quot;Hispano&quot;, &quot;Otros&quot;)) + guides(fill=FALSE) # Sin/con leyenda # ¿Cómo se distribuye el género sobre la etnia? ggplot(data, aes(ethnicity, fill=gender)) + geom_bar() + labs(title=&quot;Diagrama de barras&quot;, x= &quot;Etnia&quot;, y=&quot;Empleados&quot;) + scale_x_discrete(labels=c(&quot;Caucásico&quot;, &quot;Hispano&quot;, &quot;Otros&quot;)) + scale_fill_discrete(&quot;Género&quot;, labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) # Separado por género, pero paralelo ggplot(data, aes(ethnicity, fill=gender)) + geom_bar(position=&quot;dodge&quot;) + # también: position=position_dodge() labs(title=&quot;Distribución de empleadores según etnia&quot;, x= &quot;Raza&quot;, y=&quot;Empleados&quot;) + scale_x_discrete(labels=c(&quot;Caucásico&quot;, &quot;Hispano&quot;, &quot;Otros&quot;)) + scale_fill_brewer(palette = &quot;Accent&quot;, # Definir la paleta manualmente &quot;Género&quot;, labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) 9.7.2.11 Gráficos con plotly Ejemplo 1: g1 &lt;- ggplot(data, aes(ethnicity, fill=gender)) + geom_bar(position=&quot;dodge&quot;) + # también: position=position_dodge() labs(title=&quot;Distribución de empleadores según etnia&quot;, x= &quot;Raza&quot;, y=&quot;Empleados&quot;) + scale_x_discrete(labels=c(&quot;Caucásico&quot;, &quot;Hispano&quot;, &quot;Otros&quot;)) + scale_fill_brewer(palette = &quot;Accent&quot;, #Definir la paleta manualmente &quot;Género&quot;, labels=c(&quot;Hombre&quot;,&quot;Mujer&quot;)) g1 ggplotly(g1) Ejemplo 2: set.seed(100) #semilla d &lt;- diamonds[sample(nrow(diamonds), 1000), ] p &lt;- ggplot(data = d, aes(x = carat, y = price)) + geom_point(aes(text = paste(&quot;Clarity:&quot;, clarity)), size = 1) + geom_smooth(aes(colour = cut, fill = cut)) + facet_wrap(~ cut) p ggplotly(p) Recordatorio: Set seed o sembrar la semilla nos permite generar datos seudo aleatorizados pero replicables, de manera que cualquier persona que acceda al script sea capaz de llegar a los mismos resultados que nosotros. El número que se incluye dentro de la función actua como un “código” que genera estos datos aleatorios, y puede ser cualquiera. Ejemplo 3: Ocupar directamente plotly data(iris) fig &lt;- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length, color=~Species) fig # Sacar barra de opciones y zoom fig %&gt;% layout(yaxis = list(fixedrange = TRUE), showlegend = TRUE, xaxis = list(fixedrange = TRUE)) %&gt;% config(displayModeBar = FALSE) #Line plot x &lt;- c(1:50) random_y &lt;- rnorm(50, mean = 0) data &lt;- data.frame(x, random_y) fig &lt;- plot_ly(data, x = ~x, y = ~random_y, type = &#39;scatter&#39;, mode = &#39;lines&#39;) fig # Histogramas fig &lt;- plot_ly(x = ~rnorm(50), type = &quot;histogram&quot;) fig # Con 2 grupos fig &lt;- plot_ly(alpha = 0.6) fig &lt;- fig %&gt;% add_histogram(x = ~rnorm(500)) fig &lt;- fig %&gt;% add_histogram(x = ~rnorm(500) + 1) fig &lt;- fig %&gt;% layout(barmode = &quot;overlay&quot;) fig # Boxplot fig &lt;- plot_ly(y = ~rnorm(50), type = &quot;box&quot;) fig &lt;- fig %&gt;% add_trace(y = ~rnorm(50, 1)) fig 9.8 Análisis de datos y generación de informes con R Markdown 9.8.1 Test de hipótesis Imaginen que una compañia quiere testear la efectividad de una estrategia de venta. La estrategia A consiste en colocar un letrero que diga “oferta final”. La estrategia B consiste en dejar todo como siempre. Aleatoriamente se eligen los días y lugares en donde se utilizara la estrategia A y los días en que se utilizara la estrategia B. Luego, se registran los datos de ventas y se observan que las ventas de la compañía con la estrategia B fueron un 43,4% mayores a las tiendas con la estrategia A. La intuición se contradice con los datos, sin embargo, ¿Es esto debido al azar? ¿o bien es un resultado estadísticamente significativo? 9.8.1.1 Test de diferencia de media Una de las pruebas más comunes en estadística, la prueba t, se utiliza para determinar si las medias de dos grupos son iguales entre sí. La hipótesis de la prueba es que ambos grupos proceden de distribuciones normales con varianzas iguales. La hipótesis nula es que las dos medias son iguales, y la alternativa es que no lo son. Se sabe que bajo la hipótesis nula, podemos calcular un estadístico t que seguirá una distribución t. También existe una modificación ampliamente utilizada de la prueba t, conocida como prueba t de Welch, que ajusta el número de grados de libertad cuando se cree que las varianzas no son iguales entre sí. Este tutorial cubre los fundamentos de la realización de pruebas t en R. t.test() puede utilizarse para realizar pruebas t de una y dos muestras en vectores de datos. La función contiene una variedad de argumentos. La sintaxis es: # Datos data(&quot;midwest&quot;) names(midwest) head(midwest) skim(midwest) t.test(x, y = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), paired = FALSE, mu = 0, var.equal = FALSE, conf.level = 0.95) Aquí \\(x\\) es un vector numérico de valores de datos e \\(y\\) es un vector numérico opcional de valores de datos. Si se excluye \\(y\\), la función realiza una prueba t de una muestra sobre los datos contenidos en \\(x\\), si se incluye realiza una prueba t de dos muestras utilizando tanto \\(x\\) como \\(y\\). El argumento \\(mu\\) proporciona un número que indica el verdadero valor de la media (o la diferencia de medias si se realiza una prueba de dos muestras) bajo la hipótesis nula. Por defecto, la prueba realiza una prueba t de dos caras; sin embargo, puede realizar una hipótesis alternativa cambiando el argumento alternativo a “mayor” o “menor” dependiendo de si la hipótesis alternativa es que la media es mayor o menor que \\(mu\\), respectivamente. Por ejemplo: t.test(x, alternative = &quot;less&quot;, mu = 25) Realiza una prueba t de una muestra sobre los datos contenidos en \\(x\\) donde la hipótesis nula es que \\(mu = 25\\) y la alternativa es menor que 25. El argumento var.equal indica si se deben asumir o no varianzas iguales al realizar una prueba t de dos muestras. Por defecto se asume una varianza desigual. Por último, el argumento conf.level determina el nivel de confianza del intervalo de confianza. 9.8.1.2 t-test de una cola La prueba t de una muestra compara la media de una muestra con un valor conocido, cuando la varianza de la población es desconocida. Consideremos que queremos evaluar el porcentaje de adultos con estudios universitarios en el “midwest” y compararlo con un valor determinado. Por ejemplo, supongamos que la media nacional de adultos con estudios universitarios es del 32% (licenciatura o superior) y queremos ver si la media del “midwest” es significativamente diferente (en terminos estadísticos) de la media nacional. En concreto, queremos comprobar si la media del medio oeste es inferior a la media nacional. summary(midwest$percollege) # Gráficamente # En escala absoluta p1 &lt;- ggplot(midwest, aes(percollege)) + geom_histogram(fill = &quot;white&quot;, color = &quot;grey30&quot;) # En escala logarítmica p2 &lt;- ggplot(midwest, aes(percollege)) + geom_histogram(fill = &quot;white&quot;, color = &quot;grey30&quot;) + scale_x_log10() # La escala logarítmica es una transformación que sirve justificar el supuesto de normalidad. # La escala logarítmica hace que la distancia entre 10 y 100 sea la misma que entre 100 y 1000. grid.arrange(p1, p2, ncol = 2) # 1. Recordemos que queremos testear si el promedio del # &quot;midwest&quot; es menor que el promedio nacional. # 2. Hacemos tres test: t.test(midwest$percollege, mu = 32, alternative = &quot;less&quot;) El resultado nos indica que p &lt; 0.001 lo que nos dice que rechazamos la hipótesis nula (de que es igual a la nacional) lo que nos da evidencia estadística de que la media poblacional del “midwest” es menor que un 32%. Pero, como vimos anteriormente, tenemos problemas de no normalidad (recuerden el gráfico con las distribuciones empíricas). Para asegurarnos de que nuestra conclusión es correcta vamos a efectuar dos versiones alternativas del test: # La misma versión, pero en logaritmos... t.test(log(midwest$percollege), mu = log(32), alternative = &quot;less&quot;) # Wilcox test sirve cuando no deseamos asumir que los datos provienen de una distribución normal. wilcox.test(midwest$percollege, mu = 32, alternative = &quot;less&quot;) Ambos resultados apoyan nuestra conclusión inicial de que el porcentaje de adultos con estudios universitarios en el medio oeste es estadísticamente inferior a la media nacional. 9.8.1.3 t-test de dos colas Ahora digamos que queremos comparar las diferencias entre el porcentaje promedio de adultos con estudios universitarios en Ohio y en Michigan. En este caso, queremos realizar una prueba t de dos muestras. # Seleccionamos los datos df &lt;- midwest %&gt;% filter(state == &quot;OH&quot; | state == &quot;MI&quot;) %&gt;% select(state, percollege) # Estadisticas para ohio summary(df %&gt;% filter(state == &quot;OH&quot;) %&gt;% .$percollege) # Estadisticas para michigan summary(df %&gt;% filter(state == &quot;MI&quot;) %&gt;% .$percollege) Podemos ver que Ohio parece tener ligeramente menos adultos con estudios universitarios que Michigan. No obstante, el gráfico no nos dice si es estadísticamente significativo o no. ggplot(df, aes(state, percollege)) + geom_boxplot() p1 &lt;- ggplot(df, aes(percollege)) + geom_histogram(fill = &quot;white&quot;, color = &quot;grey30&quot;) + facet_wrap(~ state) # En logaritmos p2 &lt;- ggplot(df, aes(percollege)) + geom_histogram(fill = &quot;white&quot;, color = &quot;grey30&quot;) + facet_wrap(~ state) + scale_x_log10() grid.arrange(p1, p2, nrow = 2) # Realizamos los test # Sintaxis basica t.test(percollege ~ state, data = df) # Transformando los datos t.test(log(percollege) ~ state, data = df) # Sin asumir normalidad wilcox.test(percollege ~ state, data = df) Los resultados que aparecen a continuación muestran un valor \\(p &lt; 0.01\\) que apoya la hipótesis alternativa de que “la verdadera diferencia de medias no es igual a 0”; esencialmente afirma que existe una diferencia estadística entre las dos medias. 9.8.1.4 paired t - test Sirve para hacer test de diferencias de media para dos grupos que son estadisticamente iguales, pero que fueron sometidos a un tratamientom distinto. Lo que se quiere testear es si efectivamente la diferencia de promedios de alguna variable puede estar asociada a este tratamiento. Vamos a ocupar la base de datos de R llamada “sleep”. Imaginen queremos testear si una droga particular tiene un efecto estadisticamente significativo sobre las horas que se duerme. En particular, vamos a ver si la variable “extra” es diferente entre ambos grupos. Vamos a utilizar t.test() nuevamente, pero con la opción paired = TRUE. # Miramos los datos graficamente ggplot(sleep, aes(group, extra)) + geom_boxplot() # Testeamos t.test(extra ~ group, data = sleep, paired = TRUE) 9.8.2 Regresión Lineal en R 9.8.2.1 MCO con datos generados # Limpiar rm(list = ls()) # Fijar la semilla (Nos permite generar resultados reproducibles) set.seed(1234) # Genero número de observaciones N &lt;- 50 # Simular una variable x con una distribución uniforme runif(N,min,max) x &lt;- runif(N,1,40) x # Simular y rnorm(N,media,sd) y &lt;- 40 + 0.5*x + rnorm(N,0,2) y # Crear una base de datos (data.frame) con lo que yo genere. mco_data &lt;- data.frame(x,y) mco_data # Ahora, vamos a estimar por Minimos Cuadrados ordinarios # el modelo mco &lt;- lm(y ~ x, data = mco_data) mco summary(mco) # También lo podemos ver gráficamente ggplot(mco_data, aes(x,y))+ geom_point(with=.2, alpha=0.5) + geom_smooth(method = &quot;lm&quot;, se = TRUE) + labs(title = &quot;Diagrama de dispersión&quot;, subtitle = &quot;ScatterPlot&quot;, x = &quot;x&quot;, y = &quot;y&quot;) + theme_minimal() En resumen…. Para estimar: mco &lt;- lm(y ~ x1 + x2 + x3, data = misdatos) Para mirar resultados: summary(mco) Para mirar gráficamente: ggplot() + geom_point() + geom_smooth(method = \"lm\") 9.8.2.2 MCO con datos reales rm(list = ls()) data(&quot;CPS1985&quot;) names(CPS1985) # Correlación entre variables CPS1985 %&gt;% summarize(cor1 = cor(wage, education), cor2 = cor(wage, experience)) # Gráficamente ggplot(CPS1985, aes(education, log(wage))) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = T) + labs(title=&quot;Diagrama de dispersión&quot;, subtitle= &quot;ScatterPlot&quot;, caption=&quot;Fuente: CPS1985 (paquete AER)&quot;, x=&quot;Educación en años&quot;, y=&quot;Salario (en logaritmo)&quot;) # Estimación # Creamos variables que utilizaremos en las estimaciones # Crear dos variables: log(wage) y experiencia al cuadrado. CPS1985$lnwage &lt;- log(CPS1985$wage) CPS1985$exp2 &lt;- CPS1985$exp * CPS1985$exp # R no nos permite elevar variables al cuadrado dentro de la función # Generamos variables categóricas que queramos incluir en la regresión # Variables dicótomicas (sobre variable binarias). table(CPS1985$gender) CPS1985$sexo &lt;- ifelse(CPS1985$gender == &quot;male&quot;, 1, 0) table(CPS1985$union) CPS1985$sind &lt;- ifelse(CPS1985$union == &quot;yes&quot;, 1, 0) # Variables dicótomas (sobre variables categóricas) CPS1985 &lt;- cbind(CPS1985, dummy(factor(CPS1985$occupation), sep = &quot;_&quot;)) # Generando formas editables f0 &lt;- wage ~ education f1 &lt;- lnwage ~ education f2 &lt;- lnwage ~ education + experience f3 &lt;- lnwage ~ education + experience + exp2 f4 &lt;- lnwage ~ education + experience + exp2 + sexo f5 &lt;- lnwage ~ education + experience + exp2 + sexo + sind f6 &lt;- lnwage ~ education + experience + exp2 + sexo + sind + CPS1985_services # Estimaciones mco_0 &lt;- lm(f0, data=CPS1985, na.action = na.exclude) mco_1 &lt;- lm(f1, data=CPS1985, na.action = na.exclude) mco_2 &lt;- lm(f2, data=CPS1985, na.action = na.exclude) mco_3 &lt;- lm(f3, data=CPS1985, na.action = na.exclude) mco_4 &lt;- lm(f4, data=CPS1985, na.action = na.exclude) mco_5 &lt;- lm(f5, data=CPS1985, na.action = na.exclude) mco_6 &lt;- lm(f6, data=CPS1985, na.action = na.exclude) # Mirar los resultados de mis estimaciones summary(mco_0) summary(mco_1) summary(mco_2) summary(mco_3) summary(mco_4) summary(mco_5) summary(mco_6) # Exportar resultados # El paquete se llama &quot;stargazer&quot; setwd(here(&quot;resultados&quot;)) stargazer(mco_1, mco_2, mco_3, # Modelos type = &quot;text&quot;, title = &quot;Estimación MCO salarios vs. educ&quot;, style = &quot;qje&quot;, # Estilos notes = &quot;\\\\footnotezie ***p=.01; ** p =.05; *p=.1&quot;, notes.append = F, # Para remplazar notas puestas anteriormente. single.row = F, # Para que los errores estandar esten en la misma linea que los coeficientes no.space = T, # No se generen espacios entre los regresores. out = &quot;est1-3.doc&quot;, digits = 2) 9.8.3 R Markdown Herramienta para crear informes que sean automáticos, reproducibles y con datos que se puedan actualizar en el tiempo. Se pueden generar informes en formato word, pdf, html. R markdown (Rmd) es básicamente una intersección entre texto narrativo y código que genera visualizaciones y estimaciones con el fin de analizar datos. Rmd puede generar: texto plano, segmentos de co ́digo, gr ́aficos, tablas, tableros interactivos. R markdown es especialmente útil: Informes rutinarios: Por ejemplo, informe semanal sobre un conjunto de análisis que se actualiza en el tiempo. Informes de análisis para un subconjunto de datos: Por ejemplo, informes por país de una base de datos que contiene información de distintos países. 9.8.3.1 Conceptos Básicos Markdown es el lenguaje que permite escribir documentos en texto plano. Los archivos escritos en Markdown tienen la extensión .md. R Markdown es la variación especifica para R. Permite escribir texto plano con Markdown y adjuntar código proveniente de R. Los archivos escritos en R Markdown tienen la extensión .Rmd. knirt: Paquete de R. Sirve para leer los segmentos de código que queremos introducir en los reportes. Pandoc: Sirve para convertir el output en un archivo con formato word/pdf/html. Es un software que viene instalado automáticamente en 9.8.3.2 Proceso Fuente: https://rmarkdown.rstudio.com/authoring quick tour.html 9.8.3.3 Primer archivo en R Markdown Opciones para elegir el tipo de documento y si quiero confeccionar un documento/presentación/un tablero u otros tipos de archivo más detallados. Opciones para cambiar título y autor(a). 9.8.3.4 Directorio de trabajo El directorio de trabajo de un archivo .Rmd será donde esta guardado el archivo con esa extensión. De este modo, R buscará los archivos en la carpeta en donde este guardado el archivo .Rmd. Para este ejercicio simplemente dejaremos los datos que utilizaremos junto al archivo. 9.8.3.5 Componentes de R Markdown YAML: fijar título, fecha y tipo de output. Markdown text: introducir texto. Code Chunk: cargar paquetes, datos, visualizaciones. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
