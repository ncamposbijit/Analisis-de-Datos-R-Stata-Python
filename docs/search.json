[{"path":"index.html","id":"section","chapter":"\n1 \n\n","heading":"\n1 \n\n","text":"","code":""},{"path":"index.html","id":"programación-defensiva","chapter":"\n1 \n\n","heading":"\n1.0.1 Programación defensiva\n\n","text":"","code":""},{"path":"index.html","id":"practica-i-escribir-una-prueba-o-test","chapter":"\n1 \n\n","heading":"\n1.0.2 Practica I: escribir una prueba o test\n\n","text":"","code":""},{"path":"index.html","id":"el-comando-assert","chapter":"\n1 \n\n","heading":"\n1.0.2.1 El comando |assert|\n\n","text":"","code":""},{"path":"index.html","id":"practica-ii-no-duplicar-información","chapter":"\n1 \n\n","heading":"\n1.0.3 Practica II: no duplicar información\n\n","text":"","code":""},{"path":"index.html","id":"practica-iii-nunca-transcribir-directamente-información-todo-automatizado","chapter":"\n1 \n\n","heading":"\n1.0.3.1 Practica III: nunca transcribir directamente información, todo automatizado\n\n","text":"","code":""},{"path":"index.html","id":"practica-iv-estilo-importa","chapter":"\n1 \n\n","heading":"\n1.0.4 Practica IV: estilo importa\n\n","text":"","code":""},{"path":"index.html","id":"estilo-de-codificación-en-stata","chapter":"\n1 \n\n","heading":"\n1.1 Estilo de codificación en Stata\n\n","text":"","code":""},{"path":"index.html","id":"ser-ordenado-no-es-condición-suficiente-para-tener-un-buen-estilo-de-codificación","chapter":"\n1 \n\n","heading":"\n1.1.1 Ser ordenado no es condición suficiente para tener un buen estilo de codificación\n\n","text":"","code":""},{"path":"index.html","id":"escribe-códigos-cortos","chapter":"\n1 \n\n","heading":"\n1.1.2 Escribe códigos cortos\n\n","text":"","code":""},{"path":"index.html","id":"el-nombre-de-las-variables-debe-caracterizarla","chapter":"\n1 \n\n","heading":"\n1.1.3 El nombre de las variables debe caracterizarla\n\n","text":"","code":""},{"path":"index.html","id":"ten-especial-cuidado-cuando-uses-algebra-en-los-códigos.","chapter":"\n1 \n\n","heading":"\n1.1.4 Ten especial cuidado cuando uses algebra en los códigos.\n\n","text":"","code":""},{"path":"index.html","id":"ser-consistentes","chapter":"\n1 \n\n","heading":"\n1.1.5 Ser consistentes\n\n","text":"","code":""},{"path":"index.html","id":"chequear-errores","chapter":"\n1 \n\n","heading":"\n1.1.6 Chequear errores\n\n","text":"","code":""},{"path":"index.html","id":"separar-tipos-de-códigos","chapter":"\n1 \n\n","heading":"\n1.1.7 Separar tipos de códigos\n\n","text":"","code":""},{"path":"index.html","id":"automatizar-todo-lo-que-se-pueda-automatizar","chapter":"\n1 \n\n","heading":"\n1.1.8 Automatizar todo lo que se pueda automatizar\n\n","text":"","code":""},{"path":"index.html","id":"abstraer-para-eliminar-pasos-redundantes.","chapter":"\n1 \n\n","heading":"\n1.1.9 Abstraer para eliminar pasos redundantes.\n\n","text":"","code":""},{"path":"index.html","id":"crear-códigos-autoexplicativos-que-complementen-a-la-documentación","chapter":"\n1 \n\n","heading":"\n1.1.10 Crear códigos autoexplicativos que complementen a la documentación\n\n","text":"","code":""},{"path":"index.html","id":"pensar-siempre-en-la-unidad-de-observación-e-identificar-los-id-que-identifican-unicamente-a-la-unidad-de-observación","chapter":"\n1 \n\n","heading":"\n1.1.10.1 Pensar siempre en la unidad de observación e identificar los id que identifican unicamente a la unidad de observación\n\n","text":"","code":""},{"path":"index.html","id":"no-ser-repetitivo","chapter":"\n1 \n\n","heading":"\n1.1.11 No ser repetitivo\n\n","text":"","code":""},{"path":"index.html","id":"mensajes-finales-sobre-estilo-de-codificación","chapter":"\n1 \n\n","heading":"\n1.1.12 Mensajes finales sobre estilo de codificación\n\n","text":"","code":""},{"path":"index.html","id":"organización-de-un-proyecto-en-stata","chapter":"\n1 \n\n","heading":"\n1.2 Organización de un proyecto en Stata\n\n","text":"","code":""},{"path":"index.html","id":"flujo-de-trabajo-en-stata","chapter":"\n1 \n\n","heading":"\n1.2.1 Flujo de trabajo en Stata\n\n","text":"","code":""},{"path":"index.html","id":"organización-de-archivos-y-carpetas","chapter":"\n1 \n\n","heading":"\n1.2.2 Organización de archivos y carpetas\n\n","text":"","code":""},{"path":"index.html","id":"carpetas","chapter":"\n1 \n\n","heading":"\n1.2.2.1 Carpetas\n\n","text":"","code":""},{"path":"index.html","id":"master-do-files","chapter":"\n1 \n\n","heading":"\n1.2.3 Master do-files\n\n","text":"","code":""},{"path":"index.html","id":"rutas-de-las-carpetas","chapter":"\n1 \n\n","heading":"\n1.2.4 Rutas de las carpetas:\n\n","text":"","code":""},{"path":"index.html","id":"utilizar-algún-auxiliar-de-generación-de-código-sublime-text","chapter":"\n1 \n\n","heading":"\n1.2.5 Utilizar algún auxiliar de generación de código: Sublime Text\n\n","text":"","code":""},{"path":"index.html","id":"qué-es-sublime","chapter":"\n1 \n\n","heading":"\n1.2.5.1 ¿Qué es Sublime?\n\n","text":"","code":""},{"path":"index.html","id":"ejecutar-stata-desde-sublime","chapter":"\n1 \n\n","heading":"\n1.2.5.2 Ejecutar Stata desde Sublime\n\n","text":"","code":""},{"path":"id_01-chap02.html","id":"id_01-chap02","chapter":"2 Programar do - files","heading":"2 Programar do - files","text":"","code":""},{"path":"id_01-chap02.html","id":"programar-en-stata","chapter":"2 Programar do - files","heading":"2.1 Programar en Stata","text":"Esta sección esta basada en: introduction Stata Programing (), en notas de clase de la profesora Erin Hengel, en Advanced Stata Topics del profesor Alexander C. Lembcke, en Seventy-six Stata tips , en Top 10 ``gotchas” de y en Data Management Using Stata. Practical Handbook de .","code":""},{"path":"id_01-chap02.html","id":"qué-es-programar-en-stata","chapter":"2 Programar do - files","heading":"2.1.0.1 ¿Qué es programar en Stata?","text":"Programar en Stata es:Escribir - files: una secuencia de comandos ejecutables través de un archivo ..Escribir los que formalmente en Stata es un programa: un conjunto de comandos que incluyen el comando program. Un programa en Stata se guarda como un ado - file.Escribir lenguaje de programación matricial: denominado mata.","code":""},{"path":"id_01-chap02.html","id":"do---files","chapter":"2 Programar do - files","heading":"2.1.0.2 do - files","text":"El uso de -file garantiza la replicabilidad del análisis de datos utilizando Stata. Recordar que un -file puede llamar otros (ej. master .file. La jerarquización de los -file puede ser importante para proyectos grandes o complejos. Es importante evitar trabajar en Stata interactivamente. Únicamente hay que hacerlo para inspeccionar datos (recordar practicas Sección 3.1).","code":""},{"path":"id_01-chap02.html","id":"ado---files","chapter":"2 Programar do - files","heading":"2.1.0.3 ado - files","text":"Sirven para crear tus propios en Stata. Una vez que armes tu programa y lo guardes en la carpeta de ado-file puedes utilizarlo como cualquier otro comando de Stata. Por ejemplo, puedes agregar las opciones , range y otras. También puedes escribir un documento de ayuda (help} que explique el programa. Crear tus propios comandos es una muy buena forma de ser más eficiente al trabajar con Stata (recordar Sección 1).Mata para ado-files: Los ado-files pueden realizar tareas más complejas que involucren ejecutar el comando múltiples veces.El lenguaje de programación mata es mucho más rápido que un ado-file.Útil para realizar tareas que sean intensivas computacionalmente.es solo un lenguajes de programación que utiliza matrices. También sirve para tareas que involucran texto o listas.Próximante discutiremos este lenguaje.","code":""},{"path":"id_01-chap02.html","id":"comandos-y-funciones-claves-y-algunos-detalles","chapter":"2 Programar do - files","heading":"2.1.1 Comandos y funciones claves (y algunos detalles)}","text":"Revisaremos algunos aspectos necesarios para mejorar la eficiencia al trabajar en Stata.Directorios y uso de profile..Tipos de comandos.Tipos de datos y uso de compress.Uso de capture, preserve y restore.","code":""},{"path":"id_01-chap02.html","id":"directorios","chapter":"2 Programar do - files","heading":"2.1.1.1 Directorios","text":"cd y pwd sirven para fijar y conocer mis directorios. Es importante utilizar dobles comillas en los casos en que la ruta del directorio tenga espacios.El comando sysdir provee una lista de los directorios importantes para Stata.El comando update sirve para actualizar los comandos.La carpeta PLUS es el directorio donde se guardan comandos descargados. Si utilizas ssc, se guardara en esta carpeta.PERSONAL para tus propios ado-files.","code":""},{"path":"id_01-chap02.html","id":"profile-do","chapter":"2 Programar do - files","heading":"2.1.1.2 Profile do","text":"¿Qué es profile ?Una opción tan conocida es la utilización de profile.doEste archivo se ejecuta cada vez que Stata se inicia. Si tienes uno guardado pasa nada.Este -file permite, por ejemplo, fijar tu directorio de trabajo inicial, cambiar características de los gráficos, generar atajos.También se puede hacer que Stata abra un log-file y lo guarde siempre en el mismo lugar.Cada vez que inicies sesión Stata ira por profile.. Si lo encuentra lo va ejecutar.Stata recomienda guardar profile.en home directory (ver help profilew para ususarios de windows y profilem para Mac).Vamos ver como modificar profile.. En particular:\nAjustes generales.\nEstablecer las características de los gráficos.\nEstablecer atajos\nAbrir, cerrar y guardar un log-file con la fecha.\nAjustes generales.Establecer las características de los gráficos.Establecer atajosAbrir, cerrar y guardar un log-file con la fecha.Ejercicio 3.3.1:InstruccionesDescargar carpeta ejercicios-clase3.Abrir profile.e inspeccionar cada una de las lineas del código.PreguntasAgrega dos elementos alguna sección.Modifica las carpetas según las características de tu computadora.Guarda profile.en el home directory. Ejecutar nuevamente Stata y verificar que el código se ha ejecutado.","code":""},{"path":"id_01-chap02.html","id":"tipos-de-comandos","chapter":"2 Programar do - files","heading":"2.1.1.3 Tipos de comandos","text":"Comandos r-class y e-classMuchos comandos en Stata (ej. summarize, correlate, regress) hacen que sea posible utilizar sus resultados una vez que estos son ejecutados.summarize y correlate son comandos r-class. Es decir, comandos que guardan sus resultados en r().regress es un comando e-class. Es decir, comandos que guardan sus resultados en e().return list retorna los valores guardados en r()Su limitación es que solo están disponibles los valores del último comando r-class ejecutado.Los comandos e-class son comandos de estimación. Para verlos ereturn list y para llamarlos individualmente e(nombre). Guardan más información que los de r-class : matrices, vectores y funciones. La información de los comandos e-class sigue estando disponible una vez que utilizamos algún comando r-class`. Aquí una diferencia!Un ejemplo clásico son las estimaciones de regresión lineal.En general cualquier comando de estimación se guarda en esta clase de formato.","code":"* r-class\nsysuse auto.dta, clear \nsummarize mpg \nreturn list \n\nsummarize mpg, detail \nreturn list \ndisplay \"La asimetría de mpg es\" r(skewness) \nsummarize price, detail \nreturn list regress mpg weight length rep78 \ndisplay \"La regresión se estimo para \" e(N) \" observaciones.\" \nereturn list"},{"path":"id_01-chap02.html","id":"tipos-de-datos","chapter":"2 Programar do - files","heading":"2.1.1.4 Tipos de datos","text":"numeric y stringLa mayor distinción entre tipos de datos es entre numeric y string.Al trabajar con datos muchas veces es necesario realizar conversiones entre estos dos formatos. Los comandos destring, tostring y encode son útiles para estas tareas (si alguien tiene dudas con alguno escriben).strings pueden soportar un máximo de 244 caracteres con un byte por cada carácter.Por ejemplo, una variable del tipo str20 requiere 20 bytes por observación.Clasificación de los datosPara las variables en formato numeric los tipos de datos son: byte, int, long, float y double.Los tres primeros solo puede almacenar valores enteros. long puede almacenar todos los números de 9 dígitos, pero es limitado para 10 dígitos.float y double pueden almacenar números grandes.asumir que float será aritméticamente exacto. Por ejemplo:¿Como utilizar esto para programar efectivamente en Stata?IDs con muchos dígitos (y caracteres) guardarlos como string. como integers, float o doubles.confiar en test exactos contra una constante con datos en formato float. Utilizar formato double para cualquier serie que necesita ser precisa (ej. suma de los residuos de una regresión).Utiliza integers cuando sea apropiado (ej. variables dicotómicas). Guardar valores como int o byte ayuda utilizar de forma más eficiente el espacio en el disco.compress examina cada variable y determina si estas pueden ser guardadas de forma más eficiente. Utilizarlo.Si se especifican valores iniciales fuera de los rangos permitidos para cada tipo de dato el resultado será un missing.Notar que se genera ninguna alerta de missing cuando este se crea. Esto es distinto para variables que ya existen.Si el valor esta fuera de los rangos permitidos, la variable se guarda en un formato mayor. byte int, int long, float double. En este caso un mensaje aparece.","code":"    display float(16777216)\n        16777216\n    display float(16777217)\n        16777216clear all \nset obs 10 \ngenerate byte var1 = 101\nsummarize var1 clear all\nset obs 10\ngenerate byte var1 = 1\nreplace var1 = 101"},{"path":"id_01-chap02.html","id":"capture-preserve-y-restore","chapter":"2 Programar do - files","heading":"2.1.1.5 capture, preserve y restore","text":"Manejando errores: captureSirve para evitar que Stata aborte cuando detecta un error.Bueno para cuando quieres borrar algo que ya se encuentra. Lo malo es que suprime todos los errores y oculta todo lo que puede ir mal.Código crea var1 y var2, luego las elimina incluyendo una variable que existe. Al contrario de lo que se intuye, se borran las variables, dado que hay una variable que existe. La recomendación es siempre utilizar capture con moderación. Puede ser utilizado en bloque para así tener que utilizarlo en cada linea.preserve y restoreAlgunos comandos en Stata remplazan la base actual por una nueva (ej. collapse o contract).Utilizar preserve y restore es útil en estos casos.En caso de que queramos obtener estadística descriptiva agregada y asociarla observaciones podemos utilizar estos comandos.","code":"set obs 10\ngenerate byte var1 = 5\ngenerate byte var2 = 10 \ncapture drop var1 var2 var3 \ndescribe var1 var2sysuse auto.dta, clear\ncapture{\nreg price mpg-trunk\nreg price mpg-weight\nreg price mpg-foreign\n}\nereturn list * Ejemplo con collapse\nsysuse auto.dta, clear\ngenerate lprice = log(price)\npreserve\ncollapse (max) max_lprice=lprice max_mpg=mpg ///\n(iqr) iqr_lprince = lprice iqr_mpg = mpg if !missing(rep78), by(rep78) \nsort rep78 \ntempfile repstats\nsave `repstats'\nrestore \nsort rep78 \nmerge m:1 rep78 using `repstats'\nassert _merge != 2\nsummarize lprice max_lprice max_mpg \n\n* Ejemplo con contract\nsysuse auto.dta, clear\npreserve\ncontract mpg, cfreq(cumfreq) percent(percentage) cpercent(cumpercent)\nsort mpg \ntempfile mpgfreq\nsave `mpgfreq'\nrestore \nsort mpg\nmerge m:1 mpg using `mpgfreq'\nassert _merge != 2\nsummarize _freq cumfreq percentage cumpercent"},{"path":"id_01-chap02.html","id":"missing-values","chapter":"2 Programar do - files","heading":"2.1.1.6 Missing values","text":"En general uno utiliza variable !=. para evitar incluir missings.Mejor practica es variable <. para excluir todos los valores en missing. Otra opción es !missing(variable).Los missing están codificados internamente como valores mayores cualquier número. El menor valor de todos los missing es el punto. Al utilizar variable <. es como decir, solo utiliza los números.Como vimos en la Sección 3.1 hay muchos problemas al tratar bien los missings. Este problema se incrementa al momento de trabajar con bases de datos imposibles de inspeccionar. Veremos algunas recomendaciones que pueden ser utiles para evitar estos errores.Los comandos tratan distinto los missingsCualquier función de datos missing será missing.Cuando se calcula un promedio o una desviación estándar solo valores missing son considerados (ej. sum).Algunos comandos en Stata manejan los missing de otras formas. Por ejemplo, las funciones max, min y las funciones para filas de egen: rowmax(), rowmean(), rowmin(), rowsd() y rowtotal() ignoran los missing. Por ejemplo, rowmean(x1,x2,x3) calcula el promedio de las variables y solo retornara missing si todas lo son.Por ejemplo, rowmean(x1,x2,x3) calcula el promedio de las variables y solo retornara missing si todas lo son.collapse (sum) trata los missing como ceros.Calcular un promedio con missingsPromedioSinMissing da como resultado missings. PromedioConMissing considera missings. Es importante tratar de entender como funcionan los missings de los comandos que utilizas.Generar variables considerando missings: Al crear una variable dicotómica, gen y gen byte tratan los missing de formas distintas.sum() considera missing como ceros.max() trata los missing como si estuviesen allí.tabmiss, mvdecode y mvencodeEn ocasiones los missing difieren en notación (ej. al importar datos de otro paquete). Siempre que trabajes con una base de datos nueva es importante recodificar. Notar que todos tienen que ir un punto. Todo depende del origen de los missing.mvdecode y mvencode pueden ser útiles en este tipo de casos.mvdecode permite recodificar valores numéricos como missing. Útil cuando valores son representados como -99, -999.mvencode hace lo inverso. Mapea missing como numéricos.El comando tabmiss inspecciona todas las variables de una base de datos y reporta los missing totales y como fracción del total de observaciones.Ejercicio 3.3.2: InstruccionesAbrir ejercicios-clase3.y ejecute las líneas correspondientes al ejercicio 2.PreguntaCompare las opciones 1, 2 y 3 en relación como tratan los missing. Explique las diferencias entre cada una de estas opciones.generate, replace y missing: Sólo unas pequeños detalles.Opción 1 es la típica. Ojo esta considerando missing. Es necesario agregar & !missing(pop). Escrito de esta forma, si pop es missing, smallpop será cero. Opción 2 es más simple, pero si cualquier valor de pop es missing será evaluado como un cero también. La razón es que los missing en Stata son considerados como números muy grandes para el programa. La Opción 3 soluciona el problema.","code":"clear\nset obs 10 \ngen var1 = rnormal()\ngen var2 = 5 \ngen var3 = .\ngen promedio = (var1 + var2 + var3)/3\negen promedio1 = rowmean(var1 var2 var3)    local N = 500\n    set obs `N'\n    gen indicador = uniform() < .5\n    replace indicador =. if mod(_n, 2) == 0\n    \n    * Si indicador es missing, variable sera missing\n    gen variable_primercaso = 1 if indicador == 1 \n    replace variable_primercaso = 0 if indicador == 0\n    * Si indicador es missing, variable sera cero. \n    gen variable_segundocaso=(indicador==1)\n    \\end{minted}\n    \\end{itemize}    clear all \n    set obs 4 \n    generate byte var1 = cond(mod(_n,2)==1, 1, .)\n    generate byte var1sum = sum(var1)\n    list, noobs  display max(-5,.)clear all \nset seed 1234\nlocal N = 50\nset obs `N'\ngen income = abs(int(rnormal(0,5)))\nassert income >= 0\nreplace income =. if mod(_n, 2) == 0\n* Para ver los missing en variables\ntabmiss \n\n* Transformar un valor númerico a missing\nmvdecode income, mv(2)\n\n* Transformar varios valores númericos a missing.\nmvdecode income, mv(2 5)\n\n* Transformar varios a missing, pero identific ́andolos.\nmvdecode income, mv(3 = .a \\ 6 = .b)\n\n* Missings se cambian de vuelta a su valor original.\nmvencode income, mv(.a = 3 \\ .b = 6)clear \ncd \"$ejercicio3\"\nuse census2c, clear\n\n* Opción 1\ngen smallpop_o1 = 1 if pop<=5000\nreplace smallpop_o1 = 0 if pop>5000\n\n* Opción 2\ngen smallpop_o2 = (pop <= 5000) \n\n* Opcion 3\ngen smallpop_o3 = (pop <= 5000) if !missing(pop)"},{"path":"id_01-chap02.html","id":"string-a-numeric-y-al-revés","chapter":"2 Programar do - files","heading":"2.1.1.7 String a numeric y al revés","text":"De string numericSi las variables han sido mal clasificadas como string puedes utilizar la función real().Por ejemplo: generate idpaciente = real(pacienteid).El comando anterior genera missing para todas las observaciones que puedan ser interpretadas como numéricas.Mucho mejor es utilizar destring, replace.Otro caso usual es que tenemos datos en formato string y queremos que tengan un equivalente. El comando encode. es aconsejable utilizar este comando para valores numéricos guardados como string.De numeric stringHay veces en las que se quiere generar un equivalente string valores numéricos.Tres comandos: string() , tostring() y decode().Un ejemplo es querer mantener los 0 que estan al inicio de un código ID.El comando tostring zip, format(\\%05.0f) generate(idstring) genera un string de cinco digitos con los ceros al inicio.decode() sirve para un caso en que tengas un id en numérico, pero que la tengas en string.Strings entre comillas: importa poner las comillas bien.","code":"display \"Este es un string normal\"\n\ndisplay \"Este no es un string con \"comillas\" \"\n\ndisplay `\"Este si es un string con \"comillas\"\"'"},{"path":"id_01-chap02.html","id":"funciones-para-generar-variables","chapter":"2 Programar do - files","heading":"2.1.1.8 Funciones para generar variables","text":"generate y replace: función cond()Si quiero que un resultado sea “” si una condición es verdadera y “b” si es falsa.La función cond(x,,b) posee esta capacidad sin la necesidad de utilizar .Las observaciones en Stata estás numeradas desde el 1. _N es el mayor número de observaciones, mientras que el actual es _n. sort (ascendete) y gsort (ascendete o descendente) alteran el orden de las observaciones. Como recomendación eviten generar variables o condiciones que dependan de la posición especifica de una observación.recode para variables discretas: recode crea una nueva variable basada en otra variable.El signo (\\(=\\)) es para indicar valor antiguo valor nuevo. es necesario aplicarlo linea por linea. recode produce un código más eficiente.recode para variables continuas: recode(x,x1,x2,x3,x4,xn) para variables continuas de forma tal de generar intervalos tal que \\(x \\leq x_1 ; x_1 \\leq x \\leq x_2\\) y así sucesivamente. Los resultados son iguales los límites creados.Otros comandos que cumplen una función parecida son floor y ceil(). Ambos sirven para generar un valor entero. El primero para redondear hacia abajo y el otro hacia arriba. floor(x) retorna el entero \\(n\\) tal que \\(n \\leq x < n + 1\\) mientras que ceil(x) es tal que \\(n - 1 < x \\leq n\\).irecode para variables continuas: irecode(x,x1,x2,x3,x4,x_n) es una alternativa para categorizar grupos también. Por ejemplo:Categorizara cada grupo según el intervalo en el que este. Parte del cero!. $x x_1 $, \\(x_1 \\leq x \\leq x_2 \\rightarrow 1\\) y así sucesivamente.Crear cuartiles con xtile: Con xtile podemos querer clasificar las variables según cuantiles (quintiles, deciles, cuartiles, etc).Ejercicio 3.3.3:InstruccionesCargar la base auto.dtaPreguntaGenere una variable que sirva para redondear hacia abajo la variable mpg en en múltiplos de 5, de modo que cualquier valor de 10 14 se redondee 10, cualquier valor de 15 19 15 y así sucesivamente.","code":"* Útil para construir una tabla\ngenerate netmarr2x = cond(marr/divr > 2, 1, 2) \nlabel define netmarr2xc 1 \"marr > 2 divr\" 2 \"marr <= 2 divr\" \nlabel values netmarr2x netmarr2xc \ntabstat pop medage, by(netmarr2x) * Ejemplo 1: uso de gsort\ngsort region -pop \nby region: generate totpop = sum(pop)\n\n* Ejemplo 2: uso de _n y _N\nby region: list region totpop if _n == _N\n\n* Ejemplo 3: sort\ngenerate largepop = 0\nreplace largepop = 1 if pop > 5000 & !missing(pop)\ngen smallpop = (pop <= 5000) if !missing(pop)\ngenerate popsize = smallpop + 2*largepop\nlabel variable popsize \"Population size code\"\nlabel define popsize 1  \"<= 5 million\" 2 \"> 5 million\", modify\nlabel values popsize popsize \nbysort region popsize: egen meanpop2 = mean(pop) * Esta no es una buena opción \nreplace newcode = 5 if oldcode == 2 \nreplace newcode = 8 if oldcode == 3 \nreplace newcode = 12 if inlist(oldcode, 5, 6, 7)\n* Esta si es una buena opción \nrecode oldcode (2 = 5) (3 = 8) (5/7 = 12), gen(newcode) use census2c, clear\ngenerate breaks = recode(medage, 29, 30, 31, 32, 33)use census2c, clear\ngenerate popurbfloor = floor(popurb)\ngenerate popurbceil = ceil(popurb)generate size = irecode(pop, 1000, 4000, 8000, 20000)\nlabel define popsize 0 \"<1m\" 1 \"1-4m\" 2 \"4-8m\" 3 \">8m\" \nlabel values size popsize\ntabstat pop, stat(mean min max) by(size)* Creamos cuartiles para población \nxtile popcuart = pop , nq(4)\ntabstat pop, stat(n mean min max) by(popcuart)"},{"path":"id_01-chap02.html","id":"funciones-de-egen","chapter":"2 Programar do - files","heading":"2.1.1.9 Funciones de egen","text":"Todos y todas conocemos algunas de las típicas funciones de egen.Otras funciones son iqr(), kurt(),mad(), mdev(), median(), mode(), pc(), pctile(), rank(), sd(), skwe(), std().egenmoreMenos conocida es la colección de funciones adicionales de egen hechas por Nicholas J. Cox.Estas funciones están contenidas en el comando egenmore.bom() y eom() crean variables de fechas que corresponde al primer y último día de un mes determinado.corr() calcula correlaciones y covarianzas mientras que var() calcular la varianza.semean() calcula la desviación estándar del error de una media.record() permite calcular el valor más alto o más bajo de una serie.rall() y rany() son útiles para el análisis de datos. Evaluan una condición y genera un indicador si todas o alguna observación la cumple.","code":"clear \n* Ejemplo del uso de egen\ngenerate size = irecode(pop, 1000, 4000, 8000, 20000)\nlabel define popsize 0 \"<1m\" 1 \"1-4m\" 2 \"4-8m\" 3 \">8m\" \nlabel values size popsize\nbysort size: egen avgpop = mean(pop)\ngenerate popratio = 100 * pop / avgpop \nformat popratio \\%7.2f\nlist state pop avgpop popratio if size == 0 * Ejemplo 1: Generar variable que tenga la primera palabra de una frase (wordof)\negen firstword = wordof(make),  word(1)\nlist firstword make in  1/15\n\n* Ejemplo 2: Para generar automáticamente valores extremos (outside, 1,5 RIQ)\negen    extrmpg = outside(mpg)\ntab extrmpg,    missingset obs 12\ngen a = 1 in 1 \ngen b = 2 in 2/4\ngen c  = -3  in 5/7\ngen d = 4  in 8/10\ngen e = . in 11/12\negen any = rany(a b c d e) , c(@ > 0 & !missing(@))\negen all = rall(a b c d e) , c(@ > 0 & !missing(@)) "},{"path":"id_01-chap02.html","id":"macros-locales-y-globales","chapter":"2 Programar do - files","heading":"2.1.2 Macros locales y globales","text":"","code":""},{"path":"id_01-chap02.html","id":"nombrar-macros","chapter":"2 Programar do - files","heading":"2.1.2.1 Nombrar macros","text":"Una macro es un contenedor que puede almacenar números o nombres de variables.Puede ser local o global. La primera es temporal, la segunda .Un ejemplo de variable local es:El primer comando define la macro y sus valores. Para llamar la macro hay que utilizar las comillas (“). local nombre texto \\(\\rightarrow\\) local nombre = text \\(\\rightarrow\\) local nombre = \"text\".Ojo con las rutas:En ocasiones voy querer utilizar una macro dentro de la ruta de una carpeta.Es importante utilizar siempre / o bien \\\\. De otra forma lo reconocerá.Ejercicio 3.3.4: Compare los siguientes comandos y comente las diferencias:display \"Dos mas dos = 2 + 2\"display \"Dos mas dos= 2 + 2’“`Ojo con el signo igual: Algunas veces es bueno colocar un signo \\(=\\) al definir macros. Por ejemplo, cuando redefinimos variables.La primera parte sirve para definir la macro mientras que la segunda sirve para dar cuenta de su valor actual. Al actualizar, ocupen igual.Sin signo igual: En algunas ocasiones queremos escribir una macro dentro de un loop. En estos casos es conveniente evitar el signo igual.El local nuevalista define una macro como string que posee su propio contenido, el valor de contar y el valor del iterador.","code":"local NivelEstres Nada Medio Moderado Severo \ndisplay \"Los niveles de estrés son: `NivelEstres'\"local filename base.dta \nuse \"H:\\ECStata\\`filename'\"\nr(601);\n* Para corregir el error, dos caminos: \nuse \"H:\\ECStata\\\\`filename'\" \nuse \"H:/ECStata/`filename'\"local contador 0\nlocal NivelEstres Nada Medio Moderado Severo \nforeach a of local NivelEstres { \nlocal contador = `contador' + 1 \ndisplay \"Nivel de Estres `contador' : `a'\"\n}local contador 0\nlocal NivelEstres Nada Medio Moderado Severo \nforeach a of local NivelEstres {\nlocal contador = `contador' + 1 \nlocal nuevalista `nuevalista' `contador' 'a' \n} \ndisplay \"`nuevalista'\" "},{"path":"id_01-chap02.html","id":"generar-variables-contadores-y-condiciones-con-macros","chapter":"2 Programar do - files","heading":"2.1.2.2 Generar variables, contadores y condiciones con macros","text":"Podemos utilizar macros para renombrar variables.En este fragmento de código, Stata evalúa la expresión 1960 + '' antes de evaluar la macro externa. Por ejemplo, cuando pase por el iterador = 11, el nuevo nombre de la variable será x1971.Resumir condiciones: Podemos utilizar macros para resumir condiciones. Esto es útil para estimar modelos o generar estadística descriptiva.Agrupar en base una condición: Imaginemos que ahora queremos estimar una regresión para todas las compañías de auto que empiezan empiezan con B.La mejor manera de pensar en esto es hacer lo que hace Stata: reemplazar \"ctyname\" por su contenido substr(país,1,1)==\"ctyname’“. Al hacerlo, se convierte ensubstr(país,1,1)==”B”. Si se omiten las comillas dobles, se obtienesubstr(country,1,1)==B`, lo que da lugar un error.Contadores: Las macros también pueden ser útiles para contadores.Muy útil para hacer gráficos o guardar datos en matrices.Utilizar una macro para estimar regresiones: Imaginen que desean estimar regresiones sobre un conjunto de variables donde una parte de este conjunto esta fija y la otra parte es variable.¿Qué ocurre si queremos hacerlo por separado? ¿Uno uno?Lo que ocurre cuando hacemos referencia una macro es que su valor se introduce en ese punto. El uso de una barra invertida en su lugar hace que se introduzca la referencia de la macro, es decir, se sustituirá el valor de add_var sino el término `add_var'. Así que cada vez que llamamos la local rhs el valor actual de el local add_var es sustituido.Ejercicio 3.3.5: InstruccionesCargar la base auto.dtaPreguntasDefina una variable macro llamada control que contenga mpg, rep78 y headroom. Estime una regresión entre price y control para vehículos extranjeros y de nuevo para vehículos domésticos.Defina una variable macro llamada control que contenga mpg, rep78 y headroom. Estime una regresión entre price y control para vehículos extranjeros y de nuevo para vehículos domésticos.Ejecute summarize mpg junto con return list. Defina dos macros display local mean1 r(mean) y local mean2 = r(mean). ¿Son iguales?Ejecute summarize mpg junto con return list. Defina dos macros display local mean1 r(mean) y local mean2 = r(mean). ¿Son iguales?","code":"clear \nforvalues a = 10/20{ \ngen v`a' = rnormal()\n}\n\n* Renombramos variables \nforvalues i = 11/15 { \nrename v`i' x`=1960 + `i''\n}clear \nsysuse auto \nlocal cond \"if foreign==0\"\nlocal varlist \"mpg rep78 trunk weight turn\"\n\n* Estimar la regresión considerando la condicíòn\nreg price `varlist' `cond'local autoname B \nreg price mpg weight if substr(make,1,1)==\"`autoname'\"* Para adelante\nlocal i = 1 \nlocal ++i \ndi `i' \n\n* Para atrás\nlocal i = 1\nlocal --i \ndi `i' local rhs mpg weight\nreg price `rhs' if foreign == 0 \nlocal rhs \"`rhs' headroom trunk\"  \nreg price `rhs' if foreign == 0 local rhs \"mpg weight \\`add_var'\" \nlocal add_var \"headroom\"\nreg price `rhs' if foreign == 0  \nlocal add_var \"trunk\" \nreg price `rhs' if foreign == 0  \nlocal add_var \"turn\" \nreg price `rhs' if foreign == 0  "},{"path":"id_01-chap02.html","id":"globales","chapter":"2 Programar do - files","heading":"2.1.2.3 Globales","text":"Crear macros globalesSe crean con el comando global.Útiles para fijar directorios o programas.En otros casos es mejor utilizar local.","code":"* Para generarla \nglobal variable\n\n* En caso de querer llamarla\ndisplay $variable"},{"path":"id_01-chap02.html","id":"funciones-extendidas-de-macros","chapter":"2 Programar do - files","heading":"2.1.2.4 Funciones extendidas de macros","text":"¿Qué son las funciones de macros extendidas?Stata también define ciertas macro. Estas se denominan extended macro functions o macros extendidas.En algunos casos contienen información sobre tu sistema operativo, sobre la última estimación que se realizo o sobre la base de datos.El help extended_fcn y la documentación que la acompaña proporcionan una descripción completa de la sintaxis de cada función de macro extendida (hay muchas). Muchas tienen ligeras variaciones de sintaxis entre ellas (por ejemplo, algunas requieren que las macros estén entre comillas dobles; otras lo permiten).Mirar labelsvariable label recupera el nombre asignado una variable.Contar dentro de un localword count y word como funciones de extensión que operan sobre strings.Conocer tipos de datosOjos con las comillas dobles (nuevamente): Algunas veces las macros contienen comillas dobles. Para poder escribirlos sin errores es necesario modificar levemente la forma en que se llama la variable localEjercicio 3.3.6: PreguntasUse una macro extendida para mostrar el tipo de dato (ej. int, float, long,…) de mpg. Revise el help de extended_fcn.Use una macro extendida para retornar el valor del label asociado foreign cuando es igual 1.Use una macro extendida para mostrar solo la primera variable de `controls'.Utilice una función extendida de macros para mostrar todos los archivos de su directorio actual (sugerencia: utilice comillas compuestas cuando muestre los nombres de los archivos).","code":"* La sintaxis general es: \nlocal nombremacro: función macro extendida* Para mirar los labels de trunk\nsysuse auto, clear\nlocal tlab : variable label trunk \ndisplay \"`tlab'\"* Para contar dentro de un local\nlocal NivelEstres Nada Medio Moderado Severo \nlocal wds: word count `NivelEstres' \ndisplay \"Hay `wds' niveles de estres:\"\nforvalues i = 1/`wds' {\nlocal wd: word `i' of `NivelEstres'\ndisplay \"Nivel `i' is `wd'\"\n}* Para conocer tipos de datos\nsysuse auto, clear\nlocal stortype : type make \ndisplay \"`stortype'\"* Con error \nlocal answers yes no \"do not know\" \ndisplay \"`answers'\"\n\n* Sin error \nlocal answers yes no \"do not know\" \ndisplay `\"`answers'\"'"},{"path":"id_01-chap02.html","id":"funciones-de-macro-extendidas-para-listas","chapter":"2 Programar do - files","heading":"2.1.2.5 Funciones de macro extendidas para listas","text":"Stata también define ciertas macro para operar sobre listas. Estas funciones permiten combinar listas, buscar elementos dentro de una lista o bien buscar elementos comunes entre dos listas. Conveniente revisar help macrolist para más funciones.Función levelsof: El comando levelsof lista los valores distintos de una variable. Al agregar la opción local(nombremacro) esto se guardara como una macro.","code":"* Sintaxis: \nlocal nombre macro: list función\n\n* Ejemplo de lista\nlocal animales \"gato perro gato loro loro\" \nlocal uniqanimales : list uniq animales \ndisplay \"`uniqanimales'\"* Sintaxis básica\nsysuse auto, clear\nlevelsof rep78 \ndisplay \"`r(levels)'\"\n\n* Sintaxis cuando hay variables categóricas.\nlevelsof foreign, local(levels)\nforeach l of local levels {\ndi \"-> foreign = `: label (foreign) `l''\"\nreg price mpg if foreign == `l'\n}"},{"path":"id_01-chap02.html","id":"manipulación-de-locales-vía-listas-de-macros","chapter":"2 Programar do - files","heading":"2.1.2.6 Manipulación de locales vía listas de macros","text":"Listas de macros: Las macro lists permiten obtener el número de elementos de una macro, trabajar con valores duplicados, ordenar elementos. Veremos cuatro aplicaciones:Elementos duplicados.Agregar y remover elementos.Uniones e intersecciones.Ordenar elementos.Elementos duplicados: dups extrae todos los elementos sobrantes.Agregar y remover elementos: Es básicamente pegar elementos de una macro con otra. Definamos dos variables locales: vars y coef y peguémoslos.Para remover tenemos que definir un nuevo local con los elementos que queremos quitar y sustraerlo del original. Supongamos que queremos actualizar el contenido de vars eliminado “y”.Unión de elementos: Podemos pegar todos los elementos diferentes entre dos listas.Noten que los elementos de car fueron pegados. Notar que esto es distinto simplemente pegar macros entre sí.Intersección de elementos: Podemos hacer la intersección entre dos elementos de una lista. Esto corresponde los elementos que pertenecen ambas macros.Noten que en este caso solo car se mantiene.Ordenar elementos: En ocasiones queremos ordenar los elementos de una lista contenida en una macro.Para hacer que los elementos de una macro se ordenen aleatoriamente es conveniente utilizar mata","code":"* Deja solo los unicos\nlocal fib 0 1 1 2 3\nlocal fib_nodups : list uniq fib\ndisplay \"`fib_nodups'\"\n\n* Quita todo los duplicados\nlocal fib 0 1 1 2 3\nlocal fib_dups : list dups fib\ndisplay \"`fib_dups'\"local vars x y z \nlocal coefs a b c \nlocal vars_coefs `vars' `coefs'\ndisplay \"`vars_coefs'\"local not y\nlocal vars : list vars - not\ndisplay \"`vars'\"local A house tree car\nlocal B computer car bike\nlocal all_things : list A | B\ndisplay \"`all_things'\"local A house tree car\nlocal B computer car bike\nlocal common_things : list A & B\ndisplay \"`common_things'\"local names camila camilo pedro paula\nlocal names : list sort names\ndisplay \"`names'\"local nums 1 2 3 4 5\nmata : st_local(\"random_nums\", ///\ninvtokens(jumble(tokens(st_local(\"nums\"))')'))\ndisplay \"`random_nums'\""},{"path":"id_01-chap02.html","id":"creturn","chapter":"2 Programar do - files","heading":"2.1.2.7 creturn","text":"Macros con creturnEn algunos casos al utilizar local o global vamos querer fijar algunos parámetros.Para este propósito utilizar creturn. Algunos ejemplos son: c(current_date), c(pwd), c(current_time), c(stata_version), c(pi), c(alpha), c(Wdays).Ejercicio 3.3.7: PreguntasDefine una macro llamada comestibles con peras, manzanas, fresas, yogur, vino y queso en ella. Ponla en orden alfabético.Define una macro llamada unión que contenga los miembros de la macro animales y comestibles y luego utiliza una función de lista extendida de la macro para mostrar el número de palabras que contiene.Ordena unión y muestra la posición de la palabra “vino” utilizando una función de lista extendida de macros.","code":""},{"path":"id_01-chap02.html","id":"estructuras-de-datos","chapter":"2 Programar do - files","heading":"2.1.3 Estructuras de datos","text":"","code":""},{"path":"id_01-chap02.html","id":"escalares","chapter":"2 Programar do - files","heading":"2.1.3.1 Escalares","text":"Los escalares pueden contener valores numéricos o strings. Un escalar solo puede contener un valor.La sintaxis para generar un escalar es scalar scalar_name = exp.Para utilizar un escalar en una operación solo es necesario llamarlo por su nombre. Para listar el contenido de todos los escalares scalar list. Para borrar scalar drop scalar_name o si quiero eliminar todo scalar drop _all. Los escalares permiten parametrizar un -file.","code":"quietly: summarize mpg \nscalar mean_mpg = r(mean)\nquietly: summarize rep78\nscalar mean_rep78 = r(mean)\ndisplay \"r(mean) guarda el promedio de rep78: \" r(mean) \ndisplay \"Pero tambien podemos recuperarlo:\" mean_mpguse fem2, clear \nscalar lb1 = 80 \nscalar ub1 = 88 \nscalar lb2 = 89 \nscalar ub2 = 97 \nforvalues i = 1/2 {\ndisplay _n \"IQ\" \"lb`i'\" \"-\" \"ub`i'\" \ntabulate anxiety if inrange(iq, lb`i', ub`i')\n}"},{"path":"id_01-chap02.html","id":"elementos-y-operaciones-con-matrices","chapter":"2 Programar do - files","heading":"2.1.3.2 Elementos y operaciones con matrices","text":"Stata puede generar matrices.Las matrices son muy importantes para guardar resultados y exportarlos de forma conveniente. También son útiles para cuando se quieren hacer estimaciones de muchos parámetros y para distintos conjuntos de datos.Operaciones con matrices: Se puede operar con matrices. Por ejemplo, sabemos que \\(b = (X'X)^{-1}X'y\\) y que \\(V = \\sigma^{2}(X'X)^{-1}\\). Si queremos extraer solo la matriz \\(X'y\\) tenemos que operar, calculando \\(\\frac{1}{\\sigma^{2}}V^{-1}b\\).e(b)’ indica la traspuesta, mientras que inv() es para calcular la matriz inversa.Llamar los elementos de una matriz: Se puede acceder los escalares contenidos en las matrices. Por ejemplo, si queremos obtener los elementos de la matriz \\(e(b)\\) tenemos:También podemos utilizar las posiciones de los elementos de la matriz (b[,k)]).","code":"reg weight age age2\nmatrix b = e(b)\nmatrix list b\nmatrix V = e(V)\nmatrix list Vmatrix define b = e(b)' \nmatrix define xty = inv(V) * b /e(rmse)^2\nmatrix list xty display \"The coefficient on weight is: \" _b[weight] \ndisplay \"Its standard error is: \" _se[weight] \n* Valores predichos \ngenerate anxietyhat = _b[_cons] + _b[weight] * weight + /// \n_b[age] * age + _b[age2] * age2"},{"path":"id_01-chap02.html","id":"funciones","chapter":"2 Programar do - files","heading":"2.1.3.3 Funciones","text":"Solo disponibles para comandos e-class. Por ejemplo, si estimamos una regresión veremos que la unica función disponible es e(sample).e(sample) nos indica si una determinada observación se utilizó para estimar la regresión. Es decir, es igual uno si una observación estaba en la muestra de estimación y 0 si fue excluida.Ejercicio 3.4.1:Preguntas:Estime una regresión de mpg contra weight length rep78 utilizando base de datos auto, pero solo para los autos extranjeros (foreign == 1).Genere una nueva variable, llamada enmuestra que tome los valores dados por la función e(sample).Utilizando br, observe los valores de enmuestra. ¿Qué observa?Calcule la estadística de mpg solo para las observaciones que fueron incluidas en la regresión.","code":"sysuse auto \nregress mpg weight length rep78\nereturn list"},{"path":"id_01-chap02.html","id":"iteradores","chapter":"2 Programar do - files","heading":"2.1.4 Iteradores","text":"","code":""},{"path":"id_01-chap02.html","id":"foreach","chapter":"2 Programar do - files","heading":"2.1.4.1 Foreach","text":"foreach y forvalues: forvalues itera sobre una lista de números y foreach recorre los elementos de una macro, o los nombres de las variables de una lista de variables, o los elementos de una lista de números.Hay algunas variaciones en foreach según el tipo de lista. La sintaxis es similar la recien presentada, pero difiere en dos aspectos:se remplaza por .Hay que llamar al identificador.Veamos como iterar sobre una lista de globales y locales.\n\\end{itemize}Veamos como iterar sobre una lista de variables:Veamos como iterar sobre una lista de de nuevas variables:Veamos como iterar sobre una lista de números:","code":"* Foreach\nforeach animal in cats and dogs { \ndisplay \"`animal'\"\n}\n\n* Forvalues \nforvalues i = 1(1)100 { \ngenerate x`i' = runiform()\n}* Sobre locales y globales\nlocal money \"Franc Dollar Lira Pound\" \nforeach currency of local money { \ndisplay \"`currency'\"\n}* Sobre lista de variables\nforeach var of varlist mpg weight-turn { \nquietly summarize `var' summarize `var' if `var' > r(mean)\n}* Sobre lista de nuevas variables\nforeach var of newlist z1-z20 { \ngenerate `var' = runiform()\n}foreach num of numlist 1 4/8 13(2)21 103 { \ndisplay `num'\n}"},{"path":"id_01-chap02.html","id":"combinar-macros-con-iteradores","chapter":"2 Programar do - files","heading":"2.1.4.2 Combinar macros con iteradores","text":"foreach y forvalues combinados con macros pueden utilizarse para ahorrarnos mucho trabajo. Podemos generar tun conjunto de locales partir de iteraciones.Como ya hemos visto, el comando levelsof devuelve una lista de todos los valores distintos de una variable categórica y los guarda en la macro r(levels). Esto lo hace en el caso de que nosotros le asignemos un nombre. Podemos utilizar esta lista para los países y años de nuestra muestra para definir dos iteraciones que recorran todos los valores posibles. Para cada valor resumimos la población y definimos una macro local compuesta por el código de país y el año (por ejemplo, USA1990) que toma el valor de la población en ese año para ese país.Una aplicación del forvalues:Utilizando dos forvalues:foreach y recode:Loops anidados: foreach y forvaluesEn estos casos es bueno utilizar espacios para hacer el código más amigable. Ojo que Stata le interesa esto para ejecutar, es solo una cuestión de estilo.También útil para estimar regresiones.Tokenize: Podemos almacenar los elementos de la lista de países en macros numeradas con tokenize.Aquí los nombres de los países se almacenan como valores de las macros numeradas. Debemos referenciar doblemente la macro \\(\\). El contenido de esa macro la primera vez que se pasa por el bucle es el número 1. Para acceder al primer código de país, debemos referenciar la macro \\(`1'\\).","code":"use replicate.dta, replace\nlevelsof cty\nlocal ctries \"`r(levels)'\" \n\nforeach ctr in `ctries' { \n        sum hours_t if cty == \"`ctr'\" \n        local nombre `ctr' = `r(mean)'\n} use gdp4cty, clear \nforvalues i = 1/4 {\ngenerate double lngdp'i' = log(gdp'i')\nsummarize lngdp'i'\n}forvalues y = 1995(2)1999 {\n    forvalues i = 2(2)4 {\n        summarize gdp`i'_`y'\n    }\n}use gdp4cty, clear \nlocal ctycode 111 112 136 134 \nlocal i 0\nforeach c of local ctycode{ \n        local ++i \n        local rc \"`rc' (`i'=`c')\" \n}\ndisplay \"`rc'\"\nrecode cc `rc', gen(newcc)use gdp4cty, clear \nlocal country US UK DE FR \nlocal yrlist 1995 1999 \nforvalues i = 1/4 {\n    local cnaine: word `i' of `country'\n    display \"`cnaine'\"\n    foreach y of local yrlist {\n            rename gdp`i'_`y' gdp`cnaine'_`y'\n    } \n}sysuse auto, clear\nforeach y of varlist mpg rep78 headroom trunk weight length { \n    foreach x of varlist rep78 price displacement gear_ratio foreign { \n    regress `y' `x'\n} use gdp4cty, clear\nlocal country US UK DE FR \nlocal yrlist 1995 1999 \nlocal ncty: word count `country'\ndisplay \"`ncty'\"\ntokenize `country'\nforvalues i = 1/`ncty'{\n    foreach y of local yrlist {\n            rename gdp`i'_`y' gdp``i''_`y'\n        }\n}"},{"path":"id_01-chap02.html","id":"while-loop","chapter":"2 Programar do - files","heading":"2.1.4.3 While loop","text":"Realiza la iteración o se repite una lista de comandos mientras la condición sea verdadera. La sintaxis es:¿Cuando es útil? : Cuando este seguro() cuantas veces se realizará la iteración. Notar que si hay convergencia, va iterar infinitamente.También se puede combinar con macros. Es importante utilizar los incrementales:La primera parte define el inicio del contador, mientras que la segunda indica la condición para que sea ejecutado. El local final actualiza (incremental). Si el incremento es unitario podemos utilizar local ++.","code":"while exp { \nhace algo\n}while reldif (nueva, antigua) > 0.001 { \n}local i=1 \nwhile `i'<=5 {\ndisplay \"loop number\" `i' \nlocal i = `i'+1\n}"},{"path":"id_01-chap02.html","id":"branching","chapter":"2 Programar do - files","heading":"2.1.4.4 Branching","text":"Hacer una cosa en caso de que alguna condición sea cierta y otra cosa en caso de que sea falsa. La sintaxis básica es:Ejercicio 3.4.2:PreguntasDefina una macro llamada mimacro que sea igual un número entero aleatorio entre 1 y 99.Utilizando y else muestre un mensaje que diga si es par o impar.","code":"if algo es verdadero { \nhacer esto\n} else {\nhacer lo contrario\n}"},{"path":"id_01-chap02.html","id":"aplicaciones","chapter":"2 Programar do - files","heading":"2.1.4.5 Aplicaciones","text":"Seguir secuencias especiales: creturn posee varias constantes y valores los que se puede acceder. Por ejemplo:c(filename) nombre del último nombre del archivo guardado.c(alpha/ALPHA) lista de letras minúsculas/mayusculas.c(Mons) lista de nombres de los meses abreviados.c(Months) lista de los nombres de los meses abreviados.c(Wdays) lista de los dias de la semana abreviados tres caracteres.c(Weekdays) lista de los días de la semana abreviados.Con un iterador es posible aplicar estas listas para agregar labels. Suponga que tenemos valores de 1 12 que representan meses.","code":"clear all\nset obs 12\ngen month = _n \ntokenize `c(Months)' \nforvalues i = 1/12 {\n    label define monthlab `i' \"``i''\" , modify \n                   }\nlabel val month monthlab"},{"path":"id_01-chap02.html","id":"monitorear-un-loop","chapter":"2 Programar do - files","heading":"2.1.4.6 Monitorear un loop","text":"Ejemplo proveniente de (Stata tip 41). Cualquier loop puede ser modificado para que muestre su progreso con el comando _dots. Esto es importante para cuando se requieren hacer procesos que toman varias horas y es necesario monitorear avances.El primer comando _dots establece las lineas. Titulo y número de repeticiones son opcionales. reps solo acepta enteros como argumento. _dots ‘’ 0 tiene dos elementos:El primer argumento es el número de repetición, que registra el número de intentos en curso. En el ejemplo, esta automáticamente determinado por el loop.El segundo argumento es el código de retorno, el cual indica el tipo de símbolo. En el ejemplo tenemos un 0.Los códigos de retorno alternativos producen una “s” (-1), “.” (), “x” roja (1), una “e” (2), una “n” (3) o un “?” (cualquier otro valor).Ejemplo numérico: La idea es definir una macro local para que actue como un contador.Contar repeticiones:El número de repeticiones en rep es calculado por _dots. Es necesario contar manualmente las variables e introducir el número.Esto lo vamos hacer con la ayuda de una función de macro extendida sizeof.unab permite ingresar una lista de variable abreviada y expandirla, de forma tal de que la pueda contar.Un ejemplo más complejo es:Se ejecuta hasta que logre 70 aciertos. En este ejemplo artificial, cada iteración tiene un éxito aleatorio con una probabilidad del 80%. Los éxitos se indican con un punto (.) y los fracasos con una x.","code":"_dots 0, title(Loop ejecutando) reps(75) \nforvalues i = 1/75 { \n_dots ‘i’ 0\n}\n\n----+--- 1 ---+--- 2 ---+--- 3 ---+--- 4 ---+--- 5\n..................................................    50\n.........................sysuse auto\n_dots 0, reps(10)\nforeach var of varlist price - gear_ratio {\n  sum `var', d\n  local i = `i'+1\n  _dots `i' 0\n}sysuse auto\nunab myvars : price - gear_ratio\nlocal N : list sizeof myvars\n_dots 0, reps(`N')\nforeach var of varlist `myvars' {\n  ...\n  local i = `i'+1\n  _dots `i' 0\n}noisily _dots 0, title(Looping until 70 successes...) \nlocal rep 1 \nlocal nsuccess 0 \nwhile ‘nsuccess’ < 70 { \nlocal fail = uniform() < .2 \nlocal nsuccess = ‘nsuccess’ + (‘fail’ == 0) \nnoisily _dots ‘rep++’ ‘fail’\n}"},{"path":"id_01-chap02.html","id":"manejo-de-bases-de-datos","chapter":"2 Programar do - files","heading":"2.1.5 Manejo de bases de datos","text":"","code":""},{"path":"id_01-chap02.html","id":"prefijo-by","chapter":"2 Programar do - files","heading":"2.1.5.1 Prefijo: by","text":"Los prefijos en Stata ejecutan tareas repetitivas sin la necesidad de especificar el rango de valores sobre la tarea que es ejecutada. Un prefijo muy conocido es . Por ejemplo, varlist [, sort]: command. command es repetido para cada valor de la variable. Es más, repeticiones siguen el orden de la variable sea string o numeric.","code":"use bpress, clear\nbysort sex agegrp: summarize bp "},{"path":"id_01-chap02.html","id":"prefijo-xi","chapter":"2 Programar do - files","heading":"2.1.5.2 Prefijo: xi","text":"xi es útil para cuando queremos producir un variable indicador para las observaciones que son distintas entre si.El ejemplo le esta diciendo Stata que genere variables indicadores. Esto es muy útil para reducir los códigos.Interpretando prefijo xi: xi es comúnmente utilizado como un prefijo. La principal ventaja es cuando existen múltiples interacciones entre las variables.","code":"xi i.agegrp* Caso 1: incluye indicadores de ambas variables.\nxi: regress bp i.agegrp i.sex\n\n* Caso 2: incluye además interacciones entre ellas.\nxi: regress bp i.agegrp*i.sex\n\n* Caso 3: Interactúa una variable continua con una discreta.\nxi: regress bp i.agegrp*bp0\n\n* Caso 4: Incluye solo interacciones con variable continua (además de principal). \nxi: regress bp i.agegrp|bp0 "},{"path":"id_01-chap02.html","id":"prefijo-statsby","chapter":"2 Programar do - files","heading":"2.1.5.3 Prefijo: statsby","text":"statsby permite ampliar . Este último tiene la limitación de permitir únicamente un comando.Se produce una nueva base de datos con una observación por grupo con los estadísticos incorporados. Útil para calcular estadística descriptiva.","code":"statsby mean=r(mean) sd=r(sd) n=r(N), by(agegrp sex): summarize bp"},{"path":"id_01-chap02.html","id":"prefijo-rolling","chapter":"2 Programar do - files","heading":"2.1.5.4 Prefijo: rolling","text":"statsby permite obtener estadísticas para sub-muestras que se traslapan. rolling sirve para sub-muestras traslapadas. Por ejemplo, al trabajar con series de tiempo se quiere calcular estadísticas para datos que están traslapados (el. calcular una media móvil).Vamos calcular medias y medianas utilizando una ventana de 90 días:start y end indican el inicio y fin de la ventana.","code":"use ibm, clear \nrolling mean=r(mean) median=r(p50), window(90): summarize spx, d \ntsset end \ntsline mean"},{"path":"id_01-chap02.html","id":"merge-y-append","chapter":"2 Programar do - files","heading":"2.1.5.5 Merge y Append","text":"Recomendaciones para utilizar mergeEspecifique siempre el tipo de fusión (1:1, m:1 o 1:m). Si se especifica el tipo de fusión, se llama la versión antigua y robusta de la fusión.Nunca haga fusiones de muchos muchos (m:m), o al menos, sólo hágalo cuando tenga una muy buena razón.Incluya siempre la opción assert() para indicar qué patrón de observaciones coincidentes espera.Incluya siempre la opción keep() para indicar qué observaciones deben conservarse del conjunto de datos fusionados.Siempre que sea posible, incluya la opción keepusing() y enumere explícitamente qué variables pretende añadir al conjunto de datos; puede incluir esta opción incluso cuando mantenga todas las variables de los datos utilizados.Utilice la opción nogen, excepto cuando piense utilizar explícitamente la variable _merge más adelante. Nunca debe guardar un conjunto de datos que tenga _merge; si necesita esta variable más adelante, dele un nombre más informativo.Con respecto especificar _merge y assert notar que:Append con ciudado:El comando append es muy útil para manejos de bases de datos. Una precaución común es con respecto al nombre de las variables. Si dos variables (ej. PRECIO y precio) difieren, se generaran dos columnas nuevas al hacer el append en vez de una.Una precaución un poco menos conocida guarda relación con el tipo de variables. ¿Qué ocurre si dos variables se llaman igual, pero estan guardadas en formatos distintos?En este caso, el orden en el cual se combine la base de datos va importar y puede generar diferencias al momento de pegar datos. Esto es especialmente importante cuando una variable esta guardada en numérico en una base de datos y en string en la otra.Veamos un ejemplo con la base auto.dta. Vamos crear dos bases de datos según la procedencia de los autos y ejecutar el comando append.Notar que append genera el siguiente mensaje: foreign str 7 using data byt byte now. Noten que el contenido de la variable string se ha perdido: 22 casos son ahora missing.¿Qué ocurre si hacemos el proceso al revés? Vamos cargar autos extranjeros y le vamos pegar autos domésticos:El formato de los datos de la primer base de datos manda.\n* utilizar force sin cuidado.\n* Con distintos tipos de datos, append es sensible al orden en que los archivos son pegados. Tener cuidado y revisar consistencia en los datos. Hacer test aquí tambíen es importante.","code":"     merge ..., assert(match master) keep(match)\n\n     * Es equivalente a: \n     merge ...\n     assert _merge==1 | _merge==3\n     keep if _merge==3sysuse auto \ndrop if foreign\nsave autodom \nsysuse auto \ndrop if !foreign \nrename foreign nondom \ngenerate str foreign = \"foreign\" if nondom \nsave autofor\n\nuse autdom \nappend using autofor \ndescribe foreign use autfor \nappend using autodom, force\ndescribe foreign \ncodebook foreign"},{"path":"programar-ado---files.html","id":"programar-ado---files","chapter":"3 Programar ado - files","heading":"3 Programar ado - files","text":"","code":""},{"path":"programar-ado---files.html","id":"escribir-programas-en-stata","chapter":"3 Programar ado - files","heading":"3.0.1 Escribir programas en Stata","text":"En esta sección aprenderemos escribir nuestros propios comandos en Stata. Escribir programas en Stata tiene muchas ventajas relacionadas con las buenas practicas que vimos en la Sección 1.Vamos utilizar program para escribir los programas. Es posible equipar cualquier comando con las opciones típicas usuales (ej. o inrange. Discutiremos mayormente programas del tipo r-class y algunos comentarios con respecto los del tipo e-class.¿Por qué debiese escribir mis propios programas en Stata? Automatizar procesos que se ejecutan frecuentemente y donde los resultados dependen de algún tipo de heterogeneidad.","code":""},{"path":"programar-ado---files.html","id":"por-qué-escribir-programas-abstracción","chapter":"3 Programar ado - files","heading":"3.0.2 ¿Por qué escribir programas?: Abstracción","text":"Abstraer para eliminar pasos redundantes.Abstraer con fines de hacer códigos más claros. por otras razones.Abstracción es esencial para escribir un buen código por al menos dos razones:\nAl eliminar la redundancia se reducen las posibilidades de cometer errores.\nAumenta la claridad. Para cualquier lector será más facil leer un código redundante.\nAl eliminar la redundancia se reducen las posibilidades de cometer errores.Aumenta la claridad. Para cualquier lector será más facil leer un código redundante.Veamos un ejemplo: Supongamos que queremos ver la correlación espacial del consumo de papas fritas. Queremos testear si el consumo per-capita de papas fritas esta correlacionado con el consumo promedio percapita de las otras comunas de la misma región. Primero tenemos que calcular el consumo per-capita del resto:Ahora podemos ver si existe correlación. ¿Pero si queremos cambiar el nivel de agregación? Tal vez si existe correlación, pero nivel de área metropolitana. Copiemos el código de nuevo y calculemos esto.Noten que hay un error. Se nos olvido remplazar región por metroarea. Este error se puede propagar si seguimos haciendo operaciones. Una alternativa al copiar y pegar es escribir una función con propósito general que calcule la variable que deseamos bajo distintos parámetros.Con el programa podemos escribir los bloques de código anteriores como:Hemos escrito la función de forma totalmente general. Podemos cambiar el nivel de agregación sin inducir errores.Estructura de un programa en Stata: La sintaxis más simple es:Cuando se ha definido un programa con program, este se vuelve indistinguible de cualquier otro comando de Stata. Es importante estar seguros() de que estoy escribiendo el mismo nombre que otro programa. Para garantizar lo anterior, es bueno utilizar el comando .Guardar un programa en StataHay dos lugares en donde puedes guardar tus ado-files.Cuando se ha definido un programa con program, este se vuelve indistinguible de cualquier otro comando de Stata.En el directorio de trabajo del proyecto.Se puede hacer una carpeta nueva en la sección de códigos que indique los programas.Otra opción es guardarlo en el directorio Personal de Stata. Para entrar escriban personal en la consola.Nombrar un programa en StataEs posible darle cualquier nombre un programa mientras sea un nombre que ya es utilizado por Stata.Si por ejemplo, creas un programa llamado summarize Stata lo va ignorar y utilizará su propio comando.","code":"egen total_pc_papitas = total(pc_papitas), by(region)\negen total_obs = count(pc_papitas), by(region)\ngen consumo_papitas_resto_pc = ///\n(total_pc_papitas - pc_papitas)/(total_obs - 1)egen total_pc_papitas = total(pc_papitas), by(metroarea)\negen total_obs = count(pc_papitas), by(region)\ngen consumo_papitas_restometro_pc = ///\n(total_pc_papitas - pc_papitas)/(total_obs - 1)program consumo_papitas_resto \n    syntax, invar(varname) outvar(name) byvar(varname)\n    tempvar tot_invar count_invar \n    egen `tot_invar' = total(`invar'), by(`byvar')\n    egen `count_invar' = count('invar'), by('byvar') \n    gen `outvar' = (`tot_invar' - `invar') ///\n    / (`count_invar' - 1) \nend * Caso 1 \nconsumo_papitas_resto, invar(pc_papitas) ///\noutvar(consumo_papitas_resto_pc) byvar(region)\n\n* Caso 2\nconsumo_papitas_resto, invar(pc_papitas)  ///\noutvar(consumo_papitas_restometro_pc) byvar(metroarea)    program nombredelprograma\n        display \"Lo que va a hacer el programa\"\n    end which tabmiss"},{"path":"programar-ado---files.html","id":"mis-primeros-programas-en-stata","chapter":"3 Programar ado - files","heading":"3.0.3 Mis primeros programas en Stata","text":"","code":""},{"path":"programar-ado---files.html","id":"programa-1","chapter":"3 Programar ado - files","heading":"3.0.3.1 Programa 1","text":"Vamos escribir nuestro primer programa:Noten que al ejecutarlo se genera ningún resultado. Lo que hemos hecho es definir un comando llamado minombrees con una simple función. Esta es la idea principal de un programa. Ejecuten nuevamente el programa. Al hacer esto observaran que se genera un error. Esto es porque al igual que las variables, es posible asignar dos nombres iguales un programa. Para evitar esto, es importante utilizar program drop minombrees antes de cargar el comando nuevamente. Este es un buen momento para utilizar capture.","code":"  * Mi primer programa en Stata\n    program minombrees \n        display \"Hola, mi nombre es \"\n    end "},{"path":"programar-ado---files.html","id":"programa-2","chapter":"3 Programar ado - files","heading":"3.0.3.2 Programa 2","text":"Vamos escribir un programa que permita calcular un promedio. Vamos escribir un comando que nos permita crear una nueva variable que contenga los valores promedio y que muestre el resultado en la pantalla. Esto sería igual que escribir:Podemos evitar la repetición de estos dos comandos armando un programa:Noten que hemos utilizado `1'. Este nos indica cualquier variable que este en la primera posición. Si incluimos más de una variable. Solo va considerar la primera variable.Ejercicio 3.5.1: PreguntasEscriba un programa que permita ver todas las etiquetas (labels) de una base de datos.Aplique este programa la base auto.dta.","code":"    sysuse auto, clear\n    egen mimedia_mpg = mean(mpg)\n    tab mean_mpg program drop _all \ncapture program drop mymean\nprogram mymean \n    egen media_`1' = mean(`1')\n    tab media_`1'\nend \n\n* Aplicamos este programa \nsysuse auto, clear\nmimedia mpg"},{"path":"programar-ado---files.html","id":"programa-3","chapter":"3 Programar ado - files","heading":"3.0.3.3 Programa 3","text":"Vamos reescribir el programa para permitir un número arbitrario de variables. La macro ‘0’ contiene toda la cadena, ‘1’ el primer elemento, ‘2’ el segundo, etc. Podemos hacer una iteración sobre todos los elementos sin saber cuántos hay utilizando la técnica de desplazamiento incremental que implementaremos con macro_shift.El comando macro shift sirve como incremental. Termina cuando encuentra un vacio, lo que explica la presencia del . Notar que \"1’“` habla de la posición. Es una buena tecnica para garantizar que el iterador seguirá cuando este vacio.","code":"capture program drop mimedia \nprogram define mimedia \n    while \"`1'\"!=\"\" { \n        egen mean_`1'=mean(`1') \n        tab mean_`1' \n        macro shift\n    }\nend\n\nsysuse auto, clear\nmimedia price mpg rep78 "},{"path":"programar-ado---files.html","id":"programa-4","chapter":"3 Programar ado - files","heading":"3.0.3.4 Programa 4","text":"Modificamos un poco el programa para que despliegue los resultados en la consola de Stata. Adicionalmente, agregamos un quietly.","code":"capture program drop mimedia \nprogram define mimedia \n    while \"`1'\"!=\"\" { \n        qui: egen `1'_mean = mean(`1') \n        display \"Media de `1' = \" `1'_mean \n        macro shift\n    }\nend\n\nsysuse auto, clear\nmimedia price mpg rep78 "},{"path":"programar-ado---files.html","id":"programa-5","chapter":"3 Programar ado - files","heading":"3.0.3.5 Programa 5","text":"El comando macro shift es útil, pero puede ser lento. Agregando variables locales usuales y un incremental es mucho mejor. Ojo con las dobles comillas.","code":"capture program drop mimedia \nprogram define mimedia \n    local i = 1\n    while \"``i''\"~=\"\" { \n        qui: egen ``i''_mean = mean(``i'') \n        display \"Media de `i' = \" ``i''_mean \n        local ++i\n    }\nend"},{"path":"programar-ado---files.html","id":"programa-con-distintos-argumentos","chapter":"3 Programar ado - files","heading":"3.0.4 Programa con distintos argumentos","text":"El programa de los ejemplos anteriores soporta un solo argumento. Ahora vamos ver un programa que considere explícitamente que los argumentos de un programa pueden tomar roles distintos.tempvar crea una variable temporal que existe mientras el programa se ejecuta pero que se elimina automáticamente una vez que el programa termina su ejecución. Es importante estar seguros() de que cualquier string están en comillas dobles.Ejercicio 3.5.2:PreguntasEscriba un programa que reporte la mediana de la diferencia entre dos variables.Aplique este programa la base auto.dta.","code":"program drop _all \ncapture program drop show \nprogram define show\n    tempvar obs \n    quietly gen `obs' = `1' ///\n    if (ctycode == \"`2'\" & year ==`3')\n    sort `obs'\n    display \"`1' of country `2' in `3' is: \" `obs' \nend"},{"path":"programar-ado---files.html","id":"opción-syntax","chapter":"3 Programar ado - files","heading":"3.0.5 Opción Syntax","text":"Renombrando argumentos:De momento hemos escrito los programas utilizando '1', '2', '3' con el fin de introducir el uso de programas en Stata.Sin embargo, esta notación puede ser un poco confusa y provocar errores en la codificación. Podemos asignar nombres de mayor significado los argumentos del programa.El comandoargs asigna las variables locales var, cty, yr los valores de '1', '2', '3'. Noten que si llaman al programa con cuatro argumentos retorna un error. Un método mucho más robusto y mejor para llamar los argumentos de un programa es utilizar syntax. En vez de referirnos cada elemento de un programa por su posición, vamos especificar la “gramática” del programa.La sintaxis de Stata es:varlist denota la lista de variables, command el comando ejecutar, exp denota una expresión algebraica, range denota un rango para las observaciones mientras que ,options denota la lista de opciones propias de un comando. Utilizar syntax en un programa hace que Stata verifique si un programa satisface la sintaxis. En caso de que la cumpla, arrojara un error.Syntax\n* El comando syntax almacena en macros locales todos los elementos típicos de un comando en Stata.\n* Por ejemplo, syntax también puede definir condicionales como o .Si quieren hacer programas más complejos, con otras características es importante revisar syntax.Uno de los primeros elementos de syntax es varlist. Es posible indicar el mínimo o máximo de variables.\n* Por ejemplo: varlist(min = 2 max = 2).\n* Si tienes solo una variable puedes utilizar varname.\n* Es equivalente varlist(min = 1 max = 1).\n* La macro que guarda las variables siempre se llama varlist.Ejemplo 1: programa para calcular percentilesEl programa anterior nos permite obtener los percentiles de alguna variable al mismo tiempo que los muestra en la consola. Noten que tambíen incluido que tipo de comando es. En este caso es un comando r-class. program es quien determina el nombre del programa. syntax permite determinar los elementos de tu programa. En el ejemplo define el tipo y el límite de variables. También puede definir condicionales como o .Los escalares definidos durante el programa se pueden utilizar. Esto siempre es conveniente. Como recomendación es bueno guardar los programas como variables temporales.Ejemplo 2: programa para calcular percentiles con variables locales","code":"program drop _all \ncapture program drop show \nprogram define show\n    args var cty yr\n    tempvar obs \n    quietly gen `obs' = `var' ///\n    if (ctycode == \"`cty'\" & year ==`yr')\n    sort `obs'\n    display \"`1' of country `2' in `3' is: \" `obs' \nend[by varlist:] command [varlist] [=exp] [in range] [, options]    program ... \n        syntax varlist(min = 2) [if] [in]program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric)\n    quietly summarize `varlist', detail \n    scalar range = r(max) - r(min)\n    scalar p7525 = r(p75) - r(p25)\n    scalar p9010 = r(p90) - r(p10)\n    display as result _n \"Rangos de percentiles para `varlist'\"\n    display as txt \"75-25 : \" p7525\n    display as txt \"90-10:  \" p9010 \n    display as txt \"Range: \" range\nend program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric)\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010'\n    display as txt \"Range: \" `range'"},{"path":"programar-ado---files.html","id":"return","chapter":"3 Programar ado - files","heading":"3.0.6 Return","text":"Una característica importante de los comandos de Stata es su capacidad de reportar los resultados de forma tal de que los usuarios y usuarias podamos utilizarlos posteriormente.El comando return nos permite guardar los escalares y hacerlos accesibles, sin tener el problema que vimos en el ejemplo anterior.Notar que el lado izquierdo del escalar de retorno se refiere al nombre de la macro, el lado derecho debe hacer referencia la macro una vez más para extraer el valor almacenado en ese nombre, por lo que debe utilizar dos comillas.Ejemplo 3: programa para calcular percentiles con return","code":"program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric)\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010 '\n    display as txt \"Range: \" `range'\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\nend "},{"path":"programar-ado---files.html","id":"implementar-opciones-al-programa","chapter":"3 Programar ado - files","heading":"3.0.7 Implementar opciones al programa","text":"Podemos agregar distintas opciones al programa. Una opción es agregar como opcional que el programa de un resultado en la consola. Al incluir \\([ ]\\) en syntax significa un componente opcional en el comando.Ejemplo 4: implementar opciones al programa (imprimir por defecto)Ejemplo 5: implementar opciones al programa (imprimir por defecto)","code":"program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric) [, PRINT]\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    if \"`print'\" == \"print\" {\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010 '\n    display as txt \"Range: \" `range'\n    }\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\nend program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric) [, noPRINT]\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    if \"`print'\" != \"noprint\" {\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010 '\n    display as txt \"Range: \" `range'\n    }\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\nend "},{"path":"programar-ado---files.html","id":"incluir-ifin-al-programa","chapter":"3 Programar ado - files","heading":"3.0.8 Incluir if/in al programa","text":"Incluir un subconjunto de observacionesCualquier comando debiese incluir o range.Nuevamente, estas opciones son manejadas dentro del comando syntax. Para incluir estas opciones hay que agregar [] y [].Con estos comandos puedo ejecutar el programa para sub-muestras. Es importante asegurar que la sub-muestra es vacía. Para ello es importante calcular r(N), chequear que sea distinto de cero y agregar un título que lo indique.El comando marksample touse utiliza la información provista por o en caso de que estos sean indicados en el programa. Este comando genera una variable local touse que es igual 1 si las variables entran en el calculo que hace el programa y 0 en caso contrario.Utilizaremos 'touse' para calcular el número de observaciones que se utilizan después de aplicar los condicionales. Es necesario agregar 'touse' en cada parte del programa que trabaje con la variable de input.Ejemplo 6: Incluir un subconjunto de observaciones","code":"program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric) [, noPRINT]\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    if \"`print'\" != \"noprint\" {\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010 '\n    display as txt \"Range: \" `range'\n    }\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\nend "},{"path":"programar-ado---files.html","id":"generalizar-el-comando-para-incluir-múltiples-variables","chapter":"3 Programar ado - files","heading":"3.0.9 Generalizar el comando para incluir múltiples variables","text":"Algunas consideracionesSi quiero ejecutar el programa sobre múltiples variables, es necesario ajustar un poco el programa.Tengo que indicarle syntax que hay más de una variable. Además, tengo que indicarle al programa que en caso de que existan más variables muestre los resultados en una tabla.Guardaremos los resultados en matrices (escalares) y vamos aplicar una función de macro extendida con el fin de contar el número de filas que esta matriz debiese tener.Agregaremos también una la opción para cambiar el formato y la opción mat que permite la matriz ser guardada automáticamente con el nombre indicado.Ejemplo 7","code":"program drop _all \nprogram pctrange, rclass\nversion 17 \n    syntax varlist(min = 1 numeric ts) [if] [in] [, noPRINT FORmat(passthru) MATrix(string)] \n    marksample touse \n    quietly count if `touse'\n    if `r(N)' == 0 {\n        error 2000\n    }\n    local nvar: word count `varlist'\n    if `nvar' == 1 {\n        local res range p7525 p9010\n        tempname `res'    \n    \n        quietly summarize `varlist' if `touse', detail \n        scalar `range' = r(max) - r(min)\n        scalar `p7525' = r(p75) - r(p25)\n        scalar `p9010' = r(p90) - r(p10)\n        if \"`print'\" != \"noprint\" {\n            display as result _n \"Rangos de percentiles para `varlist', N = `r(N)'\"\n            display as txt \"75-25 : \" `p7525'\n            display as txt \"90-10:  \" `p9010 '\n            display as txt \"Range: \" `range'\n    }\n    \n    foreach r of local res {\n        return scalar `r' = ``r''\n    }\n    return scalar N = r(N)\n    }\n    else {\n    tempname rmat \n      matrix `rmat' = J(`nvar', 3, .)\n      local i 0 \n        foreach v of varlist `varlist'{\n        local ++i \n        quietly summarize `v' if `touse', detail \n        matrix `rmat'[`i' ,1] = r(max) - r(min)\n        matrix `rmat'[`i', 2] = r(p75) - r(p25)\n        matrix `rmat'[`i', 3] = r(p90) - r(p10)\n        local rown `rown' `v'\n    }\n      \n      matrix colnames `rmat' = Range P75-P25 P90-P10 \n      matrix rownames `rmat' = `rown' \n\n      if \"`print'\" != \"noprint\" {\n          local form \", noheader\"\n          if \"`format'\" != \"\" {\n                local form \"`form' `format'\" \n          }\n          matrix list `rmat' `form'\n                                }\n\n      if \"`matrix'\" != \"\" {\n            matrix `matrix' = `rmat'\n      }\n      return matrix rmat = `rmat'\n      }\n      return local varname `varlist'\nend "},{"path":"programar-ado---files.html","id":"agregar-prefijos-a-los-programas","chapter":"3 Programar ado - files","heading":"3.0.10 Agregar prefijos a los programas","text":"Prefijo byAhora vamos hacer que nuestro programa pueda utilizar el prefijo .Para agregar esta opción simplemente hay qyue modificar program.También podemos permitir que la lista de variables incluya operadores de series de tiempo (ej. L.pib, D.ingreso). Para incorporar estos elementos tenemos que modificar syntax.","code":"program pctrange, rclass byable(recall)    syntax varlist(min = 1 numeric ts) [if] [in] ///\n    [, noPRINT  FORmat(passthru) MATrix(string)]"},{"path":"programar-ado---files.html","id":"programas-para-complementar-función-egen","chapter":"3 Programar ado - files","heading":"3.0.11 Programas para complementar función egen","text":"Es posible programar funciones adicionales de egen. El nombre de estos programas deben empezar con _g. Una diferencia importante entre este tipo de programas y los que ya hemos escrito guarda relación con el hecho de que hay que tener en cuenta la nueva variable que se va crear. La sintaxis es la siguiente:El cambio que haremos en la sintaxis del programa será que incluiremos un touse para hacer la nueva variable.Ejemplo 8: programa función egenProgramas con funciones de egen con prefijo byAgregamos [, *] que corresponde las opciones. En egen el prefijo es una opción.Con el fin de permitirle al programa que pueda producir un rango de percentil separado para cada grupos utilizaremos pctile en vez de summarize.El cambio que haremos en la sintaxis del programa será que incluiremos un touse para hacer la nueva variable.Ejemplo 9: programa función egen con prefijo byGeneralización de la función egen para soportar todos los pares de cuantilesHemos desarrollado una función de egen que permite calcular un rango entre percentiles para una lista de variables especifica.Puede ser util tomar ventaja de egen pctile() para poder calcular cualquier percentil de la lista de variables especificadas.Vamos agregar dos opciones la función egen : lo() y hi(). En caso de que especifiquemos, por defecto se calcula el rango interquartil. La función de egen ahora se llamará _gpctrange.ado.Ejemplo 10: función egen más general","code":"egen [type] newvarname = fcn(arguments) [if] [in] [, options]* Programas de egen \nprogram drop _all\nprogram _gpct9010\n    syntax newvarname =/exp [if] [in] \n    tempvar touse \n    mark `touse' `if' `in' \n    quietly summarize `exp' if `touse', detail \n    quietly generate `typlist' `varlist' = r(p90) - r(p10) if `touse'\nend\n\n* Aplicación del programa\nsysuse auto, clear\negen rango9010 = pct9010(price)* Programa con opción by. \nprogram drop _all\nprogram _gpct9010\nsyntax newvarname =/exp [if] [in] [, *] \ntempvar touse p90 p10 \nmark `touse' `if' `in' \nquietly {\n          egen double `p90' = pctile(`exp') if `touse', `options' p(90) \n          egen double `p10' = pctile(`exp') if `touse', `options' p(10)\n          generate `typlist' `varlist' = `p90' - `p10' if `touse' } \nend\n\n* Aplicación del programa\nsysuse auto, clear\negen rango9010 = pct9010(price)\nbysort rep78 foreign: egen rango9010_prefijoby = pct9010(price)* Programa con opción by. \nprogram drop _all\nprogram _gpctrange\nsyntax newvarname =/exp [if] [in] [, LO(integer 25) HI(integer 75) *] \n\nif `hi' > 99 | `lo' < 1 {\n    display as error ///\n        \"Percentiles `lo' `hi' deben estar entre 1 y 99.\" \n    error 198 \n}\nif `hi' <= `lo' {\n    display as error ///\n            \"Percentiles `lo' `hi' deben estar en orden ascendente\"\n    error 198 \n}\ntempvar touse phi plo \nmark `touse' `if' `in' \nquietly {\n          egen double `phi' = pctile(`exp') if `touse', `options' p(`hi') \n          egen double `plo' = pctile(`exp') if `touse', `options' p(`lo')\n          generate `typlist' `varlist' = `phi' - `plo' if `touse' \n          } \nend\n\nsysuse auto, clear\nbysort rep78: egen iqr = pctrange(price) if inrange(rep78,3,5)\nbysort rep78: egen p8020 = pctrange(price) if inrange(rep78,3,5), hi(80) lo(20)\ntabstat iqr if inrange(rep78, 3, 5), by(rep78)\ntabstat p8020 if inrange(rep78, 3, 5), by(rep78)"},{"path":"programar-ado---files.html","id":"syntax","chapter":"3 Programar ado - files","heading":"3.0.12 Syntax","text":"Algunas consideracionesComo hemos visto hay dos formas en los que un programa de Stata puede interpretar lo que ingresamos.Por posición tal como lo hace args o como lo hicimos con los números entre las comillas.De acuerdo la gramática del programa utilizando syntax.syntax guarda los componentes en macros locales particulares las cuales podemos acceder posteriormente.Por ejemplo '' '' 'varlist' son macros locales las que podemos acceder tal como vimos la clase pasada.Ahora vamos ver algunas opciones de syntax que nos permitirán tener más herramientas para escribir nuestros programas.Al utilizar dentro de un programa paréntesis cuadrados estoy indicando que esas partes son opcionales.Por ejemplo, estas dos versiones son equivalentes, salvo que en la segunda linea todo es opcional:Vamos mirar las macros generadas por syntax:Vamos aplicar lo aprendido en un ejemplo sencillo:marksample y touse: Un error común es utilizar una muestra en una parte del programa y otra distinta en otra parte. La solución es crear una variable que contiene un 1 si la observación fue utilizada y un 0 en caso contrario.","code":"    * Nada opcional\n    syntax varlist if in title(string) adjust(real 1) \n    \n    * Todo opcional\n    syntax [varlist] [if] [, adjust (real 1) title(string)]capture program drop myprog\nprogram myprog \nsyntax varlist [if] [in] [, adjust(real 1) title(string)] \n    display \"varlist contiene |`varlist'|\" \n    display \"if contiene |`if'|\" \n    display \"in contiene |`in'|\" \n    display \"adjust contiene |`adjust'|\" \n    display \"title contiene |`title'|\" \nendcapture program drop miprograma\nprogram miprograma\n    syntax varlist [if] [in] [, adjust(real 1) title(string)]\n    display \n    if \"`title'\" != \"\" {\n        display \"`title':\"\n    }\n    foreach var of local varlist{\n        quietly summarize `var' `if' `in'\n        display \"`var'\" \" \"%9.0g r(mean)*`adjust'\n    }\nendcapture program drop miprograma\nprogram miprograma\n    syntax varlist [if] [in] [, adjust(real 1) title(string)]\n    marksample touse\n    display \n    if \"`title'\" != \"\" {\n        display \"`title':\"\n    }\n    foreach var of local varlist{\n        quietly summarize `var' `if' `touse'\n        display \"`var'\" \" \" r(mean)*`adjust'\n    }\nend"},{"path":"programar-ado---files.html","id":"varlist","chapter":"3 Programar ado - files","heading":"3.0.13 Varlist","text":"varlist específica la macro que contiene las variables que van ingresar al programa como inputs. Las opciones de varlist son:\n* default = none. Especifica como la varlist se va llenar. Por defecto se llena con todas las variables.\n* min, max especifica el número de variables permitidas.\n* numeric, string especifican que criterio deben cumplir todas las variables que ingresas al programa.\n* ts permite que la varlist contenga operadores de series de tiempo.\n* fv permite que la varlist contenga variables categóricas.","code":""},{"path":"programar-ado---files.html","id":"opciones","chapter":"3 Programar ado - files","heading":"3.0.14 Opciones","text":"Las opciones permiten hacer requeridos o opcionales. Por ejemplo, regress, noconstant. Las opciones pueden ser una numlist, una varlist (ej. (varlist) option, una namelist tal como el nombre de una matriz o de una nueva variable. Como regla general, cualquier característica que pudieran encontrar en un comando de Stata, la pueden agregar en un programa. Para las opciones es importante recordar que las mayúsculas indican la menor abreviación posible.replace, detail, constant son opciones de .noreplace, nodetail, noconstant son opciones de . Ojo las macros que retornan tienen los mismos nombres que en el caso anterior.title y adjust también son otros opcionales. El primero permite ingresar un título al comando mientras que el segundo permite ajustar los resultados por algún escalar.Hay muchas otras. Veamos un ejemplo…","code":"capture program drop miprograma\nprogram miprograma\n    syntax varlist [if] [in] [, adjust(real 1) title(string)]\n    display \n    if \"`title'\" != \"\" {\n        displa \"`title':\"\n    }\n    foreach var of local varlist{\n        quietly summarize `var' `if' `in'\n        display \"`var'\" \" \"%9.0g r(mean)*`adjust'\n    }\nend"},{"path":"programar-ado---files.html","id":"programa-para-calcular-percentiles","chapter":"3 Programar ado - files","heading":"3.0.15 Programa para calcular percentiles","text":"Una versión simple para una variable:Algunas consideracionesEl programa anterior nos permite obtener los percentiles de alguna variable al mismo tiempo que los muestra en la consola.Noten que también incluido que tipo de comando es. En este caso es un comando r-class.Los escalares definidos en el programa se pueden utilizar. Esto siempre es conveniente.Como recomendación es bueno guardar los resultados esperados de los programas como variables temporales.Versión que incluye variables guardadas localmente:","code":"program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric)\n    quietly summarize `varlist', detail \n    scalar range = r(max) - r(min)\n    scalar p7525 = r(p75) - r(p25)\n    scalar p9010 = r(p90) - r(p10)\n    display as result _n \"Rangos de percentiles para `varlist'\"\n    display as txt \"75-25 : \" p7525\n    display as txt \"90-10:  \" p9010 \n    display as txt \"Range: \" range\nend program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric)\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010'\n    display as txt \"Range: \" `range'"},{"path":"programar-ado---files.html","id":"programa-con-opción-return","chapter":"3 Programar ado - files","heading":"3.0.16 Programa con opción return","text":"Una característica importante de los comandos de Stata es su capacidad de reportar los resultados de forma tal de que los usuarios y usuarias podamos utilizarlos posteriormente. El comando return nos permite guardar los escalares y hacerlos accesibles, sin tener el problema de los escalares que vimos en los ejemplos anteriores.Programa para calcular percentiles con return:Sobre la forma en que llamamos return:Notar que el lado izquierdo de scalar('r') se refiere al nombre de los elementos de la macro res. El lado derecho hace referencia la macro una vez más para extraer el valor almacenado en ese nombre. En este caso es importante notar que se deben utilizar dos comillas (ej. \"range\"). Noten también que si utilizamos scalar list hay resultados, sin embargo, al utilizar la opción return ahora podemos rescatar los resultados con return list.Ejercicio 3.6.3: PreguntasEscriba un programa llamado misuma que sea del tipo r-class.Este programa debe entregar en la lista de return list el número total de observaciones, la suma total y el promedio de la variable.Aplicarla en auto.dta.","code":"program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric)\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010 '\n    display as txt \"Range: \" `range'\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\nend     foreach r of local res {\n        return scalar `r' = ``r''\n    }"},{"path":"programar-ado---files.html","id":"agregar-opciones-al-programa","chapter":"3 Programar ado - files","heading":"3.0.17 Agregar opciones al programa","text":"Podemos agregar distintas opciones al programa. Para este ejemplo vamos agregar como opcional que el programa de un resultado en la consola. Recordemos que incluir \\([ ]\\) en syntax significa un componente opcional en el comando.Implementar opciones al programa (imprimir por defecto):Implementar opciones al programa (imprimir por defecto):Incluir un subconjunto de observacionesCualquier comando debiese incluir o range.Nuevamente, estas opciones son manejadas dentro del comando syntax. Para incluir estas opciones hay que agregar [] y [].Con estos comandos puedo ejecutar el programa para sub-muestras. Es importante asegurar que la sub-muestra este vacía.Para ello es importante calcular r(N), chequear que sea distinto de cero y agregar un título que lo indique.El comando marksample touse utiliza la información provista por o en caso de que estos sean indicados en el programa.Este comando genera una variable local touse que es igual 1 si las variables entran en el calculo que hace el programa y 0 en caso contrario.Utilizaremos `touse' para calcular el número de observaciones que se utilizan después de aplicar los condicionales.Es necesario agregar `touse' en cada parte del programa que trabaje con la variable de input.Incluir un subconjunto de observaciones:Ejercicio 3.6.2: PreguntasAjuste el programa del ejercicio 1 con el fin de incluir la opción . Guárdelo como misuma2.Aplicarla en auto.dta.","code":"program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric) [, PRINT]\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    if \"`print'\" == \"print\" {\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010 '\n    display as txt \"Range: \" `range'\n    }\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\nend program pctrange, rclass \nversion 17 \n    syntax varlist(max = 1 numeric) [, noPRINT]\n    local res \"range p7525 p9010\"\n    tempname `res'    \n    display as result _n \"Rangos de percentiles para `varlist'\"\n    quietly summarize `varlist', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    if \"`print'\" != \"noprint\" {\n    display as txt \"75-25 : \" `p7525'\n    display as txt \"90-10:  \" `p9010 '\n    display as txt \"Range: \" `range'\n    }\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\nend program drop _all \nprogram pctrange, rclass \nsyntax varlist(max = 1 numeric) [if] [in] [, noPRINT]\n    marksample touse \n    quietly count if `touse'\n    if `r(N)' == 0 {\n        error 2000\n    }\n    local res range p7525 p9010\n    tempname `res'    \n    \n    quietly summarize `varlist' if `touse', detail \n    scalar `range' = r(max) - r(min)\n    scalar `p7525' = r(p75) - r(p25)\n    scalar `p9010' = r(p90) - r(p10)\n    if \"`print'\" != \"noprint\" {\n        display as result _n \"Rangos de percentiles para `varlist', N = `r(N)'\"\n        display as txt \"75-25 : \" `p7525'\n        display as txt \"90-10:  \" `p9010 '\n        display as txt \"Range: \" `range'\n    }\n    foreach r of local res {\n        return scalar `r' = ``r''\n        }\n    return scalar N = r(N)\n    return local varname `varlist'\n\nend "},{"path":"programar-ado---files.html","id":"generalizar-el-comando-para-incluir-múltiples-variables-1","chapter":"3 Programar ado - files","heading":"3.0.18 Generalizar el comando para incluir múltiples variables","text":"Algunas consideracionesSi quiero ejecutar el programa sobre múltiples variables, es necesario ajustar un poco el programa.Tengo que indicarle syntax que hay más de una variable. Además, tengo que indicarle al programa que en caso de que existan más variables muestre los resultados en una tabla.Guardaremos los resultados en matrices (escalares) y vamos aplicar una función de macro extendida con el fin de contar el número de filas que esta matriz debiese tener.Agregaremos también una la opción format para cambiar el formato y la opción mat que permite la matriz ser guardada automáticamente con el nombre indicado.","code":"program drop _all \nprogram pctrange, rclass\nversion 17 \n    syntax varlist(min = 1 numeric ts) [if] [in] [, noPRINT FORmat(passthru) MATrix(string)] \n    marksample touse \n    quietly count if `touse'\n    if `r(N)' == 0 {\n        error 2000\n    }\n    local nvar: word count `varlist'\n    if `nvar' == 1 {\n        local res range p7525 p9010\n        tempname `res'    \n    \n        quietly summarize `varlist' if `touse', detail \n        scalar `range' = r(max) - r(min)\n        scalar `p7525' = r(p75) - r(p25)\n        scalar `p9010' = r(p90) - r(p10)\n        if \"`print'\" != \"noprint\" {\n            display as result _n \"Rangos de percentiles para `varlist', N = `r(N)'\"\n            display as txt \"75-25 : \" `p7525'\n            display as txt \"90-10:  \" `p9010 '\n            display as txt \"Range: \" `range'\n    }\n        foreach r of local res {\n        return scalar `r' = ``r''\n    }\n    return scalar N = r(N)\n    }\n    else {\n    tempname rmat \n      matrix `rmat' = J(`nvar', 3, .)\n      local i 0 \n        foreach v of varlist `varlist'{\n        local ++i \n        quietly summarize `v' if `touse', detail \n        matrix `rmat'[`i' ,1] = r(max) - r(min)\n        matrix `rmat'[`i', 2] = r(p75) - r(p25)\n        matrix `rmat'[`i', 3] = r(p90) - r(p10)\n        local rown `rown' `v'\n        }\n      matrix colnames `rmat' = Range P75-P25 P90-P10 \n      matrix rownames `rmat' = `rown' \n\n      if \"`print'\" != \"noprint\" {\n          local form \", noheader\"\n          if \"`format'\" != \"\" {\n                local form \"`form' `format'\" \n          }\n          matrix list `rmat' `form'\n                                }\n\n      if \"`matrix'\" != \"\" {\n            matrix `matrix' = `rmat'\n      }\n      return matrix rmat = `rmat'\n      }\n      return local varname `varlist'\nend"},{"path":"programar-ado---files.html","id":"agregar-prefijos-a-los-programas-1","chapter":"3 Programar ado - files","heading":"3.0.19 Agregar prefijos a los programas","text":"prefijo : Ahora vamos hacer que nuestro programa pueda utilizar el prefijo . Para agregar esta opción simplemente hay que modificar program.También podemos permitir que la lista de variables incluya operadores de series de tiempo (ej. L.pib, D.ingreso).","code":"program pctrange, rclass byable(recall)syntax varlist(min = 1 numeric ts) "},{"path":"programar-ado---files.html","id":"programas-para-complementar-función-egen-1","chapter":"3 Programar ado - files","heading":"3.0.20 Programas para complementar función egen","text":"Es posible programar funciones adicionales de egen (extended generate). El nombre de estos programas deben empezar con _g. Una diferencia entre este tipo de programas y los ya hechos es que en estos hay que tener en cuenta la nueva variable que se va crear. La sintaxis es la siguiente:El cambio que vamos hacer en la sintaxis del programa será que incluiremos un touse para hacer la nueva variable.Vamos escribir un programa para calcular un rango en particular:Programas con funciones de egen con prefijo :Agregamos [, *] que corresponde las opciones. En egenel prefijo es una opción.Con el fin de permitirle al programa que pueda producir un rango de percentil separado para distintos grupos utilizaremos pctile en vez de summarize.","code":"egen [type] newvarname = fcn(arguments) [if] [in] [, options]* Programas de egen \nprogram drop _all\nprogram _gpct9010\n    syntax newvarname =/exp [if] [in] \n    tempvar touse \n    mark `touse' `if' `in' \n    quietly summarize `exp' if `touse', detail \n    quietly generate `typlist' `varlist' = r(p90) - r(p10) if `touse'\nend\n\n* Aplicación del programa\nsysuse auto, clear\negen rango9010 = pct9010(price)* Programa con opción by. \nprogram drop _all\nprogram _gpct9010\nsyntax newvarname =/exp [if] [in] [, *] \ntempvar touse p90 p10 \nmark `touse' `if' `in' \nquietly {\n          egen double `p90' = pctile(`exp') if `touse', `options' p(90) \n          egen double `p10' = pctile(`exp') if `touse', `options' p(10)\n          generate `typlist' `varlist' = `p90' - `p10' if `touse' } \nend\n\n* Aplicación del programa\nsysuse auto, clear\negen rango9010 = pct9010(price)\nbysort rep78 foreign: egen rango9010_prefijoby = pct9010(price)"},{"path":"programar-ado---files.html","id":"generalización-de-la-función-egen","chapter":"3 Programar ado - files","heading":"3.0.21 Generalización de la función egen","text":"Hemos desarrollado una función de egen que permite calcular un rango entre percentiles para una lista de variables especifica. Puede ser útil tomar ventaja de egen pctile() para poder calcular cualquier percentil de la lista de variables especificadas. Vamos agregar dos opciones la función egen : lo() y hi(). En caso de que especifiquemos, por defecto se calcula el rango interquartil. La función de egen ahora se llamará _gpctrange.ado.Función egen más general para percentiles:","code":"* Programa con opción by. \nprogram drop _all\nprogram _gpctrange\nsyntax newvarname =/exp [if] [in] [, LO(integer 25) HI(integer 75) *] \n\nif `hi' > 99 | `lo' < 1 {\n    display as error ///\n        \"Percentiles `lo' `hi' deben estar entre 1 y 99.\" \n    error 198 \n}\nif `hi' <= `lo' {\n    display as error ///\n            \"Percentiles `lo' `hi' deben estar en orden ascendente\"\n    error 198 \n}\ntempvar touse phi plo \nmark `touse' `if' `in' \nquietly {\n          egen double `phi' = pctile(`exp') if `touse', `options' p(`hi') \n          egen double `plo' = pctile(`exp') if `touse', `options' p(`lo')\n          generate `typlist' `varlist' = `phi' - `plo' if `touse' \n          } \nend\n\nsysuse auto, clear\nbysort rep78: egen iqr = pctrange(price) if inrange(rep78,3,5)\nbysort rep78: egen p8020 = pctrange(price) if inrange(rep78,3,5), hi(80) lo(20)\nbysort rep78: egen p8020 = pctrange(price) if inrange(rep78,3,5) ///\n& foreign==1, hi(80) lo(20)\n\n* Utilizar estas nuevas variables con otros comandos\ntabstat iqr if inrange(rep78, 3, 5), by(rep78)\ntabstat p8020 if inrange(rep78, 3, 5), by(rep78)"},{"path":"programar-ado---files.html","id":"documentar-tu-programa","chapter":"3 Programar ado - files","heading":"3.0.22 Documentar tu programa","text":"Escribir un helpEs necesario y recomendado mantener una documentación de los programas que se escriban para un proyecto.Esta documentación debe estar actualizada e incluir cualquier modificación. Importante hacerlo mientras se hace el programa y al final.Vamos aprender un poco de SMCL (Stata Markup Control Language file). Básicamente es el lenguaje con el que se escriben los help en Stata. También es el lenguaje con el que se muestran los resultados de display.Los archivos se pueden escribir en cualquier procesador de texto, pero deben ser guardados en formato .smcl. También deben ser guardados en la misma carpeta en donde se encuentra el ado-file relacionado con el archivo.Hay que empezar los códigos con \\(\\{smcl\\}\\) con el fin de indicarle Stata que el texto que viene será en formato SMCL.Las etiquetas de SMCL van entre llaves (\\(\\{\\}\\)) y se pueden leer de dos formas principalmente:\n\\(\\{tag:text\\}\\) etiquetar el texto que se esta escribiendo.\n\\(\\{tag\\}\\) etiquetar todo lo que viene hasta que se cambie en otra parte del texto.\n\\(\\{tag:text\\}\\) etiquetar el texto que se esta escribiendo.\\(\\{tag\\}\\) etiquetar todo lo que viene hasta que se cambie en otra parte del texto.Por ejemplo, si quiero poner texto en itálica tengo que ocupar la etiqueta :Etiquetas de SMCLTexto: {}, {bf}, {sf}, {ul}. Itálica, negrita, texto normal, subrayado, repectivamente.Texto en formato Stata: {cmd}, {error}, {result}, {text}.{Destacar una referencia: {hi}.Opciones de comando: {opt}.Insertar linea horizontal: {hline}.Volver dejar el texto su estado normal: {reset}.Formato de documento: {title:text}, {center}, {ralign}, {lalign}, {tab}.Párrafos: Hay dos opciones para escribirlos.\n{p #1 #2 #3 #4}.\n{p} = {p 0 0 0 0}\n{p #1 #2 #3 #4}.{p} = {p 0 0 0 0}Los números indican los siguientes elementos de un párrafo:\nEl primer número (#1) es cuántos caracteres hay que sangrar en la primera línea.\nEl segundo número (#2) es cuántos caracteres hay que sangrar en la segunda y tercera línea.\nEl tercer número (#3) es cuán lejos de la derecha debe estar el margen.\nEl cuarto número (#4) es para el ancho total del párrafo.\n{phang}: es equivalente {p 4 8 2}.\n{pstd}: es equivalente {p 4 4 2}.\n{phang2}: es equivalente {p 8 12 2}.\n{p2col}: Para separar el texto en dos columnas.\n{p_end}: Para terminar un párrafo. Útil cuando tienes dos parrafos en formatos distintos.\nEl primer número (#1) es cuántos caracteres hay que sangrar en la primera línea.El segundo número (#2) es cuántos caracteres hay que sangrar en la segunda y tercera línea.El tercer número (#3) es cuán lejos de la derecha debe estar el margen.El cuarto número (#4) es para el ancho total del párrafo.{phang}: es equivalente {p 4 8 2}.{pstd}: es equivalente {p 4 4 2}.{phang2}: es equivalente {p 8 12 2}.{p2col}: Para separar el texto en dos columnas.{p_end}: Para terminar un párrafo. Útil cuando tienes dos parrafos en formatos distintos.","code":"* Una palabra/frase en particular\n{it:este texto aparecerá en itálica}\n\n* Todo el bloque de texto\n{it} Todo lo que este aquí aparecerá en itálica. ///\nEsto va a ocurrir hasta que aparezca un nuevo ///\ntipo de etiqueta. "},{"path":"programar-ado---files.html","id":"escribir-programas-e-class","chapter":"3 Programar ado - files","heading":"3.0.23 Escribir programas e-class","text":"Vamos aprender algunos elementos que nos van permitir escribir nuestros propios comandos de estimación en Stata. Muchos de los conceptos que hemos visto aplican también para este tipo de comandos. Es necesario recordar algunas convenciones que nos van ser útiles para poder escribir nuestros programas.Los resultados se guardan en |e() y se puede acceder ellos con ereturn list.El número de observaciones es e(N) y para identificar que observaciones fueron incluidas en la estimación es necesario utilizar la función e(sample).Los coeficientes estimados se guardan en un vector e(b) y la matriz de varianza covarianza se guarda en e(V).El comando ereturn name = exp retorna un escalar, mientras que ereturn local name value y ereturn matrix name matname retorna una macro y una matriz respectivamente.El comando ereturn post envía las estimaciones de b y V sus ubicaciones oficiales.Para devolver el vector de coeficientes y su matriz de varianza, es necesario crear el vector de coeficientes, digamos \\(beta\\), y su matriz de varianza-covarianza, digamos \\(vce\\).También podemos definir la muestra de estimación incluida en la estimación con touse. Ahora es posible guardar los elementos en e(). Por ejemplo, es posible utilizar ereturn scalar, ereturn local o ereturn matrix.Es conveniente utilizar los nombres típicamente asignados para guardar resultados de los programas e(df_m) o e(df_r). Sin embargo, se pueden nombrar como deseen.","code":"ereturn post `beta' 'vce', esample(`touse')"},{"path":"programar-ado---files.html","id":"marksample-mark-y-markout","chapter":"3 Programar ado - files","heading":"3.0.24 Marksample, Mark y Markout","text":"marksample y mark son alternativas. mark es muy utilizado. Ambos comandos crean un indicador que marcan que observaciones será utilizadas. Los ocupadmos en los programas en Stata. La idea es indicarle al programa la muestra relevante. markout marca la variable con un indicador igual 0 si cualquier variable en varlist indicada contiene un missing.Marksamplemarksample se utiliza en programas en los que los argumentos se analizan mediante el comando syntax.Crea una variable temporal, almacena el nombre de la variable temporal en un local, y rellena la variable temporal con 0 y 1 según si la observación debe ser utilizada.Markmark utiliza la variable temporal touse basada en las expresiones de e .Si hay expresiones de e , touse será 1 para cada observación en los datos.Si indico una condición, solo las observaciones que cumplan esta condición tendrán un 1 en touse.Mark actualiza touse de forma tal de que revisa missing.Mark y Markoutmark parte con una variable temporal previamente creada.markout modifica la variable creada por mark poniéndola cero en las observaciones que tienen valores perdidos registrados para cualquiera de las variables en varlist.Marksample vs. Markoutmarksample es mejor que mark. Disminuye la probabilidad de que se olvide alguna restricción.markout puede ser utilizados después de mark o bien marksample.markout también puede ser usado con marksample:Ejemplo Marksample y Markout:","code":"program ....\nsyntax ...\nmarksample touse\nrest of code .... if `touse'\nendprogram ....\ntempvar touse\nmark `touse' ...\nmarkout `touse' ...\nrest of code ... if `touse'\nendprogram ...\ntempvar touse\nmark `touse' ...\nmarkout `touse' ...\nrest of code ... if `touse'\nend\n\nprgraom myprog\nsyntax varlist [if] [in]\nmarksample touse\n...\nend\n\n* Equivale a: \nprogram myprog\nversion 17.0\nsyntax varlist [if] [in]\ntempvar touse\nmark `touse' `if' `in'\nmarkout `touse' `varlist'\n...\nendprogram ...\nsyntax ... [, Denom(varname) ... ]\nmarksample touse\nmarkout `touse' `denom'\nrest of code ... if `touse'\nendprogram cwsumm\nsyntax [varlist(fv ts)] [if] [in] [aweight fweight] [, Detail noFormat]\nmarksample touse\nsummarize `varlist' [`weight'`exp'] if `touse', `detail' `format'\nend"},{"path":"programar-ado---files.html","id":"sortpreserve","chapter":"3 Programar ado - files","heading":"3.0.25 Sortpreserve","text":"Si está escribiendo un programa de Stata que cambia temporalmente el orden de los datos y quieres que los datos se ordenen en su orden original al final de la ejecución, puede ahorrar un poco de programación incluyendo sortpreserve. Para ellos debemos escribir: program miprograma, sortpreserve. Stata automáticamente reordenara las variables como estaban originalmente. Al agregar esta opción se genera una variable temporal llamada _sortindex la que contiene el orden original de los datos.","code":""},{"path":"exportar-información.html","id":"exportar-información","chapter":"4 Exportar Información","heading":"4 Exportar Información","text":"Exportar información es parte importante del proceso de análisis de datos. En esta sección veremos algunas herramientas que permiten conectar los resultados del análisis hecho en Stata con los reportes que queremos crear en Word.","code":""},{"path":"exportar-información.html","id":"stata-markdown","chapter":"4 Exportar Información","heading":"4.0.1 Stata Markdown","text":"Herramienta para crear informes que sean reproducibles y en donde los datos, códigos y operaciones están conectadas.Básicamente una intersección entre texto narrativo yEs posible generar informes en distintos formatos (ej. html, docx, pdf). Hoy veremos como generar un informe en Word y una presentación en HTML.Stata Markdown es especialmente útil para:\nInformes rutinarios: Informe semanas/mensual sobre un conjunto de datos que se actualizan constantemente.\nDocumentar análisis: Es posible integrar reportes intermedios en un trabajo de análisis de datos. Estos informes pueden ser de utilidad para detectar errores y para supervisión en equipos de trabajo.\nInformes rutinarios: Informe semanas/mensual sobre un conjunto de datos que se actualizan constantemente.Documentar análisis: Es posible integrar reportes intermedios en un trabajo de análisis de datos. Estos informes pueden ser de utilidad para detectar errores y para supervisión en equipos de trabajo.Stata Markdown y otras herramientas similares son claves para mejorar en términos de . Algunos argumentos basados en :Ayuda documentación: En la fase de procesamiento de datos podemos describir todos los pasos utilizados para convertir los datos brutos en variables de análisis, produciendo un documento con un buen formato, más claro y legible. Es posible pensar en estos informes como productos intermedios.En la fase de análisis de los datos, podemos incluir el código, explicar las razones para probar determinados modelos, incluir los resultados, las tablas y las figuras, y comentar los resultados, todo ello sin tener que cortar y pegar de forma tediosa y propensa errores.En la fase de presentación, podemos elaborar un informe centrado en los resultados, con la opción de ocultar los comandos reales utilizados para que aparezcan en el documento final.Algunos conceptosMarkdown: Es un lenguaje que permite escribir documentos en texto plano. Los archivos escritos en Markdown tienen la extensión md.Stata Markdown: Es la variación especifica para Stata. Se implementa través del comando markstat hecho por Germán Rodriguez. Estos archivos tiene extensión .stmd.","code":""},{"path":"exportar-información.html","id":"instalación","chapter":"4 Exportar Información","heading":"4.0.1.1 Instalación","text":"En Stata, ejecuta estos comandos:ssc install markstatssc install whereisInstalar padcoc desde pandoc.org/installingDecirle markstat donde encontrar pandoc. En mi caso es:","code":"whereis pandoc \"C:\\Users\\nicol\\AppData\\Local\\Pandoc\\pandoc.exe\""},{"path":"exportar-información.html","id":"funcionalidades","chapter":"4 Exportar Información","heading":"4.0.1.2 Funcionalidades","text":"Algunos elementos que vamos utilizar en Stata Markdown:Ecuaciones y notación matemática utilizando (para los que lo sepan utilizar).Hacer títulos y encabezados.Enfatizar texto (negritas e itálicas).Armas listas numeradas y numeradas.Poder mostrar líneas de código, resultados directamente de Stata. Esto incluye Mata.Insertar quiebres de páginas: \\newpage.","code":""},{"path":"exportar-información.html","id":"generar-un-documento","chapter":"4 Exportar Información","heading":"4.0.1.3 Generar un documento","text":"Como mencionamos anteriormente es posible generar un documento de word, pdf o html que contenga todos los códigos, resultados, tablas y figuras hechas en Stata. Para generar un documento es necesario:Escribir en tu editor de códigos preferido un archivo en formato stmd. En nuestro caso escribiremos un archivo que se llama ejemplo1.stmd. Guardar el archivo en tu carpeta de trabajo.Ejecutar el comando markstat según el tipo de archivo que desee generar. En nuestro caso será:","code":"markstat using ejemplo1, strict docx"},{"path":"exportar-información.html","id":"putdocx","chapter":"4 Exportar Información","heading":"4.0.2 Putdocx","text":"La ventaja de putdocx radica en que es posible personalizar un poco más las tablas. Esto permite conectar el informe final con el trabajo de análisis de datos.","code":""},{"path":"exportar-información.html","id":"algunos-comandos-útiles","chapter":"4 Exportar Información","heading":"4.0.2.1 Algunos comandos útiles","text":"Crear, pegar y guardar documentos:putdocx begin: Crea un archivo docx para exportar.putdocx describe: Describe los contenidos de archivo.putdocx save: Guarda y cierra el archivo.putdocx clear: Cierra el archivo sin guardar.putdocx append: Combina el contenido de múltiples archivos.Insertar quiebres de páginas:\n* putdocx pagebreak: Agrega una nueva página.\n* putdocx sectionbreak: Agrega una nueva sección.Agregar párrafos con texto e imágenes:\n* putdocx paragraph: Agrega un nuevo párrafo.\n* putdocx text: Agrega un bloque de texto un párrafo.\n* putdocx image: Agrega una imagen al párrafo.","code":""},{"path":"exportar-información.html","id":"tablas","chapter":"4 Exportar Información","heading":"4.0.2.2 Tablas:","text":"putdocx table: Crea una nueva tabla en el documento la que puede contener resultados de estimaciones, estadística descriptiva o datos.\nputdocx sectionbreak: Agrega una nueva sección.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"manejo-de-grandes-bases-de-datos-en-stata","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5 Manejo de grandes bases de datos en Stata","text":"","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"estilo-de-codificación-mejorar-velocidad-y-eficiencia","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.1 Estilo de codificación: mejorar velocidad y eficiencia","text":"Esta introducción esta basada en Suggestions Stata programming style de Nicholas J. Cox.Un buen estilo de codificación es por sobre todo claridad. Lo más importante es tener una estrategia y seguirla. En esta sección nos enfocaremos en algunos consejos de codificación que nos van permitir mejorar la velocidad y eficiencia en el uso de Stata. Este punto es especialmente relevante cuando se trabaja con grandes bases de datos. Una lista de formas básicas de aumentar la velocidad y eficiencia al manejar grandes bases de datos:Testear siempre las condiciones claves. Hacerlo lo antes posible.Utilizar summarize, meanonly cuando solo necesite este valor. Como regla general siempre de preguntarme si lo que estoy obteniendo es útil o .Preferir foreach y forvalues sobre . Son más rápidos.Evitar el uso de macro shift. Con muchas variables, se vuelve muy lento. Mejor ocuapar un forvalues.Evitar siempre que sea posible iterar sobre observaciones. Mata puede ser útil en este aspecto.Evite usar preserve si es posible. Es atractivo para el programar pero puede ser costoso en tiempo cuando se utilizan grandes bases de datos. Es bueno profundizar en el uso de marksample con el fin de hacer programas efectivos.Las variables temporales se eliminarán automáticamente al final de un programa, pero también considere la posibilidad de eliminarlas cuando ya sean necesarias para minimizar la sobrecarga de memoria y reducir las posibilidades de que su programa se detenga porque hay espacio para añadir más variables.Especifique el tipo de las variables temporales para minimizar la sobrecarga de memoria. Si se puede utilizar una variable de bytes, especifique: generate bytes 'myvar' en lugar de dejar que se utilice el tipo por defecto, que desperdiciaría espacio de almacenamiento.Evite utilizar una variable para mantener una constante; una macro o un escalar suele ser todo lo que se necesita.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"cargar-grandes-bases-de-datos","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.2 Cargar grandes bases de datos","text":"Cuatro aspectos que considerar siempre que se desee cargar en Stata alguna base de datos de un tamaño considerable:¿Necesita todas las variables del conjunto de datos? Si es así, cargue sólo las variables que necesite:¿Necesita todas las observaciones del conjunto de datos? Si es así, importe sólo las observaciones que necesite:¿Su conjunto de datos ocupa más espacio de almacenamiento del necesario?:Intenta leer tu conjunto de datos poco poco y optimizarlo.Además de solo importar determinadas observaciones o variables, se optimiza el espacio de almacenamiento utilizado compress.Es posible inspeccionar la base de datos sin cargarla.¿Su conjunto de datos contiene muchas observaciones idénticas?: Debe transformar el conjunto de datos en un conjunto de datos ponderado por frecuencia.","code":"use var1 var2 var3 var4 using data1, clearuse data1 if state <= 9, cleardescribe using data.1.dta"},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"reducir-el-uso-de-memoria","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.3 Reducir el uso de memoria","text":"Cuando usted trabaja con un conjunto de datos en Stata, Stata debe cargar todo el conjunto de datos en la memoria de la computadora (RAM). Afortunadamente, las computadoras portátiles de hoy tienen más memoria que la mayoría de los servidores de hace 20 años, y la mayoría de la gente nunca tiene que preocuparse por la cantidad de memoria que Stata está utilizando.¿tengo que preocupar de la memoria?Sólo tienes que preocuparte por la memoria si el tamaño de tu conjunto de datos se aproxima la cantidad de memoria del ordenador que utilizas, y si es mayor, definitivamente tienes un problema.Si usted trabaja con grandes conjuntos de datos, debe tener cuidado: tratar de usar más memoria de la que tiene terminará mal.¿Cuando es mucho?: un Laptop tipico tiene 16gb.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"reducir-el-tamaño-de-la-base-de-datos","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.4 Reducir el tamaño de la base de datos","text":"Elimina datos innecesarios. Utiliza tipos de variables pequeños: help datatypes. Siempre que crees una variable es una buena practica especificar el tipo de dato. Acortar cadenas o codificarlas: strings requieren un byte por caracteres.Sin embargo, para las observaciones todas tienen el mismo tamaño. Si tengo una variable que contiene: “Si”, “”, “lo se”. La variable utilizara 8 bytes por observación tal como si solo tuviese “lo se”. Si tu cambias “lo se” por “ns”. Ahora solo se utilizaran 2 bytes por observacion. Si tu cambias : “S”, “N”, “” solo utilizara un byte por observación.Codificar la variable de string como una variable númerica tambien reduce el espacio en memerio un byte por observación. Se recomiendo agregar labels y trabajar los string de esta forma cuando sea posible.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"eliminar-siempre-resultados-intermedios-incluso-temporales","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.5 Eliminar siempre resultados intermedios, incluso temporales","text":"Elimina resultados intermedios. Si creas variables para almacenar resultados intermedios, elimínelas tan pronto como haya terminado con ellas.Por ejemplo, el siguiente código crea una variable llamada incomePovertyRatio sólo para poder crear una variable indicadora lowIncome que identifica los sujetos cuyos ingresos son inferiores al 150% del nivel de pobrezaDebes eliminar la variable que utilizas. Hacer esto siempre.","code":"gen incomePovertyRatio = income/povertyLevel\ngen lowIncome = (incomePovertyRatio < 1.5)\ndrop incomePovertyRatio"},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"dividir-en-trozos-cuando-sea-posible","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.6 Dividir en trozos cuando sea posible","text":"Si un conjunto de datos es demasiado grande para cargarlo en la memoria, para algunas tareas puede dividirlo en un conjunto de conjuntos de datos más pequeños y trabajar con ellos de uno en uno. Puede haber una variable categórica en el conjunto de datos de tal manera que un conjunto de datos separado para cada categoría funcionaría bien, o puede dividirlo por número de observación.Dividir el conjunto de datos en trozos más pequeños probablemente sólo tiene sentido si puedes reducir el tamaño de cada trozo para que al final puedas combinarlos todos en un único conjunto de datos que pueda cargarse en la memoria.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"sort","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.7 Sort","text":"sort en Stata es razonablemente eficiente: un millón de valores aleatorios pueden ponerse en orden creciente en menos de 3 segundos con sort x. Sin embargo, el comando tiene una opción inversa para ordenar de mayor menor.gsort hace una ordenación decreciente de forma ineficiente - ordena de forma creciente en x, y luego ordena de forma creciente en menos _n. Esencialmente está haciendo:Es mejor negar por usted mismo antes de una ordenación creciente:","code":"  sort x\n  gen long sortvar = -_n\n  sort sortvar\n  drop sortvargenerate negx = -x\n  sort negx"},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"selección-de-muestra","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.8 Selección de muestra","text":"Separar la selección de variables de la inclusión. Para minimizar la cantidad de memoria utilizada, necesitamos separar la decisión de selección de la muestra de la decisión de inclusión de la variable.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"precaución-con-reshape","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.9 Precaución con reshape","text":"El comando reshape es inexplicablemente lento. 13 segundos por millón de observaciones en mi computadora. Es importante pensar en codificación y buscar más opciones para hacer reshape. Se puede escribir un archivo separado para cada año de datos, y luego concatenarlos en un largo conjunto de datos en unos 2 segundos.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"subexpresiones-comunes","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.10 Subexpresiones comunes","text":"menudo, varias sentencias generate o replace tendrán subexpresiones comunes.Una mejor opcion es precalcularlo.","code":"generate y = a if c==d & e==f\ngenerate x = b if c==d & e==fgenerate smpl = c==d & e==f\ngenerate y = a if smpl\ngenerate x = b if smpl"},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"recomendaciones-adicionales","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.11 Recomendaciones Adicionales","text":"","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"collapse","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.11.1 Collapse","text":"collapse es muy rápido. El autor, sin duda, supuso que aunque se utilizara con grandes conjuntos de datos, estaría dentro de una iteración. Pero veces lo está, y puede convertirse en el paso limitante de la velocidad en un programa de larga duración. Se puede reemplazar fácilmente con código más rápido, pero el beneficio total es tan grande como uno esperaría. Mejor ocupar gtools.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"egen","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.11.2 Egen","text":"egen tambien puede ser adecuado para que funcione más rapido. Vamos ver un ejemplo calculando el máximo de una variable.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"regresiones-en-sub-grupos","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.11.3 Regresiones en sub-grupos","text":"Exiten varias opciones para calcular regresiones según el tipo de datos que queramos incluir en nuestra muestra. Vamos ver distintas opciones y ver su desempeño en bases de datos grandes.","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"recode","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.11.4 Recode","text":"recode puede ser modificado utilizando matrices. Otra opción es utilizar ggtools","code":""},{"path":"manejo-de-grandes-bases-de-datos-en-stata.html","id":"resumen","chapter":"5 Manejo de grandes bases de datos en Stata","heading":"5.0.12 Resumen","text":"Utilizar compress.\nMantener solo las variables que se van utilizar.Mantener solo las observaciones que se van utilizar.Cargar solo las variables y observaciones necesarias.keepusing y nogen siempre para merge.ftools y gtools (más detalles sección final).Utilizar parallel (más detalles sección final).","code":""},{"path":"mata.html","id":"mata","chapter":"6 Mata","heading":"6 Mata","text":"","code":""},{"path":"mata.html","id":"qué-es-mata","chapter":"6 Mata","heading":"6.1 ¿Qué es Mata?","text":"Esta introducción esta basada en la guía de Asjad Naqvi.Mata es un lenguaje de programación matricial.Es más rápido que Stata, pero necesita más precisión (ej. dimensiones de las matrices).Muchos de los operadores de Mata son similares R o Matlab. Adicionalmente, incluye sus propios optimizadores y funciones útiles.Esta sección será una pequeña introducción al lenguaje. Tiene como objetivo mostrarles el abanico de herramientas que Mata posee con el fin de que puedan utilizarlo para hacer sus propios programas.La sintaxis básica de MataSi son utilizados dos puntos y ocurre un error, Mata será abortado, volverá Stata y aparecerá un mensaje de error.Para buscar ayuda, podemos hacer:Comandos básicos:Al igual que Stata, Mata posee sus propios comandos. Algunos útiles de recordar son:Para hacer comentarios en Mata hay que utilizar //. funcionan los asteriscos como en Stata. Todo los que se genera en Mata queda en la memoria menos que Stata se cierre, se use clear allen Stata o bien mata: mata clear. Todas las matrices y funciones definidas se mantienen en el espacio de trabajo de Mata espacio de trabajo cuando se termina mata y se puede acceder ellas cuando se vuelve entrar en\nmata.","code":"* Opción 1 \nmata \n\n< comando de mata 1 > \n< comando de mata 2 >\n< y así .... > \nend \n\n* Opción 2 \nmata <un comando de mata>\n \n* Opción 3 : la versión estricta\nmata: <un comando de mata> \n  * Ejemplo 1\nmata \nemat = 7 + 3\nemat \nend\n\n* Ejemplo 2\nmata \nemat = (\"Josefa\", \"Perez\")\nemat = (21\\8)\nmmat = (17\\6)\nvmat = (25,3\\3,11)\nemat, mmat, vmat \nend* Ejemplo 1\nhelp mata comando\nhelp mata cholesky()mata describe \nmata clear \nmata rename nombre\nmata drop nombre1 nombre2\nmata stata\n\n* Ejemplo\nmata \nmata clear \nemat = (\"Josefa\", \"Perez\")\nemat = (21\\8)\nmmat = (17\\6)\nvmat = (25,3\\3,11)\nmata describe \nemat, mmat, vmat\nmata rename vmat vmat_renombrada\nmata drop mmat\nemat, vmat_renombrada\nend"},{"path":"mata.html","id":"matrices","chapter":"6 Mata","heading":"6.1.0.1 Matrices","text":"Una forma común de generar una matriz en Mata es importándola desde los datos de Stata.Definimos la matriz X, que tiene todas las observaciones (filas) y dos columnas var1 y var2. st_ permite Mata interactuar con la interfaz de Stata con el fin de pasar información de un lado otro. Revisar help m4_stata.Otra opción es definir matrices dentro de Mata.Definimos la matriz , la cual podemos ver escribiendo mata . Veremos una matriz cuadrada de 2x2. Notar que la coma separa elementos entre columnas mientras que el slash mueve elementos entre filas. El uso de paréntesis es opcional en Mata, pero es más conveniente y es una buena práctica de estilo utilizarlos. También se pueden definir matrices especiales.Finalmente, uno puede generar matrices similar lo que se hace en Stata.Este comando genera una matriz de 2x2 rellena de unos. La sintaxis genérica es: mata J(filas, columnas, constante). Para definir una matriz identidad: mata (5).Otro operador útil es range:Crea un vector que empieza en 1 y termina en 7 saltándose de 2 en 2 (es decir, (1,3,5,7)). Si quiero generar una matriz con números aleatorios provenientes de una distribución uniforme (0,1) : mata runiform(3,3).Accediendo los elementos de una matrizPodemos acceder los elementos de una matriz.\nmata [1,2]: fila 1, columna 2.\nmata [1,.]: Todas las columnas de la fila 1.\nmata [.,2]: Todas las filas de la columna 2.\nmata [1,2]: fila 1, columna 2.mata [1,.]: Todas las columnas de la fila 1.mata [.,2]: Todas las filas de la columna 2.También podemos extraer sub-conjuntos de elementos:Pegando elementos de matrices:,B las pega horizontalmente, es decir por columnas (filas son las mismas). \\B las pega vertical, es decir por filas (columnas son las mismas).Operaciones básicas entre matrices y por elementos: Una vez definidas las matrices, también se dedica bastante tiempo sumar, restar, multiplicar y dividir matrices entre sí. Vamos ver algunas operaciones entre matrices y también entre elementos.* B es una operación entre matrices, de forma tal que el número de filas de debe ser igual al número de columnas de B. Si queremos que cada elemento de sea multiplicado por el mismo elemento en B, es necesario trasponer alguna matriz y cambiar el operador.En el primer caso la matriz resultante corresponde una matriz de 3x2, en el segundo caso es una matriz de 2x3.Operadores lógicos y funciones sobre escalares: Las matrices también pueden ser comparadas través de operadores lógicos. == B chequea si dos matrices son iguales. Esto también puede hacerse elemento por elemento (por ejemplo :>=B). Tambien se puede utilizar !=.Mata también tiene funciones sobre escalares:Algunas funciones para operar en matrices son:Mata contiene varias funciones para extraer propiedades de las matrices:Mata contiene varias funciones para operar o manipular dos matrices:Al igual que Stata, Mata puede utilizar iteradores. loop:Loops:Los valores iniciales y finales pueden extraerse de algunas declaraciones condicionales como las dimensiones de las matrices. Mata sólo permite incrementos de 1.Notar que aquí es bien distinto Stata:/else son:Mata también permite abreviar la sintaxis en caso de que existan dos o mas condiciones.“” es la condición, “b” es el valor en caso de que sea verdadero y “c” en caso de que sea falsa. Para el ejemplo, la condición sería: (x > ? 1 : 0).","code":"mata: X = st_data(.,(\"var1\", \"var2\"))* Ejemplo 4: confeccionar matrices en base a mata\nsysuse auto.dta\nmata\nmata clear \nmata: X = st_data(.,(\"price\", \"mpg\"))\nX\nmata describe\nend mata A = (1,2 \\ 3,4)* Vector fila o Vector columna\nmata 1,2,3\nmata 1\\2\\3\n\n* Vector fila o Vector columna del 1 al 4\nmata 1..4\nmata 1::4mata J(2,2,1)mata range(1,7,2)* Empezar de fila 2 columna 1 hasta fila 3 columna 3\nmata A[|2,1\\3,3|]       \n* Tomar filas 2 y 3 y columnas de la 1 a la 3.\nmata A[(2::3),(1..3)]   mata\nA = runiform(1,2)\nB = runiform(1,2)\n\n* Opción 1 : pegar horizontalmente\nA,B   \n* Opción 2 : pegar verticalmente\nA\\B   \nendmata\n A = (1,2,3\\4,5,6)\n B = (2,3\\4,5\\6,7)\n A * B\nendmata\n A = (1,2,3\\4,5,6)\n B = (2,3\\4,5\\6,7)\n  A' :* B\n  A  :* B'\nendmata\n  X = (-2, 1 \\ 0, 5)\n  abs(X)   // valor absoluto de cada elemento\n  sign(X)  // signo de cada elemento\n  exp(X)   // exponencial de cada elemento\n  sqrt(X)  // raíz cuadrada de cada elemento\n  sin(X)   // seno de cada elemento\nendmata\nA = (5,4\\6,7)\nB = (1,6\\3,2)\nend\n\n* Número de filas y columnas \nmata rows(A)\nmata cols(A)\n\n* Suma de las filas/columnas\nmata rowsum(B)\nmata colsum(B)\n\n* Calcular el promedio (retorna un vector fila)\nmata mean(B')' \n\n* Seleccionar todas las columnas sin ceros\nmata D = (1,0,2,3,0)\nmata selectindex(D) \n\n* Select para condiciones más flexibles\nselect(B, B[.,1]:>2)\n\n* Ordenar matrices \nmata sort(B,2)  // Ordena según la columna 2 de B\nmata jumble(B) // Aleatoriza las filas de Bmata\nA = (5,4\\6,7)\ndet(A)         // Determinante de A\ninvsym(A)      // Ibversa de A\ntrace(A)       // Traza de A\nrank(A)        // Rango de A\nnorm(A)        // Norma de A\nX=.            // Vectores propios\nL=.            // Valores propios\nendmata\nA = (5,4\\6,7)\nB = (1,6\\3,2)\nA' * B            // A x B\ncross(A,B)        // A x B using a solver (faster)\nendmata\n  x = 1                  // Valor inicial\n  X = 4                  // Valor final\n  while (x <= X) {       // Iniciar condición del while\n  printf(\"\\%g \\n\", x)     // Alguna operación de Mata aquí\n  x++                    // Incremental\n  }                      // Terminar el while\nend* Este for\nfor (expr1; expr2; expr3) {\nstmts\n}\n* Es equivalente a \nexpr1\nwhile (expr2) {\nstmt\nexpr3\n}mata\n   N = 4                      // Valor final\n   for (i=1; i<=N; i++) {     // for loop \n   printf(\"%g \\n\", i)         // some Mata operation here\n   }                          // end for loop\nendmata\n x = 3                       // Valor de x\n   \n  for (i=1; i<=5; i++) {     // Empezar el loop\n    if (x > i) {             // if\n      printf(\"\\%g\\n\", 0)      // Comando de mata si la condición se cumple\n      }\n    else {                   //  en caso contrario\n      printf(\"\\%g\\n\", 1)      // ejecutar este comando de mata\n      }\n  }\nend(a ? b : c)"},{"path":"mata.html","id":"de-stata-a-mata","chapter":"6 Mata","heading":"6.1.0.2 De Stata a Mata","text":"Estimar un MCO: Un ejemplo estándar es hacer un simple OLS en Mata.LoopsEn Mata, la variable dependiente se importa como el vector y las variables independientes se importan como la matriz X. esta matriz X se le añade un vector columna de unos para el intercepto.Como el vector de unos debe tener el mismo número de filas que la matriz X, nótese el uso del operador J().En el último paso, las betas y los errores estándar se exportan Stata como matrices de Stata.","code":"sysuse auto, clear\nmata\n y = st_data(.,\"price\")\n X = st_data(.,(\"mpg\", \"weight\"))\n X = X, J(rows(X),1,1)\n    beta   = invsym(cross(X,X))*cross(X,y)\n    esq    = (y - X*beta) :^ 2\n    V      = (sum(esq)/(rows(X)-cols(X)))*invsym(cross(X,X))\n    stderr = sqrt(diagonal(V))\n      st_matrix(\"b\", beta)\n      st_matrix(\"se\", stderr)\nend"},{"path":"mata.html","id":"funciones-en-mata","chapter":"6 Mata","heading":"6.1.0.3 Funciones en Mata","text":"Vector: Un ejemplo estándar es hacer un simple OLS en Mata.Matriz:Hacer opcionales:Guardar:Dos referencias recomendadas para profundizar en Mata:Introduction Stata Programming, Second Edition ()Mata Book: Book Serious Programmers Want ()","code":"* Generar un vector \nmata \nfunction zeros(c)\n{\n    a = J(c, 1, 0)\n    return(a)\n}\nb = zeros(3)\nb\nend * Generar una matriz\nmata \nfunction zerosv1(real scalar c, real scalar r)\n{\nreal matrix A \nA = J(c,r,0)\nreturn(A)\n}\nb = zerosv1(3,2)\nb\nend mata \nfunction zerosv2(real scalar c,| real scalar r)\n{\nreal matrix A\nif (args()==1) r = 1\nA = J(c, r, 0)\nreturn(A)\n}\nend \n\nmata \nzerosv2(3,2)\nend\n\nhelp m2 syntaxmata \nfunction zerosfinal(real scalar c,| real scalar r)\n{\nreal matrix A\nif (args()==1) r = 1\nA = J(c, r, 0)\nreturn(A)\n}\n\nmata stata cd \"\\$ejercicios\"\nmata mosave zerosfinal(), replace\n\nend "},{"path":"análisis-de-datos-con-python.html","id":"análisis-de-datos-con-python","chapter":"7 Análisis de Datos con Python","heading":"7 Análisis de Datos con Python","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"introducción-al-análisis-de-datos-con-python","chapter":"7 Análisis de Datos con Python","heading":"7.1 Introducción al análisis de datos con Python","text":"Objetivos de este curso:Perderle el miedo Python.Complementar el análisis de datos utilizando las ventajas que da Python.Introducirlos al programa de forma tal de que puedan ir aprendiendo cosas nuevas por su cuenta.Ventajas de Python respecto otros programasCódigo abierto.Paquetes muy potentes y en continua actualización.Múltiples usos: astronomía, física, diseños sitios web, economía, análisis de datos.Muy demandando en el mercado laboral.Rapido e intuitivoPotente con grandes bases de datos: conectar servidores externos como Google Cloud.Algunas referencias:Python Data Science Handbook Jake VanderPlas (O’Reilly). Copyright 2016 Jake VanderPlas, 978-1-491-91205-8.","code":""},{"path":"análisis-de-datos-con-python.html","id":"antes-de-partir-con-python","chapter":"7 Análisis de Datos con Python","heading":"7.1.1 Antes de partir con Python…","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"instalar-python-jupyter-notebook-y-spyder","chapter":"7 Análisis de Datos con Python","heading":"7.1.1.1 Instalar Python, Jupyter Notebook y Spyder","text":"Utilizaremos Anaconda y Python 3.Utilizaremos Anaconda y Python 3.Aunque hay varias maneras de instalar Python, la que yo sugeriría es través de la distribución multiplataforma Anaconda. Pueden encontrarlo en la página de [Anaconda] (https://www.continuum.io/downloads). Incluye tanto Python y otros paquetes preinstalados orientados la computación científica.Aunque hay varias maneras de instalar Python, la que yo sugeriría es través de la distribución multiplataforma Anaconda. Pueden encontrarlo en la página de [Anaconda] (https://www.continuum.io/downloads). Incluye tanto Python y otros paquetes preinstalados orientados la computación científica.Si tuvieron algún problema con la instalación, les recomiendo ver video.. Explica detalladamente comom instalar Python con Ananconda.Si tuvieron algún problema con la instalación, les recomiendo ver video.. Explica detalladamente comom instalar Python con Ananconda.Para programas vamos utilizar Jupyter Notebook para algunas clases y Spyder para otras clases. Pueden instalar Spyder directamente desde el navegador de Anaconda. La idea es que se familiarizen con la interfaz de ambos programas. Jupyter Notebook es útil para trabajar interactivamente, conectarse servidores y trabajar en equipo más sencillmente. Spyder es útil cuando trabajamos con bases de datos debido que la interfaz permite una programación y análisis mucho más cómodos.Para programas vamos utilizar Jupyter Notebook para algunas clases y Spyder para otras clases. Pueden instalar Spyder directamente desde el navegador de Anaconda. La idea es que se familiarizen con la interfaz de ambos programas. Jupyter Notebook es útil para trabajar interactivamente, conectarse servidores y trabajar en equipo más sencillmente. Spyder es útil cuando trabajamos con bases de datos debido que la interfaz permite una programación y análisis mucho más cómodos.Jupyter Notebook es útil para desarrollar, compartir y generar procesos reproducibles, pero mi molesta poder ver los datos. Por lo que hay ocasiones en la que prefiero utilizar Spyder, más cuando estoy haciendo algún trabajo más exploratorio.Jupyter Notebook es útil para desarrollar, compartir y generar procesos reproducibles, pero mi molesta poder ver los datos. Por lo que hay ocasiones en la que prefiero utilizar Spyder, más cuando estoy haciendo algún trabajo más exploratorio.","code":""},{"path":"análisis-de-datos-con-python.html","id":"markdown-para-jupyter-notebook","chapter":"7 Análisis de Datos con Python","heading":"7.1.1.2 Markdown para Jupyter Notebook","text":"Markdown es un lenguage de programación de texto plano.Se puede utilizar en múltiples lenguajes de programación: por ejemplo, R Markdown y Stata Markdown.Permite combinar texto plano, codificación y resultados del análisis de datos como figuras y/o tablas en un solo lugar.Aprenderemos escribir en Markdown de forma tal de mejorar la presentación de los códigos. Les servirá tambíen si es que utilzan R o Stata.Es preferible ver esto al inicio de forma tal de que podamos ir incorporando esta codificación lo largo del curso.1. TítulosLos títulos pueden ser creados utilizando\nlos símbolos ‘##’. Estos símbolos determinan el tamaño del título. Van de 1 6.Enfatizar textoPara enfatizar texto tenemos las opciones más típicascodigo aquí para escribir códigos. Lo que hacen estos símbolos es que esto paresca una función o comando escrita como codigo. Esto es útil si es que quieres escribir nombres de paquetes en un informe. Nombre del paquete. Por ejemplo paquete statsmodel.codigo aquí para escribir códigos. Lo que hacen estos símbolos es que esto paresca una función o comando escrita como codigo. Esto es útil si es que quieres escribir nombres de paquetes en un informe. Nombre del paquete. Por ejemplo paquete statsmodel.Para utilizar itálicas y subrayar.Para utilizar itálicas y subrayar.Tambien se pueden escribir de otra forma negrita, negrita, italica o bien italica, o bien negrita e itálica o negritaeitalica.Tambien se pueden escribir de otra forma negrita, negrita, italica o bien italica, o bien negrita e itálica o negritaeitalica.Para tachar textoPara tachar textoEscribir parrafos:Para escribir parrafos tengo que colocar <p> al inicio y <\/p> al final. Para hacer espacios entre parrafos simplemente presionar enter.\n___\n4. Escribir ecuacionesSe pueden escribir ecuaciones (para los/que manejan Latex)\\[= \\pi*r^{2}\\]Colocar citas anidadasTodo lo que ponga aquí se vera como una cita\n> Todo lo que ponga aquí se vera como una cita\n>> Todo lo que ponga aquí se vera como una citaLineas horizontalesEstas líneas sirven para separar los parrafos y así tener un texto mucho más ordenado.Crear listasListas ordenadasitem 1item 2importante agregar un espacio entre +! . Sino R lo reconocerá.item 3\nsub - item 1\nsub - item 1Listas ordenadasitem 1item 2Ejemplo 3: Letras funcionan para listasA. List\nB. List\n+ itemListList6. Agregar un linkOpción 1: Mostrar linkhttps://rmarkdown.rstudio.com/authoring_quick_tour.html#Using_ParametersOpción 2: Ocultar link\naquí7. Agregar una imagen8. Cambiar colores de frases Cambiar el color del texto   Tambien se puede especificar el color con códigos. ( Nota : Paletas de colores pueden encontrar ([aquí.])(https://www.colourlovers.com/) Otros colores: blue|red|green|pink|yellow.9. Agregar listas de tareasalguna tarea otra tarea10. TablasPara hacer tablas se utilizan “|” y “-”.Para cambiar manualmente la jusfiticación del texto:\n:-:\n1. Usar :-: para centrar\n2. Usar — para la derecha\n3. Usar :- para la izquierda11. Cajas de coloresEjemplos o advertencias: Utilice los cuadros amarillos para los ejemplos que están dentro de las celdas de código, o utilícelos para las fórmulas matemáticas si es necesario. Normalmente también se utiliza para mostrar mensajes de advertencia.Algo positivo: Este cuadro de alerta indica una acción exitosa o positiva.Peligro: Este cuadro de alerta indica una acción peligrosa o potencialmente negativa.12. Destacar alguna líneaÚtil para resaltar y captar la atención del lector hacia determinados puntos.13. Marcar algún texto con un color determinadoNo olvides comprar leche hoy.14. Menu de navegación","code":""},{"path":"análisis-de-datos-con-python.html","id":"atajos","chapter":"7 Análisis de Datos con Python","heading":"7.1.2 Atajos","text":"Muy necesarios para ganar tiempo y duplicar tareas, por ejemplo, cambiar de Markdown código.Nos vamos ir acostumbrando poco poco con la práctica.Para usuarios de MacCtrl: command key ⌘Shift: Shift ⇧Alt: option ⌥ModosExisten dos modos: comando y edición.El modo de edición le permite escribir código o texto en una celda y se indica con un borde de celda verde.El modo de comando vincula el teclado los comandos del nivel del cuaderno y se indica con un borde de celda gris con un margen izquierdo azul.Algunos atajos que funcionan en ambos modosShift + Enter: ejecuta la celda seleccionada y pasa la de abajo.Ctrl + Enter: ejecuta todas las celdas seleccionadas.Alt + Enter: ejecuta una celda, pero inserta una abajo.Ctrl + S: guardar el código.Atajos: modo comando (presionar esc para activar)H : mostrar todos los accesos directos.arriba: seleccionar la celda de arriba.abajo: seleccionar la celda de abajo.Shift + arriba: extender las celdas seleccionadas arriba.Shift + abajo : extender las celdas seleccionadas por debajo.: insertar celda arriba.B: insertar celda abajo.X: cortar las celdas seleccionadas.C: copiar celdas seleccionadas.V : pegar celdas abajo.Shift + V : pegar celdas arriba.D: (pulsar la tecla dos veces) borrar las celdas seleccionadas.Z: deshacer el borrado de celdas.S: guardar y comprobar.Y: cambiar el tipo de celda Código.M: cambiar el tipo de celda Markdown.P: abrir la paleta de comandos. Muy útil para ejecutar directamente atajos.Shift + espacio: desplazar el cuaderno hacia arriba.espacio: para desplazar el cuaderno hacia abajo.Alt + enter: ejecutar linea y agregar una nueva.Atajos: modo edicion(presionar enter para activar)Esc : te lleva al modo comando.tab : para completar el código o la sangría.Ctrl + : seleccionar todo.Ctrl + Z: deshacer.Ctrl + Y: rehacer.Ctrl + flecha izquierda: ir una palabra la izquierda.Ctrl + flecha derecha: ir una palabra la derecha.Ctrl + Shift + p abrir la paleta de comandos.Agregar tus propios atajosHelp > Edit Keyboard Shortcuts","code":""},{"path":"análisis-de-datos-con-python.html","id":"extensiones-de-jupyter-notebook","chapter":"7 Análisis de Datos con Python","heading":"7.1.3 Extensiones de Jupyter Notebook","text":"Es posible agregar caracteristicas adicionales Jupyter Notebook.Para ello inclimos en el Promp lo siguiente:pip install jupyter_contrib_nbextensions && jupyter contrib nbextension installVarias extensiones útiles e interesantes: índice, ocultar código, observar variables, etc.","code":""},{"path":"análisis-de-datos-con-python.html","id":"sintaxis-básica","chapter":"7 Análisis de Datos con Python","heading":"7.1.4 Sintaxis básica","text":"Comentarios marcados con #. Notar que tambíen se pueden escribir al lado derecho de los códigos.Se asignan variables con signo igual. Similar Stata, casi similar R.En Python, si quiero avanzar la línea de abajo tengo que ocupar slach. Entre parentesis es necesario.El punto y coma puede terminar opcionalmente un enunciado. Esto es útil cuando quiero funciones y/o tareas en una línea.El espacio en blanco es importante en Python. Muy importante Los bloques de código con sangría siempre van precedidos de dos puntos (:) en la línea anterior. En el ejemplo siguiente el resultado es distinto unicamente por los espacios en blanco.Los paréntesis son para agrupar, llamar valores o ocupar funciones.","code":"a = 2\na\nprint(a)\n\n# Ejemplo introductorio: vamos a ver una ejemplo sencillo, que involucra una \n# típica tarea de programación. \n\n# 1.Generamos una variable\n\nvariable = 5\n\n# 2. Generamos dos listas vacias\ninferior = []; superior = []\n\n# Distribuimos los números entre las dos listas\nfor i in range(10):\n    if (i < variable):\n        inferior.append(i)\n    else:\n        superior.append(i)\n        \nprint(\"inferior:\", inferior)\nprint(\"superior:\", superior)## Quebrar un código en dos. \n\n### Sin parentesis\nx = 1 + 2 + 3 + 4 + 5 + \\\n    6 + 7 \nprint(x)\n\n### Con parentesis \n\ny= (1 + 2 + 3 + 4 +\n     5 + 6 + 7 + 8)\n\nprint(y)# Ejemplo 1: los espacios en blanco importan\n\nif x < 4: \n    y = x * 2 \n    print(x)\n\n### Ejemplo 2: los espacios en blanco verdaderamente importan. \n\nif x < 4: \n    y = x * 2\nprint(x)\n\n\"\"\"6. A pesar de lo anterior, es importante notar que **los espacios en blanco dentro de las líneas no importan**\"\"\"\n\n# Los espacio en blanco el interior de una linea no importan, por ejemplo, estas tres expresiones son equivalentes: \n\nx=1+2\ny = 1 + 2\nz             =        1    +                2\n\nprint(x,y,z)# Agrupar \n\n(2 * 3) + 3\n\n# Llamar \n\nprint(\"valor:\", 1)\n\n# Funciones \n\nL = [4,2,3,1]\nL.sort()\nprint(L)\n\n# El patentesis en blanco indica que la función debe ser ejecutada. \n# Estos se llaman metodos, los veremos con detalle más adelante."},{"path":"análisis-de-datos-con-python.html","id":"variables-y-objetos","chapter":"7 Análisis de Datos con Python","heading":"7.1.5 Variables y objetos","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"escalares-1","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.1 Escalares","text":"","code":"# Se asignan variables utilizando signo \"=\"\n\na = 3 + 10 \nb = 2 * 4\nprint(a,b)\n\n# Luego, podemos operar con ellas y definir nuevas variables\n\nc = a ** b * 1.002\nd = a ** b * 1.2\nprint(c,d)\n\ne  = \"Hola\"\ne1 = 'Hola'\nf  = True \nf1 = False\nprint(e,e1,f,f1)\n\n# Nota: ¿Qué ocurre con los separadores de decimales?"},{"path":"análisis-de-datos-con-python.html","id":"tipos-de-escalaresvalores-en-python","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.2 Tipos de escalares/valores en Python","text":"Existen distintos tipos de objetos en Python, para saber cuales existen podemos\nutilizar type().Existen distintos tipos de objetos en Python, para saber cuales existen podemos\nutilizar type().Cada vez que escriba un () estaré refieriendo una función.Cada vez que escriba un () estaré refieriendo una función.","code":""},{"path":"análisis-de-datos-con-python.html","id":"float","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.2.1 Float","text":"Representa un número real. Tiene una parte entera u una fracciónImportante tener cuidado con la precisión. Si queremos hacer algo muy preciso, es importante tener ojo con esta caracteristica del formato float.","code":"n = 3.14126 * 3.1416 \ntype(n)0.1 + 0.2 == 0.3\n\nprint(\"0.1 = {0:.17f}\".format(0.1))\nprint(\"0.2 = {0:.17f}\".format(0.2))\nprint(\"0.3 = {0:.17f}\".format(0.3))"},{"path":"análisis-de-datos-con-python.html","id":"números-complejos.","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.2.2 Números complejos.","text":"Números con una parte real y la otra imaginaria.","code":"complex(1,2)\n\nc = 3 + 4j\n\nc.real\n\nc.imag\n\ntype(c.real)\n\ntype(c)"},{"path":"análisis-de-datos-con-python.html","id":"integers","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.2.3 Integers","text":"Números enteros","code":"# 1. int: números enteros\ndias_semana = 7 \nprint(dias_semana)\ntype(dias_semana)"},{"path":"análisis-de-datos-con-python.html","id":"boolean","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.2.4 Boolean","text":"Variables que toman como valor verdadero o falso. Estos resultados se obtienen al utilizar comparadores. Muy importantes para programar correctamente.","code":"z = True \nz1 = False \ntype(z)\ntype(z1)\n\nresult = (4 < 5)\nprint(result)\nprint(type(result))\n\n# La función bool() tambíen sirve para crear estos objetos. \nbool(2022)\n\nbool(0)\n\nbool(None)\n\nbool(\"\")\n\nbool(\"Nico\")"},{"path":"análisis-de-datos-con-python.html","id":"strings","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.2.5 Strings","text":"Texto. Se creo con dos comillas. Recordar que la versión “missing” en string son dos comillas sin texto.Alerta: Los distintos tipos de datos importan debido que se comportan distinto cuando operamos con ellos.","code":"minombre = 'nico'\n\n# Concatenar texto\ntexto = 'a' + 'b'\nprint(texto)\n\n# Concater multiples veces \ntexto_multiplicado = \"a\" * 10\nprint(texto_multiplicado)\n\n# Tamaño \nlen(texto)\n\n# Acceder a una letra en particular\ntexto[0]"},{"path":"análisis-de-datos-con-python.html","id":"none","chapter":"7 Análisis de Datos con Python","heading":"7.1.5.2.6 None","text":"Python incluye un tipo especial NoneType que solo tiene un valor posible “None”.","code":"type(None)"},{"path":"análisis-de-datos-con-python.html","id":"operadores","chapter":"7 Análisis de Datos con Python","heading":"7.1.6 Operadores","text":"Como todo lenguaje de programación Python permite hacer operaciones de distinto tipo.","code":""},{"path":"análisis-de-datos-con-python.html","id":"operaciones-básicas","chapter":"7 Análisis de Datos con Python","heading":"7.1.6.1 Operaciones básicas","text":"Alerta: La forma de hacer el exponencial en Python es distinto R o Stata.","code":"# 1. Suma \nprint(7 + 3)\n\n# 2. Resta \nprint(10 - 2)\n\n# 3. Division \nprint(5 / 10)\n\n# 4. Multiplicación \nprint(5 * 3) \n\n# 5. Modulo: retorna el resto de la división entre estos dos elementos\nprint(15 % 7)\n\n# 6. Exponencial: para elevar en potencia. Esto es distinto a R o Stata, ojo ahí. \nprint(4 ** 10)\n\n# 7. Incluir mensajes\nprint(\"el resultado es\", 44**3)\n\n# 8. División mínima \n# Floor division\nprint(11 // 2)"},{"path":"análisis-de-datos-con-python.html","id":"operadores-de-asignación","chapter":"7 Análisis de Datos con Python","heading":"7.1.6.2 Operadores de asignación","text":"","code":"# Asignar valor\na = 24\nprint(a)\na + 2\n\n# Actualizar valor\na += 2  # equivalente a \"a = a + 2\"\nprint(a)"},{"path":"análisis-de-datos-con-python.html","id":"operadores-de-comparación","chapter":"7 Análisis de Datos con Python","heading":"7.1.6.3 Operadores de comparación","text":"","code":"# 25 es impar\n25 % 2 == 1\n\n# 66 is impar\n66 % 2 == 1\n\n# Esta entre 15 y 30?\na = 25\n15 < a < 30\n\n# Es una negativo la negación del cero? \n-1 == ~0"},{"path":"análisis-de-datos-con-python.html","id":"operadores-lógicos","chapter":"7 Análisis de Datos con Python","heading":"7.1.6.4 Operadores lógicos","text":"Cuando se trabaja con valores booleanos, Python proporciona operadores para combinar los valores utilizando los conceptos estándar de “”, “” y “”. Como es de esperar, estos operadores se expresan utilizando estas palabras.","code":"# y\nx = 4\n(x < 6) and (x > 2)\n\n# o\n(x > 10) or (x % 2 == 0)\n\n# Negación\nnot (x < 6)\n\n# Distinto a \n(x > 1) != (x < 10)"},{"path":"análisis-de-datos-con-python.html","id":"operadores-de-identidad-y-afiliación","chapter":"7 Análisis de Datos con Python","heading":"7.1.6.5 Operadores de Identidad y Afiliación","text":"Ejercicio 4.1.1:  Genere un vecto llamado II igual 100, un interes igual un 1% diario y un horizonte temporal de un año. Calcule la ganancia que tendría al final del periodo si es paciente. Finalmente, genere un mensaje que diga “Partí con”YY” y ahora tengo “XX” en ganancias, donde “YY” y “XX” son los montos.","code":"a = [1, 2, 3]\nb = [1, 2, 3]\n\na == b\n\na is b\n\na is not b\n\na = [1, 2, 3]\nb = a\na is b\n\n1 in [1, 2, 3]\n\n2 not in [1, 2, 3]"},{"path":"análisis-de-datos-con-python.html","id":"estructuras-de-datos-listas-tuplas-conjuntos-diccionarios","chapter":"7 Análisis de Datos con Python","heading":"7.1.7 Estructuras de datos: listas, tuplas, conjuntos, diccionarios","text":"De momento hemos visto distintos tipos de variables y algunas funciones asociadas ellas. Ahora vamos ver estructuras que nos serás de mucha utilidad lo largo del curso. La siguiente tabla muestra todos los tipos:","code":""},{"path":"análisis-de-datos-con-python.html","id":"listas","chapter":"7 Análisis de Datos con Python","heading":"7.1.7.1 Listas","text":"Las listas son el tipo básico de colección de datos ordenados en Python. Se pueden definir con valores separados por comas entre corchetes cuiadradosAlerta: Una lista anidada es una lista de listas.","code":"a = [1.5, 1.3, 1.4, ] \nprint(a)\nlen(a) # tamaño de la lista\n\n# Estas listas pueden incluir distintos tipos de variables en la lista, por ejemplo incluir númerico y texto \n\nnumtext = [\"pepe\", 1.3, \"pepa\", 1.4]\nprint(numtext)\n\n# Pueden tambíen incluir listas anidadas y valores lógicos (lista de listas)\n\nlistasanidadas = [[1.3, 1,2], [\"pepita\", \"pepito\"], True]\n\nprint(listasanidadas)# Ahora veamos el tipo de la lista\n\ntype(listasanidadas)\nprint(type(listasanidadas))\n\n# Tambíen podemos incluir operaciones dentro\n[1 + 2, \"a\" * 5, 3]"},{"path":"análisis-de-datos-con-python.html","id":"subconjuntos-de-listas","chapter":"7 Análisis de Datos con Python","heading":"7.1.7.1.1 Subconjuntos de listas","text":"Alerta: Python cuenta el primer elemento como un cero!Alerta: El primero es incluyente mientras que el segundo excluyeFinalmente, es posible especificar un tercer entero que representa el tamaño del paso; por ejemplo, para seleccionar cada segundo elemento de la lista, podemos escribir01figuras/image.png","code":"# Vamos a ver como acceder a los elememetos de una lista. \n\n# Creamos lista de ejemplo\nlista = [True, \"pepa\", [1,3], 2]\n\n# Ahora vamos a ver distintos subconjuntos \n\nlista[1] # Segundo elemento!lista[0] # primer elemento de la lista\n\nlista[2] # tercer elemento de la lista\n\nlista[3] # cuarto elemento de la lista\n\nlista[-1] # primer elemento de derecha a izquierda!\n\n# Tambien podemos hacer subconjuntos indicado el inicio y el fin \nlista[0:2]# Tambíen se le puede decir a Python que el inicio sea por defecto\nlista[:2]L = [2, 3, 5, 7, 11]\nL[::2]  # equivalent to lista[0:len(lista):2]\n\n# El tercer elemento es quien nos muestra el salto. Distinto a Stata donde es: inicio(paso)final\n\n#  Pregunta: ¿Qué ocurre en este caso? \n\nlista2 = [1,2,3,4,\"pepe\",\"pepa\"]\nlista2[2:]\n\n# R: Selecciona hasta final!"},{"path":"análisis-de-datos-con-python.html","id":"operar-con-listas-anidadas","chapter":"7 Análisis de Datos con Python","heading":"7.1.7.1.2 Operar con listas anidadas","text":"","code":"# Podemos operar con los subconjuntos incluso cuando tienen listas \n\nlista3 = [1,2,3, [4,5],7]\nv1 = lista3[1]\nv2 = lista3[3]\nv3 = v2[1]\n\nv4 = v1 * v3 \nprint(v4)\n\n# Pregunta: ¿Cómo llegamos al 10? ¿Qué paso con v3?\n\n# Para seleccionar los elementos de una lista de listas debemos utilziar dos parentesis cuadrados\n# Por ejemplo, si quiero seleccionar el segundo elemento \n# del tercer elemento de la lista que hemos creado \n\nlista_anidada = [[1,3], [4,5,\"pepa\"]]\n\nsegundalista_tercerelemento  = lista_anidada[1][2]\nprimeralista_segundoelemento = lista_anidada[0][1]\n\nprint(segundalista_tercerelemento)\nprint(primeralista_segundoelemento)"},{"path":"análisis-de-datos-con-python.html","id":"manipulación-de-listas","chapter":"7 Análisis de Datos con Python","heading":"7.1.7.1.3 Manipulación de listas","text":"Ejercicio 4.1.2:Genere una lista llamada cada con la siguiente información:Seleccione la siguiente información desde la lista casa:Primeros 6 elementosÚltimos 4 elementosCocina y living junto sus medidasConfeccione una lista llamada casa_dos_pisos que contenga dos listas: piso 1 y piso 2. En el piso 1 deje cocina, living y comedor mientras que en el segundo piso deje pieza principal y baño.","code":"# 1. Podemos cambiar los elementos de una lista, pro ejemplo cambiemos el útilmo \n\nlista = [True, \"pepa\", [1,3], 2]\nlista[3] = \"pepito\"\nprint(lista)\n\n# Agregar elementos \n\nlistaconmascosas = lista + [\"nico\", 1.7]\nprint(listaconmascosas)\n\n# Noten que es muy parecido a cuando concatenamos valores en strings.\n\n# Remover elementos\n\nl = list(range(10)) # range: para generar 10 valores (desde el cero)\nprint(l)\ndel l[0]\nprint(l)\n\n# Notar que cuando borramos un elemento de la lista, cambian los índices!\n\n# Ordenar listas \n\n# Creamos listas\nlistaA = [11.25, 18.0, 20.0]\nlistaB = [10.75, 9.50]\n\n# Paste together first and second: full\njuntas = listaA + listaB\n\n# Sort full in descending order: full_sorted\nordenarjuntas = sorted(juntas, reverse = True)\n\n# Print out full_sorted\nprint(ordenarjuntas)\n\n# Es importante tener cuidado al manipular listas y/o generamos nuevas \n\n# Ejemplo 1: cambia original\nx = [\"a\", \"b\" , \"c\"]\ny = x # Guardamos la lista x en una lista nueva\n\n# Ahora vamos a cambiar la lista y \ny[1] = \"z\"\n\n# Miremos las listas\nprint(x) \nprint(y)\n\n# Ejemplo 2: no cambia original \n\nx = 10\ny = x\nx += 5  # Agrego 5 al valor de 5, pero no modifico x! \nprint(\"x =\", x)\nprint(\"y =\", y)\n\n\n# Cuando llamamos a x += 5, no estamos modificando el valor del objeto 10 apuntado por x; \n# más bien estamos cambiando la variable x para que apunte a un nuevo objeto entero con valor 15. \n# Por esta razón, el valor de y no se ve afectado por la operación.\n\n# Para evitar este problema es necesario crear listas de otra forma\n\nx = [\"a\", \"b\", \"c\"]\ny = list(x) # list() sirve para hacer una lista en base a otra\ny = x[:]    # Subcionjunto considerando todos los elementos\ny[1] = \"z\"\n\nprint(x)\nprint(y)\n\n# Noten que ahora cambio la lista x, pero no la lista y."},{"path":"análisis-de-datos-con-python.html","id":"tuplas","chapter":"7 Análisis de Datos con Python","heading":"7.1.7.2 Tuplas","text":"Las tuplas son en muchos aspectos similares las listas, pero se definen con paréntesis en lugar de corchetes. Tambíen pueden ser generadas sin los parentesis.¿Qué diferencia entonces las tuplas de las listas?La principal característica de las tuplas es que son inmutables: esto significa que, una vez creadas, su tamaño y contenido pueden modificarse.De este modo, las listas son mutables mientras que las tuplas son inmutables.","code":"tupla = (1, 2, 3)\nprint(tupla)\n\ntype(tupla)\n\ntupla = 1, 2, 3\nprint(tupla)\n\nlen(tupla) # tamaño\n\ntupla[0]  # Posiciontupla[1] = 4"},{"path":"análisis-de-datos-con-python.html","id":"diccionarios","chapter":"7 Análisis de Datos con Python","heading":"7.1.7.3 Diccionarios","text":"Los diccionarios son mapeos extremadamente flexibles de claves valores, y forman la base de gran parte de la implementación interna de Python. Pueden crearse mediante una lista separada por comas de pares clave:valor entre llaves.","code":"numeros = {\"uno\":1, \"dos\":2, \"tres\":3, \"cuatro\": 4}\nprint(numeros)\n\n# Para accesder debo llamar al nombre, de allí que sean como un diccionario. Son \"similares\" a las listas en R. \nnumeros[\"dos\"]\n\n# Agregar nuevos elementos al diccionario\n\nnumeros['cero'] = 0\nprint(numeros)\n\n# Noten que no hay necesarimente un sendito del orden en los diccionarios. El cero esta al final por ejemplo."},{"path":"análisis-de-datos-con-python.html","id":"conjuntos-sets","chapter":"7 Análisis de Datos con Python","heading":"7.1.7.4 Conjuntos (sets)","text":"Se definen de forma similar las listas y las tuplas, salvo que utilizan las llaves de los diccionarios.","code":"primos = {2, 3, 5, 7}\nimpares = {1, 3, 5, 7, 9}\n\n# unión\nprimos | impares      # with an operator\n\n# intersección\nprimos & impares\n\n# diferencia: que estan en el primer grupo, pero no en el segundo \nprimos - impares\n\n# diferencia simetrica: solo aparecen en un conjunto (cualquiera de los dos)\nprimos ^ impares"},{"path":"análisis-de-datos-con-python.html","id":"funciones-1","chapter":"7 Análisis de Datos con Python","heading":"7.1.8 Funciones","text":"Una función es básicamente un conjunto de tareas que podemos reutilizar lo largo de un código. Es una buena practica de programación, permite evitar repetición.Una función es básicamente un conjunto de tareas que podemos reutilizar lo largo de un código. Es una buena practica de programación, permite evitar repetición.Ya hemos visto varias: print(), type(), str(), int(), bool(), float().Ya hemos visto varias: print(), type(), str(), int(), bool(), float().En todos los programas existen funciones predeterminadas (Buil-function). Veamos algunmos ejemplos y tambíen veamos como buscar ayuda.En todos los programas existen funciones predeterminadas (Buil-function). Veamos algunmos ejemplos y tambíen veamos como buscar ayuda.","code":"lista = [1.5, 1.3, 1.2]\n\n# Algunas funciones para explorar datos\nminimo = min(lista)\nmaximo = max(lista)\nsuma = sum(lista)\npotencia = pow(2,3)\nprint(minimo, maximo, suma, potencia)\n\n# Redondear escalares  \nescalar = 5.3233232\nredondear = round(escalar,2)\n\n# Otras para ordenar listas  y medirlas\ntamaño = len(lista)\nordenar = sorted(lista)  # Por defecto ascendente\n\nprint(tamaño)\nprint(ordenar)\n\n# Ayuda! : la más importante de todas\n\nhelp(len)\nlen?\nlen??"},{"path":"análisis-de-datos-con-python.html","id":"methods","chapter":"7 Análisis de Datos con Python","heading":"7.1.9 Methods","text":"Un method es una función que pertenece un cierto objeto. Vamos ver varios ejemplos para listas y strings.","code":""},{"path":"análisis-de-datos-con-python.html","id":"methods-para-listas","chapter":"7 Análisis de Datos con Python","heading":"7.1.9.1 Methods para listas","text":"","code":"lista = [\"pepa\", 1.45, \"pepe\", 1.32, \"ema\", 1.45]\n\n# 1. Indicador de un valor determinado\nprint(lista.index(\"ema\"))\n\n# 2. Contemos valores\nprint(lista.count(1.45))\n\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# 3. Agregar elementos \nareas.append(24.5)\nareas.append(15.45)\nprint(areas)\n\n# 4. Revertir order\nareas.reverse\nprint(areas)"},{"path":"análisis-de-datos-con-python.html","id":"methods-para-strings","chapter":"7 Análisis de Datos con Python","heading":"7.1.9.2 Methods para strings","text":"En resumen…En Python todo es un objeto. Dependen del tipo de objeto.Cada objeto tiene  métodos  especificos asociados.Hay algunos metodos comunes en todos los objetos (ej. index), pero otros (ej. replace).","code":"# --------#\n# Strings  \n# --------#\n\nstring = \"pepa\"\n\n# 1. Mayusculas la primera letra\n\nstring.capitalize()\n\n# 2. Remplazamos alguna letra\n\nstring.replace(\"e\", \"a\")\n\n# 3. Todas mayusculas \n\nprint(string.upper())\n\n# 4. Contar letras\n\nprint(string.count(\"a\"))b\n\n# Para buscar ayuda en metodos\n\nL = [1,2,3]\nL.insert?\nL?\n\n# Tab Completation \n\n# ¿Cómo se los métodos disponibles? Nombre objeto + \".\" + \"tab\". Si pongo una letra al inicio puedo \n# ver cuales estan disponibles. \n\n#L.\n#L.c\n\n# Tambien es util para importar funciones desde paquetes. \n\n# Más allá del Tab, puedo utilizar * para hacer busquedas más especiales!\n\nstr.*find?# 4. Algunos metodos pueden cambiar los objetos. Veamos un ejemplo. \n\nlista = [\"pepa\", 1.45, \"pepe\", 1.32, \"ema\", 1.45]\nlista.append(\"nicolas\")\nlista\n\n# 5. Otros \"methods\" para números reales \n\nx = 4.5\nprint(x.real, \"+\", x.imag, 'i')\n\nx = 4.5\nx.is_integer()\n\nx = 4.0\nx.is_integer()\n\ntype(x.is_integer)"},{"path":"análisis-de-datos-con-python.html","id":"paquetes","chapter":"7 Análisis de Datos con Python","heading":"7.1.10 Paquetes","text":"Como cualquier lenguaje de programación existen paquetes. Los paquetes son conjuntos de funciones que cumplen un objetivo común. En las clases siguientes veremos en detalles varios de ellos.Como cualquier lenguaje de programación existen paquetes. Los paquetes son conjuntos de funciones que cumplen un objetivo común. En las clases siguientes veremos en detalles varios de ellos.Estos paquetes tienen funciones y metodos especificos.Estos paquetes tienen funciones y metodos especificos.todos los paquetes estan disponibles en Python. Hay que instalarlos.todos los paquetes estan disponibles en Python. Hay que instalarlos.Es posible colocar el nombre que uno quiera los paquetes. Por ejemploEs posible colocar el nombre que uno quiera los paquetes. Por ejemplo","code":"import numpy as np \nnp.array([1,2,3])\n\n# Tambíen es posible importar solo una función de un paquete. \nfrom numpy import array\n\n# Hay ocasiones en las que hay paquetes y subpaquetes\n\nfrom scipy.linalg import inv as my_inv\n\n# Es mejor la primer opción. Queda claro cual paquete estamos utilizando.\n\n# Ejemplo: calcular area de una circunferencia \nr = 0.43\n\n# Importamos paquete math\nimport math as mt\n\n# Calcula C\nC = 2 * mt.pi * r\n\n# Calcula A\nA = mt.pi * r ** 2\n\n# Build printout\nprint(\"Circumferencia: \" + str(C))\nprint(\"Area: \" + str(A))"},{"path":"análisis-de-datos-con-python.html","id":"algunos-paquete-fundamentales-para-el-análisis-de-datos","chapter":"7 Análisis de Datos con Python","heading":"7.1.10.1 Algunos paquete fundamentales para el análisis de datos","text":"NumPy proporciona un almacenamiento y cálculo eficientes para matrices de datos multidimensionales.NumPy proporciona un almacenamiento y cálculo eficientes para matrices de datos multidimensionales.Pandas proporciona un objeto DataFrame junto con un potente conjunto de métodos para manipular, filtrar, agrupar y transformar datos.Pandas proporciona un objeto DataFrame junto con un potente conjunto de métodos para manipular, filtrar, agrupar y transformar datos.Matplotlib proporciona una interfaz útil para la creación de gráficos y figuras con calidad de publicación.Matplotlib proporciona una interfaz útil para la creación de gráficos y figuras con calidad de publicación.","code":""},{"path":"análisis-de-datos-con-python.html","id":"librerias-principales-de-python---numpy-matplotlib-pandas","chapter":"7 Análisis de Datos con Python","heading":"7.2 Librerías principales de Python - Numpy, Matplotlib, Pandas","text":"En esta sección revisaremos todo el instrumental básico de Python: operadores, estructuras de datos, funciones y metodos.Revisaremos una de las principales librerías que se utilizan para el análisis de datos: numpy. Veremos algunos ejemplos muy básicos de matplotlib tambíen.Con estos elementos, ya pueden sentirse libres para trabajar en Python.","code":""},{"path":"análisis-de-datos-con-python.html","id":"numpy","chapter":"7 Análisis de Datos con Python","heading":"7.2.1 NumPy","text":"Los conjuntos de datos pueden ser de muchos tipos como colecciones de documentos, colecciones de imágenes,\ncolecciones de clips de sonido, colecciones de medidas numéricas o casi cualquier cosa.NumPy nos ayudará manejar datos pensados como arreglos númericos.NumPy significa numerical python.Es una alternativa las listas y sirve para manipular arreglos.Las principales librerías de Python estan basadas en numpy: Pandas, SciPy, Matplotlib","code":""},{"path":"análisis-de-datos-con-python.html","id":"sintaxis-básica-y-detalles","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.1 Sintaxis básica y detalles","text":"Para ver más detalles de la libreria es bueno recordar:\nPuedo ver el contenido de np aprentando <tab>despues de apretar np.\nPuedo revisar la documentación con np?\nMás información sobre numerical python: https://numpy.org/\nPuedo ver el contenido de np aprentando <tab>despues de apretar np.Puedo revisar la documentación con np?Más información sobre numerical python: https://numpy.org/En resumenCon listas se puede operar como nos gustaría, pero con NumPy si.NumPy asume que la lista contiene los mismos elementos. NumPy es igualador!Como vimos anteriormente, array es un tipo de objeto. Por lo tanto, sus methods pueden ser distintos.ndarray: n-dimentional array.Eje x se denomina axis 0, eje y axis1.","code":"ingreso = [890, 1023, 2500, 3223, 23434]\n\nn_hogar = [2,4,5,6,9]\n\ningreso_por_persona = ingreso / n_hogar\n# Tenemos un error!# importamos la librería o paquete \nimport numpy as np # importamos\nnp_ingreso = np.array(ingreso) \nnp_nhogar = np.array(n_hogar)\n\n# Ahora, no hay problemas en calcularlo \ningreso_por_persona = np_ingreso / np_nhogar\nprint(ingreso_por_persona)\ntype(ingreso_por_persona)# Numpy es igualador\nnp.array([1.2, \"martes\", False])\n\n# logicos + numeros: todo a números. True se convierte en uno / False en cero. \n# logicos/numerico + string: todo a string\n\n# NumPy cambia algunos operadores\nlista_numeros = [1,2,3]\nnp_numeros = np.array(lista_numeros)\nprint(np_numeros)\n\n# En una lista: + concatenar\nlista_numeros + lista_numeros\n\n# En un arreglo: + suma \nnp_numeros + np_numeros"},{"path":"análisis-de-datos-con-python.html","id":"crear-arreglos-con-numpy","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.2 Crear arreglos con NumPy","text":"Arreglo de 1,2,3 dimensiones01figuras/image.pngArreglo de 4 dimensiones01figuras/array_4d.png","code":"# Veamos los distintos tipos de arreglos que podemos hacer en numpy\nnp.random.seed(0)  # Garantizar reproducibilidad\nx1 = np.random.randint(10, size = 6)  # Arreglo de una dimensión (dimension)\nx2 = np.random.randint(10, size = (3, 4))  # Arreglo de dos dimensiones (filas x columnas)\nx3 = np.random.randint(10, size = (3, 4, 5))  # Arreglo de tres dimensiones (numero matrices, filas x columnas)\nx4 = np.random.randint(10, size =(3,4,5,6)) # Arreglo de cuatro dimensiones (numero de grupos, numero matrices, filas x columnas)\n\nprint(x1)\n\nprint(x2)\n\nprint(x3)\n\nprint(x4)\n\n# Veamos más ejemplos \n# Arreglo de una dimensión \nx = np.array([1, 2, 3])\nx\nprint(x)\n\n# Arreglo de dos dimensiones \n\n# Hago una lista de listas\nx_2d =[[3,2,5], \n      [9,7,1], \n      [4,3,6]]\n\nx_2d_array = np.array(x_2d)\nprint(x_2d_array)\n\n# Arreglo de tres dimensiones \n\nx_3d = np.array([x_2d_array,x_2d_array, x_2d_array])\nprint(x_3d)# Arreglo de cuatro dimensiones \n\nx_4d = np.array([x_3d,x_3d, x_3d, x_3d])\nprint(x_4d)"},{"path":"análisis-de-datos-con-python.html","id":"atributos-de-los-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.3 Atributos de los arreglos","text":"","code":"# Saber la dimensión\nprint(\"x3 ndim: \", x3.ndim)\n\n# Saber detalles de la dimensión\nprint(\"x3 shape:\", x3.shape)\n\n# Saber el número de elementos del objeto\nprint(\"x3 size: \", x3.size)"},{"path":"análisis-de-datos-con-python.html","id":"subconjuntos-en-un-arreglo","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.4 Subconjuntos en un arreglo","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"subconjuntos-en-arreglos-unidimensionales","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.4.1 Subconjuntos en arreglos unidimensionales","text":" Recordar : En cualquier subconjunto: parentesis cuadrados y utilizar valores lógicos.","code":"np_numeros\n\n# Puedo seleccionar subconjuntos al igual que en una lista: con la posición.\nnp_numeros[1]\n\nnp_numeros[0:2]\n\nnp_numeros[-1]\n\nnp_numeros[np_numeros > 2]\n\n# Lo anterior ocurre porque el resultado de un comparador es un valor logico\nnp_numeros > 2# Podemos escribirlo más general \ncondicion = np_numeros > 2\n\nprint(np_numeros[condicion])\n# Noten que obtenemos el mismo resultado que antes\n\nx1 = np.array([5, 0, 3, 3, 7, 9])\nprint(x1)\n\nx1[0]\n\nx = np.arange(10)\nx\n\n# Primeros cinco elementos\nx[:5]\n\n# Elements despúes del indicador cinco\nx[5:]\n\n# todos los elementos en posiciones pares\nx[::2]\n\n# Todos los elementos, en saltos de 2 en 2, pero partiendo desde el 1\nx[1::2]\n\nx[::-1]  # Todos los elementos, pero de atras a adelante"},{"path":"análisis-de-datos-con-python.html","id":"subconjuntos-en-arreglos-multidimensionales","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.4.2 Subconjuntos en arreglos multidimensionales","text":"","code":"x2 = np.array([[3, 5, 2, 4],\n       [7, 6, 8, 8],\n       [1, 6, 7, 7]])\nx2\n\nx2[0, 0]\n\nx2[2, 0]\n\nx2[2, -1]\n\nx2\n\n# dos filas, tres columnas\nx2[:2, :3]\n\n# Dar vuelta todo\nx2[::-1, ::-1]\n\nprint(x2[:, 0]) # primera columna\n\nprint(x2[0, :])  # primera fila\n\nprint(x2[0])   # equivalente a x2[0, :]"},{"path":"análisis-de-datos-con-python.html","id":"filtrar-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.5 Filtrar arreglos","text":"","code":"# Filtros con mascaras\nuno_al_diez = np.arange(1,10)\nuno_al_diez\n\n# Creo la mascara\nmask = uno_al_diez % 2 == 0 \nmask\n\n# Va a dejar unicamente los elementos donde la condición \n# es cierta \nuno_al_diez[mask]\n\ny_2d = np.array([[1,22], [2,21], [3,27], [4,26]])\ny_2d\n\n# Voy a colocar la condición dentro del subconjunto\n# Es analogo a lo que esta arriba \ny_2d[:,0][y_2d[:,1] % 2 == 0]\n\n# Otra opción es utilizar np.where()\nnp.where(y_2d[:,1] % 2 == 0)\n# Retorna un array de INDICES en donde \n# se cumplen las condiciones\n\nnp.where(y_2d == 2)\ntype(np.where(y_2d == 2))\n# El resultado es una tupla! (indice fila, indice columna)\n\nnp.where(y_2d == 2)\n\n# Puedo ocupar np.where para modificar un arreglo según \n# alguna condición \n\nnp.where(y_2d == 2, \" \", y_2d)\n# Lo primero indica la condición, lo segundo el remplazo, \n# lo tercero donde se remplaza"},{"path":"análisis-de-datos-con-python.html","id":"modificar-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.6 Modificar arreglos","text":"Es importante tener ojo al modificar los arreglos","code":"# Recordemos el arreglo x2\nprint(x2)\n\n# Hagamos un subconjunto del arreglo x2\nx2_sub = x2[:2, :2]\nprint(x2_sub)\n\n# Modifiquemos el arreglo x2\nx2_sub[0, 0] = 99\nprint(x2_sub)\n\nprint(x2) # x2 cambio!!!\n\n# Para evitar que el objeto original se modifique utilizamos el metodo copy()\nx2_sub_copy = x2[:2, :2].copy()\nprint(x2_sub_copy)\n\nx2_sub_copy[0, 0] = 42\nprint(x2_sub_copy)\n\nprint(x2) # No se ha modificado!"},{"path":"análisis-de-datos-con-python.html","id":"manipular-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.7 Manipular arreglos","text":"También es posible combinar múltiples arrays en uno solo, y la inversa, dividir un único array en múltiples arrays. Aquí veremos esas operaciones.La concatenación, o unión de dos arrays en NumPy, se realiza principalmente utilizando las rutinas np.concatenate, np.vstack y np.hstack. np.concatenate toma una tupla o lista de arrays como primer argumento, como podemos ver aquí","code":""},{"path":"análisis-de-datos-con-python.html","id":"concatenar-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.7.1 Concatenar arreglos","text":"Para trabajar con matrices de dimensiones mixtas, puede ser más claro\nutilizar las funciones np.vstack (pila vertical) y np.hstack (pila horizontal):","code":"x = np.array([1, 2, 3])\ny = np.array([3, 2, 1])\nnp.concatenate([x, y])\n\n# O de otra forma\nnp.concatenate((x,y))\n\n# Agregar un nuevo arreglo\nz = [99, 99, 99]\nprint(np.concatenate([x, y, z]))\n\ngrid = np.array([[1, 2, 3],\n                 [4, 5, 6]])\n\n# Concatenar con matrices de dimensión 2\nnp.concatenate([grid, grid])\n\n# Concatenar con matrices de dimensión 2\nnp.concatenate([grid, grid], axis = 0)\n\n# Concatenar con matrices de dimensión 2\nnp.concatenate([grid, grid], axis = 1)# Pegar arreglos verticalmente\nx = np.array([[1, 2, 3], \n              [-11, -2, -3]])\n\ny = np.array([[9, 8, 7],\n                 [6, 5, 4]])\n\nprint(np.vstack([x, y]))\n\n# Pegar arreglos horizontalmente\nnp.hstack([y, x])"},{"path":"análisis-de-datos-con-python.html","id":"separar-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.7.2 Separar arreglos","text":"","code":"# Puedo indicarle el numero de grupos \narr = np.array([1, 2, 3, 4, 5, 6])\nnewarr = np.array_split(arr, 4)\nprint(newarr)\n\n# Puedo indicarle los indices de los grupos\nx = [1, 2, 3, 99, 99, 3, 2, 1]\nx1, x2, x3 = np.split(x, [4, 5])\nprint(x1, x2, x3)"},{"path":"análisis-de-datos-con-python.html","id":"trasponer-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.7.3 Trasponer arreglos","text":"","code":"print(x)\n\nxT = np.transpose(x)\nprint(xT)\n# Noten que ahora es un vector fila\n\nnp_2d_t = np.transpose(np_2d)\nprint(np_2d_t)\n\n# Tambien puedo aplicarlo como método. \nnp_2d.T"},{"path":"análisis-de-datos-con-python.html","id":"otras-funciones-para-crear-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.7.4 Otras funciones para crear arreglos","text":"","code":"# Crea un arreglo unidimensional de zeros\nnp.zeros(10)\n\nnp.zeros((10,10))\n\n# Crear una matriz de las dimensiones que se le indican \n# Notar parentesis cuadrado\nnp.zeros([10,10])\n\n# Crea una matriz de 3x5 puntos flotantes llena de unos\nnp.ones((3, 5), dtype=float)\n\n# Crea una matriz de 3x5 llena de 3,14\nnp.full((3, 5), 3.14)\n\n# Crea un array lleno de una secuencia lineal\n# Empezando en 0, terminando en 20, pasando por 2\n# (esto es similar a la función incorporada range())\nnp.arange(0, 20, 2)\n\nnp.arange(-3,4)\n# Incluye el primer elemento, pero el último no\n\nnp.arange(-3,4,3)\n\n# Crear una matriz 3x3 de valores aleatorios normalmente distribuidos\n# con media 0 y desviación estándar 1\nnp.random.normal(0, 1, (3, 3))\n\n# Crea una matriz 3x3 de enteros aleatorios en el intervalo [0, 10)\nnp.random.randint(0, 10, (3, 3))\n\n# Crea una matriz identidad\nnp.eye(3)\n\n#  Crear una matriz de valores espaciados uniformemente\n# espaciados entre sí\n\nnp.linspace(0,2,9)\n\n# Solo valores aleatorios\nnp.random.random((2,4))\n\n# La primera parte es el modulo de numpy, el segundo \n# es el nombre de la función.\n\n# Crear un arreglo vacio\nnp.empty((3,2))\n\n# Reshape: modifica las dimensiones de un arreglo \narray = np.array([[1,2], [5,7], [6,6]])\nprint(array)\n\narray.reshape((2,3))\n\n# Flatten: toma todos los elementos de un arreglo y los \n# deja en una dimensión \n\narray.flatten()\n\n# Podemos componer funciones, volver al origina \narray.flatten().reshape((2,3))"},{"path":"análisis-de-datos-con-python.html","id":"ordenar-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.7.5 Ordenar arreglos","text":"Notar que trata cada columna/fila como arreglos independientes.Cualquier relación entre columnas/filas se pierde al ordenarlos.","code":"# Como función\nx = np.array([2, 1, 4, 3, 5])\nprint(x)\n\nprint(np.sort(x))\n\nnp.sort?\n\n# Como método\nx.sort()\nprint(x)\n\n# argsort() retorna los indices\nx = np.array([2, 1, 4, 3, 5])\ni = np.argsort(x)\nprint(i)\n# El primero es el menor\n\n\"\"\"+ **Si queremos ordenar por filas o columnas?**\"\"\"\n\n# Vamos a utilizar el argumento axis\na = np.random.randint(0, 10, (3, 3))\nprint(a)\n\n# Ordenamos por columnas\nprint(np.sort(a, axis = 0))\n\n# Ordenamos por filas\nprint(np.sort(a, axis = 1))# Puedo ordenar con la siguiente regla: \n\n# indica un número: total de elementos a ordenar\n# a la izquierda los menores\n# a la derecha los mayores\n\nx = np.array([7, 2, 3, 1, 6, 5, 4])\nnp.partition(x, 3)\n\n# Analogo para un 2d \nprint(np.partition(a, 2, axis=1))"},{"path":"análisis-de-datos-con-python.html","id":"broadcasting","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.8 Broadcasting","text":"Es una de las razones de que Python sea tan eficiente.Es una de las razones de que Python sea tan eficiente.Permite operar entre objetos (ej. matrices) que tienen distinta dimensión.Permite operar entre objetos (ej. matrices) que tienen distinta dimensión.La transmisión en NumPy sigue un estricto conjunto de reglas para determinar la interacción entre las dos matrices:\nRegla 1: Si las dos matrices difieren en su número de dimensiones, el objeto de la que tiene menos dimensiones se rellena con unos en su lado frontal (izquierdo).\nRegla 2: Si la forma de las dos matrices coincide en ninguna dimensión, la matriz con forma igual 1 en esa dimensión se estira para que coincida con la otra forma.\nRegla 3: Si en alguna dimensión los tamaños coinciden y ninguno es igual 1, se produce un error.\nLa transmisión en NumPy sigue un estricto conjunto de reglas para determinar la interacción entre las dos matrices:Regla 1: Si las dos matrices difieren en su número de dimensiones, el objeto de la que tiene menos dimensiones se rellena con unos en su lado frontal (izquierdo).Regla 1: Si las dos matrices difieren en su número de dimensiones, el objeto de la que tiene menos dimensiones se rellena con unos en su lado frontal (izquierdo).Regla 2: Si la forma de las dos matrices coincide en ninguna dimensión, la matriz con forma igual 1 en esa dimensión se estira para que coincida con la otra forma.Regla 2: Si la forma de las dos matrices coincide en ninguna dimensión, la matriz con forma igual 1 en esa dimensión se estira para que coincida con la otra forma.Regla 3: Si en alguna dimensión los tamaños coinciden y ninguno es igual 1, se produce un error.Regla 3: Si en alguna dimensión los tamaños coinciden y ninguno es igual 1, se produce un error.Más info: https://numpy.org/doc/stable/user/basics.broadcasting.htmlMás info: https://numpy.org/doc/stable/user/basics.broadcasting.html01figuras/image.png01figuras/image.png01figuras/image.png01figuras/image.pngRegla 1: tiene menos dimension, por lo tanto, la izquierda con 1.M -> (2,3)\n->(1,3)Regla 2: como difieren en la primera dimensión, estiramos la más pequeña hasta hacerla coincidirRegla 1: el de dimensión más baja con 1 la izquierda(3,1)\n(1,3)Regla 2: ambos deben coincidir\n(3,3) y (3,3).Regla 1:M -> 3,2\n-> 1,3Regla 2: La dimensión 1 se modifica hasta que sean igualesM -> 3,2\n-> 3,3","code":"array2_2 = np.array([0,0,0,0]).reshape(2,2)\narray2_1 = np.array([0,1]).reshape((2,1))\narray1_2 = np.array([0,1]).reshape((1,2))\n\nprint(array2_2)\nnp.shape(array2_2)\n\nprint(array2_1)\nnp.shape(array2_1)\n\nprint(array1_2)\nnp.shape(array1_2)\n\n# Caso 1: sumo a cada columna\nprint(array2_2 + array2_1)\n\n# Caso 2: sumo a cada fila\nprint(array2_2 + array1_2)\n\n# ¿Que hizo python? : respeto siempre las filas.\n\n# Todo lo anterior aplica para otras operaciones.\n\n# Aquí tambien hay de alguna forma bc\n\nprint(a)\n\na + 5\n# Es similar a transformar 5 en [5,5,5]\n\n# Otro ejemplo\n\nM = np.ones((3, 3))\nprint(M)\n\nprint(a)\n\nM + aM = np.ones((2, 3))\na = np.arange(3)\n\nprint(M)\n\nprint(a)\n\nprint(M.shape)\nprint(a.shape)print(M + a)\n\n# Ejemplo 2: Cuando ambos tienen que hacer broadcasting\n\na = np.arange(3).reshape((3, 1))\nb = np.arange(3)\n\nprint(a)\n\nprint(b)\n\nprint(a.shape)\nprint(b.shape)print(a + b)\n\n# Ejemplo 3\n\nM = np.ones((3, 2))\na = np.arange(3)M + a"},{"path":"análisis-de-datos-con-python.html","id":"operaciones-matematicas-en-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.9 Operaciones matematicas en arreglos","text":"","code":"# Para hacer operaciones\nnp_mat = np.array([[1, 2],\n                   [3, 4],\n                   [5, 6]])\nprint(np_mat ** 1)\n\nprint(np.array([10, 10]))\n\nprint(np_mat + np.array([10, 10]))\n\nnp_mat + np_mat"},{"path":"análisis-de-datos-con-python.html","id":"operaciones-vectorizadas-en-python","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.9.1 Operaciones vectorizadas en Python","text":"Al vectorizar operaciones se reduce el tiempo de ejecución.Las típicas operaciones que realizamos muchas veces en estos lenguajes son las que permiten disminuir el tiempo de ejecución en los programas.","code":"def dividirporvalores(valores):\n    output = np.empty(len(valores))\n    for i in range(len(valores)):\n        output[i] = 1 / valores[i]\n    return output\n        \narreglo = np.random.randint(1, 20, size=10)\ndividirporvalores(arreglo)\n\n# Commented out IPython magic to ensure Python compatibility.\narreglo_muygrande = np.random.randint(1, 100, size=1000000)\n# %timeit dividirporvalores(arreglo_muygrande)print(dividirporvalores(arreglo))\nprint(1.0 / arreglo)\n\n# Commented out IPython magic to ensure Python compatibility.\n# %timeit (1.0 / arreglo_muygrande)\n\nnp.arange(5) / np.arange(1, 6)\n\nnp.arange(100000) ** 2\n\nnp.arange(100000).sum()"},{"path":"análisis-de-datos-con-python.html","id":"operaciones-básicas-y-funciones-básicas","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.9.2 Operaciones básicas y funciones básicas","text":"","code":"x = np.arange(4)\nprint(\"x     =\", x)\nprint(\"x + 5 =\", x + 5)\nprint(\"x - 5 =\", x - 5)\nprint(\"x * 2 =\", x * 2)\nprint(\"x / 2 =\", x / 2)\nprint(\"x // 2 =\", x // 2)  # floor division\nprint(\"-x     = \", -x)\nprint(\"x ** 2 = \", x ** 2)\nprint(\"x % 2  = \", x % 2)\n# Todas estas funciones/operadores se encuentran como metodos\n\n# Funciones como metodos\nprint(np.add(x, 2))\nprint(np.multiply(x,2))\nprint(np.divide(x,2))\nprint(np.subtract(x,2))\n\na = np.array([1,2,3])\nb = np.array([[1.5,-32,-33], [4,5,6]])\nc = 2\n\nnp.absolute(b)\n\nnp.exp(b)\n\nnp.sqrt(b)\n\nnp.sin(a)\n\nnp.cos(b)\n\nnp.log(a)"},{"path":"análisis-de-datos-con-python.html","id":"operaciones-entre-distintos-tipos-de-objeto","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.9.3 Operaciones entre distintos tipos de objeto","text":"","code":"a = np.array([1,2,3])\nb = np.array([[1.5,2,3], [4,5,6]]).reshape(2,3)\nc = np.array([[2,3],[3,3],[3,5]]).reshape(3,2)\nd = 2\ne = np.array([[2,3,3]])\n\nprint(a)\n\nprint(b)\n\nprint(c)\n\nprint(d)\n\nprint(e)\n\n# Vector con escalar\na + d\n\n# Matriz con escalar\nb - d\n\n# Matriz con matriz (suma)\nb + b\n\n# Matriz con matriz (producto)\nb * c\n\n# Si quiero multiplicar matrices normalmente\nnp.dot(b,c)\n\n# Otro ejemplo\nm1 = np.array([[1,4,7],[2,5,8]])\nm2 = np.array([[1,4],[2,5],[3,6]])\n\nnp.shape(m1)\n\nnp.shape(m2)\n\nm3 = np.dot(m1,m2) \nprint(m3)\n\n# ¿Por qué aquí pudimos multiplicar? \nb * e"},{"path":"análisis-de-datos-con-python.html","id":"estadistica-exploratoria-con-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.10 Estadistica exploratoria con arreglos","text":"","code":"np_mat = np.array([[1, 2,30],\n                   [30, 4, -5],\n                   [5, 6,7 ]])\nprint(np_mat)\n\nnp_mat[:,0]\n\n# Media\nnp.mean(np_mat[:,0])\n\n# Mediana \nnp.median(np_mat[:,0])\n\n# Coeficiente de correlación \nnp.corrcoef(np_mat[:,1],np_mat[:,2] )\n\n# Desviación estandar\nnp.std(np_mat[:,1])\n\n# Suma \nnp.sum(np_mat[:,1])\n\n# Suma acumulada\nprint(np.cumsum(np_mat))\n\n# Tambien puedo utilizarlas como metodos\nnp_mat.sum()\n\nnp_mat.min()\n\nnp_mat.max()\n\nnp_mat.mean()\n\n# Generar datos aleatorios a partir de una distribución normal\ningreso = np.round(np.random.normal(1000,100,5000),1)\nn_hogar = np.round(np.random.normal(5,2,5000),0)\n\ningreso\n\ningreso.shape\n\nn_hogar\n\nn_hogar.shape\n\ndatos = np.column_stack((ingreso,n_hogar))\nprint(datos)\n\ndatos[:,0].mean()\n\n# Tambien podemos utilizar variables categoricas excluyentes\n# Fijemos colores y precios como listas\n\n# Listas : colores y precios\n\nmano = [\"diestro\", \"zurdo\", \"zurdo\", \"diestro\", \"zurdo\"]\nvalor = [20, 21, 24, 12, 15]\n\n# Convertimos a arreglos\nnp_mano = np.array(mano)\nnp_valor = np.array(valor)\n\nprint(np_mano, np_valor)\n\n# Hacemos una selección solo para diestros\nvalores_diestro = np_valor[np_mano == 'diestro']\n\n# Hacemos una selección solo para zurdos\nvalores_zurdos = np_valor[np_mano != 'diestro']\n\n# Imprimimos los resultados utilizando las estadisticas promedio\nprint(\"Mediana valor diestros: \" + str(np.median(valores_diestro )))\n\n# Print out the median height of other players. Replace 'None'\nprint(\"Mediana valor zurdos: \" + str(np.median(valores_zurdos)))\n\nnp_mat\n\n# Ojo que le puedo indicar los ejes\nnp_mat.sum(axis = 0)\n\n# Ojo que le puedo indicar los ejes\nnp_mat.sum(axis = 1)\n\nM = np.random.random((3, 4))\nprint(M)\n\nM.sum()\n\nM.min(axis=0)\n\nM.max(axis=1)\n\nnp.argmin(M)\n\nnp.percentile(M, 10)"},{"path":"análisis-de-datos-con-python.html","id":"comparativos-en-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.2.1.11 Comparativos en arreglos","text":"Ejercicio 4.2.1:Genere una lista llamada ejercicio1 que contenga cuatro listas. La primera debe tener los números (180, 78.4, 15, 18), la segunda (215, 102.7, 17, 19), la tercera (210, 98.5, 18, 20) y la cuarta (188, 75.2, 22, 21).Genere una lista llamada ejercicio1 que contenga cuatro listas. La primera debe tener los números (180, 78.4, 15, 18), la segunda (215, 102.7, 17, 19), la tercera (210, 98.5, 18, 20) y la cuarta (188, 75.2, 22, 21).Importe el paquete numpy como np. Transfore ejercicio1 en un arreglo llamado np_ejercicio1. Finalmente, imprima el tipo de arreglo y el tamaño de este (shape.)Importe el paquete numpy como np. Transfore ejercicio1 en un arreglo llamado np_ejercicio1. Finalmente, imprima el tipo de arreglo y el tamaño de este (shape.)Cree un arreglo que contenga unicamente la segunda columna de ejercicio1. Llame este arreglo segunda columna. Haga el mismo procedimiento con la primera fila.Cree un arreglo que contenga unicamente la segunda columna de ejercicio1. Llame este arreglo segunda columna. Haga el mismo procedimiento con la primera fila.Múltiplique la segunda columna por la primera fila. Llame este arreglo resultado.Múltiplique la segunda columna por la primera fila. Llame este arreglo resultado.Cree un mensaje que le muestre el promedio, la mediana y la desviación estadar de la tercera fila del arreglo ejercicio1.Cree un mensaje que le muestre el promedio, la mediana y la desviación estadar de la tercera fila del arreglo ejercicio1.","code":"a == b\n\na < 2"},{"path":"análisis-de-datos-con-python.html","id":"matplotlib","chapter":"7 Análisis de Datos con Python","heading":"7.2.2 Matplotlib","text":"Librería de Python para visualizar datos.Importante para explorar datos y para comunicar efectivamente el análisis de datos.","code":""},{"path":"análisis-de-datos-con-python.html","id":"visualizaciones-básicas","chapter":"7 Análisis de Datos con Python","heading":"7.2.2.1 Visualizaciones básicas","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"lineas","chapter":"7 Análisis de Datos con Python","heading":"7.2.2.1.1 Lineas","text":"","code":"import matplotlib.pyplot as plt ## No module named 'matplotlib'años = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.plot(años,pib)## name 'plt' is not defined"},{"path":"análisis-de-datos-con-python.html","id":"puntos","chapter":"7 Análisis de Datos con Python","heading":"7.2.2.1.2 Puntos","text":"","code":"import matplotlib.pyplot as plt ## No module named 'matplotlib'años = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.scatter(años,pib)## name 'plt' is not definedplt.show()## name 'plt' is not defined"},{"path":"análisis-de-datos-con-python.html","id":"histograma","chapter":"7 Análisis de Datos con Python","heading":"7.2.2.1.3 Histograma","text":"","code":"# Histograma de la distribución de la variable ingreso \n\ningreso = np.round(np.random.normal(1000,100,5000),1)## name 'np' is not definedplt.hist(ingreso, bins = 100)## name 'plt' is not definedplt.show()## name 'plt' is not defined\n# Por defecto Python selecciona en 10 el número de bins. Muy pocos \"bins\" sobre simplifican. \n# Muchos no permiten ver algún patrón en los datos"},{"path":"análisis-de-datos-con-python.html","id":"barras","chapter":"7 Análisis de Datos con Python","heading":"7.2.2.1.4 Barras","text":"","code":"# Grafico de barras\nx = [5,6,3,7,2]\ny= [\"A\", \"B\", \"C\", \"D\", \"E\"]\nplt.bar(y,x)## name 'plt' is not definedplt.show()## name 'plt' is not defined# Grafico de barras horizontal \n#create data for plotting\nx = [5,6,3,7,2]\ny = [\"A\", \"B\", \"C\", \"D\", \"E\"]\nplt.barh(y,x)## name 'plt' is not definedplt.show()## name 'plt' is not defined"},{"path":"análisis-de-datos-con-python.html","id":"personalizar-gráficos","chapter":"7 Análisis de Datos con Python","heading":"7.2.2.2 Personalizar gráficos","text":"Más colores en: https://matplotlib.org/3.5.0/tutorials/colors/colormaps.htmlPara ver más opciones de la librería: https://matplotlib.org/3.5.0/api/","code":"# Agregar  escalas logaritmicas \nimport matplotlib.pyplot as plt ## No module named 'matplotlib'años = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.scatter(años,pib)## name 'plt' is not definedplt.yscale(\"log\") # Para cambiar las escalas a logaritmos## name 'plt' is not definedplt.show()## name 'plt' is not defined# Modificar el tamaño\nplt.figure(figsize=(8,6))## name 'plt' is not definedplt.plot([10,11,12,13,14], [15,16,17,18,19])## name 'plt' is not definedplt.grid(True)## name 'plt' is not definedplt.show()## name 'plt' is not defined# Modificar simbolos\nplt.plot([10,11,12,13,14], [15,16,17,18,19], \"gd\")## name 'plt' is not definedplt.grid(True)## name 'plt' is not definedplt.show()## name 'plt' is not defined# gd: green dot.\n\n# Modificar color\nx = [0,1,2,3,4,5]\nx2 = [0,1,4,9,16,25]\nplt.scatter(x,x2, s=10, color = \"red\")## name 'plt' is not definedplt.show()## name 'plt' is not defined# Agregar titulos y nombres de los ejes\nimport matplotlib.pyplot as plt ## No module named 'matplotlib'años = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.plot(años,pib)## name 'plt' is not definedplt.xlabel(\"Años\") # Titulo eje x## name 'plt' is not definedplt.ylabel(\"Producto interno bruto\") # Titulo eje y## name 'plt' is not definedplt.title(\"Evolución PIB en el tiempo\") # Titulon grafico## name 'plt' is not defined# Modificar los números de los ejes\nimport matplotlib.pyplot as plt ## No module named 'matplotlib'años = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.plot(años,pib)## name 'plt' is not definedplt.xlabel(\"Años\") # Titulo eje x## name 'plt' is not definedplt.ylabel(\"Producto interno bruto\") # Titulo eje y## name 'plt' is not definedplt.title(\"Evolución PIB en el tiempo\") # Titulon grafico## name 'plt' is not definedplt.yticks([0,500,1000,1500,2000]) # Eje y## name 'plt' is not definedplt.xticks([2010, 2011, 2012, 2013, 2014])# Eje x## name 'plt' is not defined# Colocar una etiqueta en el eje\nimport matplotlib.pyplot as plt ## No module named 'matplotlib'años = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.plot(años,pib)## name 'plt' is not definedplt.xlabel(\"Años\") # Titulo eje x## name 'plt' is not definedplt.ylabel(\"Producto interno bruto\") # TItulo eje y## name 'plt' is not definedplt.title(\"Evolución PIB en el tiempo\")## name 'plt' is not definedplt.yticks([0,500,1000,1500,2000], [0, \"0.5M\", \"1M\", \"1.5M\", \"2M\" ])## name 'plt' is not defined# Cambiar la opacidad \nx = [2,1,6,4,2,4,8,9,4,2,4,10,6,4,5,7,7,3,2,7,5,3,5,9,2,1]\nplt.hist(x, \n         bins = 10, \n         color='blue', \n         alpha=0.2)## name 'plt' is not definedplt.show()## name 'plt' is not defined# Alpha: cambia la opacidad del grafico. Entre 0 y 1.\n\n# Cambiar color\nx = [5,6,3,7,2]\ny = [\"A\", \"B\", \"C\", \"D\", \"E\"]\nplt.bar(y,x, color = \"green\")## name 'plt' is not definedplt.show()## name 'plt' is not defined# Grafico de barras horizontal y con color\n#create data for plotting\nx = [5,6,3,7,2]\ny = [\"A\", \"B\", \"C\", \"D\", \"E\"]\nplt.barh(y,x, color =\"yellowgreen\")## name 'plt' is not definedplt.show()## name 'plt' is not defined# Agregar texto al grafico\nplt.plot([10,11,12,13,14], [15,16,17,18,19], 'rd')## name 'plt' is not definedplt.axis([10, 16, 15, 20])## name 'plt' is not definedplt.xlabel('Eje x')## name 'plt' is not definedplt.ylabel('Eje y')## name 'plt' is not definedplt.text(11.2, 15.9, r'Este es el segundo punto')## name 'plt' is not definedplt.show()## name 'plt' is not defined# Agregar una linea horizontal\naños = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.plot(años,pib)## name 'plt' is not definedplt.axhline(y=1200, ls='--', c='r')## name 'plt' is not definedplt.show()## name 'plt' is not defined# Agregar una linea vertical\naños = [2010, 2011, 2012, 2013, 2014]\npib  = [1000, 1150, 1090, 1200, 1500]\nplt.plot(años,pib)## name 'plt' is not definedplt.axvline(x = 2011, ls='--', c='r')## name 'plt' is not definedplt.show()## name 'plt' is not defined# Agregar caja con nombres \nx = np.linspace(0, 10, 30) # Devuelve números espaciados uniformemente en un intervalo especificado.## name 'np' is not definedplt.plot(x, np.sin(x), label='seno')## name 'plt' is not definedplt.plot(x, np.cos(x), label='coseno')## name 'plt' is not definedplt.legend()## name 'plt' is not definedplt.show()## name 'plt' is not defined# Otra forma de agregar caja con nombres\nplt.plot(x, np.sin(x))## name 'plt' is not definedplt.plot(x, np.cos(x))## name 'plt' is not definedplt.legend([\"seno\", \"cos\"])## name 'plt' is not definedplt.show()## name 'plt' is not defined# Cambiar la ubicación de la leyenda\nplt.plot(x, np.sin(x))## name 'plt' is not definedplt.plot(x, np.cos(x))## name 'plt' is not definedplt.legend([\"seno\", \"cos\"], loc = \"upper right\")## name 'plt' is not definedplt.show()## name 'plt' is not defined# Visualización más compleja con más de una leyanda\n\nlines = []\nline_styles = ['-', '-.', '--', ':']\nline_colors = ['red', 'blue', 'green', 'black']\nx = np.linspace(0, 10, 1000)## name 'np' is not definedfor i in range(4):\n    line, = plt.plot(\n        x, \n        np.sin(x - i * np.pi / 2), \n        line_styles[i], \n        color=line_colors[i]\n    )\n    lines.append(line)## name 'plt' is not definedlegend1 = plt.legend(lines[:2], ['part A', 'part B'], loc='upper right')## name 'plt' is not definedlegend2 = plt.legend(lines[2:], ['part C', 'part B'], loc='lower right')## name 'plt' is not definedplt.gca().add_artist(legend1)## name 'plt' is not definedplt.gca().add_artist(legend2)## name 'plt' is not definedplt.show()## name 'plt' is not defined# Visualizar cuatro elemento: variable 1 y 2 con los ejes, variable 3 con color y variable 4 con tamaño- \nx = np.random.randn(100)## name 'np' is not definedy = np.random.randn(100)## name 'np' is not definedcolors = np.random.rand(100)## name 'np' is not definedsizes = 100 * np.random.rand(100)## name 'np' is not definedplt.scatter(x, y, c=colors, s=sizes, alpha=0.8, cmap='viridis') # cmap: color maps. Otro ejemplo: plasma## name 'plt' is not definedplt.show()## name 'plt' is not defined# Agregar barra de colores\n# Visualizar cuatro elemento: variable 1 y 2 con los ejes, variable 3 con color y variable 4 con tamaño- \nx = np.random.randn(100)## name 'np' is not definedy = np.random.randn(100)## name 'np' is not definedcolors = np.random.rand(100)## name 'np' is not definedsizes = 100 * np.random.rand(100)## name 'np' is not definedplt.scatter(x, y, c=colors, s=sizes, alpha=0.8, cmap='viridis') # cmap: color maps. Otro ejemplo: plasma## name 'plt' is not definedplt.colorbar(orientation = \"horizontal\")## name 'plt' is not definedplt.show()## name 'plt' is not definedx = np.linspace(0, 10, 11)## name 'np' is not definedx# Combinar tres histogramas en un grafico\n\nh1 = np.random.normal(0, 0.8, 1000)## name 'np' is not definedh2 = np.random.normal(-3, 1.2, 1000)## name 'np' is not definedh3 = np.random.normal(4, 0.5, 1000)## name 'np' is not definedplt.hist(h1, bins=40, alpha=0.5)## name 'plt' is not definedplt.hist(h2, bins=40, alpha=0.8)## name 'plt' is not definedplt.hist(h3, bins=40, alpha=0.5)## name 'plt' is not definedplt.show()## name 'plt' is not defined# Modificar la opacidad del grafico\nx = [2,1,6,4,2,4,8,9,4,2,4,10,6,4,5,7,7,3,2,7,5,3,5,9,2,1]\n#plot for a histogram\nplt.hist(x, bins = 10, color='blue', alpha=0.2)## name 'plt' is not definedplt.show()## name 'plt' is not defined# Alpha: cambia la opacidad del grafico. Entre 0 y 1.\n\n# Generar graficos, combinarlos, pero uno despues del otro\nfig = plt.figure() # Genero un espacio para la figura## name 'plt' is not definedax1 = fig.add_axes([0.1, 0.5, 0.8, 0.4], ylim=(-2, 2))## name 'fig' is not definedax2 = fig.add_axes([0.1, 0.0, 0.8, 0.4], ylim=(-2, 2))## name 'fig' is not definedx = np.linspace(0, 10)## name 'np' is not definedax1.plot(np.sin(x))## name 'ax1' is not definedax2.plot(np.cos(x))## name 'ax2' is not definedplt.show()## name 'plt' is not defined# Generate normal distributed sample points in 2-D\nx, y = np.random.multivariate_normal([0, 0], [[1, 1], [1, 2]], 3000).T## name 'np' is not defined# Define subplot styles\nfig = plt.figure(figsize=(10, 10))## name 'plt' is not definedgrid = plt.GridSpec(4, 4, hspace=0.2, wspace=0.2)## name 'plt' is not definedax = fig.add_subplot(grid[:-1, 1:])## name 'fig' is not definedax_x = fig.add_subplot(grid[-1, 1:])## name 'fig' is not definedax_y = fig.add_subplot(grid[:-1, 0])## name 'fig' is not defined# Plot the data to subplots\nax.scatter(x, y, s=3, alpha=0.5)## name 'ax' is not definedax_x.hist(x, 40, orientation='vertical')## name 'ax_x' is not definedax_y.hist(y, 40, orientation='horizontal')## name 'ax_y' is not definedax_x.invert_yaxis()## name 'ax_x' is not definedax_y.invert_xaxis()## name 'ax_y' is not definedplt.show()## name 'plt' is not defined"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-de-generación-de-visualizaciones","chapter":"7 Análisis de Datos con Python","heading":"7.2.2.3 Ejemplo de generación de visualizaciones","text":"Método 1: Aplicar plot()Método 2: Aplicar la función plot(x,y)Método 3: Crear una figura vacía y aplicar metodosMúltiples sub-plotsOtro ejemplo: combinación método 1 y método 3Ejercicio 4.2.1: Escribir una función que pregunte al usuario o usuaria por las ventas de un rango de años y muestre por pantalla un diagrama de líneas con la evolución de las ventas.","code":"# Preambulo\n\nimport pandas as pd                    # datos\nimport matplotlib as mpl               # graficos\nimport datetime as dt                  # fechas\nimport numpy as np                     # arreglos\n\n# Chequear todo\nprint('Pandas version: ', pd.__version__)\nprint('Matplotlib version: ', mpl.__version__)\nprint('Today: ', dt.date.today())\n\n# PIB y consumo USA\ngdp  = [13271.1, 13773.5, 14234.2, 14613.8, 14873.7, 14830.4, 14418.7,\n        14783.8, 15020.6, 15369.2, 15710.3]\npce  = [8867.6, 9208.2, 9531.8, 9821.7, 10041.6, 10007.2, 9847.0, 10036.3,\n        10263.5, 10449.7, 10699.7]\nyear = list(range(2003,2014))        # use range for years 2003-2013 \n\n# Creamos data frame\nus = pd.DataFrame({'gdp': gdp, 'pce': pce}, index=year) \nprint(us.head(3))\n\n# Pib percapita (World Bank data, 2013, thousands of USD) \ncode    = ['USA', 'FRA', 'JPN', 'CHN', 'IND', 'BRA', 'MEX']\ncountry = ['United States', 'France', 'Japan', 'China', 'India',\n             'Brazil', 'Mexico']\ngdppc   = [53.1, 36.9, 36.3, 11.9, 5.4, 15.0, 16.5]\n\nwbdf = pd.DataFrame({'gdppc': gdppc, 'country': country}, index=code)\nwbdfus\n\nus.plot()\n\n# Solo pib\nus['gdp'].plot()\n\n# ahora en barras\nus.plot(kind='bar')\n\n# Comp puntos \n# Orden importa \nus.plot.scatter('pce', 'gdp')# import pyplot module of Matplotlib \nimport matplotlib.pyplot as plt\n\nus\n\nus.index\n\nplt.plot(us.index, us['pce'])\n\n# Dos lineas juntas \nplt.plot(us.index, us['gdp'])\nplt.plot(us.index, us['pce'])\n\n# Noten como python agrega\n\nus\n\n# Grafico de barras \nplt.barh(us.index, us['gdp'], align = 'center')\n\n# Agregamos nuevos elementos \nplt.plot(us.index, us['gdp']) \nplt.plot(us.index, us['pce']) \n\nplt.title('US GDP', fontsize=12, loc='left') # Títutlo\nplt.ylabel('Billions of 2009 USD')           # Eje y\nplt.xlabel(\"Years\")\nplt.xlim(2002.5, 2013.5)                     # Limites eje x\nplt.tick_params(labelcolor='gray')            # Colores\nplt.legend(['GDP', 'Consumption'])           # Nombres variables \nplt.show()# Creamos figura vacía\nfig, ax = plt.subplots()\n\n# Creamos con más cosas \nfig, ax = plt.subplots()\n\n# Agregamos cosas con metodos de ax\nax.plot(us.index, us['gdp'], linewidth=2, color='magenta')\nax.set_title('US GDP', fontsize=14, loc='left')\nax.set_ylabel('Billions of USD')\nax.set_xticks([2004, 2008, 2012])\nax.grid(True)\n\n# Guardamos la figura\nfig.savefig('resultados\\\\us_gdp.pdf')# Creamos un arreglo de 2d \nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)  \nprint('Objeto ax tiene dimensión', len(ax))\n\n# Agregamos contenido\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n\nax[0].plot(us.index, us['gdp'], color='green')   # first plot \nax[1].plot(us.index, us['pce'], color='red')     # second plot# grab the axis\nax = us.plot()\n\n# Aplicamos métodos\nax = us.plot()  \nax.set_title('US GDP and Consumption', fontsize=14, loc='left')\nax.set_ylabel('Billions of 2013 USD')\nax.legend(loc='center right')# Respuesta: \n\ninicio = int(input(\"Introduce el año inicial: \"))\nfin    = int(input(\"Introduce el año final: \"))\n\n# Definimos un diccionario vacio donde ir guardando los datos \n\nventas = {}\n\n# Hacer una iteración \n\nfor i in range(inicio, fin + 1): \n    ventas[i] = input(\"Introduce las ventas del año: \" + str(i))\n\n# Armamos la figura: \n\nfig, ax = plt.subplots()\nax.plot(ventas.keys(), ventas.values())\nplt.show()"},{"path":"análisis-de-datos-con-python.html","id":"pandas","chapter":"7 Análisis de Datos con Python","heading":"7.2.3 Pandas","text":"Esta librería nos permitirá analizar data como tablas, rectangular o data frames.Esta librería nos permitirá analizar data como tablas, rectangular o data frames.En las que las filas y columnas se identifican con etiquetas en lugar de con simples índices enterosEn las que las filas y columnas se identifican con etiquetas en lugar de con simples índices enterosLa documentación la encuentra aquí: https://pandas.pydata.org/docs/La documentación la encuentra aquí: https://pandas.pydata.org/docs/","code":""},{"path":"análisis-de-datos-con-python.html","id":"sintaxis-básica-ejemplo-1","chapter":"7 Análisis de Datos con Python","heading":"7.2.3.1 Sintaxis básica: Ejemplo 1","text":"","code":"# Importamos librería pandas \nimport pandas as pd\n\ncolumnas = [\"ind1\", \"ind2\", \"ind3\"]\nprint(columnas)\n\n# Transformemoslo \ndata = pd.DataFrame(columnas)\n\n# Ahora los datos estan en un data.frame\nprint(data)\ntype(data)\n\n# Ahora vamos a agregar nombres y nuevas columnas con un diccionario \n\nnombres_columna = {\"nombre\": columnas,\n                  \"altura\": [1.67, 1.9, 0.23], \n                  \"peso\": [54, 100, 2]}\n\ndata = pd.DataFrame(nombres_columna)\nprint(data)\n\n# Si queremos acceder a elementos \ndata[\"altura\"]\n\n# Si queremos acceder a un valor en particular, similar a las listas\nseleccion_columna = data[\"altura\"][2]\nprint(seleccion_columna)\n\nseleccion_filas = data.iloc[2]\n\n#data1.iloc?\n\nprint(seleccion_filas)\n\n# Agregar nueva información\nbmi = []\n\nfor i in range(len(data)): \n    bmi_score = data[\"peso\"][i]/(data[\"altura\"][i]**2)\n    bmi.append(bmi_score)\n\nbmi\n\ndata[\"bmi\"] = bmi\n\ndata\n\n# Guardemos \ndata.to_csv(\"datos_ejemplo1.csv\")"},{"path":"análisis-de-datos-con-python.html","id":"sintaxis-básica-ejemplo-2","chapter":"7 Análisis de Datos con Python","heading":"7.2.3.2 Sintaxis básica: Ejemplo 2","text":"","code":"# Importamos datos que utilizaremos durante la clase\nfrom gapminder import gapminder\n\ngapminder"},{"path":"análisis-de-datos-con-python.html","id":"inspeccionar-un-data-frame","chapter":"7 Análisis de Datos con Python","heading":"7.2.3.3 Inspeccionar un data frame","text":"","code":"df = gapminder \ndf.head(5)\n\n# Mirar las primeras 10 filas\ndf.head(5).tail(2)\n\ndf.tail()\n\n# Mirar los tipos de datos\ndf.dtypes\n# object se refiere a datos de texto.\n\n# Mirar las columnas\nprint(df.columns)\ntype(df.columns)\n\n# Mostrar N fila- M columna\nprint(df.shape)\n\n# Mirar una columna en particular\n# Caso 1\ndf['country']\n\n# Todas: resumen estadistico\ndf.describe()\n\n# Texto\ndf.country.describe()\n\n# Numerica\ndf.lifeExp.describe()\n\nprint(\"min:\", df.gdpPercap.min())\nprint(\"max:\", df.gdpPercap.max())\nprint(\"mean:\", df.gdpPercap.mean())\nprint(\"std:\", df.gdpPercap.std())\nprint(\"count:\", df.gdpPercap.count())"},{"path":"análisis-de-datos-con-python.html","id":"manipular","chapter":"7 Análisis de Datos con Python","heading":"7.2.3.4 Manipular","text":"Renombrar:Filtrar:Reemplazar:Remover:Agregar nuevas filas:Describir una base de datos:","code":"# Renombrar variables: diccionario + rename \n\ndf.rename(columns = {'country': 'pais', 'continent':'continent', \n                   'year':'año', 'lifExp':'esperanza_vida', \n                   'pop':'poblacion', 'gdpPercap':'pib_percapita'})\n\n# Ojo que no hemos asignado\n\ndf\n\ndf2 = df.rename(columns = {'country': 'pais', 'continent':'continente', \n                   'year':'año', 'lifeExp':'esperanza_vida', \n                   'pop':'poblacion', 'gdpPercap':'pib_percapita'})\n\n# Recomendable generar una nueva base de datos, va dejando un registro\n\ndf2solo_asia = df[df[\"continent\"] == \"Asia\"]\n\n# Lo que esta adentro es un vecto booleano\ndf[\"continent\"] == \"Asia\"\n\ndf_asia = solo_asia\nprint(df_asia)datos_remplazados = df.replace(\"Asia\", \"asia\")\n\nprint(datos_remplazados)remover_columna = df.drop(\"pop\", axis = 1)\n\nprint(remover_columna)\n\nremover_dos_columnas = df.drop([\"gdpPercap\", \"pop\"], axis = 1)\n\nprint(remover_dos_columnas)\n\nremover_filas = df.iloc[0:100]\n\nprint(remover_filas)nuevas_filas = {\"country\": \"nuevo\", \n                \"continent\": \"Asia\", \n                \"year\" : \"2022\", \n               \"gdpPercap\" : \"100000000\"}\n\n\ndf.append(nuevas_filas, ignore_index = True)\n# Para agregar un diccionario es importante la úñtima opción# Agrupar un data frame: group_by \n\ndf_agrupado = df.groupby('year')\ndf_agrupado.mean()\n\n# Sobre una variable en particular\ndf_agrupado = df[['gdpPercap', 'year']].groupby('year')\ndf_agrupado.mean()\n\n# Podemos agrupar varias operaciones\ndf.groupby(\"year\").agg({\"lifeExp\": np.mean, \"gdpPercap\": np.size})\n\n# Agrupar por mas de una variable: año y continente\ndf.groupby([\"year\", \"continent\"]).agg({\"lifeExp\": np.mean, \"gdpPercap\": np.size})"},{"path":"análisis-de-datos-con-python.html","id":"herramientas-de-programación","chapter":"7 Análisis de Datos con Python","heading":"7.3 Herramientas de programación","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"controladores-de-flujo","chapter":"7 Análisis de Datos con Python","heading":"7.3.1 Controladores de flujo","text":"Controlar los caminos de los códigos que programamos.Permite sofisticar procesos de análisis de datos.Veremos algunos típicos + otros que son propiamente de Python.","code":""},{"path":"análisis-de-datos-con-python.html","id":"if-elif-else","chapter":"7 Análisis de Datos con Python","heading":"7.3.2 if-elif-else","text":"La operación nos permite evaluar si se cumple una condición.La operación nos permite evaluar si se cumple una condición.Por ejemplo: Iteramos sobre una lista entre (0, 10), si el valor es mayor que 5 muestra el resultado, caso contrario si es que .Por ejemplo: Iteramos sobre una lista entre (0, 10), si el valor es mayor que 5 muestra el resultado, caso contrario si es que .Como vimos en el ejemplo anterior, para poder realizar flujos de códigos que consideren varias situaciones vamos usar , elif y else.Como vimos en el ejemplo anterior, para poder realizar flujos de códigos que consideren varias situaciones vamos usar , elif y else.Vamos crear una lista utilizando valores de una segunda lista B con valores entre 0-10.Vamos crear una lista utilizando valores de una segunda lista B con valores entre 0-10.Vamos iterar sobre los valores de la lista y realizar las siguientes operaciones:\nSi el valor está entre 0-3 guardamos ese mismo valor en B\nSi el valor es mayor que 3 y menor que 5 lo multiplicamos por 2\nSi el valor es igual 5 pedir ingresar un valor con la función input()\nVamos iterar sobre los valores de la lista y realizar las siguientes operaciones:Si el valor está entre 0-3 guardamos ese mismo valor en BSi el valor está entre 0-3 guardamos ese mismo valor en BSi el valor es mayor que 3 y menor que 5 lo multiplicamos por 2Si el valor es mayor que 3 y menor que 5 lo multiplicamos por 2Si el valor es igual 5 pedir ingresar un valor con la función input()Si el valor es igual 5 pedir ingresar un valor con la función input()Para todo el resto eleva el número al cuadradoPara todo el resto eleva el número al cuadradoLa función input() nos va pedir entregar un valor. Esto es útil cuando necesitamos que el usuario nos reporte algún dato.Ejercicio 4.3.1: Escriba un programa que tenga como resultado su nombre si la edad ingresada es multiplo de 5. Note que la edad debe ser ingresada por el mismo usuario. Si la edad es multiplo de 5 el programa debe dar como resultado su apellido.","code":"# Sintaxis básica\n\nx = -100\n\nif x == 0:\n    print(x, \"es cero\")\nelif x > 0:\n    print(x, \"es positivo\")\nelif x < 0:\n    print(x, \"es negativo\")\nelse:\n    print(x, \"cualquier cosa.\")# Ejemplo sencillo\nfor i in range(0, 10): #iteración\n    if i > 5: #condición\n        print(i) #operación\n\n# Puedo puedo definir variables dentro\n\na = 10 \n\nif a == 10: \n    x = a\nprint(x)\n\n# Otros operadores con texto\na = 'texto2'\n\nif a != 'texto':\n    print(\"ok\")\n\n# Dentro de una lista \n# Es importante entender el flujo de los códigos que escribamos\n\na = [0, 'x', 3]\n\nfor i in range(0, len(a)): \n    print(a[i])\n    if i == 0:\n        print(\"ok\")\n    elif i == 1:\n        print(\"ok2\")\n\n# Noten lo importante que es la sangria\n\nif a == 10: \n    x = a\nprint(x)\n\n# ¿Qué ocurre si movemos el print()?# 1. Cargamos numpy\nimport numpy as np\n\n# 1. Creamos las listas\n\nA = np.arange(0, 11, 1)\nB = []\n\n# 2. Iteramos sobre A\nfor i in A: \n    \n    # Creamos la primera condición\n    if i >= 0 and i <= 3: \n        B.append(i)\n        \n    # Segunda condición\n    elif i > 3 and i < 5: \n        B.append(i * 2)\n        \n    # Tercera condición    \n    elif i == 5:\n        B.append(float(input(\"Agregar un valor \")))\n        B.append(input(\"cualquier cosa: \"))\n        \n    # Para todo el resto\n    else: \n        B.append(i**2)\n        \n# ¿Qué ocurre de especial con el formato float()?\n\nprint(\"A:\", A)\nprint(\"B:\", B)input()\n\n# Noten que es un string. Tenemos que cambiarle el formato. \nx = input(\"Su edad aquí: \")\ntype(x)\n\n# Otro ejemplo con múltiples condiciones \n\nllueve = True\nsalir = True\ntemperatura = 30\n\n# Multiples condiciones\nif llueve and salir:\n    usar_paraguas = True\nelif llueve and not salir:\n    usar_paraguas = False\nelif not llueve and salir and temperatura > 25:\n    usar_bloqueador = True\nelse:\n    vender_paragua = True\n\n# Noten que el if no sigue en caso de que una condición ya esta saldada. \nprint(\"Paraguas:\", usar_paraguas)\nprint(\"Bloqueador:\", usar_bloqueador)\n\nprint(\"Vender Paraguas:\", vender_paragua)\n\n# Defino variables\n\ncuarto = \"cocina\"\narea = 10.0\n\n# Para habitaciones\n\nif cuarto == \"cocina\" :\n    print(\"Mira en la cocina\")\nelif cuarto == \"cuarto\":\n    print(\"Mira en el cuarto\")\nelse :\n    print(\"Mira en cualquier lugar\")\n\n# Para areas \n\nif area > 15 :\n    print(\"Gran tamaño\")\nelif area > 10 :\n    print(\"Tamaño normal\")\nelse :\n    print(\"Bastante pequeño\")edad = int(input(\"Diga su edad: \"))\nif edad % 5 == 0: \n    print(\"Nicolás\")\nelse: \n    print(\"Campos\")"},{"path":"análisis-de-datos-con-python.html","id":"iteradores-básicos","chapter":"7 Análisis de Datos con Python","heading":"7.3.3 Iteradores básicos","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"for-loop","chapter":"7 Análisis de Datos con Python","heading":"7.3.3.1 For loop","text":"Veamos dos versiones alternativas de hacer el mismo , pero anidado","code":"# For loops \nfor N in [2, 3, 5, 7, 9, 11]:\n    print(N)\n\n# For \nfor i in range(10):\n    print(i, end = \" \")\n    \n# Todo en la misma linea\n\n# Podemos iterar sobre una lista: iteramos en el rango de valores que tiene la lista\nx = [0, 1, 'a', True]\n\nfor i in range(0, len(x)):\n    print(i, end = \" \")\n\nfor i in range(0, len(x)):\n    print(x[i], end = \" \")\n\n# Podemos iterar sobre un array\nimport numpy as np\n\nx = np.array([0, 1, 2, 3])\n\nfor i in range(0, len(x)):\n    print(i,end = \" \")\n\n# Podemos iterar sobre un string\n\nfor a in \"Nicolas\" :\n    print(a.capitalize(), end = \" \")\n\n# Podemos iterar en una lista de listas \n\n# Lista de listas \n\ncasa = [[\"living\", 11.25], \n         [\"cocina\", 18.0], \n         [\"comedor\", 20.0], \n         [\"habitacion\", 10.75], \n         [\"baño\", 9.50]]\nprint(casa)\n\n# Iteramos\nfor x in casa :\n    print(\"La \" + x[0] + \" es de \" + str(x[1]) + \" m2\")\n    \n# Noten que el x en si mismo es una lista con dos elemenos. \n# Utilizando subconjuntos de listas podemos ir llamando a los elemenos del iterador.\n\n# Creamos x y y fuera de la iteración\nx = 0 \ny = 0\n\nfor i in range(1, 5): \n    z = 0 # creamos z dentro de la iteración\n    x = x + i\n    y = i \n    z = z + i\n    print(\"i:\", i, \", x=\", x, \", y=\", y, \", z=\", z)\n\n# Podemos generar objetos para ir iterando sobre ellos. Recordar: linspace (inicio, término, cantidad de elementos)\na = np.linspace(1, 10, 5)\nprint(\"type de a:\", type(a))\nprint(\"a: \", a)\n\n#arange(inicio, término, separador)\nb = np.arange(1, 10,1)\nprint(\"type de b:\", type(b))\nprint(\"b: \", b)print(a,b)\n\n# For anidados. \n\n# 1. Creamos una lista\nlista1 = []\n\n# 2. Iteramos sobre los valoes de a y b\nfor i in a: \n    for j in b: \n        #3. Anexamos una operación a la lista\n#         print((i, j), \"multiplicacion:\", i * j)\n        lista1.append(i * j) \nprint(lista1)\n\n#1. Creamos una lista 2\nlista2 = []\n#2. Iteramos sobre el largo de las listas a y b\nfor i in range(0, len(a)): \n    for j in range(0, len(b)): \n        #3. Anexamos una operación a la lista\n        lista2.append(a[i] * b[j]) \nprint(lista2)\n\n#Podemos sumar los valores de una lista con sum()\nprint(\"suma de la lista:\", sum(lista2))\n\nlista1 == lista2\n\n# Puedo ingresar tuplas de forma tal de recuperar el indice y el resultado! \nlista = [1.73, 1.68, 1.71, 1.89]\n\nfor index, a in enumerate(lista):\n    print(\"indice \" + str(index) + \": \" + str(a))\n    \n# Noten que utilizamos un enumerate(). \n# Esto es un interador. \n# Revisaremos más de estos en la siguiente sección\n# Como pueden ver auenta nuestra posibilidades en Python\n# Se pueden saltar en saltos de más de uno? (en tuplas?)"},{"path":"análisis-de-datos-con-python.html","id":"while-loop-1","chapter":"7 Análisis de Datos con Python","heading":"7.3.3.2 While loop","text":"muy comúnÚtil para resolver problemas de optimización","code":"error = 50 \n\nwhile error > 1:\n    error = error/4\n    print(error)\n\n# Otra forma más típica de hacer la actualización \n\ni = 0\n\nwhile i < 10:\n    print(i, end=' ')\n    i += 1\n\n# While loop: pueden ser eternos. ¿Utilidad practica? \n i = 0\n while i < 10:\n    print(i, end=' ')\n    i = i +  1\n\n# Recuerden que puedo agregar condicionales \n\n# Condición inicial\ninicial = 10\n\nwhile inicial != 0 :\n    print(\"corregir...\")\n    if inicial > 0 :\n        inicial = inicial - 1\n    else:\n        inicial = inicial + 1\n    print(inicial)"},{"path":"análisis-de-datos-con-python.html","id":"opcionales-break-y-continue","chapter":"7 Análisis de Datos con Python","heading":"7.3.3.3 Opcionales: break y continue","text":"break: quiebra loopcontinue: siguiente","code":"# Se lo salta\nfor n in range(10):\n    if n % 2 == 0:\n        continue\n    print(n, end=' ')\n\n# No continua si se llega a un nivel predeterminado \na, b = 0, 1\namax = 1000\n\nL = []\n\nwhile True:\n    (a, b) = (b, a + b)\n    if a > amax:\n        break\n    L.append(a)\n\nprint(L)"},{"path":"análisis-de-datos-con-python.html","id":"iterar-sobre-diccionarios-y-arreglos","chapter":"7 Análisis de Datos con Python","heading":"7.3.3.4 Iterar sobre diccionarios y arreglos","text":"Hay que tener cuidado al iterar sobre estos objetosVeamos dos ejemplos","code":"# Diccionarios \nmundo = {\"Burgos\": 30.55, \n        \"Villasana de Mena\": 2.77, \n        \"Santiago\": 39}\n\nmundo\n\nfor key, value in mundo: \n    print(key + \":\" + str(value))\n\n# Tenemos que utilizar un metodo\nfor key, value in mundo.items(): \n    print(key + \": \" + str(value))\n\nmundo.items()\n\ntype(mundo.items())\n\n# Ejemplo: \neuropa = {'spain':'madrid', 'france':'paris', 'germany':'berlin',\n          'norway':'oslo', 'italy':'rome', 'poland':'warsaw', 'austria':'vienna' }\n          \n# Iterar sobre un diccionario \nfor key, value in europa.items() :\n     print(\"the capital of \" + str(key) + \" is \" + str(value))\n        \n        \n# Noten que hacermos el for con una tupla!\n\n# Arreglo 2d\n\nnp_altura = np.array([1.73,1.68,1.71,1.89,1.79])\nnp_peso = np.array([65.4,32.2,42.3,42.3,89.2])\n\nmeas = np.array([np_altura,np_peso])\n\nprint(meas)\n\nfor val in meas: \n    print(val)\n\nfor val in np.nditer(meas): \n    print(val)"},{"path":"análisis-de-datos-con-python.html","id":"iterar-sobre-pandas","chapter":"7 Análisis de Datos con Python","heading":"7.3.3.5 Iterar sobre Pandas","text":"Hay que tener cuidado al iterar sobre estos objetosVeamos dos ejemplos","code":"from gapminder import gapminder\n\ngapminder\n\n# Primer intento\nfor val in gapminder: \n    print(val)\n\n# No entiende que queremos...\n\n# Utilizamos iterrows method\nfor lab,row in gapminder.iterrows(): \n    print(lab)\n    print(row)\n\n# Lab esta asociado al ID de un data frame\n# row es toda la fila asociada\n\n# Podemos hacerlo selectivamente\nfor lab,row in gapminder.iterrows(): \n    print(str(lab) + \": \" + row[\"continent\"])\n\n# Agregar una columna con un iterador \n\nfor lab,row in gapminder.iterrows(): \n    gapminder.loc[lab, \"tamaño_continente\"] = len(row[\"continent\"])\n\n# Es solo un ejemplo, no es muy eficiente.\n\n# Otro ejemplo, utilizando una función en vez de un método\n\nfor lab,row in gapminder.iterrows(): \n    gapminder.loc[lab, \"continente_mayus\"] = row[\"continent\"].upper()\n\n# Es solo un ejemplo, no es muy eficiente.\n\ngapminder\n\n# Opcion alternativa: buscar un method y vectorizar\n\ngapminder[\"tamaño_continente_vect\"] = gapminder[\"continent\"].apply(len)\n\n# Esta es la opción eficiente.\n\ngapminder\n\ngapminder[\"Cont_mayus_vect\"] = gapminder[\"continent\"].apply(str.upper)\n\ngapminder"},{"path":"análisis-de-datos-con-python.html","id":"iteradores-adicionales-en-python","chapter":"7 Análisis de Datos con Python","heading":"7.3.4 Iteradores adicionales en Python","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"enumerate","chapter":"7 Análisis de Datos con Python","heading":"7.3.4.1 Enumerate","text":"usar cuando: queremos tener registro tanto del indice como del valor asociado","code":"L = [2, 4, 6, 8, 10]\n\nfor i in range(len(L)):\n    print(i, L[i])\n\nfor i, val in enumerate(L):\n    print(i, val)"},{"path":"análisis-de-datos-con-python.html","id":"zip","chapter":"7 Análisis de Datos con Python","heading":"7.3.4.2 Zip","text":"usar cuando: Se quiera iterar sobre múltiples listas simultaneamente","code":"L = [2, 4, 6, 8, 10]\nR = [3, 6, 9, 12, 15]\n\nfor lval, rval in zip(L, R):\n    print(lval, rval)\n\n# Si son de distinto tamaño?\nL = [2, 4, 6, 8, 10]\nR = [3, 6, 9]\nfor lval, rval in zip(L, R):\n    print(lval, rval)\n    \n# Va a utilizar como referencia el más corto"},{"path":"análisis-de-datos-con-python.html","id":"map","chapter":"7 Análisis de Datos con Python","heading":"7.3.4.3 Map","text":"usar cuando: quiera tomar una función y aplicarla al valor dentro de un objeto","code":"# Defino una función\ncuadrado = lambda x: x ** 2\n\nfor val in map(cuadrado, range(100)):\n    print(val, end = \" \")\n    \n# Nota: Una función lambda puede tomar cualquier número de argumentos, pero sólo puede tener una expresión.\n\n# Otro ejemplo: importar funciones especificas a una librearía \nimport math\n\nfor val in map(math.sqrt, range(100)):\n    print(val, end = \" \")"},{"path":"análisis-de-datos-con-python.html","id":"filter","chapter":"7 Análisis de Datos con Python","heading":"7.3.4.4 Filter","text":"similar map, excepto que pasa valores que son considerados verdaderos por la función de filtradoEjercicio 4.3.2: Genere un código que tome una lista de strings y calcule su tamaño.","code":"# Itera sobre los valores para los cuales se cumple la condición\nes_par = lambda x: x % 2 == 0\n\nfor val in filter(es_par, range(100)):\n    print(val, end=' ')lista_strings = [\"Rodrigo\", \"Santiago\", \"Tania\", \"Laura\", \"Nicolás\"]\nlista_final = list(map(len,lista_strings))\nprint(lista_final)"},{"path":"análisis-de-datos-con-python.html","id":"list-comprehension","chapter":"7 Análisis de Datos con Python","heading":"7.3.5 List Comprehension","text":"Es una forma de comprimir la construcción de un objeto utilizando un iterador.01figuras/image.png01figuras/image.png","code":"[i for i in range(20) if i % 2 == 0]\n# El resultado es una lista que solo considera los números pares.\n\n# Un for-loop\nL = []\n\nfor n in range(12):\n    L.append(n ** 2)\nL\n\n[n ** 2 for n in range(12)]\n\n# construir una lista formada por el cuadrado de n para cada n hasta 12"},{"path":"análisis-de-datos-con-python.html","id":"iteración-múltiple","chapter":"7 Análisis de Datos con Python","heading":"7.3.5.1 Iteración Múltiple","text":"objetivo: armar -loops anidados en una linea","code":"[(i, j) for i in range(2) for j in range(3)]\n\n[(i, j, z) for i in range(2) for j in range(3) for z in range(3)]"},{"path":"análisis-de-datos-con-python.html","id":"condicionales-en-el-iterador","chapter":"7 Análisis de Datos con Python","heading":"7.3.5.2 Condicionales en el iterador","text":"Idea: agregar condicionales al final de la expresión.Ejercicio 4.3.3: Escriba los siguientes procesos iterativos en formato de list comprehension:","code":"# Construir una lista de valores para cada valor hasta 20, pero sólo si el valor no es divisible por 3\"\n\n[val for val in range(20) if val % 3 > 0]\n\n# Equivalente a\nL = []\n\nfor val in range(20):\n    if val % 3 > 0:\n        L.append(val)\nL\n\nval = -1000\n\nval if val >= 0 else -val\n\n# ¿Qué función es esta?\n\n# Una más compleja\n# construir una lista, dejando fuera los múltiplos de 3, y haciendo negativos todos los múltiplos de 2.\n\n[val if val % 2 else -val for val in range(20) if val % 3]\n\n# Se puede extender para otro tipo de objetos, por ejemplo a conjuntos\n\n{n**2 for n in range(12)}\n\n{a % 3 for a in range(1000)}# Parte a\nnumeros = [3,5,45,97,32,22,10,19,39,43]\nresultado = []\n\nfor n in numeros:\n  if n % 2 == 0:\n    resultado.append(n)\nprint(resultado)\n\n[n for n in numeros if n % 2 == 0]\n\n# Parte b: Cuente el número de espacios en un string cualquiera\n\nstring = \"Mi nombre es Nicolás Campos \"\n\nlen([x for x in string if x == \" \"])"},{"path":"análisis-de-datos-con-python.html","id":"generadores","chapter":"7 Análisis de Datos con Python","heading":"7.3.6 Generadores","text":"Similar los iteradores de listas, pero ahorra bastate memoria, es eficiente.Un generador es básicamente un recipiente en donde se van colocar valores.Un generador calcula los valores menos que sea necesario, lo cual aumenta su eficiencia.Un generador se puede utilizar una vez, un iteador normal muchas veces","code":"# Hasta el momento\n\n[n ** 3 for n in range(12)]\n\n# Un generador\n(n ** 2 for n in range(12))\n\n# No genera el contenido.\n\n# Si lo quiero expresar\nG = (n ** 2 for n in range(12))\n\nlist(G)# Generadores utilizan parentesis\nG = (n ** 2 for n in range(12))\nlist(G)\n\n# Una lista es una colección de valores\nL = [n ** 2 for n in range(12)]\n\nfor val in L:\n    print(val, end=' ')\n\n# Un generador es un recipiente para guardar valores\nG = (n ** 2 for n in range(12))\n\nfor val in G:\n    print(val, end=' ')\n    \n# El potencial del generador es mayor\n\nL1 = [n ** 2 for n in range(10)]\n\nL2 = []\nfor n in range(10):\n    L2.append(n ** 2)\n\nprint(L1)\nprint(L2)\n\nG1 = (n ** 2 for n in range(10))\n\ndef gen():\n    for n in range(10):\n        yield n ** 2\n\nG2 = gen()\nprint(*G1)\nprint(*G2)\n\n# Utiliza yield para representar una secuencia potencialmente infinita de valores\n\n# Recordemos que estos dos aspectos son iguales\n\nL1 = [n ** 2 for n in range(12)]\n\nL2 = []\nfor n in range(12):\n    L2.append(n ** 2)\n\nprint(L1)\nprint(L2)\n\n# Si queremos hacer la equivalencia, pero con generadores tenemos\n\nG1 = (n ** 2 for n in range(12))\n\ndef gen():\n    for n in range(12):\n        yield n ** 2\n\nG2 = gen()\n\nprint(*G1)\nprint(*G2)\n\n# Noten que tuve que ingresar la función yield en vez de return. \n# La idea es que pueda incluir potencialmente una lista infinita de valores.L = [n ** 2 for n in range(12)]\n\n# Normal \nfor val in L:\n    print(val, end=' ')\nprint()\n\n# Generador\nG = (n ** 2 for n in range(12))\nlist(G)\n\nlist(G)"},{"path":"análisis-de-datos-con-python.html","id":"algoritmo-de-erastotenes","chapter":"7 Análisis de Datos con Python","heading":"7.3.6.1 Algoritmo de erastotenes","text":"Vamos generar un algoritmo que nos permita obtener todos los numeros primos bajo un cierto número natural dado.Es conveniente utilizar generadores cuando se trabajan con grandes bases de datos.Es importante recordar que el generador solo puede ser iterado una vez.es conveniente utilizar generadores cuando necesitamos los valores previos.\nRecuerden que solo itera una vez.Cuando tengo bases de datos pequeñas es necesario.¿Puedo escribir lo anterior con un ? : Si, el generador es por la actualización y el espacio.","code":"# Generamos una lista de candidatos \nL = [n for n in range(2, 10)]\nprint(L)\n\n# Removemos todos los múltiplos de dos salvo si mismo\nL = [n for n in L if n == L[0] or n % L[0] > 0]\nprint(L)\n\n# Removemos todos los múltiplos de tres salvo si mismo\nL = [n for n in L if n == L[1] or n % L[1] > 0]\nprint(L)\n\n# Asi sucesivamente hasta converger\nL = [n for n in L if n == L[2] or n % L[2] > 0]\nprint(L)\n\n# Asi sucesivamente hasta converger\nL = [n for n in L if n == L[3] or n % L[3] > 0]\nprint(L)\n\n# Generamos esto como un generador \n\ndef gen_primos(N):\n    \"\"\"Generar primos hasta N\"\"\"\n    primos = set()\n    for n in range(2, N):\n        if all(n % p > 0 for p in primos):\n            primos.add(n)\n            yield n\n\nprint(*gen_primos(1000))"},{"path":"análisis-de-datos-con-python.html","id":"funciones-2","chapter":"7 Análisis de Datos con Python","heading":"7.3.7 Funciones","text":"Parte importante de cualquier proceso sofisticado de análisis de datosParte importante de cualquier proceso sofisticado de análisis de datosSon lo que conocemos como programas en Stata.Son lo que conocemos como programas en Stata.Tres grupos principales:\nFunciones incorporadas, como help() para pedir ayuda, min() para obtener el valor mínimo, print() para imprimir un objeto en la terminal. Listado: https://docs.python.org/3/library/functions.html\nFunciones definidas por el usuario (UDFs), que son funciones que los usuarios crean para mejorar sus procesos.\nFunciones anónimas, que también se llaman funciones lambda porque se declaran con la palabra clave def estándar.\nTres grupos principales:Funciones incorporadas, como help() para pedir ayuda, min() para obtener el valor mínimo, print() para imprimir un objeto en la terminal. Listado: https://docs.python.org/3/library/functions.htmlFunciones incorporadas, como help() para pedir ayuda, min() para obtener el valor mínimo, print() para imprimir un objeto en la terminal. Listado: https://docs.python.org/3/library/functions.htmlFunciones definidas por el usuario (UDFs), que son funciones que los usuarios crean para mejorar sus procesos.Funciones definidas por el usuario (UDFs), que son funciones que los usuarios crean para mejorar sus procesos.Funciones anónimas, que también se llaman funciones lambda porque se declaran con la palabra clave def estándar.Funciones anónimas, que también se llaman funciones lambda porque se declaran con la palabra clave def estándar.Diferencia entre metodos y funciones?\nMetodo es especifico una clase de objeto\nTodos los metodos son funciones, pero todas las funciones son metodos.\nDiferencia entre metodos y funciones?Metodo es especifico una clase de objetoTodos los metodos son funciones, pero todas las funciones son metodos.","code":""},{"path":"análisis-de-datos-con-python.html","id":"sintaxis-básica-y-algunos-ejemplos","chapter":"7 Análisis de Datos con Python","heading":"7.3.7.1 Sintaxis básica y algunos ejemplos","text":"Una variable local va existir en un lugar específico.Una variable local va existir en un lugar específico.Dentro de la función ejemplo3() definimos =, esta variable existe sólo dentro de la función ejemplo3(), es decir es una variable local.Dentro de la función ejemplo3() definimos =, esta variable existe sólo dentro de la función ejemplo3(), es decir es una variable local.Cuando decimos que = ejemplo3(“hola”) estamos definiendo una variable global, que existe en todo el espacio del Jupyter, una vez definida puede ser llamada en en cualquier parte.Cuando decimos que = ejemplo3(“hola”) estamos definiendo una variable global, que existe en todo el espacio del Jupyter, una vez definida puede ser llamada en en cualquier parte.","code":"def miprimerafuncion(a): #declaramos la función y establecemos el input\n    b = a * 2  #definimos qué hace la función\n    return b #Establecemos un output\n\n# llamamos la función\nmiprimerafuncion(\"nico\")\n\ndef miprimerafuncion_sr(a):\n     a * 2\n\nmiprimerafuncion_sr(15)\n\nb = 100\nprint(miprimerafuncion(b))\nprint(miprimerafuncion(1000))\n\n# Funciones con más de un argumento\n\ndef ejemplo2(a, b): \n    return a * b, a ** b\n\ntype(ejemplo2(10,2))\n\n# Funciones que puedo asignar como tuplas\nX, Y = ejemplo2(10,2)\nprint(\"X =\", X, \"Y =\", Y)\n\n# Funciones sin argumentos \ndef hello():\n  print(\"Hola mundo\") \n  return\n\nhello()\n\ndef fibonacci(N):\n    L = []\n    a, b = 0, 1\n    while len(L) < N:\n        a, b = b, a + b\n        L.append(a)\n    return L\n\nfibonacci()\n\n## Valores por parametros y opciones por defecto\ndef fibonacci_cd(N, a = 0, b = 1):\n    L = []\n    while len(L) < N:\n        a, b = b, a + b\n        L.append(a)\n    return L\n\nfibonacci(10)\n\n# Por defecto\nfibonacci_cd(10)\n\n# Variables locales y variables globales \ndef ejemplo3(a):\n    AA = []\n    AA.append(a)\n    return AA\n\nC = ejemplo3(\"hola\")\nprint(C)\n\nprint(C)# Ahora vamos a crear una función que nos permita interactuar con los/las usuarias\n\n#1. Creamos la función con un input A\ndef ejemplo4(A):\n    \n    #2. Creamos una lista  vacía\n    B = []\n\n    #3. Iteramos sobre A\n    for i in A: \n        \n        #Creamos la primera condición\n        if i >= 0 and i<= 3: \n            B.append(i)\n        #Segunda condición\n        elif i > 3 and i<5: \n            B.append(i*2)\n        #Tercera condición\n        elif i==5:\n            B.append(input(\"Agregar un valor\"))\n        #Todo el resto\n        else: \n            B.append(i**2)\n\n    #4. Output\n    return B\n\n# Creamos una lista entre 0-10\nA = np.arange(0, 11, 1)\n\n#Llamamos la función y guardamos el resultado en una variable global\nB = ejemplo4(A)\n\nprint(\"A:\", A)\nprint(\"B:\", B)\n\n# Otro ejemplo\ndef hello():\n  name = str(input(\"Tu nombre aquí: \"))\n  if name:\n    print (\"Hola \" + str(name))\n  else:\n    print(\"Hola no más\") \n  return \n  \nhello()\n\n# Notemos el uso de return \n\ndef hola():\n  print(\"Hola mundo\") \n  return(\"hola\")\n\nhola() * 2\n\ndef hola_sinreturn():\n  print(\"Hola mundo\")\n\nhola_sinreturn() * 2\n# Arroja un error\n\n# Funciones anidadas (revisar prox clase)\n\ndef display(name):\n    def message():\n        return \"Hello \"\n    return name\n\nprint(message(), display(\" Siva\"))"},{"path":"análisis-de-datos-con-python.html","id":"argumentos-de-las-funciones-en-python","chapter":"7 Análisis de Datos con Python","heading":"7.3.7.2 Argumentos de las funciones en Python","text":"","code":"## Defecto \ndef plus(a,b = 2):\n  return a + b\n  \nplus(1)\n\nplus(1, 3)\n\n# Requeridos: son los que necesita la funcion\ndef plus(a,b):\n  return a + b\n\nplus()\n\n# Argumentos clave\n\ndef plus(a,b):\n  return a - b\n  \nplus(2,3)\n\nplus(b = 2, a = 4)\n\nplus(2,4)"},{"path":"análisis-de-datos-con-python.html","id":"args-and-kwargs-argumentos-flexibles","chapter":"7 Análisis de Datos con Python","heading":"7.3.7.3 args and kwargs: argumentos flexibles","text":"idea: Querer escribir una función en la que sepas inicialmente cuántos argumentos va tener.*args y **kwargs permiten trabajar con este tipo de problemasAquí lo importante son los nombres args y kwargs, sino los caracteres * que los preceden.Aquí lo importante son los nombres args y kwargs, sino los caracteres * que los preceden.args y kwargs son sólo los nombres de las variables que se utilizan menudo por convención, abreviatura de “argumentos” y “argumentos de palabras clave”.args y kwargs son sólo los nombres de las variables que se utilizan menudo por convención, abreviatura de “argumentos” y “argumentos de palabras clave”.La diferencia operativa son los caracteres de asterisco: un solo * antes de una variable significa “expandir esto como una secuencia”, mientras que un doble ** antes de una variable significa “expandir esto como un diccionario”.La diferencia operativa son los caracteres de asterisco: un solo * antes de una variable significa “expandir esto como una secuencia”, mientras que un doble ** antes de una variable significa “expandir esto como un diccionario”.","code":"def catch_all(*args, **kwargs):\n    print(\"args =\", args)\n    print(\"kwargs = \", kwargs)\n\ncatch_all(1, 2, 3, 4,5,65, a = 4, b = 5)\n\ncatch_all('a', keyword=2)# Tambien puedo ocupar esta sintaxis (* y **) al llamar variables dentro de una función\ninputs = (1, 2, 3)\nkeywords = {'pi': 3.14}\ncatch_all(*inputs, **keywords)"},{"path":"análisis-de-datos-con-python.html","id":"anonymous-lambda-functions","chapter":"7 Análisis de Datos con Python","heading":"7.3.7.4 Anonymous (lambda) Functions","text":"Idea: quiero hacer funciones que sean anonimasCaso 1 y Caso 2 son equivalentes¿Por qué quiero escribir entonces funciones lambda?:\nFunción por un periodo corto de tiempo.\nO cuando tenemos una función que tiene como input otras funciones.\nFunción por un periodo corto de tiempo.O cuando tenemos una función que tiene como input otras funciones.","code":"# Caso 1\nadd = lambda x, y: x + y\nadd(1, 2)\n\n# Caso 2 \ndef add(x, y):\n    return x + y\n\n# Noten que esta función no tiene nombre\ndouble = lambda x: x*2\n\ndouble(5)\n\n# Con dos argumentos \nsum = lambda x, y: x + y;\nsum(4,5)from functools import reduce\n\nlista = [1,2,3,4,5,6,7,8,9,10]\n\n# Filtro: filtra según un criterio. En este caso el criterio es una función creada por mi y anonima \nfiltrado_list = list(filter(lambda x: (x*2 > 10), lista))\n\n# Mapeo: aplica una función a un objeto. En este caso una anonima\nmapeo_list = list(map(lambda x: x*2, lista))\n\n# Reducir: reduce la secuencia a un simple valor. Suma el acumulado finalmente.\nreducedico_list = reduce(lambda x, y: x+y, lista)\n\nprint(filtrado_list)\nprint(mapeo_list)\nprint(reducedico_list)\n\n#reduce?"},{"path":"análisis-de-datos-con-python.html","id":"documentar-funciones","chapter":"7 Análisis de Datos con Python","heading":"7.3.7.5 Documentar funciones","text":"Es útil documentar lo que hacen las funciones que hacemosEjercicio 4.3.4:Crear una función que calcule la estimación del ingreso según la ecuación de Mincer:\\(Ln(Y)=\\beta_0+\\beta_1S+\\beta_2^2 Exp+ Exp2\\)Donde:\\(S\\): años de educación\\(Exp\\): experiencia\\(Exp2\\): experiencia al cuadradoUtilice: _0=9.7, _1=0.14, _2=0.07, _3=−0.001","code":"def count_letter(content, letter):\n    \n  \"\"\"Cuenta el número de veces que una `letra` aparece en `contenido`.\n  \n  Argumentos:\n    contenido (str): La palabra (string) donde vamos a ir a buscar.\n    letra (str): La letra que vamos a ir a buscar. \n\n  Retorna:\n    int\n  \"\"\"\n  return len([char for char in content if char == letter])\n\nresult = count_letter(\"nicolascampos\", \"s\")\n\nprint(result)\n\ncount_letter?def mincer(S , Exp):\n    \n    ß0 = 9.7\n    ß1 = 0.14\n    ß2 = 0.07\n    ß3 = -0.001\n    return ß0 + ß1*S + ß2*Exp + ß3*Exp**2\n\neduc_list = np.array([14,8,20])\nexp_list =  np.array([2,20,1])\n\nprint(mincer(educ_list, exp_list))"},{"path":"análisis-de-datos-con-python.html","id":"manipulación-de-bases-de-datos-parte-i","chapter":"7 Análisis de Datos con Python","heading":"7.4 Manipulación de bases de datos (Parte I)","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"importar-e-inspeccionar-datos-repaso","chapter":"7 Análisis de Datos con Python","heading":"7.4.1 Importar e inspeccionar datos (repaso)","text":"¿Cuál es mi directorio?loc se refiere al nombre de la unidad de observación (labeling indexing).iloc se refiere al índice (positional indexing).En este caso son iguales dado que los nombres son efectivamente las posiciones. En casos donde esto\nsea así, es importante considerar la diferencia al utilizar estas funciones.","code":"import os\nos.getcwd()\n\nos.listdir()\n\n# Carpetas \nprincipal = os.getcwd()\ndatos = principal + \"\\\\datos\"\ndatos\nresultados = principal + \"\\\\resultados\"\nresultados\n\n# Recordar como es un dataframe de pandas \nimport pandas as pd \nexam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura', 'Kevin', 'Jonas'],\n        'score': [12.5, 9, 16.5, np.nan, 9, 20, 14.5, np.nan, 8, 19],\n        'attempts': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes']}\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndf = pd.DataFrame(exam_data , index = labels)\ndf\n\n# Importamos datos que utilizaremos durante la clase\nfrom gapminder import gapminder\ngapminder\n\ndf = gapminder\n\ndf.head(10)\n\n# Acceder al nombre de las columnas\ndf.columns\n\n# Acceder al nombre de las filas \nprint(df.index)\n\n# Acceder a valores\ndf.values\n\n# Nuevamente el tipo \ntype(df)\n\n# Forma\ndf.shape\n\n# Inspeccion detallada de las columnas \ndf.info()\n\n# Mirar una sola columna\ndf['country']\n\n# Crear un nuevo dataset\ncountry_df = df[\"country\"]\n\ncountry_df\n\ntype(country_df)\n\n# Tenemos una serie: cada columna es una serie para python. Similar a un arreglo 1D.\n\n# Sunconjuntos de variables \nsubset = df[[\"country\", \"continent\", \"year\"]]\n\nsubset.head()\n\ndf\n\n# Buscamos la fila con el indice 2\ndf.loc[3]\n\n# Buscamos las filas con el id en 0 u 2\ndf.loc[[3,0,100]]\n\ndf.iloc[3]\n\ndf.iloc[[3,0,100]]# Recordar filas y columnas\nsubset = df.loc[:,[\"year\", \"pop\"]]\n\nsubset.head()\n\n# Para hacer filtros en base a condiciones\n# Filtramos todas las filas que tienen como año 1967\ndf.loc[df[\"year\"] == 1967, [\"year\", \"pop\"]]\n\n# Ahora con múltiples condiciones\n# Ocupar paretesis redondos\n# Ocupar entre ellos iperadores de comparación\n# Note como escrbimimos el número\n\ndf.loc[(df[\"year\"] == 1967) & (df[\"pop\"] > 1_000_000), [\"year\", \"pop\"]]\n\n1_000_000\n\nscientists=pd.DataFrame(\ndata={'Occupation':['Chemist','Statistician'],\n'Born':['1920-07-25', '1876-06-13'],\n'Died':['1958-04-16', '1937-10-16'],\n'Age':[37,61]},\nindex=['Rosaline Franklin','William Gosset'],\ncolumns=['Occupation', 'Born','Died','Age'])\n\nscientists\n\nimport os\nos.getcwd()\n\nos.listdir()\n\n# Carpetas \nimport os \nprincipal = os.getcwd()\ndatos = principal + \"\\\\datos\"\ndatos\nresultados = principal + \"\\\\resultados\"\nresultados\n\nscientists = pd.read_csv('datos\\\\scientists.csv')\n\nscientists\n\nages = scientists['Age']\n\nages\n\nages = scientists.Age\nages\n\ntype(ages)\n\nages.mean()\n\nages.shape\n\nages.min()\n\nages.describe()\n\nages[ages > ages.mean()]\n\nages[(ages > ages.mean()) & (ages > 75)]\n\nages[(ages > ages.mean()) & ~(ages > 75)]\n\nages.describe()\n\nages + 100\n\nscientists\n\nscientists[scientists['Age'] > scientists['Age'].mean()]\n\nscientists['Age'] > scientists['Age'].mean()\n\nscientists\n\n# Ordenar valores\nscientists.sort_values(by=\"Age\", ascending = False)\n\nscientists.to_csv('resultados\\\\scientists_clean.csv', index=False)"},{"path":"análisis-de-datos-con-python.html","id":"manipular-repaso","chapter":"7 Análisis de Datos con Python","heading":"7.4.2 Manipular (repaso)","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"renombrar","chapter":"7 Análisis de Datos con Python","heading":"7.4.2.1 Renombrar","text":"","code":"# Renombrar variables: diccionario + rename \n\ndf.rename(columns = {'country': 'pais', 'continent':'continent', \n                   'year':'año', 'lifExp':'esperanza_vida', \n                   'pop':'poblacion', 'gdpPercap':'pib_percapita'})\n\n# Ojo que no hemos asignado\n\ndf\n\ndf2 = df.rename(columns = {'country': 'pais', 'continent':'continente', \n                   'year':'año', 'lifeExp':'esperanza_vida', \n                   'pop':'poblacion', 'gdpPercap':'pib_percapita'})\n\n# Recomendable generar una nueva base de datos, va dejando un registro\n\ndf2"},{"path":"análisis-de-datos-con-python.html","id":"filtrar","chapter":"7 Análisis de Datos con Python","heading":"7.4.2.2 Filtrar","text":"","code":"solo_asia = df[df[\"continent\"] == \"Asia\"]\n\n# Lo que esta adentro es un vecto booleano\ndf[\"continent\"] == \"Asia\"\n\ndf_asia = solo_asia\nprint(df_asia)"},{"path":"análisis-de-datos-con-python.html","id":"replazar","chapter":"7 Análisis de Datos con Python","heading":"7.4.2.3 Replazar","text":"","code":"datos_remplazados = df.replace(\"Asia\", \"asia\")\n\nprint(datos_remplazados)"},{"path":"análisis-de-datos-con-python.html","id":"remover","chapter":"7 Análisis de Datos con Python","heading":"7.4.2.4 Remover","text":"Ejercicio 4.4.1: Escriba una función llamada ins_simpl que le permita usted obtener un resumen exploratorio de su base de datos.La función debe tener como input un dataframe y retornar 5 outputs:\nPrimerar y últimas 8 filas.\nTipo de datos para todas las columnas\nNombre de las columnas\nDescripción completa de la base de datos (metodo: describe)\nEl mínimo, máximo y el promedio de la columna 4 de la base de datos.\nPrimerar y últimas 8 filas.Tipo de datos para todas las columnasNombre de las columnasDescripción completa de la base de datos (metodo: describe)El mínimo, máximo y el promedio de la columna 4 de la base de datos.","code":"remover_columna = df.drop(\"pop\", axis = 1)\n\nprint(remover_columna)\n\nremover_dos_columnas = df.drop([\"gdpPercap\", \"pop\"], axis = 1)\n\nprint(remover_dos_columnas)\n\nremover_filas = df.iloc[0:100]\n\nprint(remover_filas)\n\n#### Agregar nuevas filas\n\nnuevas_filas = {\"country\": \"nuevo\", \n                \"continent\": \"Asia\", \n                \"year\" : \"2022\", \n               \"gdpPercap\" : \"100000000\"}\n\n\ndf.append(nuevas_filas, ignore_index = True)\n# Para agregar un diccionario es importante la úñtima opción"},{"path":"análisis-de-datos-con-python.html","id":"agrupar-calculos","chapter":"7 Análisis de Datos con Python","heading":"7.4.3 Agrupar calculos","text":"","code":"df = gapminder\n\ndf.head()\n\ndf.groupby('year')['gdpPercap'].mean()\n\ndf.groupby(['year', 'continent'])[['lifeExp', 'gdpPercap']].mean()\n\n# Otra forma de escribir lo mismo\ndf\\\n    .groupby(['year', 'continent'])[['lifeExp', 'gdpPercap']]\\\n    .mean()\n\n# Para dejarlos como un dataframe -> reset_index\n(df\n    .groupby(['year', 'continent'])[['lifeExp', 'gdpPercap']]\n    .mean()\n    .reset_index()\n)\n\n# Combinar lo que obtengo con visualizaciones \nimport matplotlib.pyplot as plt\n\n(df\n     .groupby(['year'])\n     [['lifeExp']]\n     .mean()\n     .plot()\n)\nplt.show()\n\n# Imprima un subconjunto de datos\nsolo_asia = gapminder.groupby('continent')\nasiaDf = solo_asia.get_group('Asia')\nasiaDf\n\n# Contar valores por grupp\ngapminder[\"continent\"].value_counts()\n\n# Agrupar un data frame: group_by \n\ndf_agrupado = df.groupby('year')\ndf_agrupado.mean()\n\n# Sobre una variable en particular\ndf_agrupado = df[['gdpPercap', 'year']].groupby('year')\ndf_agrupado.mean()\n\n# Podemos agrupar varias operaciones\ndf.groupby(\"year\").agg({\"lifeExp\": np.mean, \"gdpPercap\": np.size})\n\n# Agrupar por mas de una variable: año y continente\ndf.groupby([\"year\", \"continent\"]).agg({\"lifeExp\": np.mean, \"gdpPercap\": np.size})"},{"path":"análisis-de-datos-con-python.html","id":"aplicar-funciones","chapter":"7 Análisis de Datos con Python","heading":"7.4.4 Aplicar funciones","text":"Idea: Aplicar funciones columnas de una dataframeEjercicio 4.4.2:Importe Automobile data. Llamela df_autoImprima las últimas dos filas de las primeras cinco filas¿Cuál es el auto más caro de la compañía?Cree un data frame con toda la info de los autos toyota.¿Cuál es el número total de autos de la compañia?¿Cuàl es el auto más caro de cada compañia?¿Cuales son las millas promedio por compañia?Ordene la base de datos por precio de los autos.","code":"def my_sq(x):\n    return x ** 2\n\nmy_sq(4)\n\ndef avg_2(x, y):\n    return (x + y) / 2\n\navg_2(10, 20)\n\nassert avg_2(10, 20) == 15.0\n\n# Esta es una función muy importante para programar defensivamente!!!\n\n# Apliquemos funciones a un dataframe\ndf=pd.DataFrame({'a':[10,20,30],\n                 'b':[20,30,40]})\ndf\n\ndf['a'] ** 2\n\ntype(df['a'])\n\ndef my_sq(x):\n    # assert isinstance(x, int)\n    return x ** 2\n\n# Con el metodo apply lo utilizamos \ndf['a'].apply(my_sq)\n\ndef my_exp(x, e):\n    return x ** e\n\nmy_exp(2, 2)\n\nmy_exp(2, 10)\n\ndf['a'].apply(my_exp, e=10)\n\ndef print_me(x):\n    print(x)\n\ndf\n\ndf.apply(print_me, axis=1)\n\nimport numpy as np\n\ndf.apply(np.mean)"},{"path":"análisis-de-datos-con-python.html","id":"datos-como-tablas-problemas-y-soluciones","chapter":"7 Análisis de Datos con Python","heading":"7.4.5 Datos como tablas: problemas y soluciones","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-1-encabezados-de-las-columnas-son-valores-y-no-nombres-de-variables","chapter":"7 Análisis de Datos con Python","heading":"7.4.5.1 Ejemplo 1: encabezados de las columnas son valores y no nombres de variables","text":"Noten que esta es una base de datos donde tenemos información en las columnas","code":"# Fijar directorio principal \nmain = os.getcwd()\n\nmain\n\n# Fijar directorios secundarios en base al principal\ndatos = main + \"\\\\datos\"\n\ndatos\n\npew = pd.read_csv('datos\\\\pew-raw.csv')\npew\n\npew.head()# Formato wide a formato 2 \npew_long = pd.melt(pew, \n                   id_vars = \"religion\")\n\npew_long.head()\n\npew_long = pd.melt(pew, \n                   id_vars = \"religion\", \n                   var_name = \"ingreso\")\n\npew_long.head()\n\npew_long = pd.melt(pew, \n                   id_vars = \"religion\", \n                   var_name = \"ingreso\", \n                   value_name = \"Casos\")\n\npew_long.head()\n\npew\n\nbillboard = pd.read_csv(\"datos\\\\billboard.csv\")\n\nbillboard\n\nbillboard_long = pd.melt(\n    billboard, \n    id_vars = [\"year\", \"artist\", \"track\", \"time\", \"date.entered\"],\n    var_name = \"week\", \n    value_name = \"rating\"\n)\n\nbillboard_long\n\nbillboard.shape\n\nbillboard_long.shape"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-2-múltiples-variables-guardadas-en-la-columna-1","chapter":"7 Análisis de Datos con Python","heading":"7.4.5.2 Ejemplo 2: múltiples variables guardadas en la columna 1","text":"","code":"ebola = pd.read_csv(\"datos\\\\country_timeseries.csv\")\n\nebola.head()\n\nebola_long = pd.melt(ebola, \n                     id_vars = [\"Date\", \"Day\"])\n\nebola_long.head()\n\n# Vamos a quitar \"Cases\" en variables\n\n\"cases_Guinea\".split(\"_\")\n\n# Columna + tipo de acceso + metodo\nvariable_split = ebola_long[\"variable\"].str.split(\"_\")\n\nvariable_split\n\n# Veamos que tiene por partes \ntype(variable_split)\n\ntype(variable_split[0])\n\ntype(variable_split[0][0])\n\nvariable_split.str.get(0)\n\nvariable_split.str.get(1)\n\n# Ahora vamos a utilizar estos dos para actualizar base de datos \n\nebola_long[\"stats\"] = variable_split.str.get(0)\nebola_long[\"country\"] = variable_split.str.get(1)\n\nebola_long\n\n# Otra forma de hacer lo anterior\n\nebola_long[[\"stats_e\", \"country_e\"]] = (ebola_long[\"variable\"].str.split(\"_\", expand = True))\n\nebola_long"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-3-mútiples-tipos-de-unidades-de-observación-en-la-misma-tabla","chapter":"7 Análisis de Datos con Python","heading":"7.4.5.3 Ejemplo 3: Mútiples tipos de unidades de observación en la misma tabla","text":"","code":"# Variables guradads en filas y columnas \nweather = pd.read_csv(\"datos\\\\weather.csv\")\n\nweather\n\nweather_melt = pd.melt(\n        weather, \n        id_vars = [\"id\", \"year\", \"month\", \"element\"], \n        var_name = \"day\",\n        value_name = \"temp\"\n)\n\nweather_melt\n\nweather_tidy = weather_melt.pivot_table(\n    index = [\"id\", \"year\", \"month\", \"day\"], \n    columns = \"element\", \n    values = \"temp\"\n)\n\nweather_tidy\n# Nos permite ver la jerarquí ad elos datos que tenemos\n\nweather_tidy.reset_index()\n\n# Multiple Types of Observational Unit in Same Table (i.e De-nomalized Table)\n\nbillboard_long.head()\n\nbillboard_long.loc[billboard_long[\"track\"] == \"Loser\"]\n\nbillboard_songs = billboard_long[[\"year\", \"artist\", \"track\", \"time\"]]\n\nbillboard_songs.shape\n\nbillboard_songs.head()\n\n# Eliminar duplicados \nbillboard_songs = billboard_songs.drop_duplicates()\n\nbillboard_songs[\"id\"] = range(len(billboard_songs))\n\nbillboard_songs.head()\n\n# Guardamos \nresultados = main + \"\\\\resultados\"\nbillboard_songs.to_csv(\"resultados\\\\billboard_songs.csv\", index =  False)\n\n# Combinemos dos bases de datos \nbillboard_ratings = billboard_long.merge(\n\n    billboard_songs, \n    on = [\"year\", \"artist\", \"track\", \"time\"]\n\n\n)\n\nbillboard_ratings.head()\n\nbillboard_ratings = billboard_ratings[[\"id\", \"date.entered\", \"week\", \"rating\"]]\n\nbillboard_ratings.head()\n\nbillboard_ratings.sample(20)\n\nbillboard_songs.to_csv(\"resultados\\\\billboard_ratings.csv\", index =  False)\n\nbillboard_ratings.info()\n\nbillboard_long.info()"},{"path":"análisis-de-datos-con-python.html","id":"concatenar-data-frames","chapter":"7 Análisis de Datos con Python","heading":"7.4.6 Concatenar data frames","text":"","code":"import pandas as pd\n\ndf1 = pd.read_csv('datos\\\\concat_1.csv')\ndf2 = pd.read_csv('datos\\\\concat_2.csv')\ndf3 = pd.read_csv('datos\\\\concat_3.csv')\n\ndf1\n\ndf2\n\ndf3\n\nrow_concat = pd.concat([df1, df2, df3])\nrow_concat\n\nrow_concat.loc[0]\n\nrow_concat.iloc[0]\n\nrow_concat.ix[0]\n\nrow_concat_reset = pd.concat([df1, df2, df3], ignore_index=True)\nrow_concat_reset\n\nnew_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])\nnew_row_series\n\npd.concat([df1, new_row_series])\n\nnew_row_data = pd.DataFrame([['n1', 'n2', 'n3', 'n4']],\n                            columns=['A', 'B', 'C', 'D'])\nnew_row_data\n\npd.concat([df1, new_row_data])\n\ncol_concat = pd.concat([df1, df2, df3], axis=1)\ncol_concat\n\ncol_concat['A']\n\ncol_concat_ignore = pd.concat([df1, df2, df3], axis=1, ignore_index=True)\ncol_concat_ignore\n\ndf1.columns=['A','B','C','D']\ndf2.columns=['E','F','G','H']\ndf3.columns=['A','H','F','C']\n\ndf1\n\ndf2\n\ndf3\n\npd.concat([df1, df2, df3])"},{"path":"análisis-de-datos-con-python.html","id":"manipulación-de-bases-de-datos-parte-ii","chapter":"7 Análisis de Datos con Python","heading":"7.5 Manipulación de bases de datos (Parte II)","text":"","code":"# Preambulo\n\n# Librerías: \n\nimport pandas as pd                    # datos\nimport matplotlib as mpl               # graficos\nimport numpy as np                     # arreglos\nimport datetime as dt                  # fechas \nimport os                              # directorios \n\n# Fecha\nprint(dt.date.today())"},{"path":"análisis-de-datos-con-python.html","id":"aplicar-funciones-a-columnas-de-una-base-de-datos","chapter":"7 Análisis de Datos con Python","heading":"7.5.1 Aplicar funciones a columnas de una base de datos","text":"Idea: Aplicar funciones columnas de una dataframeRecuerden todo el instrumental que aprendimos en las clases anteriores respecto al uso de funciones!Ejercicio 4.5.1:  + Escribir una función que reciba una muestra de números en una lista y devuelva un diccionario con su media, varianza y desviación estandar.","code":"# Apliquemos funciones a un dataframe\n\ndf = pd.DataFrame({'a':[10,20,30],\n                 'b':[20,30,40]})\ndf\n\n# Aplico directamente a una columna, la cual llamo por su nombre\ndf2 = df['a'] ** 2\ndf2\n\ndf['b'] ** 2\n\ntype(df['a'])\n\n# Defino una función cualquiera, tan compleja como lo desee\ndef my_sq(x):\n    return x ** 2\n\ndf['b'].apply(my_sq)\n\n# Podemos hacer lo mismo con funciones que tengan más de un parámetro\ndef my_exp(x, e):\n    return x ** e\n\nmy_exp(2, 2)\n\nmy_exp(2, 10)\n\n# Noten que puedo incluir valores para los argumentos adicionales al principal! \ndf['a'].apply(my_exp, e=5)\n\ndef print_me(x):\n    print(x)\n\ndf\n\ndf.apply(print_me, axis = 1)\n\n# Tambien puedo incluir funciones de otras librerías de python \ndf.apply(np.mean)import math\ndef statistics(sample): \n    \"\"\"\n    Función que calcula media, varianza y desv.estandar \n    \n    Parámetros:\n    sample: Es una lista de números \n    \n    Output \n    Diccionario con media, varianza y desv. estandar. \n    \n    \"\"\"\n    \n    stat = {}\n    stat['media'] = sum(sample)/len(sample)\n    stat['varianza'] = sum(np.square(sample))/len(sample)-stat['media']**2\n    stat['desviacion estandar'] = stat['varianza']**0.5\n    return stat\n\nprint(statistics([1, 2, 3, 4, 5]))"},{"path":"análisis-de-datos-con-python.html","id":"dividir-una-base-de-datos-por-grupos","chapter":"7 Análisis de Datos con Python","heading":"7.5.2 Dividir una base de datos por grupos","text":"Ejercicio 4.5.2: Análisis exploratorio de una base de datosImporte desde su carpeta de trabajo auto.csv, llame esta base de datos df.Imprima las últimas dos filas de las primeras cinco filas de df.¿Cuál es el auto más caro de la compañía?Cree un data frame con toda la información disponible para los autos toyota.¿Cuál es el número total de autos de la compañia?¿Cuál es el auto más caro de cada compañia?¿Cuales son las millas promedio por compañia?Ordene la base de datos por precio de los autos.","code":"df = pd.read_csv(\n'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv')\ndf\n\ndf.groupby('sexo').groups\n# groups devuelve un diccionario cuyas claves categoricas y valores son indices\n\n# Si quiero obtener uno de estos grupos utilizo get_group\ndf.groupby('sexo').get_group('M')\n\ndf.groupby('sexo').get_group('H')\n\n# Hacer esto es útil si es que quiero aplicar una función a ciertos subconjuntos de observaciones \n\n# Voy a agrupar por sexo y voy a calcular el promedio de cada variable\ndf.groupby('sexo').agg(np.mean)\n\n# Noten que utilizamos agg. Este metodo aplica la función a todo el dataframe\n\ndf.groupby('sexo').agg(np.sum)\n\ndf.groupby('sexo').agg(np.std)# Importar datos \n\nmain = os.getcwd()\ndatos = main + \"\\\\datos\"\ndatos\n\ndf = pd.read_csv(\"datos\\\\auto.csv\")\ndf\n\ndf.tail()\n\n# 2. Inspeccionar \ndf.head(5).tail(2)\n\n# El auto más caro \ndf[['company','price']][df.price==df['price'].max()]\n\n# 4. Imprima todos los autos de toyota y su información\n\ntoyotaDf = df.groupby('company').get_group('toyota')\ntoyotaDf\n\n# 5. ¿Cuál es el número total de autos por compañia? \ndf['company'].value_counts()\n\n# 6. ¿Cuál es el auto más caro de cada compañia?\n\ncompanias = df.groupby('company')\nprecioMaxComp = companias['price'].max()\nprecioMaxComp\n\n# 7. Millas promedio por cada compañia \n\ngrupo = df.groupby('company')\nmillasDF = grupo['company','average-mileage'].mean()\nmillasDF\n\n# 8. Ordene por precio columna \ndf = df.sort_values(by=['price'], ascending=True)\ndf.head(5)"},{"path":"análisis-de-datos-con-python.html","id":"librería-datetime","chapter":"7 Análisis de Datos con Python","heading":"7.5.3 Librería datetime","text":"Para manejar fechas en Python se suele utilizar la librería datetimePara manejar fechas en Python se suele utilizar la librería datetimeEsta incluye los tipos de datos date, time y datetime para representar fechas y funciones para manejarlas.Esta incluye los tipos de datos date, time y datetime para representar fechas y funciones para manejarlas.Algunas de las operaciones más habituales que permite son:Algunas de las operaciones más habituales que permite son:Acceder los distintos componentes de una fecha (año, mes, día, hora, minutos, segundos y microsegundos).Acceder los distintos componentes de una fecha (año, mes, día, hora, minutos, segundos y microsegundos).Convertir cadenas con formato de fecha en los tipos date, time o datetime.Convertir cadenas con formato de fecha en los tipos date, time o datetime.Convertir fechas de los tipos date, time o datetime en cadenas formateadas de acuerdo diferentes formatos de fechas.Convertir fechas de los tipos date, time o datetime en cadenas formateadas de acuerdo diferentes formatos de fechas.Hacer aritmética de fechas (sumar o restar fechas).Hacer aritmética de fechas (sumar o restar fechas).Comparar fechas.Comparar fechas.Los tipos de datos date, time y datetimeLos tipos de datos date, time y datetimedate(año, mes, dia) : Devuelve un objeto de tipo date que representa la fecha con el año, mes y dia indicados.date(año, mes, dia) : Devuelve un objeto de tipo date que representa la fecha con el año, mes y dia indicados.time(hora, minutos, segundos, microsegundos) : Devuelve un objeto de tipo time que representa un tiempo la hora, minutos, segundos y microsegundos indicados.time(hora, minutos, segundos, microsegundos) : Devuelve un objeto de tipo time que representa un tiempo la hora, minutos, segundos y microsegundos indicados.datetime(año, mes, dia, hora, minutos, segundos, microsegundos) : Devuelve un objeto de tipo datetime que representa una fecha y hora con el año, mes, dia, hora, minutos, segundos y microsegundos indicados.datetime(año, mes, dia, hora, minutos, segundos, microsegundos) : Devuelve un objeto de tipo datetime que representa una fecha y hora con el año, mes, dia, hora, minutos, segundos y microsegundos indicados.Usualmente vamos querer cambiar formato de fechas, para hacerlo ocupamos marcadores de posición:\n%Y (año completo),\n%y (últimos dos dígitos del año),\n%m (mes en número),\n%B (mes en palabra),\n%d (día),\n%(día de la semana),\n%(día de la semana abrevidado),\n%H (hora en formato 24 horas),\n%(hora en formato 12 horas),\n%M (minutos), %S (segundos),\n%p (o PM),\n%C (fecha y hora completas),\n%x (fecha completa),\n%X (hora completa).\nUsualmente vamos querer cambiar formato de fechas, para hacerlo ocupamos marcadores de posición:%Y (año completo),%y (últimos dos dígitos del año),%m (mes en número),%B (mes en palabra),%d (día),%(día de la semana),%(día de la semana abrevidado),%H (hora en formato 24 horas),%(hora en formato 12 horas),%M (minutos), %S (segundos),%p (o PM),%C (fecha y hora completas),%x (fecha completa),%X (hora completa).","code":"# Devuelve un objeto de tipo date que representa la fecha con el año, mes y dia indicados.\nfecha1 = dt.date(2020, 12, 25)\nprint(fecha1)\n\ntype(dt.time(13,30,5))\n\n# Devuelve un objeto de tipo time que representa un tiempo la hora, minutos, segundos y microsegundos indicados.\n\nprint(dt.datetime(2020, 12, 25, 13, 30, 5))\n# Representa una fecha y hora con el año, mes, dia, hora, minutos, segundos y microsegundos indicados.\n\n# Acceder a los elementos de una fecha\nprint(dt.date.today())\n\nfrom datetime import date, time, datetime\n\ndt.month\n\ndt.day\n\ndt.hour\n\ndt.minute\n\ndt.second\n\ndt.microsecond# Conversión de fechas en cadenas con diferentes formatos\nfrom datetime import date, time, datetime\nd = datetime.now()\nprint(d.strftime('%d-%m-%Y'))\nprint(d.strftime('%A, %d %B, %y'))\nprint(d.strftime('%H:%M:%S'))\nprint(d.strftime('%H horas, %M minutos y %S segundos'))\n\n# Función strptime(s, formato):\nprint(datetime.strptime('15/4/2020', '%d/%m/%Y'))\n\nprint(datetime.strptime('2020-4-15 20:50:30', '%Y-%m-%d %H:%M:%S'))\n\n# Para hacer aritmetica de fechas utilizar \n# timedelta(dias, segundos, microsegundos) : intervalo de tiempo con los dias, segundos y micorsegundos indicados.\nfrom datetime import date, time, datetime, timedelta\nd1 = datetime(2020, 1, 1)\nd2 = d1 + timedelta(200000, 3600)\nprint(d2)\n\n## Convertir una columna con to_datetime para que sea del tipo fecha\n\n# to_datetime(columna, formato): devuelve la serie que resulta de convertir las cadenas de la columna con el \n# nombre columna en fechas del tipo datetime con el formado especificado en formato\n\ndf = pd.DataFrame({'Name': ['María', 'Carlos', 'Carmen'], 'Nacimiento':['05-03-2000', '20-05-2001', '10-12-1999']})\ndf\n\ndf['Nacimiento']\n\ndf1 = pd.to_datetime(df.Nacimiento, format = '%d-%m-%Y')\ndf1"},{"path":"análisis-de-datos-con-python.html","id":"reestructurar-un-dataframe","chapter":"7 Análisis de Datos con Python","heading":"7.5.4 Reestructurar un DataFrame","text":"Formato ancho largo y viceversaDiferencias entre ambos?01figuras/image.png","code":"datos = {'nombre':['Pepe', 'Pepa', 'Nico'],\n'edad':[18, 22, 20],\n 'Matemáticas':[8.5, 9, 3.5],\n 'Economía':[8, 8.5, 5],\n'Programación':[6.5, 9.2, 9]}\n\ndf = pd.DataFrame(datos)\n\ndf\n# Esta en formato ancho\n\n# Para pasar a formato largo utilizo melt()\n\ndf1 = df.melt(id_vars=['nombre', 'edad'], \n              var_name='asignatura', \n              value_name='nota')\ndf1\n\n# Ahora pasemosla de vuelta a un formato ancho el metodo pivot()\n\ndf2 = df1.pivot(index=['nombre', 'edad'],\n          columns='asignatura', \n          values= \"nota\")\n\ndf2"},{"path":"análisis-de-datos-con-python.html","id":"reestructurar-datos-para-arreglar-posibles-problemas","chapter":"7 Análisis de Datos con Python","heading":"7.5.5 Reestructurar datos para arreglar posibles problemas","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"caso-1-encabezados-de-las-columnas-son-valores-y-no-nombres-de-variables","chapter":"7 Análisis de Datos con Python","heading":"7.5.5.1 Caso 1: encabezados de las columnas son valores y no nombres de variables","text":"Noten que esta es una base de datos donde tenemos información en las columnas","code":"# Fijar directorio principal \nmain = os.getcwd()\n\nmain\n\n# Fijar directorios secundarios en base al principal\ndatos = main + \"\\\\datos\"\n\ndatos\n\npew = pd.read_csv('datos\\\\pew-raw.csv')\npew\n\npew.head()# Formato wide a formato 2 \npew_long = pd.melt(pew, \n                   id_vars = \"religion\")\n\npew_long.head(10)\n\npew_long = pd.melt(pew, \n                   id_vars = \"religion\", \n                   var_name = \"ingreso\")\n\npew_long.head()\n\npew_long = pd.melt(pew, \n                   id_vars = \"religion\", \n                   var_name = \"ingreso\", \n                   value_name = \"Casos\")\n\npew_long.head()\n\npew\n\nbillboard = pd.read_csv(\"datos\\\\billboard.csv\")\n\nbillboard\n\nbillboard_long = pd.melt(\n    billboard, \n    id_vars = [\"year\", \"artist\", \"track\", \"time\", \"date.entered\"],\n    var_name = \"week\", \n    value_name = \"rating\"\n)\n\nbillboard_long\n\nbillboard.shape\n\nbillboard_long.shape\n\n317*76"},{"path":"análisis-de-datos-con-python.html","id":"caso-2-múltiples-tipos-de-datos-en-una-misma-columna","chapter":"7 Análisis de Datos con Python","heading":"7.5.5.2 Caso 2: múltiples tipos de datos en una misma columna","text":"","code":"ebola = pd.read_csv(\"datos\\\\country_timeseries.csv\")\n\nebola.head()\n\nebola_long = pd.melt(ebola, \n                     id_vars = [\"Date\", \"Day\"])\n\nebola_long\n\n# Vamos a quitar \"Cases\" en variables\n\n\"cases_Guinea\".split(\"_\")\n\n# Columna + tipo de acceso + metodo\nvariable_split = ebola_long[\"variable\"].str.split(\"_\")\n\nvariable_split\n\n# Veamos que tiene por partes \ntype(variable_split)\n\ntype(variable_split[0])\n\nvariable_split[0][1]\n\nvariable_split.str.get(0)\n\nvariable_split.str.get(1)\n\n# Ahora vamos a utilizar estos dos para actualizar base de datos \n\nebola_long[\"stats\"] = variable_split.str.get(0)\nebola_long[\"country\"] = variable_split.str.get(1)\n\nebola_long\n\n# Otra forma de hacer lo anterior\n\nebola_long[[\"stats_e\", \"country_e\"]] = (ebola_long[\"variable\"].str.split(\"_\", expand = True))\n\nebola_long"},{"path":"análisis-de-datos-con-python.html","id":"caso-3-multiples-variables-guardadas-en-una-columna","chapter":"7 Análisis de Datos con Python","heading":"7.5.5.3 Caso 3: Multiples variables guardadas en una columna","text":"notes raw data set:columns starting “m” “f” contain multiple variables:\nSex (“m” “f”)\nAge Group (“0-14”,“15-24”, “25-34”, “45-54”, “55-64”, “65”, “unknown”)\nSex (“m” “f”)Age Group (“0-14”,“15-24”, “25-34”, “45-54”, “55-64”, “65”, “unknown”)Mixture 0s missing values(“NaN”). due data collection process distinction important dataset.","code":"df = pd.read_csv(\"datos\\\\tb-raw.csv\")\ndf\n\ndf = pd.melt(df, id_vars=[\"country\",\"year\"], value_name=\"cases\", var_name=\"sex_and_age\")\n\n# Extract Sex, Age lower bound and Age upper bound group\ntmp_df = df[\"sex_and_age\"].str.extract(\"(\\D)(\\d+)(\\d{2})\", expand=False)    \n\n# Name columns\ntmp_df.columns = [\"sex\", \"age_lower\", \"age_upper\"]\n\n# Create `age`column based on `age_lower` and `age_upper`\ntmp_df[\"age\"] = tmp_df[\"age_lower\"] + \"-\" + tmp_df[\"age_upper\"]\n\n# Merge \ndf = pd.concat([df, tmp_df], axis=1)\n\n# Drop unnecessary columns and rows\ndf = df.drop(['sex_and_age',\"age_lower\",\"age_upper\"], axis=1)\ndf = df.dropna()\ndf = df.sort_values(ascending=True,by=[\"country\", \"year\", \"sex\", \"age\"])\ndf.head(10)"},{"path":"análisis-de-datos-con-python.html","id":"caso-4-variables-guardadas-en-filas-y-columnas","chapter":"7 Análisis de Datos con Python","heading":"7.5.5.4 Caso 4: Variables guardadas en filas y columnas","text":"Ejercicio 4.5.3: Reestructurar datosCree una base de datos llamada productos con la siguiente informaciónTransforme la base de datos de formato largo formato ancho.","code":"# Variables guradads en filas y columnas \nweather = pd.read_csv(\"datos\\\\weather.csv\")\n\nweather\n\nweather_melt = pd.melt(\n        weather, \n        id_vars = [\"id\", \"year\", \"month\", \"element\"], \n        var_name = \"day\",\n        value_name = \"temp\"\n)\n\nweather_melt\n\nweather_tidy = weather_melt.pivot_table(\n    index = [\"id\", \"year\", \"month\", \"day\"], \n    columns = \"element\", \n    values = \"temp\"\n)\n\nweather_tidy\n# Nos permite ver la jerarquí ad elos datos que tenemos\n\nweather_tidy.reset_index()productos = pd.DataFrame({'categoria': ['Cleaning', 'Cleaning', 'Entertainment', 'Entertainment', 'Tech', 'Tech'],\n        'tienda': ['Walmart', 'Dia', 'Walmart', 'Fnac', 'Dia','Walmart'],\n        'precio':[1142, 2350, 1999, 1595, 5575, 11134],\n        'puntaje': [4, 3, 5, 7, 5, 8]})\n\nproductos\n\nproductos_ancho = productos.pivot(index = \"categoria\", \n                                  columns = \"tienda\", \n                                  values = \"precio\")\nproductos_ancho"},{"path":"análisis-de-datos-con-python.html","id":"missing-values-1","chapter":"7 Análisis de Datos con Python","heading":"7.5.6 Missing values","text":"Datos del mundo real rara vez son homogeneos.Muchos valores van encontrarse por múltiples razones.Vamos discutir un poco sobre este tipo de valores en la librería de pandas.NaN puede utilizarse como valor numérico en operaciones matemáticas, mientras que None puedeNaN es un valor numérico flotantes None es un tipo interno de Python ( NoneType ) y sería más como “inexistente” o “vacío” que “numéricamente inválido” en este contexto.","code":""},{"path":"análisis-de-datos-con-python.html","id":"realidad-cuando-importamos-datos","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.1 Realidad cuando importamos datos","text":"","code":"df = pd.read_csv('datos//survey_visited.csv')\n\npd.read_csv('datos//survey_visited.csv', keep_default_na=False)\n\npd.read_csv('datos//survey_visited.csv', \n            na_values=[619, 622])"},{"path":"análisis-de-datos-con-python.html","id":"dos-funciones-claves","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.2 Dos funciones claves","text":"","code":"df\n\n# Funciones is.na() y notna()\n\npd.isna(df[\"dated\"])\n\n# Resultado: valor booleano donde me indica que valores son na\n\n# Toda la base de datos \npd.isna(df)\n\npd.notna(df[\"dated\"])\n\n# Resultado: inverso de lo anterior\n\npd.notna(df)\n\ndf.notna()\n\ndf.isna()"},{"path":"análisis-de-datos-con-python.html","id":"calculos-con-missing-values","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.3 Calculos con missing values","text":"Entre objetos los missing values se traspasan. se tratan como un cero.","code":"a = pd.DataFrame({\"one\": [None, None, 0.11, -2.1, -2.2], \n                   \"two\":[32, 4, 0.11, -2.1, -2.2] })\n\na\n\nb = pd.DataFrame({\"one\": [None, None, 0.11, -2.1, None], \n                  \"two\":[32, 4, 0.11, None, -2.2], \n                  \"three\":[322, 43, 1.3, 2.1, 2.2]})\n\nb\n\nc = a + b\nca\n\n# ¿Qué ocurre cuando queremos hacer estadistica descriptiva? \na[\"one\"].sum()\n# Cuando sumamos todos los valores son tratados como ceros.\n\nc\n\nc[\"three\"].sum()\n\n# Si todos los valors son NA la suma será cero.\n\n# Otros tipos de funciones, por ejemplo, cumsum() ignoar NA por defecto, \n# pero los preservan en los resultados \n\na\n\na.cumsum()\n\n# Si quiero incorporar el hecho de que existen missing values, ocupo la opción skipna = False\na.cumsum(skipna = False)\n\na"},{"path":"análisis-de-datos-con-python.html","id":"qué-hacer-con-los-missing-values","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.4 ¿Qué hacer con los missing values?","text":"","code":"df = pd.read_csv('datos//banglore.csv')\ndf.head(10)\n\ndf.shape\n\ndf.isnull().head(10)\n# Verdadero: significa NaN\n\ndf.isnull?\n\n# Ahora vamos a sumar los missing values por columna\ndf.isna().sum()\n\n5502/13320\n\n# Contamos el total\ndf.isnull().sum().sum()"},{"path":"análisis-de-datos-con-python.html","id":"fillna","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.5 fillna()","text":"","code":"# Vamos a remplazar los valores missing por alguno arbitrario\ndf2 = df.fillna(value = 0)\n\ndf.head(10)\n\ndf2.head(10)\n\ndf2.isnull().sum().sum()\n\ndf3 = df.fillna(value = 5)\n\ndf3.isnull().sum().sum()\n\ndf.head(10)\n\n# Llenar valores con alguno previo\n\ndf4 = df.fillna(method = \"pad\")\ndf4.head(10)\n\n# Llenar valores con algun valor posterior\n\ndf5 = df.fillna(method = \"bfill\")\ndf5.head(10)\n\ndf.head(10)\n\ndf6 = df.fillna(method = \"pad\", axis = 1)\ndf6.head(10)\n\ndf7 = df.fillna(method = \"bfill\", axis = 1)\ndf7.head(10)\n\ndf.head(10)\n\n# Llenar diferentes valores en diferentes columnas\ndf8 = df.fillna({\"society\":\"abc\", \n                 \"balcony\": \"defg\"})\ndf8.tail(10)\n\n# Ahora vamos a completarlo con el promedio de la columna \n\ndf9 = df.fillna(value = df[\"balcony\"].mean())\ndf9.head(10)\n\n# alternativamente podemos ocupar max() o min()"},{"path":"análisis-de-datos-con-python.html","id":"drop-na","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.6 Drop na ()","text":"","code":"df\n\ndf10 = df.dropna()\ndf10\n\ndf11 = df.dropna(how = \"all\")\ndf11.shape\n\ndf11a = df.dropna(how = \"any\")\ndf11a.shape"},{"path":"análisis-de-datos-con-python.html","id":"replace","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.7 replace ()","text":"","code":"# Remplaza cualquier valor\ndf12 = df.replace(to_replace = np.nan, value = 343434)\ndf12\n\ndf13 = df.replace(to_replace = 3.0, value = 343434)\ndf13"},{"path":"análisis-de-datos-con-python.html","id":"interpolate","chapter":"7 Análisis de Datos con Python","heading":"7.5.6.8 interpolate()","text":"Cambia por el promedio de una lista ordenada","code":"df[\"balcony\"] = df[\"balcony\"].interpolate(method = \"linear\")\n\ndf.head(10)"},{"path":"análisis-de-datos-con-python.html","id":"juntar-bases-de-datos-merge-y-append","chapter":"7 Análisis de Datos con Python","heading":"7.5.7 Juntar bases de datos: merge y append","text":"01figuras/image.png01figuras/image.png","code":""},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-1-juntar-por-filas-append","chapter":"7 Análisis de Datos con Python","heading":"7.5.7.1 Ejemplo 1: Juntar por filas (append)","text":"","code":"df1 = pd.DataFrame({\"Nombre\":[\"Carmen\", \"Luis\"], \n\"Sexo\":[\"Mujer\", \"Hombre\"], \"Edad\":[22, 18]}).set_index(\"Nombre\")\ndf1\n\ndf2 = pd.DataFrame({\"Nombre\":[\"María\", \"Pedro\", \"Carmen\"], \n\"Sexo\":[\"Mujer\", \"Hombre\", \"Mujer\"], \"Edad\":[25, 30, 22], \"FT\": [1,1,1]}).set_index(\"Nombre\")\ndf2\n\ndf3 = pd.DataFrame({\"Nombre\":[\"María\", \"Pedro\", \"Carmen\"], \n\"Sexo\":[\"Mujer\", \"Hombre\", \"Mujer\"], \"Edad\":[25, 30, 22]}).set_index(\"Nombre\")\ndf3\n\ndf = pd.concat([df1, df3])\ndf"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-2-juntar-por-columnas-merge","chapter":"7 Análisis de Datos con Python","heading":"7.5.7.2 Ejemplo 2: Juntar por columnas (merge)","text":"Sintaxis básicadf.merge(df1, df2, = clave, = tipo)TiposTiposinner (por defecto): El DataFrame resultante solo contiene las filas cuyos valores en la clave están en los dos DataFramesinner (por defecto): El DataFrame resultante solo contiene las filas cuyos valores en la clave están en los dos DataFramesinner-join.gifouter:\n+ El DataFrame resultante contiene todas las filas de los dos DataFrames.\n+ Si una fila de un DataFrame puede emparejarse con otra los mismos valores en la clave en el otro DataFrame, la fila se añade igualmente al DataFrame resultante rellenando las columnas del otro DataFrame con el valor NaN.\n+ Es equivalente la unión de conjuntos.full-join.gifleft:El DataFrame resultante contiene todas las filas del primer DataFrame.Descarta las filas del segundo DataFrame que pueden emparejarse con alguna fila del primero-left-join.gifright:El DataFrame resultante contiene todas las filas del segundo DataFrameDescarta las filas del primer DataFrame que pueden emparejarse con alguna fila del segundo DataFrame través de la clave.right-join.gif","code":""},{"path":"análisis-de-datos-con-python.html","id":"aplicaciones-a-bases-de-datos-append","chapter":"7 Análisis de Datos con Python","heading":"7.5.7.3 Aplicaciones a bases de datos : append","text":"","code":"df1 = pd.read_csv('datos\\\\concat_1.csv')\ndf2 = pd.read_csv('datos\\\\concat_2.csv')\ndf3 = pd.read_csv('datos\\\\concat_3.csv')\n\ndf1\n\ndf2\n\ndf3\n\nrow_concat = pd.concat([df1, df2, df3])\nrow_concat\n\nrow_concat.loc[0]\n\nrow_concat.iloc[0]\n\n# Si quiero resetear el indice utilizo opcion ignore_index = True\nrow_concat_reset = pd.concat([df1, df2, df3], ignore_index = True)\nrow_concat_reset\n\nrow_concat_reset.loc[0]\n\n# ¿Qué ocurre si es que pego un data frame con una serie?\nnew_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])\nnew_row_series\n\ndf1\n\npd.concat([df1, new_row_series])\n\n# Miren lo que ocurre si ahora defino columnas similares\nnew_row_data = pd.DataFrame([['n1', 'n2', 'n3', 'n4']],\n                            columns=['A', 'B', 'C', 'D'])\nnew_row_data\n\npd.concat([df1, new_row_data])\n\n# Tambien puedo modificar los ejes de forma tal de pegar hacia el lado\ncol_concat = pd.concat([df1, df2, df3], axis=1)\ncol_concat\n\n# Puedo hacer algunas selecciones que pueden generar tablas de interes \ncol_concat['A']\n\n# Tambien puedo resetear los indices, en este caso el de las columnmas \ncol_concat_ignore = pd.concat([df1, df2, df3], axis=1, ignore_index=True)\ncol_concat_ignore\n\n# Miremos un ultimo ejemplo: junta lo común, genera nuevos en caso de que no lo sean\ndf1.columns=['A','B','C','D']\ndf2.columns=['E','F','G','H']\ndf3.columns=['A','H','F','C']\n\ndf1\n\ndf2\n\ndf3\n\npd.concat([df1, df2, df3])"},{"path":"análisis-de-datos-con-python.html","id":"aplicaciones-a-bases-de-datos-merge","chapter":"7 Análisis de Datos con Python","heading":"7.5.7.4 Aplicaciones a bases de datos : merge","text":"Ejercicio 4.5.4: Genere dos bases de datos (df1 y df2) con la siguiente información:df1df2Aplique los cuatro tipos de “join”: inner, outer, left y right. Compare sus resultados","code":"person=pd.read_csv('datos\\\\survey_person.csv')\nsite=pd.read_csv('datos\\\\survey_site.csv')\nsurvey=pd.read_csv('datos\\\\survey_survey.csv')\nvisited=pd.read_csv('datos\\\\survey_visited.csv')\n\nperson\n\nsite\n\nsurvey\n\nvisited\n\nvisited_sub = visited.loc[[0, 2, 6]]\nvisited_sub\n\nsite\n\n# Por defecto va a ser inner, noten que le indico left_on y right_on. ¿Razón? \no2o = site.merge(visited_sub, \n                 left_on=['name'], \n                 right_on='site')\no2o\n\no2o.shape\n\nvisited\n\nsite\n\nm2o = site.merge(visited, \n                 left_on='name', \n                 right_on='site')\n\nm2odf1 = pd.DataFrame({\"Nombre\":[\"Carmen\", \"Luis\", \"María\"],  \"Sexo\":[\"Mujer\", \"Hombre\", \"Mujer\"]})\ndf2 = pd.DataFrame({\"Nombre\":[\"María\", \"Pedro\", \"Luis\"], \"Edad\":[25, 30, 18]})\n\ndf1\n\ndf2\n\ndf_inner = pd.merge(df1, df2, on=\"Nombre\")\ndf_outer = pd.merge(df1, df2, on=\"Nombre\", how=\"outer\")\ndf_left = pd.merge(df1, df2, on=\"Nombre\", how=\"left\")\ndf_right = pd.merge(df1, df2, on=\"Nombre\", how=\"right\")\n\ndf_inner\n\ndf_outer\n\ndf_left\n\ndf_right"},{"path":"análisis-de-datos-con-python.html","id":"análisis-de-datos-parte-i","chapter":"7 Análisis de Datos con Python","heading":"7.6 Análisis de datos (Parte I)","text":"La clase de hoy tiene como objetivo revisar algunos elementos releacionados con análisis estadístico.Veremos algunos concepts básicos y su aplicación en python.En la segunda parte de la clase revisaremos varios ejemplos.","code":"# Preambulo\n\n# Librerías: \n\nimport pandas as pd                    # datos/dataframes\nimport numpy as np                     # arreglos\nimport datetime as dt                  # fechas \nimport os                              # directorios \nimport matplotlib.pyplot as plt        # Visualización básicas y no tan básicas\nimport scipy.stats as stats            # Librería estadística\nimport random                          # Números aleatorios\nimport math                            # Operaciones matemáticas\n\n# Fecha de hoy\nprint(dt.date.today())\n\n# Carpetas\n\nmain = os.getcwd()\ndatos = main + \"\\\\datos\"\nresultados = main + \"\\\\resultados\"\ntablas = resultados + \"\\\\tablas\"\nfiguras = resultados + \"\\\\figuras\"\n\nmain"},{"path":"análisis-de-datos-con-python.html","id":"estadistica-descriptiva","chapter":"7 Análisis de Datos con Python","heading":"7.6.1 Estadistica Descriptiva","text":"Las estadísticas descriptivas son medidas que resumen características importantes de los datos.Las estadísticas descriptivas son medidas que resumen características importantes de los datos.La elaboración de estadísticas descriptivas es un primer paso común después de limpiar y preparar un conjunto de datos para su análisis.La elaboración de estadísticas descriptivas es un primer paso común después de limpiar y preparar un conjunto de datos para su análisis.Ya hemos visto varios ejemplos de estadísticas descriptivas en clases anteriores, en esta clase formalizaremos algunos temas y veremos muchos ejemplos.Ya hemos visto varios ejemplos de estadísticas descriptivas en clases anteriores, en esta clase formalizaremos algunos temas y veremos muchos ejemplos.","code":""},{"path":"análisis-de-datos-con-python.html","id":"medidas-de-tendencia-central-media-mediana-y-moda","chapter":"7 Análisis de Datos con Python","heading":"7.6.1.1 Medidas de tendencia central : media, mediana y moda","text":"Las medidas de centralidad son estadísticas que nos dan una idea del “centro” de una variable numérica.Las medidas de centralidad son estadísticas que nos dan una idea del “centro” de una variable numérica.En otras palabras, las medidas de centralidad dan una idea del valor típico que se espera ver en una distribución de datos.En otras palabras, las medidas de centralidad dan una idea del valor típico que se espera ver en una distribución de datos.Las medidas de centralidad más comunes son la media, la mediana y la moda.Las medidas de centralidad más comunes son la media, la mediana y la moda.La media es simplemente un promedio: la suma de los valores dividida por el número total de registros. Como hemos visto en lecciones anteriores, podemos utilizar df.mean() para obtener la media de cada columna de un DataFrame:La media es simplemente un promedio: la suma de los valores dividida por el número total de registros. Como hemos visto en lecciones anteriores, podemos utilizar df.mean() para obtener la media de cada columna de un DataFrame:Recuerden que también podemos incluir el promedio por filaLa mediana de una distribución es el valor en el que el 50% de los datos se encuentra por debajo y el 50% por encima.En esencia, la mediana divide los datos por la mitad.La mediana también se conoce como el percentil del 50%, ya que el 50% de las observaciones se encuentran por debajo de ella.Se puede obtener la mediana utilizando la función df.median():Aunque la media y la mediana nos dan una idea del centro de una distribución, siempre son iguales.Aunque la media y la mediana nos dan una idea del centro de una distribución, siempre son iguales.La mediana siempre nos da un valor que divide los datos en dos mitades, mientras que la media es un promedio numérico, por lo que los valores extremos pueden tener un impacto significativo en la media.La mediana siempre nos da un valor que divide los datos en dos mitades, mientras que la media es un promedio numérico, por lo que los valores extremos pueden tener un impacto significativo en la media.En una distribución simétrica, la media y la mediana serán iguales.En una distribución simétrica, la media y la mediana serán iguales.En una distribución simetrica (como la normal) la media y la mediana son igualesEn una distribución asimétrica, la media y la mediana difieren.La media también está muy influenciada por los valores atípicos, mientras que la mediana resiste la influencia de los mismos:Dado que la mediana tiende resistir los efectos de la asimetría y los valores atípicos, se conoce como una estadística “robusta”.Dado que la mediana tiende resistir los efectos de la asimetría y los valores atípicos, se conoce como una estadística “robusta”.La mediana suele dar una mejor idea del valor típico en una distribución con asimetrías o valores atípicos significativos.La mediana suele dar una mejor idea del valor típico en una distribución con asimetrías o valores atípicos significativos.Finalmente, la moda de una variable es simplemente el valor que aparece con más frecuencia. diferencia de la media y la mediana, se puede tomar el modo de una variable categórica y es posible tener múltiples modas.Finalmente, la moda de una variable es simplemente el valor que aparece con más frecuencia. diferencia de la media y la mediana, se puede tomar el modo de una variable categórica y es posible tener múltiples modas.Las columnas con modas múltiples (valores múltiples con la misma cuenta) devuelven valores múltiples como el modo.Las columnas sin moda (ningún valor que aparezca más de una vez) devuelven NaN.En resumen…01figuras/image.png01figuras/image.png01figuras/image.png","code":"# Importamos datos\nmtcars = pd.read_csv(\"datos\\\\mtcars.csv\")\nmtcars\n\ndel mtcars[\"model\"]\nmtcars\n\nmtcars.head(10)\n\nmtcars.tail()\n\nmtcars.columns\n\nlist(mtcars.columns)\n\nsorted(mtcars.columns)\n\n# Promedio cada columna\npromedio = mtcars.mean()   \npromedio\n\nt1 = pd.DataFrame(promedio) \nt1\n\nt1 = t1.rename(columns = {0: \"Promedio\"})\nt1\n\n# Guardamos nuestra T1\nos.chdir(tablas)\nt1.to_excel(\"t1.xlsx\", \n            sheet_name='t1', \n            index=True)# Promedio por fila \nmtcars.mean(axis=1)# Mediana\nmediana = mtcars.median()  \nt2 = pd.DataFrame(mediana) \nt2 = t2.rename(columns = {0: \"Mediana\"})\n\nt2\n\n# Guardamos nuestra T2\nos.chdir(tablas)\nt2.to_excel(\"t2.xlsx\", \n            sheet_name='t2', \n            index=False)\n\n# ¿Si queremos juntar? \nt3 = pd.concat([t1, t2], axis = 1)\nt3\n\n# Guardamos nuestra T3\nos.chdir(tablas)\nt3.to_excel(\"t3.xlsx\", \n            sheet_name='t3', \n            index=True)\n\n# Index = True/False me permite incluir el nombre de las filas.norm_data = pd.DataFrame(np.random.normal(size=100000))\n\nnorm_data.plot(kind=\"density\",\n              figsize=(10,10));\n\n\nplt.vlines(norm_data.mean(),     # Media\n           ymin=0, \n           ymax=0.4,\n           linewidth=5.0);\n\nplt.vlines(norm_data.median(),   # Mediana\n           ymin=0, \n           ymax=0.4, \n           linewidth=2.0,\n           color=\"red\");skewed_data = pd.DataFrame(np.random.exponential(size=100000))\n\nskewed_data.plot(kind=\"density\",\n              figsize=(10,10),\n              xlim=(-1,5));\n\n\nplt.vlines(skewed_data.mean(),     # Media\n           ymin=0, \n           ymax=0.8,\n           linewidth=5.0);\n\nplt.vlines(skewed_data.median(),   # Mediana\n           ymin=0, \n           ymax=0.8, \n           linewidth=2.0,\n           color=\"red\");norm_data = np.random.normal(size=50)\noutliers = np.random.normal(15, size=3)\ncombined_data = pd.DataFrame(np.concatenate((norm_data, outliers), axis=0))\n\ncombined_data.plot(kind=\"density\",\n              figsize=(10,10),\n              xlim=(-5,20));\n\n\nplt.vlines(combined_data.mean(),     # Media (azul)\n           ymin=0, \n           ymax=0.2,\n           linewidth=5.0);\n\nplt.vlines(combined_data.median(),   # Mediana (rojo)\n           ymin=0, \n           ymax=0.2, \n           linewidth=2.0,\n           color=\"red\");moda = mtcars.mode()\nmodamoda.dropna(axis = 0, \n            how = 'any', \n            inplace = True)\n\nmoda\n\n# Guardamos nuestra T4\nos.chdir(tablas)\nmoda.to_excel(\"t4.xlsx\", \n            sheet_name='t4', \n            index=True)\n\n# Index = True/False me permite incluir el nombre de las filas."},{"path":"análisis-de-datos-con-python.html","id":"medidas-de-dispersión","chapter":"7 Análisis de Datos con Python","heading":"7.6.1.2 Medidas de dispersión","text":"Las medidas de dispersión son estadísticas que describen cómo varían los datos.Las medidas de dispersión son estadísticas que describen cómo varían los datos.Mientras que las medidas de centro nos dan una idea del valor típico, las medidas de dispersión nos dan una idea de cuánto tienden divergir los datos del valor típico.Mientras que las medidas de centro nos dan una idea del valor típico, las medidas de dispersión nos dan una idea de cuánto tienden divergir los datos del valor típico.Una de las medidas de dispersión más sencillas es el rango.Una de las medidas de dispersión más sencillas es el rango.El rango es la distancia entre las observaciones máxima y mínima:El rango es la distancia entre las observaciones máxima y mínima:Como se ha señalado anteriormente, la mediana representa el percentil 50 de un conjunto de datos.Se puede utilizar un resumen de varios percentiles para describir la dispersión de una variable.Podemos extraer el valor mínimo (percentil 0), el primer cuartil (percentil 25), la mediana, el tercer cuartil (percentil 75) y el valor máximo (percentil 100) utilizando el método quantile():El rango intercuartil (IQR) es otra medida común de dispersión. El IQR es la distancia entre el tercer cuartil y el primer cuartil:En un grafico de caja y bigote es posible ver estas medidas de dispersión de forma gráfica:La varianza y la desviación estándar son otras dos medidas comunes de la dispersión.La varianza de una distribución es la media de las desviaciones (diferencias) al cuadrado de la media.Utilice df.var() para comprobar la varianza:La desviación estandar es la raíz cuadrada de la varianza.La desviación estandar es la raíz cuadrada de la varianza.La desviación estándar puede ser más interpretable que la varianza, ya que la desviación estándar se expresa en términos de las mismas unidades que la variable en cuestión, mientras que la varianza se expresa en términos de unidades al cuadrado.La desviación estándar puede ser más interpretable que la varianza, ya que la desviación estándar se expresa en términos de las mismas unidades que la variable en cuestión, mientras que la varianza se expresa en términos de unidades al cuadrado.Utilice df.std() para comprobar la desviación estándar:Utilice df.std() para comprobar la desviación estándar:Dado que tanto la varianza como la desviación estándar se derivan de la media, son susceptibles la influencia de la asimetría de los datos y de los valores atípicos.Dado que tanto la varianza como la desviación estándar se derivan de la media, son susceptibles la influencia de la asimetría de los datos y de los valores atípicos.La desviación absoluta de la mediana es una medida alternativa de la dispersión basada en la mediana, que hereda la solidez de la mediana frente la influencia de la asimetría y los valores atípicos.La desviación absoluta de la mediana es una medida alternativa de la dispersión basada en la mediana, que hereda la solidez de la mediana frente la influencia de la asimetría y los valores atípicos.Es la mediana del valor absoluto de las desviaciones de la mediana:Es la mediana del valor absoluto de las desviaciones de la mediana:","code":"# Rango\nrango = max(mtcars[\"mpg\"]) - min(mtcars[\"mpg\"])\nrangofive_num = [mtcars[\"mpg\"].quantile(0),   \n            mtcars[\"mpg\"].quantile(0.25),\n            mtcars[\"mpg\"].quantile(0.50),\n            mtcars[\"mpg\"].quantile(0.75),\n            mtcars[\"mpg\"].quantile(1)]\n\nfive_num\n\nt5 = pd.DataFrame(mtcars.describe())\nt5\n\n# Guardamos nuestra T4\nos.chdir(tablas)\nmoda.to_excel(\"t5.xlsx\", \n            sheet_name='t5', \n            index=True)\n\n# Index = True/False me permite incluir el nombre de las filas.IQR = mtcars[\"mpg\"].quantile(0.75) - mtcars[\"mpg\"].quantile(0.25)\nIQRmtcars.boxplot(column=\"mpg\",\n               return_type='axes',\n               figsize=(8,8))\n\n# Agregar texto\nplt.text(x=0.74, y=22.25, s=\"3er cuartil\")\nplt.text(x=0.8, y=18.75, s=\"Mediana\")\nplt.text(x=0.75, y=15.5, s=\"Primer cuartil\")\nplt.text(x=0.9, y=10, s=\"Min\")\nplt.text(x=0.9, y=33.5, s=\"Max\")\nplt.text(x=0.7, y=19.5, s=\"IQR\", rotation=360, size=15);mtcars[\"mpg\"].var()mtcars[\"mpg\"].std()abs_median_devs = abs(mtcars[\"mpg\"] - mtcars[\"mpg\"].median())\nabs_median_devs.median()"},{"path":"análisis-de-datos-con-python.html","id":"asimetría-y-curtosis","chapter":"7 Análisis de Datos con Python","heading":"7.6.1.3 Asimetría y curtosis","text":"Además de las medidas de centro y dispersión, las estadísticas descriptivas incluyen medidas que dan una idea de la forma de una distribución.Además de las medidas de centro y dispersión, las estadísticas descriptivas incluyen medidas que dan una idea de la forma de una distribución.La simetría mide el sesgo o la asimetría de una distribución, mientras que la curtosis mide la cantidad de datos que se encuentran en las colas de una distribución en comparación con el centro.La simetría mide el sesgo o la asimetría de una distribución, mientras que la curtosis mide la cantidad de datos que se encuentran en las colas de una distribución en comparación con el centro.vamos entrar en los cálculos exactos que hay detrás de la asimetría y la curtosis, pero son esencialmente estadísticas que llevan la idea de la varianza un paso más allá: mientras que la varianza implica elevar al cuadrado las desviaciones de la media, la asimetría implica elevar al cubo las desviaciones de la media y la curtosis implica elevar las desviaciones de la media la cuarta potencia.vamos entrar en los cálculos exactos que hay detrás de la asimetría y la curtosis, pero son esencialmente estadísticas que llevan la idea de la varianza un paso más allá: mientras que la varianza implica elevar al cuadrado las desviaciones de la media, la asimetría implica elevar al cubo las desviaciones de la media y la curtosis implica elevar las desviaciones de la media la cuarta potencia.Pandas ha incorporado funciones para comprobar la asimetría y la curtosis, df.skew() y df.kurt() respectivamente:Pandas ha incorporado funciones para comprobar la asimetría y la curtosis, df.skew() y df.kurt() respectivamente:Ahora vamos comprobar la asimetría de cada una de las distribuciones. Dado que la simetría se mide con la asimetría, esperaríamos ver una asimetría baja para todas las distribuciones excepto la asimétrica, porque todas las demás son aproximadamente simétricas:Veamos la curtosis:Como podemos ver en el resultado, los datos con distribución normal tienen una curtosis cercana cero, la distribución plana tiene una curtosis negativa y las dos distribuciones con más datos en las colas frente al centro tienen mayor curtosis.01figuras/image.png","code":"mtcars[\"mpg\"].skew()  # Simetría\n\nmtcars[\"mpg\"].kurt()  # Kurtosis\n\nnorm_data = np.random.normal(size=100000)\n\nskewed_data = np.concatenate((np.random.normal(size=35000)+2, \n                             np.random.exponential(size=65000)), \n                             axis=0)\nuniform_data = np.random.uniform(0,2, size=100000)\n\npeaked_data = np.concatenate((np.random.exponential(size=50000),\n                             np.random.exponential(size=50000)*(-1)),\n                             axis=0)\n\ndata_df = pd.DataFrame({\"norm\":norm_data,\n                       \"skewed\":skewed_data,\n                       \"uniform\":uniform_data,\n                       \"peaked\":peaked_data})\n\ndata_df.plot(kind=\"density\",\n            figsize=(10,10),\n            xlim=(-5,5));data_df.skew()data_df.kurt()"},{"path":"análisis-de-datos-con-python.html","id":"resumen-sección-1","chapter":"7 Análisis de Datos con Python","heading":"7.6.1.4 Resumen sección 1","text":"Las estadísticas descriptivas le ayudan explorar las características de sus datos, como el centro, la dispersión y la forma, resumiéndolas con medidas numéricas.Las estadísticas descriptivas le ayudan explorar las características de sus datos, como el centro, la dispersión y la forma, resumiéndolas con medidas numéricas.Los estadísticos descriptivos ayudan informar sobre la dirección de un análisis y le permiten comunicar sus ideas otros de forma rápida y sencilla. Además, ciertos valores, como la media y la varianza, se utilizan en todo tipo de pruebas estadísticas y modelos predictivos.Los estadísticos descriptivos ayudan informar sobre la dirección de un análisis y le permiten comunicar sus ideas otros de forma rápida y sencilla. Además, ciertos valores, como la media y la varianza, se utilizan en todo tipo de pruebas estadísticas y modelos predictivos.Es importante hacer siempre una tabla con estos estadisticos al inicio de cualquier ejercicio de análisis de datos.Es importante hacer siempre una tabla con estos estadisticos al inicio de cualquier ejercicio de análisis de datos.","code":""},{"path":"análisis-de-datos-con-python.html","id":"la-distribución-normal","chapter":"7 Análisis de Datos con Python","heading":"7.6.2 La distribución normal","text":"La normal o distribución gaussiana es una distribución de probabilidad continua caracterizada por una curva simétrica en forma de campana.La normal o distribución gaussiana es una distribución de probabilidad continua caracterizada por una curva simétrica en forma de campana.Una distribución normal se define por su centro (media) y su dispersión (desviación estándar). La mayor parte de las observaciones generadas partir de una distribución normal se sitúan cerca de la media, que se encuentra en el centro exacto de la distribución: como regla general, aproximadamente el 68% de los datos se sitúan dentro de una desviación estándar de la media, el 95% dentro de dos desviaciones estándar y el 99,8% dentro de tres desviaciones estándar.Una distribución normal se define por su centro (media) y su dispersión (desviación estándar). La mayor parte de las observaciones generadas partir de una distribución normal se sitúan cerca de la media, que se encuentra en el centro exacto de la distribución: como regla general, aproximadamente el 68% de los datos se sitúan dentro de una desviación estándar de la media, el 95% dentro de dos desviaciones estándar y el 99,8% dentro de tres desviaciones estándar.La distribución normal es quizá la más importante de toda la estadística. Resulta que muchos fenómenos del mundo real, como las puntuaciones de los tests de inteligencia y las alturas humanas, siguen aproximadamente una distribución normal, por lo que se utiliza menudo para modelar variables aleatorias. Muchas pruebas estadísticas comunes asumen que las distribuciones son normales.La distribución normal es quizá la más importante de toda la estadística. Resulta que muchos fenómenos del mundo real, como las puntuaciones de los tests de inteligencia y las alturas humanas, siguen aproximadamente una distribución normal, por lo que se utiliza menudo para modelar variables aleatorias. Muchas pruebas estadísticas comunes asumen que las distribuciones son normales.El apodo de scipy para la distribución normal es norm. Vamos investigar la distribución normal:El apodo de scipy para la distribución normal es norm. Vamos investigar la distribución normal:El resultado muesta muestra que:Aproximadamente el 16% de los datos generados por una distribución normal con media 0 y desviación estándar 1 está por debajo de -1,Aproximadamente el 16% de los datos generados por una distribución normal con media 0 y desviación estándar 1 está por debajo de -1,El 16% está por encima de 1 y el 68% se encuentra entre -1 y 1.El 16% está por encima de 1 y el 68% se encuentra entre -1 y 1.Lo anterior es coherente con la regla 68, 95, 99,7.Lo anterior es coherente con la regla 68, 95, 99,7.Grafiquemos la distribución normal e inspeccionemos las áreas que hemos calculado:Grafiquemos la distribución normal e inspeccionemos las áreas que hemos calculado:01figuras/image.pngEl gráfico anterior muestra la forma de campana de la distribución normal, el área por debajo y por encima de una desviación estándar y el área dentro de una desviación estándar de la media.El gráfico anterior muestra la forma de campana de la distribución normal, el área por debajo y por encima de una desviación estándar y el área dentro de una desviación estándar de la media.Encontrar los cuantiles de la distribución normal es una tarea común cuando se realizan pruebas estadísticas. Puede comprobar los cuantiles de la distribución normal con stats.norm.ppf():Encontrar los cuantiles de la distribución normal es una tarea común cuando se realizan pruebas estadísticas. Puede comprobar los cuantiles de la distribución normal con stats.norm.ppf():El resultado del cuantil anterior confirma que aproximadamente el 5% de los datos se encuentra más de 2 desviaciones estándar de la media.Esto es conocido como el valor z. Básicamente una medida de cuantas desviaciones estandar un punto esta alejado sobre la media.","code":"# Estas caracteristicas matematicas nos permiten saber la posición de ciertos valores\nprob_under_minus1 = stats.norm.cdf(x= -1,  \n                                loc = 0,               \n                                scale = 1)     \n\nprob_over_1 = 1 - stats.norm.cdf(x = 1,  \n                                loc = 0,               \n                                scale= 1) \n\nbetween_prob = 1-(prob_under_minus1 + prob_over_1)\n\nprint(prob_under_minus1, prob_over_1, between_prob)plt.rcParams[\"figure.figsize\"] = (9,9)\n                                  \nplt.fill_between(x=np.arange(-4,-1,0.01), \n                 y1= stats.norm.pdf(np.arange(-4,-1,0.01)) ,\n                 facecolor='red',\n                 alpha=0.35)\n\nplt.fill_between(x=np.arange(1,4,0.01), \n                 y1= stats.norm.pdf(np.arange(1,4,0.01)) ,\n                 facecolor='red',\n                 alpha=0.35)\n\nplt.fill_between(x=np.arange(-1,1,0.01), \n                 y1= stats.norm.pdf(np.arange(-1,1,0.01)) ,\n                 facecolor='blue',\n                 alpha=0.35)\n\nplt.text(x=-1.8, y=0.03, s= round(prob_under_minus1,3))\nplt.text(x=-0.2, y=0.1, s= round(between_prob,3))\nplt.text(x=1.4, y=0.03, s= round(prob_over_1,3));print(stats.norm.ppf(q=0.50))\n\nprint(stats.norm.ppf(q=0.975) )"},{"path":"análisis-de-datos-con-python.html","id":"de-una-muestra-aleatoria-a-la-población","chapter":"7 Análisis de Datos con Python","heading":"7.6.2.1 De una muestra aleatoria a la población","text":"Hasta ahora, esta guía se ha centrado en las funciones y la sintaxis necesarias para manipular, explorar y describir los datos.Hasta ahora, esta guía se ha centrado en las funciones y la sintaxis necesarias para manipular, explorar y describir los datos.La limpieza de datos y el análisis exploratorio son menudo pasos preliminares hacia el objetivo final de extraer información de los datos través de la inferencia estadística o el modelado predictivo.La limpieza de datos y el análisis exploratorio son menudo pasos preliminares hacia el objetivo final de extraer información de los datos través de la inferencia estadística o el modelado predictivo.La inferencia estadística es el proceso de análisis de datos de muestra para obtener información sobre la población de la que se recogieron los datos y para investigar las diferencias entre las muestras de datos.La inferencia estadística es el proceso de análisis de datos de muestra para obtener información sobre la población de la que se recogieron los datos y para investigar las diferencias entre las muestras de datos.En el análisis de datos, menudo estamos interesados en las características de alguna población grande, pero recoger datos de toda la población puede ser inviable.En el análisis de datos, menudo estamos interesados en las características de alguna población grande, pero recoger datos de toda la población puede ser inviable.","code":""},{"path":"análisis-de-datos-con-python.html","id":"estimaciones-puntuales","chapter":"7 Análisis de Datos con Python","heading":"7.6.2.2 Estimaciones puntuales","text":"Las estimaciones puntuales son estimaciones de los parámetros de la población basadas en datos de la muestra.Las estimaciones puntuales son estimaciones de los parámetros de la población basadas en datos de la muestra.Por ejemplo, si quisiéramos saber la edad media de los votantes podríamos realizar una encuesta entre los votantes registrados y utilizar la edad media de los encuestados como una estimación puntual de la edad media de la población en su conjunto. La media de una muestra se conoce como media muestral.Por ejemplo, si quisiéramos saber la edad media de los votantes podríamos realizar una encuesta entre los votantes registrados y utilizar la edad media de los encuestados como una estimación puntual de la edad media de la población en su conjunto. La media de una muestra se conoce como media muestral.La media de la muestra suele ser exactamente igual la media de la población. Esta diferencia puede deberse muchos factores, como un diseño deficiente de la encuesta, métodos de muestreo sesgados y la aleatoriedad inherente la extracción de una muestra de una población.La media de la muestra suele ser exactamente igual la media de la población. Esta diferencia puede deberse muchos factores, como un diseño deficiente de la encuesta, métodos de muestreo sesgados y la aleatoriedad inherente la extracción de una muestra de una población.Investiguemos las estimaciones puntuales generando una población de datos de edad aleatorios y extrayendo luego una muestra de ella para estimar la media:Investiguemos las estimaciones puntuales generando una población de datos de edad aleatorios y extrayendo luego una muestra de ella para estimar la media:Nuestra estimación puntual basada en una muestra aleatoria de 500 individuos subestima la verdadera media de la población en 0,6 años, pero se aproxima.Nuestra estimación puntual basada en una muestra aleatoria de 500 individuos subestima la verdadera media de la población en 0,6 años, pero se aproxima.Esto ilustra un punto importante: podemos obtener una estimación bastante precisa de una gran población mediante el muestreo e un subconjunto relativamente pequeño de individuos.Esto ilustra un punto importante: podemos obtener una estimación bastante precisa de una gran población mediante el muestreo e un subconjunto relativamente pequeño de individuos.Otra estimación puntual que puede ser de interés es la proporción de la población que pertenece alguna categoría o subgrupo.Otra estimación puntual que puede ser de interés es la proporción de la población que pertenece alguna categoría o subgrupo.Por ejemplo, nos gustaría saber la ocupación de cada votante que encuestamos, para tener una idea de la demografía general de la base de votantes. Se puede hacer una estimación puntual de este tipo de proporción tomando una muestra y comprobando después la proporción en la muestra:Por ejemplo, nos gustaría saber la ocupación de cada votante que encuestamos, para tener una idea de la demografía general de la base de votantes. Se puede hacer una estimación puntual de este tipo de proporción tomando una muestra y comprobando después la proporción en la muestra:Obsérvese que las estimaciones de las proporciones se acercan las verdaderas proporciones de la población subyacente.","code":"np.random.seed(10)\npopulation_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000)\npopulation_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000)\npopulation_ages = np.concatenate((population_ages1, population_ages2))\n\npopulation_ages\n\npopulation_ages.mean()\n\nnp.random.seed(6)\nsample_ages = np.random.choice(a= population_ages,\n                               size=500)            # Muestra aleatoria\n\nprint ( sample_ages.mean() )                         # Media muestral\n\npopulation_ages.mean() - sample_ages.mean()   # Miremos la diferenciarandom.seed(10)\npopulation_occ = ([\"occ1\"]*100000) + ([\"occ2\"]*50000) +\\\n                   ([\"occ3\"]*50000) + ([\"occ4\"]*25000) +\\\n                   ([\"occ5\"]*25000)\n\ndemo_sample = random.sample(population_occ, 1000)   # muestra aleatoria\n\nfor occ in set(demo_sample):\n    print( occ + \" proportion estimate:\" )\n    print( demo_sample.count(occ)/1000 )"},{"path":"análisis-de-datos-con-python.html","id":"distribuciones-de-muestreo-y-el-teorema-del-límite-central","chapter":"7 Análisis de Datos con Python","heading":"7.6.2.3 Distribuciones de muestreo y el teorema del límite central","text":"Muchos procedimientos estadísticos asumen que los datos siguen una distribución normal, porque la distribución normal tiene buenas propiedades como la simetría y que la mayoría de los datos se agrupan dentro de unas pocas desviaciones estándar de la media.Muchos procedimientos estadísticos asumen que los datos siguen una distribución normal, porque la distribución normal tiene buenas propiedades como la simetría y que la mayoría de los datos se agrupan dentro de unas pocas desviaciones estándar de la media.Los datos del mundo real suelen tener una distribución normal y la distribución de una muestra tiende reflejar la distribución de la población.Los datos del mundo real suelen tener una distribución normal y la distribución de una muestra tiende reflejar la distribución de la población.Esto significa que una muestra tomada de una población con una distribución sesgada también tenderá ser sesgada. Vamos investigarlo trazando los datos y la muestra que hemos creado antes y comprobando la inclinación:Esto significa que una muestra tomada de una población con una distribución sesgada también tenderá ser sesgada. Vamos investigarlo trazando los datos y la muestra que hemos creado antes y comprobando la inclinación:La distribución tiene poca asimetría, pero el gráfico revela que los datos son claramente normales:\nen lugar de una curva de campana simétrica, tiene una distribución bimodal con dos picos de alta densidad.\nLa muestra que extrajimos de esta población debería tener aproximadamente la misma forma y asimetría:\nLa distribución tiene poca asimetría, pero el gráfico revela que los datos son claramente normales:en lugar de una curva de campana simétrica, tiene una distribución bimodal con dos picos de alta densidad.La muestra que extrajimos de esta población debería tener aproximadamente la misma forma y asimetría:La muestra tiene aproximadamente la misma forma que la población subyacente. Esto sugiere que podemos aplicar las técnicas que suponen una distribución normal este conjunto de datos, ya que es normal.La muestra tiene aproximadamente la misma forma que la población subyacente. Esto sugiere que podemos aplicar las técnicas que suponen una distribución normal este conjunto de datos, ya que es normal.En realidad, sí podemos, gracias al teorema del límite central.En realidad, sí podemos, gracias al teorema del límite central.El teorema del límite central es uno de los resultados más importantes de la teoría de la probabilidad y sirve de base muchos métodos de análisis estadístico.El teorema del límite central es uno de los resultados más importantes de la teoría de la probabilidad y sirve de base muchos métodos de análisis estadístico.El teorema afirma que la distribución de muchas medias muestrales, conocida como distribución muestral, se distribuirá normalmente. Esta regla es válida incluso si la propia distribución subyacente está distribuida normalmente. En consecuencia, podemos tratar la media muestral como si se dibujara una distribución normal.El teorema afirma que la distribución de muchas medias muestrales, conocida como distribución muestral, se distribuirá normalmente. Esta regla es válida incluso si la propia distribución subyacente está distribuida normalmente. En consecuencia, podemos tratar la media muestral como si se dibujara una distribución normal.Para ilustrarlo, creemos una distribución de muestreo tomando 200 muestras de nuestra población y haciendo luego 200 estimaciones puntuales de la media:Para ilustrarlo, creemos una distribución de muestreo tomando 200 muestras de nuestra población y haciendo luego 200 estimaciones puntuales de la media:La distribución del muestreo parece ser aproximadamente normal, pesar de la distribución bimodal de la población de la que se extrajeron las muestras.La distribución del muestreo parece ser aproximadamente normal, pesar de la distribución bimodal de la población de la que se extrajeron las muestras.Además, la media de la distribución muestral se aproxima la media real de la población:Además, la media de la distribución muestral se aproxima la media real de la población:Cuantas más muestras tomemos, mejor será nuestra estimación del parámetro poblacional. La media muestral converge la media poblacional.","code":"pd.DataFrame(population_ages).hist(bins=58,\n                                  range=(17.5,75.5),\n                                  figsize=(9,9))\n\nprint(stats.skew(population_ages))pd.DataFrame(sample_ages).hist(bins=58,\n                                  range=(17.5,75.5),\n                                  figsize=(9,9));\n\nprint(stats.skew(sample_ages))np.random.seed(10)\n\npoint_estimates = []         # Lista vacía\n\nfor x in range(2000):         # Generamos 200 muestras aleatorias \n    sample = np.random.choice(a= population_ages, size=500)\n    point_estimates.append(sample.mean())\n    \npd.DataFrame(point_estimates).plot(kind=\"density\",  # Graficamos la media\n                                   figsize=(9,9),\n                                   xlim=(41,45));population_ages.mean() - np.array(point_estimates).mean()population_ages.mean()"},{"path":"análisis-de-datos-con-python.html","id":"intervalos-de-confianza","chapter":"7 Análisis de Datos con Python","heading":"7.6.2.4 Intervalos de confianza","text":"Una estimación puntual puede dar una idea aproximada de un parámetro de la población, como la media, pero las estimaciones son propensas errores y puede que sea factible tomar múltiples muestras para obtener mejores estimaciones.Una estimación puntual puede dar una idea aproximada de un parámetro de la población, como la media, pero las estimaciones son propensas errores y puede que sea factible tomar múltiples muestras para obtener mejores estimaciones.Un intervalo de confianza es un rango de valores por encima y por debajo de una estimación puntual que capta el verdadero parámetro de la población con un nivel de confianza predeterminado.Un intervalo de confianza es un rango de valores por encima y por debajo de una estimación puntual que capta el verdadero parámetro de la población con un nivel de confianza predeterminado.Por ejemplo, si quiere tener un 95% de posibilidades de capturar el verdadero parámetro de la población con una estimación puntual y un intervalo de confianza correspondiente, establecería su nivel de confianza en el 95%. Los niveles de confianza más altos dan lugar intervalos de confianza más amplios.Por ejemplo, si quiere tener un 95% de posibilidades de capturar el verdadero parámetro de la población con una estimación puntual y un intervalo de confianza correspondiente, establecería su nivel de confianza en el 95%. Los niveles de confianza más altos dan lugar intervalos de confianza más amplios.Calcule un intervalo de confianza tomando una estimación puntual y luego sumando y restando un margen de error para crear un rango. El margen de error se basa en el nivel de confianza deseado, la dispersión de los datos y el tamaño de la muestra. La forma de calcular el margen de error depende de si se conoce la desviación estándar de la población o .Calcule un intervalo de confianza tomando una estimación puntual y luego sumando y restando un margen de error para crear un rango. El margen de error se basa en el nivel de confianza deseado, la dispersión de los datos y el tamaño de la muestra. La forma de calcular el margen de error depende de si se conoce la desviación estándar de la población o .El IC resultante nos dice que si repetimos la aleatorización múltiples veces, el 95% de los intervalos de confianza que generemos contendrán el valor real. Un IC de un 95% significa que hay un 95% de probabilidades que el valor real se encuentre en un intervalo de confianza particular.El IC resultante nos dice que si repetimos la aleatorización múltiples veces, el 95% de los intervalos de confianza que generemos contendrán el valor real. Un IC de un 95% significa que hay un 95% de probabilidades que el valor real se encuentre en un intervalo de confianza particular.Si conoce la desviación estándar de la población, el margen de error es igual :Si conoce la desviación estándar de la población, el margen de error es igual :\\[z * \\frac{\\sigma}{\\sqrt{n}}\\]Donde σ (sigma) es la desviación estándar de la población, n es el tamaño de la muestra, y z es un número conocido como el valor crítico z.Donde σ (sigma) es la desviación estándar de la población, n es el tamaño de la muestra, y z es un número conocido como el valor crítico z.El valor z-crítico es el número de desviaciones estándar que habría que alejar de la media de la distribución normal para capturar la proporción de los datos asociada al nivel de confianza deseado. Por ejemplo, sabemos que aproximadamente el 95% de los datos de una distribución normal se encuentra dentro de 2 desviaciones estándar de la media, por lo que podríamos utilizar 2 como valor z-crítico para un intervalo de confianza del 95% (aunque es más exacto obtener los valores z-críticos con stats.norm.ppf().).El valor z-crítico es el número de desviaciones estándar que habría que alejar de la media de la distribución normal para capturar la proporción de los datos asociada al nivel de confianza deseado. Por ejemplo, sabemos que aproximadamente el 95% de los datos de una distribución normal se encuentra dentro de 2 desviaciones estándar de la media, por lo que podríamos utilizar 2 como valor z-crítico para un intervalo de confianza del 95% (aunque es más exacto obtener los valores z-críticos con stats.norm.ppf().).Calculemos un 95% de confianza para nuestra estimación puntual de la media:Calculemos un 95% de confianza para nuestra estimación puntual de la media:Nota: Utilizamos stats.norm.ppf(q = 0,975) para obtener el valor z-crítico deseado en lugar de q = 0,95 porque la distribución tiene dos colas.Nota: Utilizamos stats.norm.ppf(q = 0,975) para obtener el valor z-crítico deseado en lugar de q = 0,95 porque la distribución tiene dos colas.Observa que el intervalo de confianza que hemos calculado captura la verdadera media poblacional de 43,0023.Observa que el intervalo de confianza que hemos calculado captura la verdadera media poblacional de 43,0023.Vamos crear varios intervalos de confianza y trazarlos para tener una mejor idea de lo que significa “capturar” el verdadero valorVamos crear varios intervalos de confianza y trazarlos para tener una mejor idea de lo que significa “capturar” el verdadero valorObserve que en el gráfico anterior, todos los intervalos de confianza del 95%, excepto uno, se superponen la línea roja que marca la media real.Observe que en el gráfico anterior, todos los intervalos de confianza del 95%, excepto uno, se superponen la línea roja que marca la media real.Esto es de esperar: dado que un intervalo de confianza del 95% capta la media real el 95% de las veces, es de esperar que nuestro intervalo capte la media real el 5% de las veces.Esto es de esperar: dado que un intervalo de confianza del 95% capta la media real el 95% de las veces, es de esperar que nuestro intervalo capte la media real el 5% de las veces.Si se conoce la desviación estandar de la población, hay que utilizar la desviación estandar de la muestra para crear intervalos de confianza.Si se conoce la desviación estandar de la población, hay que utilizar la desviación estandar de la muestra para crear intervalos de confianza.Dado que la desviación estándar de la muestra puede coincidir con el parámetro de la población, el intervalo tendrá más error cuando se conozca la desviación estándar de la población. Para tener en cuenta este error, utilizamos lo que se conoce como valor crítico t en lugar del valor crítico z.Dado que la desviación estándar de la muestra puede coincidir con el parámetro de la población, el intervalo tendrá más error cuando se conozca la desviación estándar de la población. Para tener en cuenta este error, utilizamos lo que se conoce como valor crítico t en lugar del valor crítico z.El valor crítico t se extrae de lo que se conoce como distribución t, una distribución que se asemeja mucho la distribución normal pero que se hace cada vez más amplia medida que disminuye el tamaño de la muestra. La distribución t está disponible en scipy.stats con el nombre de “t” para que podamos obtener los valores críticos de t con stats.t.ppf().El valor crítico t se extrae de lo que se conoce como distribución t, una distribución que se asemeja mucho la distribución normal pero que se hace cada vez más amplia medida que disminuye el tamaño de la muestra. La distribución t está disponible en scipy.stats con el nombre de “t” para que podamos obtener los valores críticos de t con stats.t.ppf().Tomemos una nueva muestra más pequeña y luego creemos un intervalo de confianza sin la desviación estándar de la población, utilizando la distribución t:Tomemos una nueva muestra más pequeña y luego creemos un intervalo de confianza sin la desviación estándar de la población, utilizando la distribución t:Nota: cuando se utiliza la distribución t, hay que proporcionar los grados de libertad (df). Para este tipo de prueba, los grados de libertad son iguales al tamaño de la muestra menos 1. Si el tamaño de la muestra es grande, la distribución t se aproxima la distribución normal.Nota: cuando se utiliza la distribución t, hay que proporcionar los grados de libertad (df). Para este tipo de prueba, los grados de libertad son iguales al tamaño de la muestra menos 1. Si el tamaño de la muestra es grande, la distribución t se aproxima la distribución normal.Observe que el valor crítico de t es mayor que el valor crítico de z que utilizamos para el intervalo de confianza del 95%. Esto permite que el intervalo de confianza arroje una red más grande para compensar la variabilidad causada por el uso de la desviación estándar de la muestra en lugar de la desviación estándar de la población. El resultado final es un intervalo de confianza mucho más amplio (un intervalo con un mayor margen de error).Observe que el valor crítico de t es mayor que el valor crítico de z que utilizamos para el intervalo de confianza del 95%. Esto permite que el intervalo de confianza arroje una red más grande para compensar la variabilidad causada por el uso de la desviación estándar de la muestra en lugar de la desviación estándar de la población. El resultado final es un intervalo de confianza mucho más amplio (un intervalo con un mayor margen de error).Si tiene una muestra grande, el valor crítico t se acercará al valor crítico z, por lo que hay poca diferencia entre utilizar la distribución normal frente la distribución t:Si tiene una muestra grande, el valor crítico t se acercará al valor crítico z, por lo que hay poca diferencia entre utilizar la distribución normal frente la distribución t:También podemos hacer un intervalo de confianza para una estimación puntual de una proporción poblacional. En este caso, el margen de error es igual \\[z * \\sqrt{\\frac{p(1-p)}{n}}\\]Donde z es el valor crítico de z para nuestro nivel de confianza, p es la estimación puntual de la proporción poblacional y n es el tamaño de la muestra. Calculemos un intervalo de confianza del 95% para los de ocupación 3 según la proporción de la muestra que hemos calculado antes (0,192):El resultado muestra que el intervalo de confianza capturó el verdadero parámetro poblacional de 0,2.El resultado muestra que el intervalo de confianza capturó el verdadero parámetro poblacional de 0,2.De forma similar nuestras estimaciones puntuales de la media de la población, podemos utilizar la función scipy stats.distribution.interval() para calcular un intervalo de confianza para una proporción de la población para nosotros.De forma similar nuestras estimaciones puntuales de la media de la población, podemos utilizar la función scipy stats.distribution.interval() para calcular un intervalo de confianza para una proporción de la población para nosotros.En este caso estamos trabajando con valores z-críticos por lo que queremos trabajar con la distribución normal en lugar de la distribución t:En este caso estamos trabajando con valores z-críticos por lo que queremos trabajar con la distribución normal en lugar de la distribución t:En resumen…En resumen…La estimación de los parámetros de la población través del muestreo es una forma de inferencia sencilla, pero potente.La estimación de los parámetros de la población través del muestreo es una forma de inferencia sencilla, pero potente.Las estimaciones puntuales combinadas con los márgenes de error nos permiten crear intervalos de confianza que capturan el verdadero parámetro de la población con una alta probabilidad.Las estimaciones puntuales combinadas con los márgenes de error nos permiten crear intervalos de confianza que capturan el verdadero parámetro de la población con una alta probabilidad.La próxima vez ampliaremos los conceptos de esta lección aprendiendo sobre las pruebas de hipótesis estadísticas.La próxima vez ampliaremos los conceptos de esta lección aprendiendo sobre las pruebas de hipótesis estadísticas.Ejercicio 4.6.1:Escriba una función que le permita calcular los intervalos de confianza de la media muestral. El input debe ser una columna de una data.frame.Escriba una función que le permita calcular los intervalos de confianza de la media muestral. El input debe ser una columna de una data.frame.Utilice la función que acaba de generar junto con un iterador que le permita generar este intervalo de confianza para todas las columnas de una base de datos cualquiera. Pruebe su iterador con la base de datos mtcars.Utilice la función que acaba de generar junto con un iterador que le permita generar este intervalo de confianza para todas las columnas de una base de datos cualquiera. Pruebe su iterador con la base de datos mtcars.","code":"np.random.seed(10)\n\nsample_size = 1000\nsample = np.random.choice(a= population_ages, size = sample_size)\nsample_mean = sample.mean()\n\nz_critical = stats.norm.ppf(q = 0.975)  # Valor critico*\n\nprint(\"z-critical value:\")              # Miramos el valor critico\nprint(z_critical)                        \n\npop_stdev = population_ages.std()  # Get the population standard deviation\n\nmargin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size))\n\nconfidence_interval = (sample_mean - margin_of_error,\n                       sample_mean + margin_of_error)  \n\nprint(\"Confidence interval:\")\nprint(confidence_interval)\n\nsample_meannp.random.seed(12)\n\nsample_size = 1000\nintervals = []\nsample_means = []\n\nfor sample in range(25):\n    sample = np.random.choice(a= population_ages, size = sample_size)\n    sample_mean = sample.mean()\n    sample_means.append(sample_mean)\n\n    z_critical = stats.norm.ppf(q = 0.975)  # Get the z-critical value*         \n\n    pop_stdev = population_ages.std()  # Get the population standard deviation\n\n    stats.norm.ppf(q = 0.025)\n\n    margin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size))\n\n    confidence_interval = (sample_mean - margin_of_error,\n                           sample_mean + margin_of_error)  \n    \n    intervals.append(confidence_interval)\n\nplt.figure(figsize=(9,9))\n\nplt.errorbar(x=np.arange(0.1, 25, 1), \n             y=sample_means, \n             yerr=[(top-bot)/2 for top,bot in intervals],\n             fmt='o')\n\nplt.hlines(xmin=0, xmax=25,\n           y=43.0023, \n           linewidth=2.0,\n           color=\"red\");np.random.seed(10)\n\nsample_size = 25\nsample = np.random.choice(a= population_ages, size = sample_size)\nsample_mean = sample.mean()\n\nt_critical = stats.t.ppf(q = 0.975, df=24)  # Valor t*\n\nprint(\"t-critical value:\")                  # Chequear valor t\nprint(t_critical)                        \n\nsample_stdev = sample.std(ddof=1)    # Obtenemos desv estandar muestral\n\nsigma = sample_stdev/math.sqrt(sample_size)  # estimamos \nmargin_of_error = t_critical * sigma\n\nconfidence_interval = (sample_mean - margin_of_error,\n                       sample_mean + margin_of_error)  \n\nprint(\"Confidence interval:\")\nprint(confidence_interval)# Chequeamos la diferencia entre valores criticos con un tamaño muestral de 1000\n\nstats.t.ppf(q=0.975, df= 999) - stats.norm.ppf(0.975)\n\n\"\"\"En lugar de calcular un intervalo de confianza para una estimación puntual de la media a mano, puede calcularlo utilizando la función de Python stats.t.interval():stats.t.interval(alpha = 0.95,              # Nivel de confianza\n                 df= 24,                    # Grados de libertad\n                 loc = sample_mean,         # Media muestral\n                 scale = sigma)             # Desviación estandar muestralz_critical = stats.norm.ppf(0.975)      # Valor critico\n\np = 0.192                               # Estimación puntual de la proporción\n\nn = 1000                                # Tamaño muestral\n\nmargin_of_error = z_critical * math.sqrt((p*(1-p))/n)\n\nconfidence_interval = (p - margin_of_error,  # Intervalo de confianza\n                       p + margin_of_error) \n\nconfidence_intervalstats.norm.interval(alpha = 0.95,    # Nivel de confianza           \n                   loc =  0.192,     # Estimación puntual de la proporción\n                   scale = math.sqrt((p*(1-p))/n))  # Escalandodef mean_confidence_interval(data, confidence=0.05):\n    \n    mean = data.mean()\n    sd = data.std()\n    n = len(data)\n    \n    tcritico = stats.t.ppf(q = (1 - confidence/2), df= n-1)\n    se =  sd/np.sqrt(n)\n    \n    lcv = mean - tcritico * se\n    ucv = mean + tcritico * se\n    \n    return mean, lcv, ucv\n\nlist(mean_confidence_interval(mtcars[\"mpg\"], 0.05))\n\nmtcars.columns\n\nlista = []\n        \nfor col in mtcars.columns:\n    a = mean_confidence_interval(mtcars[col], 0.05)\n    a_list = list(a)\n    lista.append(a_list)\n\nlista\n\nic = pd.DataFrame(lista, index=mtcars.columns)\nic\n\nic = ic.rename(columns = {0: \"Promedio\", 1: \"LIC\", 2 : \"UIC\"})\nic"},{"path":"análisis-de-datos-con-python.html","id":"test-de-hipotesis","chapter":"7 Análisis de Datos con Python","heading":"7.6.3 Test de hipotesis","text":"Las estimaciones puntuales y los intervalos de confianza son herramientas básicas de inferencia que sirven de base para otra técnica de inferencia: las pruebas de hipótesis estadísticas.Las estimaciones puntuales y los intervalos de confianza son herramientas básicas de inferencia que sirven de base para otra técnica de inferencia: las pruebas de hipótesis estadísticas.La prueba de hipótesis estadística es un marco para determinar si los datos observados se desvían de lo esperado. La biblioteca scipy.stats de Python contiene una serie de funciones que facilitan la realización de pruebas de hipótesis.La prueba de hipótesis estadística es un marco para determinar si los datos observados se desvían de lo esperado. La biblioteca scipy.stats de Python contiene una serie de funciones que facilitan la realización de pruebas de hipótesis.","code":""},{"path":"análisis-de-datos-con-python.html","id":"aspectos-básicos","chapter":"7 Análisis de Datos con Python","heading":"7.6.3.1 Aspectos básicos","text":"Las pruebas de hipótesis estadísticas se basan en una afirmación llamada hipótesis nula que supone que hay nada interesante entre las variables que se están probando (lo que quiero que ocurra).Las pruebas de hipótesis estadísticas se basan en una afirmación llamada hipótesis nula que supone que hay nada interesante entre las variables que se están probando (lo que quiero que ocurra).La forma exacta de la hipótesis nula varía de un tipo de prueba otro: si se está probando si los grupos difieren, la hipótesis nula afirma que los grupos son iguales. Por ejemplo, si quieres comprobar si la edad media de los votantes de tu comuna difiere de la media nacional, la hipótesis nula sería que hay diferencia entre las edades medias.La forma exacta de la hipótesis nula varía de un tipo de prueba otro: si se está probando si los grupos difieren, la hipótesis nula afirma que los grupos son iguales. Por ejemplo, si quieres comprobar si la edad media de los votantes de tu comuna difiere de la media nacional, la hipótesis nula sería que hay diferencia entre las edades medias.El propósito de una prueba de hipótesis es determinar si es probable que la hipótesis nula sea cierta dados los datos de la muestra. Si hay pocas pruebas en contra de la hipótesis nula teniendo en cuenta los datos, se acepta la hipótesis nula. Si la hipótesis nula es improbable teniendo en cuenta los datos, puede rechazar la hipótesis nula favor de la hipótesis alternativa.El propósito de una prueba de hipótesis es determinar si es probable que la hipótesis nula sea cierta dados los datos de la muestra. Si hay pocas pruebas en contra de la hipótesis nula teniendo en cuenta los datos, se acepta la hipótesis nula. Si la hipótesis nula es improbable teniendo en cuenta los datos, puede rechazar la hipótesis nula favor de la hipótesis alternativa.La forma exacta de la hipótesis alternativa dependerá de la prueba específica que se realice. Siguiendo con el ejemplo anterior, la hipótesis alternativa sería que la edad media de los votantes de su estado difiere de hecho de la media nacional.La forma exacta de la hipótesis alternativa dependerá de la prueba específica que se realice. Siguiendo con el ejemplo anterior, la hipótesis alternativa sería que la edad media de los votantes de su estado difiere de hecho de la media nacional.Una vez que tenga la hipótesis nula y la alternativa en la mano, elija un nivel de significancia (menudo denotado por la letra griega α.).Una vez que tenga la hipótesis nula y la alternativa en la mano, elija un nivel de significancia (menudo denotado por la letra griega α.).El nivel de significacia es un umbral de probabilidad que determina cuándo se rechaza la hipótesis nula.El nivel de significacia es un umbral de probabilidad que determina cuándo se rechaza la hipótesis nula.Tras realizar una prueba, si la probabilidad de obtener un resultado tan extremo como el que se observa debido al azar es inferior al nivel, se rechaza la hipótesis nula favor de la alternativa. Esta probabilidad de ver un resultado tan extremo o más extremo que el observado se conoce como valor p.Tras realizar una prueba, si la probabilidad de obtener un resultado tan extremo como el que se observa debido al azar es inferior al nivel, se rechaza la hipótesis nula favor de la alternativa. Esta probabilidad de ver un resultado tan extremo o más extremo que el observado se conoce como valor p.La prueba T es una prueba estadística utilizada para determinar si una muestra de datos numéricos difiere significativamente de la población o si dos muestras difieren entre sí.La prueba T es una prueba estadística utilizada para determinar si una muestra de datos numéricos difiere significativamente de la población o si dos muestras difieren entre sí.","code":""},{"path":"análisis-de-datos-con-python.html","id":"test---t-en-una-muestra","chapter":"7 Análisis de Datos con Python","heading":"7.6.3.2 Test - T en una muestra","text":"Una prueba t de una muestra comprueba si la media de una muestra difiere de la media de la población.Creemos algunos datos de edad ficticia para la población de votantes de todo el país y una muestra de votantes de santiago.Comprobemos si la media de edad de los votantes de santiago difiere de la de la población:Observa que hemos utilizado una combinación de distribuciones ligeramente diferente para generar los datos de la muestra de Santiago, por lo que sabemos que las dos medias son diferentes.Observa que hemos utilizado una combinación de distribuciones ligeramente diferente para generar los datos de la muestra de Santiago, por lo que sabemos que las dos medias son diferentes.Realicemos una prueba t con un nivel de confianza del 95% y veamos si rechaza correctamente la hipótesis nula de que la muestra procede de la misma distribución que la población.Realicemos una prueba t con un nivel de confianza del 95% y veamos si rechaza correctamente la hipótesis nula de que la muestra procede de la misma distribución que la población.Para realizar una prueba t de una muestra, podemos la función stats.ttest_1samp():Para realizar una prueba t de una muestra, podemos la función stats.ttest_1samp():El resultado de la prueba muestra que el estadístico de prueba “t” es igual -2,574.Este estadístico de prueba nos indica cuánto se desvía la media de la muestra de la hipótesis nula.Si el estadístico t se encuentra fuera de los cuantiles de la distribución t correspondientes nuestro nivel de confianza y grados de libertad, rechazamos la hipótesis nula.Podemos comprobar los cuantiles con stats.t.ppf():Podemos calcular las probabilidades de ver un resultado tan extremo como el que observamos (conocido como valor p) pasando el estadístico t como cuantil la función stats.t.cdf():Nota: La hipótesis alternativa que estamos comprobando es si la media de la muestra difiere (es igual) la media de la población. Dado que la muestra puede diferir en dirección positiva o negativa, multiplicamos el valor por dos.Observe que este valor es el mismo que el valor p que aparece en el resultado de la prueba t original.Observe que este valor es el mismo que el valor p que aparece en el resultado de la prueba t original.Un valor p de 0,01311 significa que esperaríamos ver datos tan extremos como nuestra muestra debido al azar alrededor del 1,3% de las veces si la hipótesis nula fuera cierta.Un valor p de 0,01311 significa que esperaríamos ver datos tan extremos como nuestra muestra debido al azar alrededor del 1,3% de las veces si la hipótesis nula fuera cierta.En este caso, el valor p es inferior nuestro nivel de significacia α (igual 1-conf.nivel o 0,05), por lo que deberíamos rechazar la hipótesis nula.En este caso, el valor p es inferior nuestro nivel de significacia α (igual 1-conf.nivel o 0,05), por lo que deberíamos rechazar la hipótesis nula.Veamos que ocurre con los intervalos de confianza:Veamos que ocurre con los intervalos de confianza:Por otra parte, dado que hay un 1,3% de posibilidades de ver un resultado tan extremo debido al azar, es significativo al nivel de confianza del 99%. Esto significa que si construyéramos un intervalo de confianza del 99%, capturaría la media de la población:Con un nivel de confianza más alto, construimos un intervalo de confianza más amplio y aumentamos las probabilidades de que capte la verdadera media, con lo que es menos probable que rechacemos la hipótesis nula.Con un nivel de confianza más alto, construimos un intervalo de confianza más amplio y aumentamos las probabilidades de que capte la verdadera media, con lo que es menos probable que rechacemos la hipótesis nula.En este caso, el valor p de 0,013 es mayor que nuestro nivel de significación de 0,01 y rechazamos la hipótesis nula.En este caso, el valor p de 0,013 es mayor que nuestro nivel de significación de 0,01 y rechazamos la hipótesis nula.","code":"np.random.seed(6)\n\npopulation_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000)\npopulation_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000)\npopulation_ages = np.concatenate((population_ages1, population_ages2))\n\nsantiago_ages1 = stats.poisson.rvs(loc=18, mu=30, size=30)\nsantiago_ages2 = stats.poisson.rvs(loc=18, mu=10, size=20)\nsantiago_ages = np.concatenate((santiago_ages1, santiago_ages2))\n\nprint( population_ages.mean() )\nprint( santiago_ages.mean() )stats.ttest_1samp(a = santiago_ages,               # Datos muestrales\n                 popmean = population_ages.mean())  # Media poblacionalstats.t.ppf(q=0.025,  # Cuantile para chequear\n            df=49)  # Grados de libertad\n\nstats.t.ppf(q=0.975,  #  # Cuantile para chequear\n            df=49)  # Grados de libertadstats.t.cdf(x = -2.5742,      # Estadistico T\n               df= 49) * 2   # Por dos (dos colas)sigma = santiago_ages.std()/math.sqrt(50)  # Desv. est. muestral\n\nstats.t.interval(0.95,                        # Intervalo de confianza\n                 df = 49,                     # Grados libertad\n                 loc = santiago_ages.mean(), #  Media muestral\n                 scale= sigma)                # Desviacion estandar muestralstats.t.interval(alpha = 0.99,                # Intervalo de confianza\n                 df = 49,                     # Grados libertad\n                 loc = santiago_ages.mean(), # Media muestra\n                 scale= sigma)                # Desviacion estandar muestral"},{"path":"análisis-de-datos-con-python.html","id":"test-t-en-dos-muestras","chapter":"7 Análisis de Datos con Python","heading":"7.6.3.3 Test T en dos muestras","text":"Una prueba t de dos muestras investiga si las medias de dos muestras de datos independientes difieren entre sí.Una prueba t de dos muestras investiga si las medias de dos muestras de datos independientes difieren entre sí.En una prueba de dos muestras, la hipótesis nula es que las medias de ambos grupos son iguales. diferencia de la prueba de una muestra, en la que se compara con un parámetro poblacional conocido, la prueba de dos muestras sólo incluye las medias de las muestras.En una prueba de dos muestras, la hipótesis nula es que las medias de ambos grupos son iguales. diferencia de la prueba de una muestra, en la que se compara con un parámetro poblacional conocido, la prueba de dos muestras sólo incluye las medias de las muestras.Puede realizar una prueba t de dos muestras pasando con la función stats.ttest_ind().Puede realizar una prueba t de dos muestras pasando con la función stats.ttest_ind().Vamos generar una muestra de datos de la edad de los votantes de Valparaiso y probarla contra la muestra que hicimos antes:Vamos generar una muestra de datos de la edad de los votantes de Valparaiso y probarla contra la muestra que hicimos antes:La prueba arroja un valor p de 0,0907, lo que significa que hay un 9% de posibilidades de que veamos los datos de la muestra tan separados si los dos grupos analizados son realmente idénticos.La prueba arroja un valor p de 0,0907, lo que significa que hay un 9% de posibilidades de que veamos los datos de la muestra tan separados si los dos grupos analizados son realmente idénticos.Si utilizáramos un nivel de confianza del 95%, podríamos rechazar la hipótesis nula, ya que el valor p es mayor que el nivel de significancia correspondiente del 5%.Si utilizáramos un nivel de confianza del 95%, podríamos rechazar la hipótesis nula, ya que el valor p es mayor que el nivel de significancia correspondiente del 5%.","code":"np.random.seed(12)\nvalparaiso_ages1 = stats.poisson.rvs(loc=18, mu=33, size=30)\nvalparaiso_ages2 = stats.poisson.rvs(loc=18, mu=13, size=20)\nvalparaiso_ages = np.concatenate((valparaiso_ages1, valparaiso_ages2))\n\nprint(valparaiso_ages.mean() )\n\nstats.ttest_ind(a= santiago_ages,\n                b= valparaiso_ages,\n                equal_var=False)    # asumimos que tienen igual varianza?"},{"path":"análisis-de-datos-con-python.html","id":"t-test-pareados","chapter":"7 Análisis de Datos con Python","heading":"7.6.3.4 T-Test pareados","text":"La prueba t básica de dos muestras está diseñada para probar las diferencias entre grupos independientes.La prueba t básica de dos muestras está diseñada para probar las diferencias entre grupos independientes.En algunos casos, puede interesarle probar las diferencias entre muestras del mismo grupo en diferentes momentos. Por ejemplo, un hospital podría querer probar si un medicamento para perder peso funciona comprobando los pesos de los pacientes del mismo grupo antes y después del tratamiento.En algunos casos, puede interesarle probar las diferencias entre muestras del mismo grupo en diferentes momentos. Por ejemplo, un hospital podría querer probar si un medicamento para perder peso funciona comprobando los pesos de los pacientes del mismo grupo antes y después del tratamiento.Una prueba t pareada permite comprobar si las medias de las muestras del mismo grupo difieren.Una prueba t pareada permite comprobar si las medias de las muestras del mismo grupo difieren.Podemos realizar una prueba t emparejada utilizando la función scipy stats.ttest_rel(). Generemos algunos datos de peso de pacientes ficticios y hagamos una prueba t pareada:Podemos realizar una prueba t emparejada utilizando la función scipy stats.ttest_rel(). Generemos algunos datos de peso de pacientes ficticios y hagamos una prueba t pareada:El resumen muestra que los pacientes perdieron alrededor de 1,23 libras de media después del tratamiento. Realicemos una prueba t pareada para ver si esta diferencia es significativa con un nivel de confianza del 95%:Rechazamos la hipotesis nula de que ambos promedios son iguales.Ejercicio 4.6.2: Tome los siguientes valores de colesterol de los pacientes antes y despues de una dietatest_results_before_diet=[224, 235, 223, 253, 253, 224, 244, 225, 259, 220, 242, 240, 239, 229, 276, 254, 237, 227]test_results_after_diet=[198, 195, 213, 190, 246, 206, 225, 199, 214, 210, 188, 205, 200, 220, 190, 199, 191, 218]Realice un test de hipotesis pareado que le permita chequear si los niveles de colesterol han bajado posterior la dieta utilizando un nivel de significancia del 0.05%. Asuma que efectivamente los datos se distribuyen normalmente.","code":"np.random.seed(11)\n\nbefore= stats.norm.rvs(scale=30, loc=250, size=100)\n\nafter = before + stats.norm.rvs(scale=5, loc=-1.25, size=100)\n\nweight_df = pd.DataFrame({\"weight_before\":before,\n                          \"weight_after\":after,\n                          \"weight_change\":after-before})\n\nweight_df.describe()             # Chequeamosstats.ttest_rel(a = before,\n                b = after)test_results_before_diet=[224, 235, 223, 253, 253, 224, 244, 225, 259, 220, 242, 240, 239, 229, 276, 254, 237, 227]\ntest_results_after_diet=[198, 195, 213, 190, 246, 206, 225, 199, 214, 210, 188, 205, 200, 220, 190, 199, 191, 218]\n\ntest_stat, p_value_paired = stats.ttest_rel(test_results_before_diet,test_results_after_diet)\nprint(\"p value:%.6f\" % p_value_paired , \"one tailed p value:%.6f\" %(p_value_paired/2))\n\nif p_value_paired <0.05:\n    print(\"Reject null hypothesis\")\nelse:\n    print(\"Fail to reject null hypothesis\")"},{"path":"análisis-de-datos-con-python.html","id":"error-tipo-i-y-tipo-ii","chapter":"7 Análisis de Datos con Python","heading":"7.6.3.5 Error tipo I y tipo II","text":"El resultado de una prueba de hipótesis estadística y la correspondiente decisión de rechazar o aceptar la hipótesis nula son infalibles.El resultado de una prueba de hipótesis estadística y la correspondiente decisión de rechazar o aceptar la hipótesis nula son infalibles.Una prueba proporciona pruebas favor o en contra de la hipótesis nula y luego se decide si se acepta o se rechaza en función de esas pruebas, pero éstas pueden carecer de la fuerza necesaria para llegar la conclusión correcta. Las conclusiones incorrectas obtenidas partir de las pruebas de hipótesis se clasifican en una de las dos categorías siguientes: error de tipo y error de tipo II.Una prueba proporciona pruebas favor o en contra de la hipótesis nula y luego se decide si se acepta o se rechaza en función de esas pruebas, pero éstas pueden carecer de la fuerza necesaria para llegar la conclusión correcta. Las conclusiones incorrectas obtenidas partir de las pruebas de hipótesis se clasifican en una de las dos categorías siguientes: error de tipo y error de tipo II.El error de tipo describe una situación en la que se rechaza la hipótesis nula cuando en realidad es verdadera. Este tipo de error también se conoce como “falso positivo” o “falso acierto”. La tasa de error de tipo 1 es igual al nivel de significación α, por lo que establecer un nivel de confianza más alto (y, por tanto, un alfa más bajo) reduce las posibilidades de obtener un falso positivo.El error de tipo describe una situación en la que se rechaza la hipótesis nula cuando en realidad es verdadera. Este tipo de error también se conoce como “falso positivo” o “falso acierto”. La tasa de error de tipo 1 es igual al nivel de significación α, por lo que establecer un nivel de confianza más alto (y, por tanto, un alfa más bajo) reduce las posibilidades de obtener un falso positivo.El error de tipo II describe una situación en la que se rechaza la hipótesis nula cuando en realidad es falsa. El error de tipo II también se conoce como “falso negativo” o “fallo”. Cuanto más alto sea el nivel de confianza, más probabilidades habrá de cometer un error de tipo II.El error de tipo II describe una situación en la que se rechaza la hipótesis nula cuando en realidad es falsa. El error de tipo II también se conoce como “falso negativo” o “fallo”. Cuanto más alto sea el nivel de confianza, más probabilidades habrá de cometer un error de tipo II.01figuras/image.pngH0: esta embarazadaH0: esta embarazadaH1: Esta embarazadaH1: Esta embarazadaETI: Rechazar la nula (rechazo “estar embarazada”) cuando es verdadera (“esta embarazado”).ETI: Rechazar la nula (rechazo “estar embarazada”) cuando es verdadera (“esta embarazado”).ETII: rechazar la nula (“digo: esta embarazada”), cuando es falsa (“esta embaradaza”).ETII: rechazar la nula (“digo: esta embarazada”), cuando es falsa (“esta embaradaza”).Investiguemos estos errores con un gráfico:Investiguemos estos errores con un gráfico:En el gráfico anterior, las áreas rojas indican los errores de tipo que se producen cuando la hipótesis alternativa es diferente de la nula para una prueba de dos caras con un nivel de confianza del 95%.En el gráfico anterior, las áreas rojas indican los errores de tipo que se producen cuando la hipótesis alternativa es diferente de la nula para una prueba de dos caras con un nivel de confianza del 95%.El área azul representa los errores de tipo II que se producen cuando la hipótesis alternativa es diferente de la nula, como muestra la distribución de la derecha.El área azul representa los errores de tipo II que se producen cuando la hipótesis alternativa es diferente de la nula, como muestra la distribución de la derecha.Tenga en cuenta que la tasa de error de tipo II es el área bajo la distribución alternativa dentro de los cuantiles determinados por la distribución nula y el nivel de confianza.Tenga en cuenta que la tasa de error de tipo II es el área bajo la distribución alternativa dentro de los cuantiles determinados por la distribución nula y el nivel de confianza.Podemos calcular la tasa de error de tipo II para las distribuciones anteriores de la siguiente manera:Podemos calcular la tasa de error de tipo II para las distribuciones anteriores de la siguiente manera:Con las distribuciones normales anteriores, rechazaríamos la hipótesis nula en un 30% de las ocasiones porque las distribuciones están lo suficientemente cerca como para tener un solapamiento significativo.","code":"plt.figure(figsize=(12,10))\n\n\nplt.fill_between(x=np.arange(-4,-2,0.01), \n                 y1= stats.norm.pdf(np.arange(-4,-2,0.01)) ,\n                 facecolor='red',\n                 alpha=0.35)\n\nplt.fill_between(x=np.arange(-2,2,0.01), \n                 y1= stats.norm.pdf(np.arange(-2,2,0.01)) ,\n                 facecolor='grey',\n                 alpha=0.35)\n\nplt.fill_between(x=np.arange(2,4,0.01), \n                 y1= stats.norm.pdf(np.arange(2,4,0.01)) ,\n                 facecolor='red',\n                 alpha=0.5)\n\nplt.fill_between(x=np.arange(-4,-2,0.01), \n                 y1= stats.norm.pdf(np.arange(-4,-2,0.01),loc=3, scale=2) ,\n                 facecolor='grey',\n                 alpha=0.35)\n\nplt.fill_between(x=np.arange(-2,2,0.01), \n                 y1= stats.norm.pdf(np.arange(-2,2,0.01),loc=3, scale=2) ,\n                 facecolor='blue',\n                 alpha=0.35)\n\nplt.fill_between(x=np.arange(2,10,0.01), \n                 y1= stats.norm.pdf(np.arange(2,10,0.01),loc=3, scale=2),\n                 facecolor='grey',\n                 alpha=0.35)\n\nplt.text(x=-0.8, y=0.15, s= \"Null Hypothesis\")\nplt.text(x=2.5, y=0.13, s= \"Alternative\")\nplt.text(x=2.1, y=0.01, s= \"Type 1 Error\")\nplt.text(x=-3.2, y=0.01, s= \"Type 1 Error\")\nplt.text(x=0, y=0.02, s= \"Type 2 Error\");lower_quantile = stats.norm.ppf(0.025)  # Lower cutoff value\nupper_quantile = stats.norm.ppf(0.975)  # Upper cutoff value\n\n# Area bajo la alternativa,\nlow = stats.norm.cdf(lower_quantile,    \n                     loc=3,             \n                     scale=2)\n\n# Area under alternative, to the left the upper cutoff value\nhigh = stats.norm.cdf(upper_quantile, \n                      loc=3, \n                      scale=2)          \n\n# Area under the alternative, between the cutoffs (Type II error)\nhigh-low"},{"path":"análisis-de-datos-con-python.html","id":"poder-estadístico","chapter":"7 Análisis de Datos con Python","heading":"7.6.3.6 Poder estadístico","text":"La potencia de una prueba estadística es la probabilidad de que la prueba rechace la hipótesis nula cuando la alternativa es realmente diferente de la nula.La potencia de una prueba estadística es la probabilidad de que la prueba rechace la hipótesis nula cuando la alternativa es realmente diferente de la nula.En otras palabras, la potencia es la probabilidad de que la prueba detecte que hay algo interesante cuando realmente hay algo interesante. La potencia es igual uno menos la tasa de error de tipo II. La potencia de una prueba estadística está influida porEn otras palabras, la potencia es la probabilidad de que la prueba detecte que hay algo interesante cuando realmente hay algo interesante. La potencia es igual uno menos la tasa de error de tipo II. La potencia de una prueba estadística está influida porEl nivel de significancia elegido para la prueba.El tamaño de la muestra.El tamaño del efecto de la prueba.la hora de elegir un nivel de significación para una prueba, existe un equilibrio entre el error de tipo y el de tipo II. Un nivel de significación bajo, como 0,01, hace que sea poco probable que una prueba tenga errores de tipo (falsos positivos), pero es más probable que tenga errores de tipo II (falsos negativos) que una prueba con un valor mayor del nivel de significancia α. Una convención común es que una prueba estadística debe tener una potencia de al menos 0,8.la hora de elegir un nivel de significación para una prueba, existe un equilibrio entre el error de tipo y el de tipo II. Un nivel de significación bajo, como 0,01, hace que sea poco probable que una prueba tenga errores de tipo (falsos positivos), pero es más probable que tenga errores de tipo II (falsos negativos) que una prueba con un valor mayor del nivel de significancia α. Una convención común es que una prueba estadística debe tener una potencia de al menos 0,8.Un mayor tamaño de la muestra reduce la incertidumbre de la estimación puntual, haciendo que la distribución de la muestra se estreche, lo que da lugar menores tasas de error de tipo II y una mayor potencia.Un mayor tamaño de la muestra reduce la incertidumbre de la estimación puntual, haciendo que la distribución de la muestra se estreche, lo que da lugar menores tasas de error de tipo II y una mayor potencia.Tamaño del efecto es un término general que describe una medida numérica del tamaño de algún fenómeno. Hay muchas medidas diferentes del tamaño del efecto que surgen en diferentes contextos. En el contexto de la prueba T, un tamaño del efecto simple es la diferencia entre las medias de las muestras. Este número puede estandarizarse dividiéndolo por la desviación estándar de la población o la desviación estándar conjunta de las muestras. Esto pone el tamaño del efecto en términos de desviaciones estándar, por lo que un tamaño del efecto estandarizado de 0,5 se interpretaría como que la media de una muestra está 0,5 desviaciones estándar de otra (en general, 0,5 se considera un tamaño del efecto “grande”).Tamaño del efecto es un término general que describe una medida numérica del tamaño de algún fenómeno. Hay muchas medidas diferentes del tamaño del efecto que surgen en diferentes contextos. En el contexto de la prueba T, un tamaño del efecto simple es la diferencia entre las medias de las muestras. Este número puede estandarizarse dividiéndolo por la desviación estándar de la población o la desviación estándar conjunta de las muestras. Esto pone el tamaño del efecto en términos de desviaciones estándar, por lo que un tamaño del efecto estandarizado de 0,5 se interpretaría como que la media de una muestra está 0,5 desviaciones estándar de otra (en general, 0,5 se considera un tamaño del efecto “grande”).Dado que la potencia estadística, el nivel de significación, el tamaño del efecto y el tamaño de la muestra están relacionados, es posible calcular cualquiera de ellos para valores dados de los otros tres. Esto puede ser una parte importante del proceso de diseño de una prueba de hipótesis y del análisis de los resultados. Por ejemplo, si quiere realizar una prueba con un nivel de significancia determinado (digamos el estándar 0,05) y una potencia (digamos el estándar 0,8) y está interesado en un tamaño del efecto determinado (digamos 0,5 para la diferencia estandarizada entre las medias de la muestra), podría utilizar esa información para determinar el tamaño de la muestra que necesita.Dado que la potencia estadística, el nivel de significación, el tamaño del efecto y el tamaño de la muestra están relacionados, es posible calcular cualquiera de ellos para valores dados de los otros tres. Esto puede ser una parte importante del proceso de diseño de una prueba de hipótesis y del análisis de los resultados. Por ejemplo, si quiere realizar una prueba con un nivel de significancia determinado (digamos el estándar 0,05) y una potencia (digamos el estándar 0,8) y está interesado en un tamaño del efecto determinado (digamos 0,5 para la diferencia estandarizada entre las medias de la muestra), podría utilizar esa información para determinar el tamaño de la muestra que necesita.En python, la biblioteca statsmodels contiene funciones para resolver cualquier parámetro de la potencia de las pruebas T. Utilice statsmodels.stats.power.tt_solve_power para pruebas t de una muestra y statsmodels.stats.power.tt_ind_solve_power para una prueba t de dos muestras.En python, la biblioteca statsmodels contiene funciones para resolver cualquier parámetro de la potencia de las pruebas T. Utilice statsmodels.stats.power.tt_solve_power para pruebas t de una muestra y statsmodels.stats.power.tt_ind_solve_power para una prueba t de dos muestras.Comprobemos el tamaño de la muestra que debemos utilizar dados los valores de los parámetros estándar anteriores para una prueba t de una muestra:Comprobemos el tamaño de la muestra que debemos utilizar dados los valores de los parámetros estándar anteriores para una prueba t de una muestra:En este caso, querríamos un tamaño de muestra de al menos 34 para hacer un estudio con la potencia y el nivel de significación deseados capaz de detectar un tamaño de efecto grande.","code":"from statsmodels.stats.power import tt_solve_power\n\ntt_solve_power(effect_size = 0.5,\n               alpha = 0.05,\n               power = 0.8)"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-1-resumen-exploratorio-de-datos","chapter":"7 Análisis de Datos con Python","heading":"7.6.4 Ejemplo 1: resumen exploratorio de datos","text":"Como sacar la primera impresión de tus datosNo olvidar: http://www.danielmsullivan.com/pages/tutorial_stata_to_python.html","code":"# Basics\nimport pandas as pd\nimport numpy as np\n\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt"},{"path":"análisis-de-datos-con-python.html","id":"datos","chapter":"7 Análisis de Datos con Python","heading":"7.6.4.1 Datos","text":"","code":"# Load the data\ndf = sns.load_dataset('tips')\n\n# View first 5 rows\ndf.head()\n\n# Miramos tipos de datos\ndf.info()\n\n# Tamaño\ndf.shape\n\n# Tipos de datos\ndf.dtypes"},{"path":"análisis-de-datos-con-python.html","id":"missing-values-2","chapter":"7 Análisis de Datos con Python","heading":"7.6.4.2 Missing Values","text":"","code":"# Contamos missing values por variable\nmissing_values = df.isnull().sum()\nmissing_values\n\nmissing_values_df = pd.DataFrame(missing_values)\nmissing_values_df.rename(columns = {0 : \"Cantidad\"})\n\n# Guardamos nuestra T6\nmissing_values_df.to_excel(\"t6.xlsx\", sheet_name='t6', index=False)\n\n# Insertamos NA para ejemplificar\ndfna = pd.DataFrame([[10, 1.2, 'Female', np.nan, '?', 'Dinner', 2]]*4, columns=df.columns)\n\ndf2 = df.append(dfna).reset_index(drop=True)\n\n# tail of the df2, where NAs and nulls were inserted\ndf2.tail()\n\n# Heatmap to see missing values\nsns.heatmap(df2.isnull(), cbar=False);\n\n# Chequiamos por otros posibles valores nilos - [?.,:*]  solpom columnas de texto \nfor col in df2.select_dtypes(exclude='number').columns:\n  print(f\"{col}: {df2[col].str.contains('[?.,:*]').sum()} possible null(?.,:*) values\")\n\n# Remplazamos ? \ndf2.replace({'?':np.nan}, inplace=True)\ndf2.tail(2)\n\n# Ahora, vamos a los calores más comunes \nmost_common_smoker = df.smoker.value_counts().index[0]\nmost_common_day = df.day.value_counts().index[0]\n\n# Fill NA Values with the most common value\ndf2.fillna(value={'smoker':most_common_smoker, 'day':most_common_day})\n\n# Drop NA values\ndf2.dropna(inplace=True)\ndf2\n\ndf2.describe()"},{"path":"análisis-de-datos-con-python.html","id":"análisis-univariado","chapter":"7 Análisis de Datos con Python","heading":"7.6.4.3 Análisis univariado","text":"Bien, ahora sabemos que factura_total, punta y tamaño son variables numéricas, por lo que podemos analizar de la misma manera, con un boxplot o histograma.Los valores atípicos se confirman y tiran de la media hacia arriba. Ver los bigotes más largos para total_bill y propina.Para las variables categóricas, podemos trazar barras con recuentos.","code":"df.info()# Grafico caja y bigote\n\nfig, g = plt.subplots(1, 3, figsize=(20,9))\n\ng1 = sns.boxplot(data=df, \n                 y='total_bill', \n                 color='royalblue', \n                 ax=g[0])\n\ng1.set_title('Boxplot of Total_Bill', size=15)\n\ng2 = sns.boxplot(data=df, \n                 y='tip', \n                 color='coral', \n                 ax=g[1])\n\ng2.set_title('Boxplot of Tip', \n             size=15)\n\ng3 = sns.boxplot(data=df, \n                 y='size', \n                 color='gold', \n                 ax=g[2])\n\ng3.set_title('Boxplot of Size', size=15);# Countplots of categorical variables\nfig, g = plt.subplots(1, 4, figsize=(28,9))\n\ng1 = sns.countplot(data=df, x='sex', color='royalblue', ax=g[0])\ng1.set_title('Countplot of Sex', size=15)\n\ng2 = sns.countplot(data=df, x='day', color='coral', ax=g[1])\ng2.set_title('Countplot of Day', size=15)\n\ng3 = sns.countplot(data=df, x='time', color='gold', ax=g[2])\ng3.set_title('Countplot of Time', size=15)\n\ng4 = sns.countplot(data=df, x='smoker', color='gold', ax=g[3])\ng4.set_title('Countplot of Smoker', size=15);"},{"path":"análisis-de-datos-con-python.html","id":"correlaciones","chapter":"7 Análisis de Datos con Python","heading":"7.6.4.4 Correlaciones","text":"La correlación es la medida estadística de la relación lineal entre dos variables numéricas.La correlación es la medida estadística de la relación lineal entre dos variables numéricas.Así que, como vamos hacer un análisis multivariante, creo que es interesante calcular las correlaciones, porque esto puede orientar tus esfuerzos.Así que, como vamos hacer un análisis multivariante, creo que es interesante calcular las correlaciones, porque esto puede orientar tus esfuerzos.En caso de que tengas muchas variables, será mejor comprobar principalmente (si sólo) las que tengan correlaciones más altas.En caso de que tengas muchas variables, será mejor comprobar principalmente (si sólo) las que tengan correlaciones más altas.La prueba estándar es Pearson, pero si los datos son normales, el método Spearman es más adecuado.Si aplicamos el método de Pearson, los resultados son tan diferentes. Esto tiene dos razones principales: 1. la muestra es pequeña; 2. lo que probablemente afecta más la distribución son los valores atípicos.Si aplicamos el método de Pearson, los resultados son tan diferentes. Esto tiene dos razones principales: 1. la muestra es pequeña; 2. lo que probablemente afecta más la distribución son los valores atípicos.En el gráfico anterior, vemos que la factura total está más relacionada con la propina que con el tamaño de la fiesta.En el gráfico anterior, vemos que la factura total está más relacionada con la propina que con el tamaño de la fiesta.","code":"import scipy.stats as stats\n\nstats.shapiro(mtcars[\"mpg\"])\n\n# Test para evaluar normalidad \n# H0: normal\n\n# Función para testear normalidad con un alpha de 0.05 \n\ndef test_normality(data):\n  stat, p = stats.shapiro(data)\n  if p < 0.05:\n    print(f'p-Value: {p}. Not normaly distributed.')\n  else:\n    print(f'p-Value: {p}. Normaly distributed.')\n\n# Tests\n\nfor col in df.select_dtypes(include=['int', 'float']).columns:\n  test_normality(df[col])# Data not normal, use Spearman\ncorrelation_calc = df.corr(method='spearman')\nsns.heatmap( correlation_calc , annot=True, cmap='RdBu');"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-2-datos-con-precios-de-casas","chapter":"7 Análisis de Datos con Python","heading":"7.6.5 Ejemplo 2: datos con precios de casas","text":"","code":"main\n\ndatos\n\ndf = pd.read_csv(\"datos\\\\MELBOURNE_HOUSE_PRICES_LESS.csv\")\ndf.head()\n\ndf.info()\n\naverage = df['Price'].mean()\nprint(average)\n\nmed = df['Price'].median()\nprint(med)\n\nstandard_deviation = df['Price'].std()\nprint(standard_deviation)\n\n# Commented out IPython magic to ensure Python compatibility.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# %matplotlib inline\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10,8))\nax = sns.boxplot(x='Price', data=df, orient=\"v\")\n\nos.chdir(figuras)\n\nplt.figure(figsize=(12,10))\nax = sns.boxplot(x='Type', y='Price', data=df, orient=\"v\")\nplt.savefig(\"f1.pdf\")\n\nfilter_data = df.dropna(subset=['Price'])\nplt.figure(figsize=(14,8))\nsns.distplot(filter_data['Price'], kde=False)\n\ndf['Type'].unique()\n\ntype_counts = df['Type'].value_counts()\ndf2 = pd.DataFrame(\n                    {'house_type': type_counts}, \n                   index = ['t', 'h', 'u']\n                  )\ndf2.plot.pie(y='house_type', figsize=(10,10), autopct='%1.1f%%')\n\nsns.set(style='darkgrid')\nplt.figure(figsize=(20,10))\nax = sns.countplot(x='Regionname', data=df)"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-3","chapter":"7 Análisis de Datos con Python","heading":"7.6.6 Ejemplo 3","text":"Dispersión","code":"os.chdir(datos)\ndf = pd.read_csv(\"house_prices.csv\")\ndf.head()\n\ndf.head()\n\ndf.shape\n\ndf.info()\n\n# Histograma básico exploratorio\n\nsaleprice = df['SalePrice']\n\nmean=saleprice.mean()\nmedian=saleprice.median()\nmode=saleprice.mode()\n\nprint ('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\nplt.figure(figsize=(10,5))\nplt.hist(saleprice,bins=100,color='grey')\nplt.axvline(mean,color='red',label='Mean')\nplt.axvline(median,color='yellow',label='Median')\nplt.axvline(mode[0],color='green',label='Mode')\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\nsaleprice.cumsum().head()saleprice.min()\n\nsaleprice.max()\n\nsaleprice.max()-saleprice.min()\n\n#varianza\nsaleprice.var()\n\nfrom math import sqrt\n\nstd = sqrt(saleprice.var())\nstd\n\nsaleprice.skew()\n\nsaleprice.kurt()\n\n# Pasamos a numpy para ocupar funciones de numpy\nh = np.array(df['SalePrice'])\nh = sorted(h)\n \n# Generamos una distribución normal con la misma media y sd\nfit = stats.norm.pdf(h, np.mean(h), np.std(h)) \n \n#plot both series on the histogram\nplt.plot(h, fit,'-',linewidth = 4,label=\"Normal distribution with same mean and var\")\nplt.hist(h, bins = 100,label=\"Actual distribution\", density = True)    \nplt.legend()\nplt.show()\n\nfit\n\n# Chequeamos correlaciones \n\ncorelation=df[['LotArea','GrLivArea','GarageArea','SalePrice']].corr()\nprint (corelation)\n\nsns.heatmap(corelation)\n\n# Chequeamos covarianza\ndf[['LotArea','GrLivArea','GarageArea','SalePrice']].cov().head()\n\n# #50 percentile i.e median\n# np.percentile(df['salary'], 50)\n\nsaleprice.quantile(0.5)\n\n# q75 = np.percentile(df['salary'], 75)\n# q75\n\nq3 = saleprice.quantile(0.75)\nq3\n\n#25th percentile\n# q25 = np.percentile(df['salary'], 25)\nq1 = saleprice.quantile(0.25)\nq1\n\n#interquartile range\nIQR = q3  - q1\nIQR\n\nplt.boxplot(saleprice)\nplt.show()"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-4","chapter":"7 Análisis de Datos con Python","heading":"7.6.7 Ejemplo 4","text":"","code":"df = pd.read_csv('insurance.csv')\ndf\n\ndf.head()\n\ndf.info()\n\ndf.shape\n\ndf.columns\n\ndf.describe()\n\ndf.describe(include='O')\n\nlist(df.sex.unique())\n\ndf.isnull().sum()\n\ndf[df.duplicated(keep='first')]\n\ndf.drop_duplicates(keep='first',inplace=True)\n\nplt.figure(figsize=(10,6))\nsns.distplot(df.expenses,color='r')\nplt.title('Expenses Distribution',size=18)\nplt.xlabel('Expenses',size=14)\nplt.ylabel('Density',size=14)\nplt.show()\n\nplt.figure(figsize=(10,6))\nsns.histplot(df.age)\nplt.title('Age Distribution',size=18)\nplt.xlabel('Age',size=14)\nplt.ylabel('Count',size=14)\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df.bmi,color='y')\nplt.title('BMI Distribution',size=18)\nplt.show()\n\nplt.figure(figsize = (10,6))\nsns.boxplot(df.expenses)\nplt.title('Distribution Charges',size=18)\nplt.show()\n\nQ1 = df['expenses'].quantile(0.25)\nQ3 = df['expenses'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\ndf[(df['expenses']< Q1-1.5* IQR) | (df['expenses']> Q3+1.5* IQR)]\n\nplt.figure(figsize=(10,6))\nsns.countplot(x = 'sex', data = df)\nplt.title('Total Number of Male and Female',size=18)\nplt.xlabel('Sex',size=14)\nplt.show()\n\nplt.figure(figsize = (10,6))\nsns.countplot(df.children)\nplt.title('Children Distribution',size=18)\nplt.xlabel('Children',size=14)\nplt.ylabel('Count',size=14)\nplt.show()\n\nplt.figure(figsize = (10,6))\nsns.countplot(df.smoker)\nplt.title('Smoker Distribution',size=18)\nplt.xlabel('Smoker',size=14)\nplt.ylabel('Count',size=14)\nplt.show()\n\ndf.smoker.value_counts()\n\nplt.figure(figsize = (10,6))\nsns.countplot(df.region,palette='Blues')\nplt.title('Region Distribution',size=18)\nplt.xlabel('Region',size=14)\nplt.ylabel('Count',size=14)\nplt.show()\n\nplt.figure(figsize = (10,6))\nsns.scatterplot(x='age',y='expenses',color='r',data=df)\nplt.title('Age vs Charges',size=18)\nplt.xlabel('Age',size=14)\nplt.ylabel('Charges',size=14)\nplt.show()\n\nprint('Correlation between age and charges is : {}'.format(round(df.corr()['age']['expenses'],3)))\n\nplt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.boxplot(x='smoker',y='expenses',data=df)\nplt.title('Smoker vs Expenses',size=18);\n\nsns.pairplot(df, \n                 markers=\"+\",\n                 diag_kind=\"kde\",\n                 kind='reg',\n                 plot_kws={'line_kws':{'color':'#aec6cf'}, \n                           'scatter_kws': {'alpha': 0.7, \n                                           'color': 'red'}},\n                 corner=True);\n\nplt.figure(figsize = (10,6))\nsns.heatmap(df.corr(),annot=True,square=True,\n            cmap='RdBu',\n            vmax=1,\n            vmin=-1)\nplt.title('Correlations Between Variables',size=18);\nplt.xticks(size=13)\nplt.yticks(size=13)\nplt.show()"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-5-desigualdad-de-ingreso-usando-gapminder","chapter":"7 Análisis de Datos con Python","heading":"7.6.8 Ejemplo 5 : Desigualdad de ingreso usando Gapminder","text":"\nIntroducción\n\nData Wrangling\n\nExploratory Data Analysis\n\nConclusions\n","code":""},{"path":"análisis-de-datos-con-python.html","id":"introducción","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.1 Introducción","text":"Este análisis se centra en la desigualdad de ingresos medida por el Índice de Gini y su asociación con métricas económicas como el PIB per cápita, las inversiones como % del PIB y los ingresos fiscales como % del PIB. También se incluye una métrica política, el índice de democracia de la EIU.Esta investigación puede considerarse un punto de partida para cuestiones complejas como¿Está asociado un mayor ingreso fiscal como % del PIB con una menor desigualdad de ingresos?¿Se asocia un mayor índice de democracia de la EIU con una menor desigualdad de ingresos?¿Se asocia un mayor PIB per cápita con una menor desigualdad de ingresos?¿Se asocian las inversiones más altas como porcentaje del PIB con una menor desigualdad de ingresos?Este análisis utiliza el conjunto de datos gapminder de la Fundación Gapminder. La Fundación Gapminder es una empresa sin ánimo de lucro registrada en Estocolmo, Suecia, que promueve el desarrollo global sostenible y la consecución de los Objetivos de Desarrollo del Milenio de las Naciones Unidas mediante un mayor uso y comprensión de las estadísticas y otra información sobre el desarrollo social, económico y medioambiental nivel local, nacional y global.*El Índice de Gini es una medida de dispersión estadística que pretende representar la distribución de la renta o la riqueza de los residentes de una nación, y es la medida de desigualdad más utilizada. Fue desarrollado por el estadístico y sociólogo italiano Corrado Gini y publicado en su documento de 1912 Variabilidad y Mutabilidad.","code":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import plotly \n#import plotly.express as px\n#%matplotlib inline\n\npd.set_option('display.max_rows', 10)\npd.options.display.max_columns = 100\npd.set_option(\"display.precision\", 2)"},{"path":"análisis-de-datos-con-python.html","id":"data-wrangling","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2 Data Wrangling","text":"En esta sección del informe se han cargado los datos, se ha comprobado su limpieza y se han comunicado los resultados.\n##### Propiedades generalesEl conjunto de datos contiene datos de los siguientes conjuntos de datos de GapMinder:Índice de democracia de la EIU:“Este índice de democracia utiliza los datos de Economist Inteligence Unit para expresar la calidad de las democracias como un número entre 0 y 100. Se basa en 60 aspectos diferentes de las sociedades que son relevantes para la democracia: sufragio universal para todos los adultos, participación de los votantes, percepción de la protección de los derechos humanos y libertad para formar organizaciones y partidos.\nEl índice de democracia se calcula partir de los 60 indicadores, divididos en cinco”“subíndices”“, que sonÍndice de pluralismo electoral;Índice de gobierno;Índice de participación políticam;Índice de cultura política;Índice de libertad civil.Los subíndices se basan en la suma de las puntuaciones de aproximadamente 12 indicadores por subíndice, convertidos en una puntuación entre 0 y 100.\n(Economist publica el índice con una escala de 0 10, pero Gapminder lo ha convertido en 0 100 para facilitar su comunicación en forma de porcentaje)“.\nhttps://docs.google.com/spreadsheets/d/1d0noZrwAWxNBTDSfDgG06_aLGWUz4R6fgDhRaUZbDzE/edit#gid=935776888Ingresos: PIB per cápita, dólares constantes de la PPA:\nEl PIB per cápita mide el valor de todo lo producido en un país durante un año, dividido por el número de personas. La unidad está en dólares internacionales, precios fijos de 2011. Los datos se ajustan para tener en cuenta la inflación y las diferencias en el coste de la vida entre los países, los llamados dólares PPA. El final de la serie temporal, entre 1990 y 2016, utiliza los últimos datos del PIB per cápita del Banco Mundial, procedentes de sus Indicadores de Desarrollo Mundial. Para retroceder en el tiempo antes de que la serie del Banco Mundial comience en 1990, hemos utilizado varias fuentes, como Angus Maddison.\nhttps://www.gapminder.org/data/documentation/gd001/Ingresos: PIB per cápita, dólares constantes de la PPA:\nEl PIB per cápita mide el valor de todo lo producido en un país durante un año, dividido por el número de personas. La unidad está en dólares internacionales, precios fijos de 2011. Los datos se ajustan para tener en cuenta la inflación y las diferencias en el coste de la vida entre los países, los llamados dólares PPA. El final de la serie temporal, entre 1990 y 2016, utiliza los últimos datos del PIB per cápita del Banco Mundial, procedentes de sus Indicadores de Desarrollo Mundial. Para retroceder en el tiempo antes de que la serie del Banco Mundial comience en 1990, hemos utilizado varias fuentes, como Angus Maddison.\nhttps://www.gapminder.org/data/documentation/gd001/Inversiones (% del PIB)\nLa formación de capital es un término utilizado para describir la acumulación neta de capital durante un período contable para un país determinado. El término se refiere las adiciones de bienes de capital, como equipos, herramientas, activos de transporte y electricidad. Los países necesitan bienes de capital para sustituir los más antiguos que se utilizan para producir bienes y servicios. Si un país puede sustituir los bienes de capital cuando llegan al final de su vida útil, la producción disminuye. En general, cuanto mayor sea la formación de capital de una economía, más rápido podrá crecer su renta agregada.Inversiones (% del PIB)\nLa formación de capital es un término utilizado para describir la acumulación neta de capital durante un período contable para un país determinado. El término se refiere las adiciones de bienes de capital, como equipos, herramientas, activos de transporte y electricidad. Los países necesitan bienes de capital para sustituir los más antiguos que se utilizan para producir bienes y servicios. Si un país puede sustituir los bienes de capital cuando llegan al final de su vida útil, la producción disminuye. En general, cuanto mayor sea la formación de capital de una economía, más rápido podrá crecer su renta agregada.Ingresos fiscales (% del PIB)\nSe refiere las transferencias obligatorias al gobierno central para fines públicos. incluye la seguridad social.\nhttps://data.worldbank.org/indicator/GC.TAX.TOTL.GD.ZSIngresos fiscales (% del PIB)\nSe refiere las transferencias obligatorias al gobierno central para fines públicos. incluye la seguridad social.\nhttps://data.worldbank.org/indicator/GC.TAX.TOTL.GD.ZS","code":""},{"path":"análisis-de-datos-con-python.html","id":"initial-analysis-of-the-datasets","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2.1 Initial Analysis of the Datasets","text":"Ingresos fiscales en porcentaje del PIBA continuación se presentan los resultados de un primer análisis:Los archivos csv para este análisis se descargaron del sitio web de GapMinder. Pueden encontrarse aquí:https://github.com/psterk1/data_analytics/tree/master/intro/final_projectEl primer archivo es ‘tax_revenue_percent_of_gdp.csv’Un primer análisis reveló que aproximadamente la mitad de los años con > 0,50 valores perdidos. Véase más abajo:Debido los resultados que se presentan continuación, los años con el menor porcentaje de valores nulos se encontraban en el rango de 10 años de 2006 2016. En consecuencia, se seleccionó este intervalo anual para el resto de los conjuntos de datosRenta por persona - PIB per cápitaA continuación se presentan los resultados de un primer análisis:Los resultados siguientes muestran que hay nulos.Inversión Porcentaje del PIBA continuación se presentan los resultados de un primer análisis:Los resultados que aparecen continuación muestran que los años con el menor porcentaje de valores nulos se encontraban en el intervalo de 10 años comprendido entre 2006 y 2016. En consecuencia, se seleccionó este intervalo anual para el resto de los conjuntos de datos.Índice de Democracia de la EIUA continuación se presentan los resultados del análisis inicial:Los resultados que aparecen continuación muestran que hay nulos en el conjunto de datos.Conjunto de datos de GiniA continuación se muestran los resultados del análisis inicial:Los resultados que aparecen continuación muestran que hay nulos en el conjunto de datos.","code":"tax = pd.read_csv(\"datos\\\\tax_revenue_percent_of_gdp.csv\")\ntax\n\nprint(\"number of rows: \", tax.shape[0])\nprint(\"number of columns: {}\".format(tax.shape[1]))\nprint(\"number of duplicates: {}\".format(tax.duplicated().sum()))\nprint(\"datatypes:\\n\")\nprint(tax.dtypes)\nprint(\"\\nSample:\")\ntax.head(3)tax_null = tax.isnull().sum()/tax.shape[0]\ntax_null.to_frame().transpose()income = pd.read_csv('datos\\\\income_per_person_gdppercapita_ppp_inflation_adjusted.csv')\n\nprint(\"number of rows: \", income.shape[0])\nprint(\"number of columns: {}\".format(income.shape[1]))\nprint(\"number of duplicates: {}\".format(income.duplicated().sum()))\nprint(\"datatypes:\\n\")\nprint(income.dtypes)\nincome.head(3)income_null = income.isnull().sum()/income.shape[0]\nincome_null.to_frame().transpose()invest = pd.read_csv('datos\\\\investments_percent_of_gdp.csv')\n\nprint(\"number of rows: \", invest.shape[0])\nprint(\"number of columns: {}\".format(invest.shape[1]))\nprint(\"number of duplicates: {}\".format(invest.duplicated().sum()))\nprint(\"datatypes:\\n\")\nprint(invest.dtypes)\nprint(\"\\nSample:\")\ninvest.head(3)invest_null = invest.isnull().sum()/invest.shape[0]\ninvest_null.to_frame().transpose()demo = pd.read_csv('datos\\\\demox_eiu.csv')\n\nprint(\"number of rows: \", demo.shape[0])\nprint(\"number of columns: {}\".format(demo.shape[1]))\nprint(\"number of duplicates: {}\".format(demo.duplicated().sum()))\nprint(\"datatypes:\\n\")\nprint(demo.dtypes)\nprint(\"\\nSample:\")\ndemo.head(3)demo_null = demo.isnull().sum()/demo.shape[0]\ndemo_null.to_frame().transpose()gini = pd.read_csv('datos\\\\gini.csv')\n\nprint(\"number of rows: \", gini.shape[0])\nprint(\"number of columns: {}\".format(gini.shape[1]))\nprint(\"number of duplicates: {}\".format(gini.duplicated().sum()))\nprint(\"datatypes:\\n\")\nprint(gini.dtypes)\nprint(\"\\nSample:\")\ngini.head(3)gini_null = gini.isnull().sum()/gini.shape[0]\ngini_null.to_frame().transpose()"},{"path":"análisis-de-datos-con-python.html","id":"conclusiones-y-próximos-pasos","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2.2 Conclusiones y próximos pasos","text":"continuación se exponen las conclusiones del análisis inicial:Dado que el Índice de Democracia de la EIU solo tiene datos para los años 2006 - 2018, se seleccionará un rango de años similar para los otros conjuntos de datos.El porcentaje de nulos en los ingresos fiscales como porcentaje del PIB fue menor entre los años 2006 y 2016. La mayoría de los valores fueron nulos para 2017. Como resultado de esto, más el resultado en 1. anterior, el período de años 2006 - 2016 será seleccionado para este conjunto de datos.Próximos pasosLos conjuntos de datos anteriores tendrán la columna del país más los años 2006 - 2016.Los conjuntos de datos serán pivotados para tener el continente, el país, el año y los conjuntos de datos anteriores. continuación se muestra un ejemplo de cómo será el conjunto de datos combinado final:\nLos conjuntos de datos serán pivotados para tener el continente, el país, el año y los conjuntos de datos anteriores. continuación se muestra un ejemplo de cómo será el conjunto de datos combinado final:","code":""},{"path":"análisis-de-datos-con-python.html","id":"reorganizamos-los-datos","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2.3 Reorganizamos los datos","text":"EIU Democracy IndexIncome Per Person (GDP per Capita)Investment Percent GDPTax Revenue Percent GDPGini IndexJuntamos las bases de datosJuntamos continente con país. En este paso, emparejamos cada país con su continente. Esto permitirá el análisis nivel de continente para la detección de tendencias más amplias.","code":"demo_last_10 = demo.iloc[:, :-2]\ndemo_last_10.head(3)\n\ndemo_last_10 = demo_last_10.melt(id_vars=['country'], var_name='year', value_name='demox_eiu')\ndemo_last_10.sort_values(['country','year'], inplace=True)\ndemo_last_10.head(3)income_last_10 = income.iloc[:, np.r_[:1, 207:218]]\nincome_last_10\n\nincome_last_10 = income_last_10.melt(id_vars=['country'], var_name='year', value_name='income_per_person')\nincome_last_10.sort_values(['country', 'year'], inplace=True)\n\nincome_last_10.head(3)invest_last_10 = invest.iloc[:, np.r_[:1, 47:58]]\ninvest_last_10 = invest_last_10.melt(id_vars=['country'], var_name='year', value_name='invest_%_gdp')\ninvest_last_10.sort_values(['country', 'year'], inplace=True)\ninvest_last_10.head(3)tax_last_10 = tax.iloc[:, np.r_[:1, 35:46]]\ntax_last_10 = tax_last_10.melt(id_vars=['country'], var_name='year', value_name='tax_%_gdp')\ntax_last_10.sort_values(['country', 'year'], inplace=True)\ntax_last_10.head(3)gini_last_10 = gini.iloc[:, np.r_[:1, 207:218]]\ngini_last_10 = gini_last_10.melt(id_vars=['country'], var_name='year', value_name='gini_index')\ngini_last_10.sort_values(by=['country', 'year'], inplace=True)\ngini_last_10.head(3)combined = demo_last_10.merge(income_last_10, left_on=['country', 'year'], right_on=['country', 'year'])\ncombined = combined.merge(invest_last_10, left_on=['country', 'year'], right_on=['country', 'year'])\ncombined = combined.merge(tax_last_10, left_on=['country', 'year'], right_on=['country', 'year'])\ncombined = combined.merge(gini_last_10, left_on=['country', 'year'], right_on=['country', 'year'])\ncombined\n\ncont = pd.read_csv('datos\\\\continent_country.csv')\ncontcombined_final = cont.merge(combined, left_on=['country'], right_on=['country'])\ncombined_final"},{"path":"análisis-de-datos-con-python.html","id":"limpieza-de-datos","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2.4 Limpieza de datos","text":"continuación se detallan las medidas adoptadas para garantizar la calidad del conjunto de datos:Missing Values: continuación se muestra un resumen de los valores que faltan (nulos) en el conjunto de datos:Una opción para tratar los valores “tax_%_gdp” que faltan sería sustituirlos por la media del país. Sin embargo, algunos de los países tienen todos nulos y otros tienen la mayoría de nulos para esta columna.Una segunda opción es eliminar las filas con nulos. En aras de la simplicidad, utilizaremos esta opción.Duplicates. hay duplicados en el conjunto de datos:","code":"combined_final.isna().sum()combined_final.dropna(inplace=True)\ncombined_final.isna().sum()combined_final.duplicated().sum()"},{"path":"análisis-de-datos-con-python.html","id":"estadísticas-descriptivas","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2.5 Estadísticas descriptivas","text":"continuación se presentan las estadísticas descriptivas del conjunto de datos. Un examen de los valores indica que los valores mínimo, máximo y medio parecen razonables.","code":"combined_final.describe()"},{"path":"análisis-de-datos-con-python.html","id":"resumen-1","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2.6 Resumen","text":"Dado que los resultados del análisis inicial indican que el conjunto de datos está limpio, es necesario realizar más pasos de limpieza.","code":"print(\"number of rows: \", combined_final.shape[0])\nprint(\"number of columns: {}\".format(combined_final.shape[1]))\nprint(\"datatypes:\\n\")\nprint(combined_final.dtypes)\n\n# Estos son los continentes incluidos en el conjunto de datos. Todos los valores parecen razonables.\n\ncombined_final.continent.unique()\n\ncombined_final.country.unique()"},{"path":"análisis-de-datos-con-python.html","id":"guardar-el-conjunto-de-datos-limpiado","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.2.7 Guardar el conjunto de datos limpiado","text":"","code":"combined_final.to_csv('datos\\\\combined_final_last_10_years.csv', index=False)\ncombined_final.to_excel('datos\\\\combined_final_last_10_years.xlsx', index=False)"},{"path":"análisis-de-datos-con-python.html","id":"análisis-exploratorio","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.3 Análisis exploratorio","text":"Pregunta de investigación 1 - ¿La desigualdad de ingresos está empeorando o mejorando en los últimos 10 años?Mejor significa que el índice de Gini está bajando.Media global de Gini por año:plot shows, mean global gini index going last 10 years, meaning global income inequality improving.Índice de Gini global medio por continente:El gráfico siguiente revela que, por continentes, todos disminuyeron o se mantuvieron prácticamente estables, excepto África.Pregunta de investigación 2 - ¿Qué 10 países tienen la menor y la mayor desigualdad de ingresos?La más baja. En general, la mayoría de los países con menor desigualdad de ingresos se encuentran en Europa.Más alto: En general, la mayoría de los países con menor desigualdad de ingresos están en África y en América.Pregunta de investigación 3 - ¿Se asocia una mayor recaudación fiscal como % del PIB con una menor desigualdad de ingresos?La hipótesis es que los países con mayores ingresos fiscales en % del PIB están asociados una menor desigualdad de ingresos. La hipótesis es que los mayores ingresos fiscales se distribuyen los estratos económicos más bajos en forma de prestaciones sociales. Veamos qué muestran los datos.Es difícil ver una tendencia en el gráfico de dispersión de abajo:Si se observa el logaritmo de ambos valores, se observa que la correlación entre las dos variables es esencialmente plana: hay pruebas convincentes de que un mayor porcentaje de impuestos sobre el PIB conduzca una menor desigualdad de ingresos.La correlación de Pearson es ligeramente negativa: -0,08:Pregunta de investigación 4 - ¿Se asocia una mayor renta por persona -PIB per cápita- con una menor desigualdad de ingresos?La hipótesis es que una mayor renta por persona indica que una mayor parte del PIB del país se distribuye de forma equitativa entre su población.En este caso, el coeficiente de correlación de Persona es -0,34, lo que indica que existe una débil correlación entre log(renta_por_persona) y log(índice_gini):Pregunta de investigación 5 - ¿Se asocia una mayor inversión como porcentaje del PIB con una menor desigualdad de ingresos?La hipótesis es que una mayor inversión como porcentaje del PIB indica que una mayor parte del PIB del país se invierte en mejoras de capital, lo que distribuye los beneficios de los ingresos entre un amplio segmento de la población, lo que conduce una mayor igualdad entre su población.El coeficiente de Pearson de -0,03 indica que hay correlación entre estas dos variables.Pregunta de investigación 6 - ¿Un mayor Índice de Democracia de la EIU está asociado una menor desigualdad de ingresos?La hipótesis es que los países con un mayor Índice de Democracia de la EIU atienden las necesidades de un segmento más amplio de la población, lo que conduce una menor desigualdad de ingresos.En este caso, el coeficiente de correlación de Person es -0,2, lo que indica que existe una débil correlación entre log(demox_eiu) y log(gini_index):","code":"columns = ['year', 'gini_index']\ngini = combined_final[columns]\ngini\n\ngini_annual_average = gini.groupby('year')['gini_index'].mean()\ngini_annual_averageplt.plot(gini_annual_average.index, gini_annual_average)\nplt.title('Mean Global Gini Index by Year')\nplt.xlabel('Year')\nplt.ylabel('Mean Global Gini Index');columns = ['year', 'continent', 'gini_index']\ngini = combined_final[columns]\ngini\n\ngini_cont_average = gini.groupby(['year', 'continent'])['gini_index'].mean()\ngini_cont_averagegini_cont_average.unstack(level=1).plot(kind='line', subplots=False, \\\n                                        title='Mean Global Gini Index by Continent by Year').\\\n                                        set_ylabel(\"Gini Index\");\n\ncolumns = ['year', 'continent', 'country', 'gini_index']\ngini = combined_final[columns]\nginigini.groupby(['country', 'continent'])['gini_index'].mean().to_frame().sort_values(by=['gini_index']).head(10)gini.groupby(['country', 'continent'])['gini_index'].mean().to_frame().sort_values(by=['gini_index'], ascending=False).head(10)columns = ['continent', 'country', 'year', 'tax_%_gdp', 'gini_index']\ntax = combined_final[columns]\ntaxtax.plot(x='tax_%_gdp', y='gini_index', kind='scatter', title='Mean Global Gini Index by Tax % of GDP');tax_plot = tax.plot(x='tax_%_gdp', y='gini_index', kind='scatter', loglog=True, \\\n                    title='log Mean Global Gini Index by log Tax % of GDP')\ntax_plot.set_xlabel('log(tax % gdp)')\ntax_plot.set_ylabel('log(gini index)');tax_log = np.log(tax['tax_%_gdp']).to_frame()\ntax_log['log_gini_index'] = np.log(tax['gini_index'])\ntax_log.corr()columns = ['continent', 'country', 'year', 'income_per_person', 'gini_index']\nincome = combined_final[columns]\nincome\n\nincome.plot(x='income_per_person', y='gini_index', kind='scatter', title='Mean Gini Index by Income Per Person');\n\nincome_plot = income.plot(x='income_per_person', y='gini_index', kind='scatter', loglog=True, \\\n                    title='log Mean Gini Index by Income Per Person')\nincome_plot.set_xlabel('log(income_per_person)')\nincome_plot.set_ylabel('log(gini index)');income_log = np.log(income['income_per_person']).to_frame()\nincome_log['log_gini_index'] = np.log(tax['gini_index'])\nincome_log.corr()columns = ['continent', 'country', 'year', 'invest_%_gdp', 'gini_index']\ninvest = combined_final[columns]\ninvest\n\ninvest = invest[invest['invest_%_gdp'] > 0]\n\ninvest.plot(x='invest_%_gdp', y='gini_index', kind='scatter', title='Mean Gini Index by Investment % GDP');\n\ninvest_plot = invest.plot(x='invest_%_gdp', y='gini_index', kind='scatter', loglog=True, \\\n                    title='log Mean Global Gini Index by log Invest % of GDP')\ninvest_plot.set_xlabel('log(invest % gdp)')\ninvest_plot.set_ylabel('log(gini index)');invest_log = np.log(invest['invest_%_gdp']).to_frame()\ninvest_log['log_gini_index'] = np.log(tax['gini_index'])\ninvest_log.corr()columns = ['continent', 'country', 'year', 'demox_eiu', 'gini_index']\ndemo = combined_final[columns]\ndemo\n\ndemo.plot(x='demox_eiu', y='gini_index', kind='scatter', title='Mean Gini Index by EIU Democracy Index');\n\ndemo_plot = demo.plot(x='demox_eiu', y='gini_index', kind='scatter', loglog=True, \\\n                    title='log Mean Global Gini Index by log EIU Democracy Index')\ndemo_plot.set_xlabel('log(demox_eiu)')\ndemo_plot.set_ylabel('log(gini index)');demo_log = np.log(demo['demox_eiu']).to_frame()\ndemo_log['log_gini_index'] = np.log(tax['gini_index'])\ndemo_log.corr()"},{"path":"análisis-de-datos-con-python.html","id":"conclusiones","chapter":"7 Análisis de Datos con Python","heading":"7.6.8.4 Conclusiones","text":"Las conclusiones de este análisis son las siguientes:Pregunta de investigación 1: ¿Ha empeorado o mejorado la desigualdad de ingresos en los últimos 10 años?Respuesta:Sí, está mejorando, pasando del 38,7 al 37,3En cuanto los continentes, todos han disminuido o se han mantenido estables, excepto África.Pregunta de investigación 2: ¿Qué 10 países tienen la menor y la mayor desigualdad de ingresos?Respuesta:La más baja: Eslovenia, Ucrania, República Checa, Noruega, República Eslovaca, Dinamarca, Kazajstán, Finlandia, Bielorrusia, República KirguisaLa más alta: Colombia, Lesoto, Honduras, Bolivia, República Centroafricana, Zambia, Surinam, Namibia, Botsuana, SudáfricaPregunta de investigación 3: ¿Se asocia una mayor recaudación fiscal como porcentaje del PIB con una menor desigualdad de ingresos?Respuesta: NoPregunta de investigación 4: - ¿Se asocia una mayor renta por persona - PIB per cápita con una menor desigualdad de ingresos?Respuesta: , pero sí una débil correlación negativa.*Pregunta de investigación 5: - ¿Se asocia una mayor inversión como porcentaje del PIB con una menor desigualdad de ingresos?Respuesta: *Pregunta de investigación 6: - ¿Se asocia un mayor índice de democracia de la EIU con una menor desigualdad de ingresos?Respuesta: , pero sí una débil correlación negativa.Los resultados anteriores sugieren que hay otros factores que impulsan la reducción general de la desigualdad de ingresos. Deberían realizarse más análisis de los factores adicionales.Firm Level Innovation CEO Compensation\nUtilizando los datos de panel sobre las patentes de Estados Unidos de 1992 2006 y los datos sobre el valor de las opciones sobre acciones concedidas los directores ejecutivos (CEO) de las empresas estadounidenses emparejadas, este estudio tiene como objetivo investigar la relación entre el valor de las opciones sobre acciones concedidas los CEO y la actividad de innovación nivel de empresa, medida por el número de patentes concedidas cada año.\nEste estudio tiene como objetivo proporcionar evidencia empírica para la teoría de Manso(2011), que predice que el esquema de incentivos óptimos presenta una tolerancia sustancial al fracaso temprano la recompensa por el éxito largo plazo y el compromiso con un plan de compensación largo plazo para directores generales por parte del consejo de administración\nUtilizando los datos de panel sobre las patentes de Estados Unidos de 1992 2006 y los datos sobre el valor de las opciones sobre acciones concedidas los directores ejecutivos (CEO) de las empresas estadounidenses emparejadas, este estudio tiene como objetivo investigar la relación entre el valor de las opciones sobre acciones concedidas los CEO y la actividad de innovación nivel de empresa, medida por el número de patentes concedidas cada año.Este estudio tiene como objetivo proporcionar evidencia empírica para la teoría de Manso(2011), que predice que el esquema de incentivos óptimos presenta una tolerancia sustancial al fracaso temprano la recompensa por el éxito largo plazo y el compromiso con un plan de compensación largo plazo para directores generales por parte del consejo de administraciónData Cleaning: Los datos primarios de este estudio proceden de la base de datos de patentes de la Oficina Nacional de Investigación Económica (NBER) y de la base de datos de compensación de ejecutivos de Wharton Research Data Service (WRDS) Compustat.Datos de la patente: Los datos de patentes patsic06_mar09_ip.csv contienen información sobre el año de solicitud y de concesión, la clase de patente estadounidense, junto con todos los IPC originales de la patente, el número de cesionario original y todos los números pdpass que resultan de la división de patentes de propiedad conjunta, la estandarización de nombres, etc.Descripción de la variableDatos de patentesappyear registra el año de presentación y solicitud de la patente; gyear es el año de concesión de la patente. Cada patente concedida tiene un número de patente único como patent. Los datos cat, icl_class, icl_maingroup, nclass, subcat, subclass son la clase de patente o la clase de tecnología de cada patente, que son menos relevantes en mi estudioIdentificador del cesionario. El assignee es el número que se asigna una empresa al solicitar o conceder una patente, creado originalmente por la Oficina de Patentes de los Estados Unidos (U.S.P.O). El pdpass es también un número único de cesionario y se utiliza para cotejar los datos con los de Compustat. Un pdpass puede ser asignado por varios números de asignatario.Tratamiento del cesionario emparejadoHay patentes que coinciden con el cesionario mediante el algoritmo de coincidencia de nombres desarrollado por la base de datos de patentes NBER, estas observaciones se muestran como cesionario = 0 o pdpass = NaN. Estas observaciones se eliminan automáticamente\n““”Añadir gvkeyLa gvkey es el identificador de empresas del Wharton Rearch Data Service (WRDS)La gvkey es el identificador de empresas del Wharton Rearch Data Service (WRDS)El archivo dynasscsv.csv mantiene un registro de todos los cambios dinámicos en la propiedad de las patentes. Los cambios en la titularidad de las patentes pueden deberse la fusión y adquisición de empresas, o la venta de patentes. En este estudio, sólo centro en el cesionario original (como pdpco1 y gvkey1) de la patente y tengo en cuenta ningún cambio dinámico en la propiedad de la patente. Esto se debe que estoy más interesado en la relación entre la patente y la compensación través de la decisión de los ejecutivos sobre el propio proyecto de investigación de la empresa y el proceso de exploración de nuevas ideas, través de la compra de patentes o la adquisición de patentes de otras empresas, ya que la compra de patentes es una decisión con recompensas conocidasEl archivo dynasscsv.csv mantiene un registro de todos los cambios dinámicos en la propiedad de las patentes. Los cambios en la titularidad de las patentes pueden deberse la fusión y adquisición de empresas, o la venta de patentes. En este estudio, sólo centro en el cesionario original (como pdpco1 y gvkey1) de la patente y tengo en cuenta ningún cambio dinámico en la propiedad de la patente. Esto se debe que estoy más interesado en la relación entre la patente y la compensación través de la decisión de los ejecutivos sobre el propio proyecto de investigación de la empresa y el proceso de exploración de nuevas ideas, través de la compra de patentes o la adquisición de patentes de otras empresas, ya que la compra de patentes es una decisión con recompensas conocidasEn muchos casos, pdpass es igual gvkey para una empresa. Mi objetivo es que la “clave de acceso” sea el identificador único de las empresas, por lo que elimino cualquier “paso de acceso” que coincida con una “clave de acceso” y realizo la fusión interna. Además, nos gustaría investigar cualquier cambio en el recuento de patentes después de que los ejecutivos hayan concedido alguna opción de compra de acciones; dado que el rango de datos para la opción de compra de acciones de los ejecutivos es de 1992 2006, sólo nos centraríamos en el subconjunto del recuento de patentes que es posterior 1992Tratamiento del recuento de patentes del año que faltaPara cada empresa (identificada por gvkey), la razón del valor que falta en el recuento de patentes puede ser 1) que haya patentes concedidas en ese año, pero que coincidan con el pdpass y, por tanto, con la empresa en la base de datos de patentes del NBER; 2) que simplemente haya ninguna patente concedida en ese año. En consecuencia, la regla general para tratar el valor faltante del recuento de patentes cada año es simplemente sustituirlo por 0 en este estudioPreprocessingSólo consideré las características con menos de 10 valores perdidos. Rellené el resto de los valores que faltaban con los últimos datos disponibles en https://data.worldbank.org/. Los valores de Canadá y Etiopía para la característica “Crédito nacional al sector privado por parte de los bancos (% del PIB)” son, con mucho, los más antiguos. Son de 2008.Pib por trabajador: El PIB per cápita es una medida útil cuando queremos evaluar la riqueza de un país. Sin embargo, yo quería identificar predictores de la productividad de un país. Una medida mejor sería en este caso el PIB por trabajador, ya que sólo tiene en cuenta la población activa. La medida ideal sería el PIB por horas trabajadas, pero este conjunto de datos incluye la información necesaria para construir esta característica y el PIB por trabajador también es ya suficiente.Urban Population: La característica “Población urbana (%)” describe qué porcentaje de la población vive en las ciudades.Domestic credit private sector banks (% GDP): Esta característica describe la cantidad de recursos financieros proporcionados al sector privado por los bancos en relación con el PIB.Fixed broadband subscriptions (per 100 people): Suscripciones para el acceso de alta velocidad la Internet pública.Age dependency ratio, old (% working-age population): Número de personas mayores de 64 años en relación con el número de personas entre 15 y 64 años.Age dependency ratio, young (% working-age population): Number people younger 15 relation number people 15 64.Unemployment, total (% total labor force) (modeled ILO estimate): Unemployment estimate modeled Iternational Labor Organization.Final consumption expenditure (% GDP): Final household government consumption expenditure relative GDP.Análisis de regresión (intro)Existe una fuerte correlación entre “Tasa de dependencia de la edad, ancianos (% de la población en edad de trabajar)” y “Abonados la banda ancha fija (por cada 100 personas)”, ya que el coeficiente de correlación de Pearson es de 0,897830. Esto puede causar problemas la hora de interpretar el modelo de regresión. Intento resolver este problema construyendo tres modelos. El primero tendrá en cuenta todas las características, para el segundo modelo dejaré fuera la “Tasa de dependencia de la edad, ancianos (% de la población en edad de trabajar)” y el tercer modelo incluirá las “Suscripciones la banda ancha fija (por cada 100 personas)”.El coeficiente de “Población urbana (%)” es positivo y el efecto es significativo en los tres modelos. El efecto de “Crédito interno al sector privado por parte de los bancos (% del PIB)” es significativo en ninguno de los modelos. “Abonados la banda ancha fija (por cada 100 personas)” sólo se incluyó en los modelos 1 y 2 y el efecto es positivo y significativo en ambos. “Tasa de dependencia de la edad, mayores (% de la población en edad de trabajar)” tiene un efecto positivo y significativo en el modelo 1 y el modelo 3, mientras que “Tasa de dependencia de la edad, jóvenes (% de la población en edad de trabajar)” tiene un efecto significativo en ningún modelo. Lo mismo puede decirse de “Desempleo, total (% de la población activa total) (estimación modelada de la OIT)”. Sin embargo, el “Gasto en consumo final (% del PIB)” muestra un efecto negativo y altamente significativo en todos los modelos. La falta de significación de algunas de las variables puede deberse la multicolinealidad, especialmente en lo que respecta “Dependencia de la edad, jóvenes”.Por último, examinaremos la eficacia de cada modelo para predecir el PIB por trabajador de un país. El pequeño tamaño de la muestra es ideal para construir un modelo de predicción, pero permite la validación cruzada “Leave-one-”, que llevaría mucho tiempo en el caso de grandes conjuntos de datos.","code":"# Commented out IPython magic to ensure Python compatibility.\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n#import qeds\n\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\n\n# %matplotlib inlinedatos\n\npats = pd.read_stata(\"datos\\\\pat76_06_ipc.dta\")\npats.head()# observations with pdpass not missing \npats = pats[pats['pdpass'].notnull()]\n\n# count number of unique patent numbers to have patent count for a assignee in a year\npat_yc1 = pats.groupby([\"pdpass\",\"gyear\"])[\"patent\"].nunique().to_frame().reset_index()\\\n.rename(columns={\"patent\":\"patent_count\"})\n\npat_yc1.head()\n\npat_yc1.info()dynass = pd.read_stata(\"datos\\\\dynass.dta\")\ndynass = dynass[[\"pdpass\",\"pdpco1\",\"gvkey1\"]] \ndynass = dynass.rename(columns = {\"pdpco1\":\"pdpco\", \"gvkey1\":\"gvkey\"})\ndynass = dynass.astype({\"pdpass\": float})\ndynass.head()pgvkey = pd.merge(pat_yc1, dynass, on=\"pdpass\",how=\"inner\")\n\n# align with executive compensation data (1992 onwards)\npgvkey1 = pgvkey.loc[pgvkey[\"gyear\"]>=1992]\npgvkey1.head()pgvkey2 = pgvkey1[[\"gvkey\",\"gyear\",\"patent_count\"]]\npgvkey3 = pgvkey2.pivot_table(index=\"gvkey\", columns =\"gyear\", values=\"patent_count\",\n                             aggfunc= \"sum\")\npgvkey3.head()\n\npgvkey3 = pgvkey3.stack(\"gyear\",dropna=False).to_frame().reset_index()\\\n    .rename(columns={0: 'patent_count'})\npgvkey3.head()\n\npgvkey3[\"patent_count\"] = pgvkey3[\"patent_count\"].fillna(0)\npgvkey3.head()\n\npgvkey3.info()\n\nimport numpy as np \nimport pandas as pd\n#import geopandas as gpd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import LeaveOneOut\nimport statsmodels.api as sm\nimport sklearn.metrics as sklm\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.4f}'.format\n\ndata = pd.read_csv('datos\\\\final_demographics_data.csv')data['log GDP per worker'] = np.log(data['GDP (constant 2010 US$)']/data['Labor force, total'])\ndata['Urban population (%)'] = (data['Urban population']/data['Population, total']) * 100\ndata['log population'] = np.log(data['Population, total'])\ndata = data[data['log GDP per worker'].notna()]\ngdp_per_worker = data['GDP (constant 2010 US$)']/data['Labor force, total']\n\nfig = plt.subplots(figsize=(15,15))\nplt.title('Heatmap of Missing Values')\nsns.heatmap(data.isna())\nplt.show()# only considerng features with less than 10 missing values and a sensible relationship with the producitivity\nfeatures = ['country', 'log GDP per worker', 'Urban population (%)', 'Domestic credit to private sector by banks (% of GDP)', \n            'Fixed broadband subscriptions (per 100 people)', 'Age dependency ratio, old (% of working-age population)', \n            'Age dependency ratio, young (% of working-age population)', 'Unemployment, total (% of total labor force) (modeled ILO estimate)', \n            'Final consumption expenditure (% of GDP)'] \n\n# Filling nan with last available data from https://data.worldbank.org/\n\n# Domestic credit to private sector by banks (% of GDP) \ndata['Domestic credit to private sector by banks (% of GDP)'].iloc[data['country'] == 'Argentina'] = 15.4 # 2017\ndata['Domestic credit to private sector by banks (% of GDP)'].iloc[data['country'] == 'Ethiopia'] = 17.6 # 2008\ndata['Domestic credit to private sector by banks (% of GDP)'].iloc[data['country'] == 'Canada'] = 124.1 # 2008\ndata['Domestic credit to private sector by banks (% of GDP)'].iloc[data['country'] == 'Iraq'] = 9.2 # 2018\ndata['Domestic credit to private sector by banks (% of GDP)'].iloc[data['country'] == 'Saudi Arabia'] = 54.0 # 2018\ndata['Domestic credit to private sector by banks (% of GDP)'].iloc[data['country'] == 'Switzerland'] = 168.5 # 2016\ndata['Domestic credit to private sector by banks (% of GDP)'].iloc[data['country'] == 'Viet Nam'] = 137.9 #2019\n\n# fixed broadband\ndata['Fixed broadband subscriptions (per 100 people)'].iloc[data['country'] == 'Peru'] = 7.93 # 2018\ndata['Fixed broadband subscriptions (per 100 people)'].iloc[data['country'] == 'Philippines'] = 5.48 # 2019\ndata['Fixed broadband subscriptions (per 100 people)'].iloc[data['country'] == 'Ethiopia'] = 0.06 # 2017\ndata['Fixed broadband subscriptions (per 100 people)'].iloc[data['country'] == 'Nepal'] = 2.82 # 2018\n\n# final consumption\ndata['Final consumption expenditure (% of GDP)'].iloc[data['country'] == 'China'] = 56.0 # 2019\ndata['Final consumption expenditure (% of GDP)'].iloc[data['country'] == 'Ghana'] = 77.6 # 2019\ndata['Final consumption expenditure (% of GDP)'].iloc[data['country'] == 'Iraq'] = 65.4 # 2019\ndata['Final consumption expenditure (% of GDP)'].iloc[data['country'] == 'Kazakhstan'] = 61.4 # 2019\ndata['Final consumption expenditure (% of GDP)'].iloc[data['country'] == 'Kenya'] = 95.5 # 2019\ndata['Final consumption expenditure (% of GDP)'].iloc[data['country'] == 'Tunisia'] = 92.9 # 2019\ndata['Final consumption expenditure (% of GDP)'].iloc[data['country'] == 'United States of America'] = 81.8 # 2020\n\ndata = data[features]\ndata = data.dropna()\n\ndatafig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.histplot(gdp_per_worker, ax=ax1, color='salmon')\nax1.set_title('GDP per worker')\nsns.histplot(data['log GDP per worker'], ax=ax2, color='salmon')\nax2.set_title('Log GDP per worker')\nplt.show()fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.regplot(ax=ax1, x='Urban population (%)', y='log GDP per worker', data=data, color = 'salmon')\nsns.histplot(data['Urban population (%)'], ax=ax2, color='salmon')\nplt.show()\n\nprint('Pearson-r and P-Value: ' + str(pearsonr(data['Urban population (%)'], data['log GDP per worker'])))fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.regplot(ax=ax1, x='Domestic credit to private sector by banks (% of GDP)', y='log GDP per worker', data=data, color = 'salmon')\nsns.histplot(data['Domestic credit to private sector by banks (% of GDP)'], ax=ax2, color='salmon')\nplt.show()\n\nprint('Pearson-r and P-Value: ' + str(pearsonr(data['Domestic credit to private sector by banks (% of GDP)'], data['log GDP per worker'])))fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.regplot(ax=ax1, x='Fixed broadband subscriptions (per 100 people)', y='log GDP per worker', data=data, color = 'salmon')\nsns.histplot(data['Fixed broadband subscriptions (per 100 people)'], ax=ax2, color='salmon')\nplt.show()\n\nprint('Pearson-r and P-Value: ' + str(pearsonr(data['Fixed broadband subscriptions (per 100 people)'], data['log GDP per worker'])))fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.regplot(ax=ax1, x='Age dependency ratio, old (% of working-age population)', y='log GDP per worker', data=data, color = 'salmon')\nsns.histplot(data['Age dependency ratio, old (% of working-age population)'], ax=ax2, color='salmon')\nplt.show()\n\nprint('Pearson-r and P-Value: ' + str(pearsonr(data['Age dependency ratio, old (% of working-age population)'], data['log GDP per worker'])))fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.regplot(ax=ax1, x='Age dependency ratio, young (% of working-age population)', y='log GDP per worker', data=data, color = 'salmon')\nsns.histplot(data['Age dependency ratio, young (% of working-age population)'], ax=ax2, color='salmon')\nplt.show()\n\nprint('Pearson-r and P-Value: ' + str(pearsonr(data['Age dependency ratio, young (% of working-age population)'], data['log GDP per worker'])))fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.regplot(ax=ax1, x='Unemployment, total (% of total labor force) (modeled ILO estimate)', y='log GDP per worker', data=data, color = 'salmon')\nsns.histplot(data['Unemployment, total (% of total labor force) (modeled ILO estimate)'], ax=ax2, color='salmon')\nplt.show()\n\nprint('Pearson-r and P-Value: ' + str(pearsonr(data['Unemployment, total (% of total labor force) (modeled ILO estimate)'], data['log GDP per worker'])))fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\nsns.regplot(ax=ax1, x='Final consumption expenditure (% of GDP)', y='log GDP per worker', data=data, color = 'salmon')\nsns.histplot(data['Final consumption expenditure (% of GDP)'], ax=ax2, color='salmon')\nplt.show()\n\nprint('Pearson-r and P-Value: ' + str(pearsonr(data['Final consumption expenditure (% of GDP)'], data['log GDP per worker'])))data.corr().style.background_gradient(cmap='coolwarm')X = data.drop(['country'], axis = 1)\ny = X.pop('log GDP per worker')\n\n# columns for model 2\nmodel2_l = ['const','Urban population (%)', 'Domestic credit to private sector by banks (% of GDP)', \n            'Fixed broadband subscriptions (per 100 people)', 'Final consumption expenditure (% of GDP)',\n            'Age dependency ratio, young (% of working-age population)', 'Unemployment, total (% of total labor force) (modeled ILO estimate)']\n\n# columns for model 3\nmodel3_l = ['const', 'Urban population (%)', 'Domestic credit to private sector by banks (% of GDP)', \n          'Age dependency ratio, old (% of working-age population)', 'Final consumption expenditure (% of GDP)',\n          'Age dependency ratio, young (% of working-age population)', 'Unemployment, total (% of total labor force) (modeled ILO estimate)']\n\nX_lin = sm.tools.add_constant(X)\nmodel1 = sm.OLS(y, X_lin.astype('float')).fit()\nmodel2 = sm.OLS(y, X_lin[model2_l].astype('float')).fit()\nmodel3 = sm.OLS(y, X_lin[model3_l].astype('float')).fit()\n\ndef return_results(model, model_list):\n    return [model.pvalues[i] for i in model_list]\n\nresults = pd.DataFrame(index=X_lin.columns)\nresults['Model 1 Coeff.'] = pd.Series(model1.params)\nresults['Model 1 P-Values'] = pd.Series(return_results(model1, X_lin.columns), index = X_lin.columns)\nresults['Model 2 Coeff.'] = pd.Series(model2.params)\nresults['Model 2 P-Values'] = pd.Series(return_results(model2, model2_l), index = model2_l)\nresults['Model 3 Coeff.'] = pd.Series(model3.params)\nresults['Model 3 P-Values'] = pd.Series(return_results(model3, model3_l), index = model3_l)\nresults = results.fillna('---')\n\nresultsloo = LeaveOneOut()\nm1_score = []\nm2_score = []\nm3_score = []\nfor train_index, test_index in loo.split(X_lin, gdp_per_worker[X_lin.index]):\n    X_train, X_test = X_lin.iloc[train_index], X_lin.iloc[test_index]\n    y_train, y_test = gdp_per_worker.iloc[train_index], gdp_per_worker.iloc[test_index]\n    model1 = sm.OLS(list(y_train), X_train.astype('float')).fit()\n    model2 = sm.OLS(list(y_train), X_train[model2_l].astype('float')).fit()\n    model3 = sm.OLS(list(y_train), X_train[model3_l].astype('float')).fit()\n    m1_score.append(np.sqrt(sklm.mean_squared_error(y_test, model1.predict(X_test.astype('float')))))\n    m2_score.append(np.sqrt(sklm.mean_squared_error(y_test, model2.predict(X_test[model2_l].astype('float')))))\n    m3_score.append(np.sqrt(sklm.mean_squared_error(y_test, model3.predict(X_test[model3_l].astype('float')))))\n\nprint('GDP per worker std.:        ' + str(np.std(gdp_per_worker[X_lin.index])))\nprint('Model 1 LOOCV Error (RMSE): ' + str(np.mean(m1_score)))\nprint('Model 2 LOOCV Error (RMSE): ' + str(np.mean(m2_score)))\nprint('Model 3 LOOCV Error (RMSE): ' + str(np.mean(m3_score)))"},{"path":"análisis-de-datos-con-python.html","id":"análisis-de-datos-parte-ii","chapter":"7 Análisis de Datos con Python","heading":"7.7 Análisis de datos (Parte II)","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"optimización-con-scipy","chapter":"7 Análisis de Datos con Python","heading":"7.7.1 Optimización con Scipy","text":"Mínimo global: corresponde al valor mínimo en el dominio de la función.Mínimo global: corresponde al valor mínimo en el dominio de la función.Mínimo local: corresponde al valor mínimo en una región particular de la curva.Mínimo local: corresponde al valor mínimo en una región particular de la curva.En el ejemplo tenemos un mínimo local y un mínimo global.En el ejemplo tenemos un mínimo local y un mínimo global.Mediante un algoritmo de scipy podemos encontrar estos valores, por ejemplo podemos usar la función fmin_bfgs (método BFGS) para encontrar el mínimo.Mediante un algoritmo de scipy podemos encontrar estos valores, por ejemplo podemos usar la función fmin_bfgs (método BFGS) para encontrar el mínimo.Por la naturaleza del método podemos quedar en el mínimo local si partimos del “lado correcto”Podemos encontrar un mínimo local (dentro de un intervalo), usando fminboundSi la función tiene más de una raíz, tenemos que cambiar el valor inicial para partir de un punto que nos lleve la segunda (tercera…) soluciónSupongamos que f tiene un poco de ruidoLa forma funcional original es \\(f(x)=x2+10∗sin(x)\\)Entonces podemos generalizar mediante\\[f(x,,b)=∗x^2+b∗sin(x)\\]Mediante curve_fit() podemos encontrar los valores de y b.","code":"import numpy as np\nfrom scipy import linalg\n\narr = np.array([[1, 2],\n               [3, 4]])\nlinalg.\n\narr\n\narr = np.array([[3, 2],\n                [6, 4]])\n\nlinalg.det(arr)\n\n#2. Inversa de una matriz\nlinalg.inv(arr)\n# calcular la inversa de una matriz singular (su determinante es cero) generará un error\n\narr = np.array([[1, 2],\n               [3, 4]])\nlinalg.inv(arr)\n\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return x**2 + 10*np.sin(x)\n\nx = np.arange(-10, 10, 0.1)\nplt.plot(x, f(x)) \nplt.show()# Usamos el código optmize, la función fmin_bfgs\n# La sintaxis es fmin_bfgs(función a optimizar, valor inicial)\nopt = optimize.fmin_bfgs(f, 0)\nprint(opt)\n\n#El resultado nos entrega: \n#1) Valor de la función evaluada en el punto mínimo\n#2) Número de iteraciones que se demoró en encontrar el mínimo\n#3) Número de evaluaciones de la función\n#4) Número de evaluaciones del gradiente\n#5) Parámetro que minimiza la funciónoptimize.fmin_bfgs(f, 3)xmin_local = optimize.fminbound(f, 0, 10)    \nprint(xmin_local)\n\nroot = optimize.fsolve(f, 1)  # our initial guess is 1\nrootroot2 = optimize.fsolve(f, -2.5)\nroot2\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(x, f(x), 'b-', label=\"f(x)\")\nxmins = np.array([xmin_global[0], xmin_local])\nax.plot(xmins, f(xmins), 'go', label=\"Minima\")\nroots = np.array([root, root2])\nax.plot(roots, f(roots), 'kv', label=\"Roots\")\nax.legend()\nax.set_xlabel('x')\nax.set_ylabel('f(x)')xdata = np.linspace(-10, 10, num=20)\nydata = f(xdata) + np.random.randn(xdata.size)\n\n\nplt.plot(x, f(x), 'b-', label=\"f(x)\")\nplt.plot(xdata, ydata, color=\"red\")def f2(x,a ,b):\n    return a*x**2 + b**np.sin(x)\n\n#Valor inicial\nguess = [2,2]\n\n# curve_fit(f, xdata, ydata, valor inicial)\nparams, params_covariance = optimize.curve_fit(f2, xdata, ydata, guess)\n\n# retorna: \n#1) Valores óptimos que minimizan f(xdata) - ydata\n#2) Covarianza estimada de valores óptimos\nparams\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(x, f(x), 'b-', label=\"f(x)\")\nax.plot(x, f2(x, *params), 'r--', label=\"Curve fit result\")\n\nxmins = np.array([xmin_global[0], xmin_local])\nax.plot(xmins, f(xmins), 'go', label=\"Minima\")\nroots = np.array([root, root2])\nax.plot(roots, f(roots), 'kv', label=\"Roots\")\nax.legend()\nax.set_xlabel('x')\nax.set_ylabel('f(x)')"},{"path":"análisis-de-datos-con-python.html","id":"aplicación-de-predicción","chapter":"7 Análisis de Datos con Python","heading":"7.7.1.1 Aplicación de predicción","text":"\\(y_i = 0.5 + x_i - 0.1x^2 + \\epsilon_i\\)","code":"A = [[1, 2],    \n     [3, 4]]\n\nB = [[11, 12],\n     [13, 14]]\n\nprint(A)\n\nprint(B)\n\nA_np = np.array(A)\n\nB_np = np.array(B)\n\nA_np * B_np\n\nA_np@B_np\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(0)T = 2000\nx = np.random.normal(5,2,T)  ## media, sd, largo de serie\nϵ = np.random.normal(0,1,T)\ny = 0.5 + x - 0.1*x**2+ϵ\n\ny\n\nplt.scatter(x,y,s=5,color='blue')\nplt.xlabel('x')\nplt.ylabel('y')"},{"path":"análisis-de-datos-con-python.html","id":"polonomio-de-orden-1","chapter":"7 Análisis de Datos con Python","heading":"7.7.1.2 Polonomio de orden 1","text":"np.linalg.lstq -> Return least-squares solution linear matrix equation.\nhttps://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html","code":"X_1 = np.column_stack([np.ones_like(x),x])\nX_1[0:3,:]β_1 = np.linalg.lstsq(X_1,y,rcond=None)[0]\nprint(β_1)\n\nplt.scatter(x,y,s=5,color='blue')\nplt.scatter(x,X_1@β_1,s=5,color='red')"},{"path":"análisis-de-datos-con-python.html","id":"polinomio-de-orden-2","chapter":"7 Análisis de Datos con Python","heading":"7.7.1.3 Polinomio de orden 2","text":"","code":"X_2 = np.column_stack([np.ones_like(x),x,x**2])\nX_2[0:3,:]\n\nβ_2 = np.linalg.lstsq(X_2,y,rcond=None)[0]\nprint(β_2)\n\nplt.scatter(x,y,s=5,color='blue')\nplt.scatter(x,X_2@β_2,s=5,color='black')\n\nK =10\nX_K = np.column_stack([x**k for k in range(K+1)])\n#X_K[0:3,:]\n\nβ_K = np.linalg.lstsq(X_K,y,rcond=None)[0]\nprint(β_K)\n\nplt.scatter(x,y,s=5,color='blue')\nplt.scatter(x,X_K@β_K,s=5,color='orange')\n\nECM_1 = np.mean((y-X_1@β_1)**2)\nECM_2 = np.mean((y-X_2@β_2)**2)\nECM_K = np.mean((y-X_K@β_K)**2)\nprint(ECM_1,ECM_2,ECM_K)"},{"path":"análisis-de-datos-con-python.html","id":"modelo-de-regresión-lineal","chapter":"7 Análisis de Datos con Python","heading":"7.7.2 Modelo de regresión lineal","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"introducción-1","chapter":"7 Análisis de Datos con Python","heading":"7.7.2.1 Introducción","text":"La biblioteca scikit-learn de Python contiene una amplia gama de funciones para el modelado predictivo. Vamos cargar su función de entrenamiento de regresión lineal y ajustar una línea los datos de mtcars:Podemos tener una idea de la parte de la varianza de la variable de respuesta que explica el modelo utilizando la función model.score():El resultado de la función de puntuación para la regresión lineal es el “R-cuadrado”, un valor que va de 0 1 y que describe la proporción de la varianza de la variable de respuesta que explica el modelo. En este caso, el peso del coche explica aproximadamente el 75% de la varianza en mpg.El resultado de la función de puntuación para la regresión lineal es el “R-cuadrado”, un valor que va de 0 1 y que describe la proporción de la varianza de la variable de respuesta que explica el modelo. En este caso, el peso del coche explica aproximadamente el 75% de la varianza en mpg.La medida R-cuadrado se basa en los residuos: las diferencias entre lo que el modelo predice para cada punto de datos y el valor real de cada punto de datos. Podemos extraer los residuos del modelo haciendo una predicción con el modelo sobre los datos y luego restando el valor real de cada predicción:La medida R-cuadrado se basa en los residuos: las diferencias entre lo que el modelo predice para cada punto de datos y el valor real de cada punto de datos. Podemos extraer los residuos del modelo haciendo una predicción con el modelo sobre los datos y luego restando el valor real de cada predicción:Ahora que tenemos un modelo lineal, vamos trazar la línea la que se ajusta en nuestro gráfico de dispersión para tener una idea de lo bien que se ajusta los datos:La línea de regresión parece un ajuste razonable y se ajusta nuestra intuición: medida que aumenta el peso del coche, cabe esperar que el consumo de combustible disminuya.Los valores atípicos pueden tener una gran influencia en los modelos de regresión lineal: como la regresión consiste en minimizar los residuos al cuadrado, los residuos grandes tienen una influencia desproporcionada en el modelo. Trazar el resultado nos ayuda detectar los valores atípicos influyentes. En este caso parece haber ningún valor atípico influyente. Añadamos un valor atípico -un coche superpesado de bajo consumo- y tracemos un nuevo modelo de regresión:Aunque se trata de un caso extremo y artificial, el gráfico anterior ilustra la influencia que puede tener un solo valor atípico en un modelo de regresión lineal.Aunque se trata de un caso extremo y artificial, el gráfico anterior ilustra la influencia que puede tener un solo valor atípico en un modelo de regresión lineal.En un modelo de regresión lineal que se comporte bien, nos gustaría que los residuos se distribuyeran de forma aproximadamente normal. Es decir, nos gustaría que el error se distribuyera de forma más o menos uniforme por encima y por debajo de la línea de regresión.En un modelo de regresión lineal que se comporte bien, nos gustaría que los residuos se distribuyeran de forma aproximadamente normal. Es decir, nos gustaría que el error se distribuyera de forma más o menos uniforme por encima y por debajo de la línea de regresión.Podemos investigar la normalidad de los residuos con un gráfico Q-Q (cuantil-cuantil). Haga un qqplot pasando los residuos la función stats.probplot() en la biblioteca scipy.stats:Podemos investigar la normalidad de los residuos con un gráfico Q-Q (cuantil-cuantil). Haga un qqplot pasando los residuos la función stats.probplot() en la biblioteca scipy.stats:Cuando los residuos se distribuyen normalmente, tienden situarse lo largo de la línea recta en el gráfico Q-Q. En este caso, los residuos parecen seguir un patrón ligeramente lineal: los residuos se inclinan un poco lejos de la línea de normalidad en cada extremo. Esto es una indicación de que una simple línea recta podría ser suficiente para describir completamente la relación entre el peso y el mpg.Cuando los residuos se distribuyen normalmente, tienden situarse lo largo de la línea recta en el gráfico Q-Q. En este caso, los residuos parecen seguir un patrón ligeramente lineal: los residuos se inclinan un poco lejos de la línea de normalidad en cada extremo. Esto es una indicación de que una simple línea recta podría ser suficiente para describir completamente la relación entre el peso y el mpg.Después de hacer las predicciones del modelo, es útil tener algún tipo de métrica para evaluar lo bien que funcionó el modelo. El R-cuadrado ajustado es una medida útil, pero sólo se aplica al modelo de regresión en sí: nos gustaría tener alguna métrica de evaluación universal que nos permita comparar el rendimiento de diferentes tipos de modelos.Después de hacer las predicciones del modelo, es útil tener algún tipo de métrica para evaluar lo bien que funcionó el modelo. El R-cuadrado ajustado es una medida útil, pero sólo se aplica al modelo de regresión en sí: nos gustaría tener alguna métrica de evaluación universal que nos permita comparar el rendimiento de diferentes tipos de modelos.El error medio cuadrático (RMSE) es una métrica de evaluación común para las predicciones con números reales. El error medio cuadrático es la raíz cuadrada de la media del error cuadrático (residuos). Si recuerda, escribimos una función para calcular el RMSE en la lección 12:El error medio cuadrático (RMSE) es una métrica de evaluación común para las predicciones con números reales. El error medio cuadrático es la raíz cuadrada de la media del error cuadrático (residuos). Si recuerda, escribimos una función para calcular el RMSE en la lección 12:En lugar de definir su propia función RMSE, puede utilizar la función de error cuadrático medio de la biblioteca scikit-learn y tomar la raíz cuadrada del resultado:","code":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nmatplotlib.style.use('ggplot')\n\nmtcars.head()\n\n# Load mtcars data set\nmtcars = pd.read_csv(\"mtcars.csv\")\n\nmtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\");from sklearn import linear_model\n\n# Modelo inicial\nregression_model = linear_model.LinearRegression()\n\n# Entrenar al modelo\nregression_model.fit(X = pd.DataFrame(mtcars[\"wt\"]), \n                     y = mtcars[\"mpg\"])\n\n# Chequear coeficientes\nprint(regression_model.intercept_)\n\nprint(regression_model.coef_)regression_model.score(X = pd.DataFrame(mtcars[\"wt\"]), \n                       y = mtcars[\"mpg\"])train_prediction = regression_model.predict(X = pd.DataFrame(mtcars[\"wt\"]))\n\n# Actual - prediction = residuals\nresiduals = mtcars[\"mpg\"] - train_prediction\n\nresiduals.describe()\n\nSSResiduals = (residuals**2).sum()\n\nSSTotal = ((mtcars[\"mpg\"] - mtcars[\"mpg\"].mean())**2).sum()\n\n# R-squared\n1 - (SSResiduals/SSTotal)mtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\",\n           xlim = (0,7))\n\n# Graficamos\nplt.plot(mtcars[\"wt\"],      # Variables explicativas\n         train_prediction,  # Variables predichas\n         color=\"blue\");mtcars_subset = mtcars[[\"mpg\",\"wt\"]]\n\nsuper_car = pd.DataFrame({\"mpg\":50,\"wt\":10}, index=[\"super\"])\n\nnew_cars = mtcars_subset.append(super_car)\n\n# Initialize model\nregression_model = linear_model.LinearRegression()\n\n# Train the model using the new_cars data\nregression_model.fit(X = pd.DataFrame(new_cars[\"wt\"]), \n                     y = new_cars[\"mpg\"])\n\ntrain_prediction2 = regression_model.predict(X = pd.DataFrame(new_cars[\"wt\"]))\n\n# Plot the new model\nnew_cars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\", xlim=(1,11), ylim=(10,52))\n\n# Plot regression line\nplt.plot(new_cars[\"wt\"],     # Explanatory variable\n         train_prediction2,  # Predicted values\n         color=\"blue\");plt.figure(figsize=(9,9))\n\nstats.probplot(residuals, dist=\"norm\", plot=plt);def rmse(predicted, targets):\n    \"\"\"\n    Computes root mean squared error of two numpy ndarrays\n    \n    Args:\n        predicted: an ndarray of predictions\n        targets: an ndarray of target values\n    \n    Returns:\n        The root mean squared error as a float\n    \"\"\"\n    return (np.sqrt(np.mean((targets-predicted)**2)))\n\nrmse(train_prediction2, new_cars[\"mpg\"])\n\nrmse(train_prediction, mtcars[\"mpg\"])\n\nrmse(train_prediction2, new_cars[\"mpg\"])from sklearn.metrics import mean_squared_error\n\nRMSE = mean_squared_error(train_prediction, mtcars[\"mpg\"])**0.5\n\nRMSE"},{"path":"análisis-de-datos-con-python.html","id":"agregando-polinomios","chapter":"7 Análisis de Datos con Python","heading":"7.7.2.2 Agregando polinomios","text":"Vamos trazar la línea curva definida por el nuevo modelo para ver si el ajuste es mejor que el anterior. Para empezar, vamos crear una función que toma una matriz de valores x, coeficientes del modelo y un término de intercepción y devuelve los valores x y los valores y ajustados correspondientes esos valores x.La función cuadrática parece ajustarse los datos un poco mejor que la lineal. Investiguemos más fondo utilizando el nuevo modelo para hacer predicciones sobre los datos originales y comprobemos el error medio cuadrático:Como el RMSE del modelo cuadrático es menor que el anterior y el R-cuadrado ajustado es mayor, probablemente sea un modelo mejor. Sin embargo, tenemos que tener cuidado con el sobreajuste de los datos de entrenamiento (“overfitting”).La sobreadaptación describe una situación en la que nuestro modelo se ajusta demasiado los datos que utilizamos para crearlo (datos de entrenamiento), lo que provoca una mala generalización los nuevos datos.La sobreadaptación describe una situación en la que nuestro modelo se ajusta demasiado los datos que utilizamos para crearlo (datos de entrenamiento), lo que provoca una mala generalización los nuevos datos.Por eso, generalmente queremos utilizar los datos de entrenamiento para evaluar un modelo: nos da una evaluación sesgada, normalmente demasiado optimista. Uno de los puntos fuertes de la regresión lineal de primer y segundo orden es que son tan simples que es poco probable que sobreajusten los datos. Cuanto más complejo sea el modelo que creemos y más libertad tenga para ajustarse los datos de entrenamiento, mayor será el riesgo de sobreajuste.Por eso, generalmente queremos utilizar los datos de entrenamiento para evaluar un modelo: nos da una evaluación sesgada, normalmente demasiado optimista. Uno de los puntos fuertes de la regresión lineal de primer y segundo orden es que son tan simples que es poco probable que sobreajusten los datos. Cuanto más complejo sea el modelo que creemos y más libertad tenga para ajustarse los datos de entrenamiento, mayor será el riesgo de sobreajuste.Por ejemplo, podríamos seguir incluyendo más términos polinómicos en nuestro modelo de regresión para ajustarnos más los datos de entrenamiento y conseguir puntuaciones de RMSE más bajas con respecto al conjunto de entrenamiento, pero es casi seguro que esto se generalizaría bien los nuevos datos. Ilustremos este punto ajustando un modelo de 10º orden los datos de mtcars:Por ejemplo, podríamos seguir incluyendo más términos polinómicos en nuestro modelo de regresión para ajustarnos más los datos de entrenamiento y conseguir puntuaciones de RMSE más bajas con respecto al conjunto de entrenamiento, pero es casi seguro que esto se generalizaría bien los nuevos datos. Ilustremos este punto ajustando un modelo de 10º orden los datos de mtcars:Obsérvese que la puntuación R-cuadrado ha aumentado sustancialmente con respecto nuestro modelo cuadrático. Vamos trazar la línea de mejor ajuste para investigar lo que hace el modelo:Observe cómo el modelo polinómico de 10º orden se curva de forma salvaje en algunos lugares para ajustarse los datos de entrenamiento. Aunque este modelo se ajusta mejor los datos de entrenamiento, es casi seguro que se generalizará bien los nuevos datos, ya que conduce predicciones absurdas, como que un coche tiene menos de 0 mpg si pesa 5.000 libras.","code":"# Iniciamos el modelo\npoly_model = linear_model.LinearRegression()\n\n# Data frmame\npredictors = pd.DataFrame([mtcars[\"wt\"],           # Include weight\n                           mtcars[\"wt\"]**2]).T     # Include weight squared\n\n# \"Entrenar\" el modelo\npoly_model.fit(X = predictors, \n               y = mtcars[\"mpg\"])\n\n# Chequear\nprint(\"Model intercept\")\nprint(poly_model.intercept_)\n\nprint(\"Model Coefficients\")\nprint(poly_model.coef_)\n\n\nprint(\"Model Accuracy:\")\nprint(poly_model.score(X = predictors, \n                 y = mtcars[\"mpg\"]))# Plot the curve from 1.5 to 5.5\npoly_line_range = np.arange(1.5, 5.5, 0.1)\n\n# Get first and second order predictors from range\npoly_predictors = pd.DataFrame([poly_line_range,\n                               poly_line_range**2]).T\n\n# Get corresponding y values from the model\ny_values = poly_model.predict(X = poly_predictors)\n\nmtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\",\n           xlim = (0,7))\n\n# Plot curve line\nplt.plot(poly_line_range,   # X-axis range\n         y_values,          # Predicted values\n         color=\"blue\");preds = poly_model.predict(X=predictors)\n\nrmse(preds , mtcars[\"mpg\"])# Initialize model\npoly_model = linear_model.LinearRegression()\n\n# Make a DataFrame of predictor variables\npredictors = pd.DataFrame([mtcars[\"wt\"],           \n                           mtcars[\"wt\"]**2,\n                           mtcars[\"wt\"]**3,\n                           mtcars[\"wt\"]**4,\n                           mtcars[\"wt\"]**5,\n                           mtcars[\"wt\"]**6,\n                           mtcars[\"wt\"]**7,\n                           mtcars[\"wt\"]**8,\n                           mtcars[\"wt\"]**9,\n                           mtcars[\"wt\"]**10]).T     \n\n# Train the model using the new_cars data\npoly_model.fit(X = predictors, \n               y = mtcars[\"mpg\"])\n\n# Check trained model y-intercept\nprint(\"Model intercept\")\nprint(poly_model.intercept_)\n\n# Check trained model coefficients (scaling factor given to \"wt\")\nprint(\"Model Coefficients\")\nprint(poly_model.coef_)\n\n# Check R-squared\npoly_model.score(X = predictors, \n                 y = mtcars[\"mpg\"])p_range = np.arange(1.5, 5.45, 0.01)\n\npoly_predictors = pd.DataFrame([p_range, p_range**2, p_range**3,\n                              p_range**4, p_range**5, p_range**6, p_range**7, \n                              p_range**8, p_range**9, p_range**10]).T  \n\n# Get corresponding y values from the model\ny_values = poly_model.predict(X = poly_predictors)\n\nmtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\",\n           xlim = (0,7))\n\n# Plot curve line\nplt.plot(p_range,   # X-axis range\n         y_values,          # Predicted values\n         color=\"blue\");"},{"path":"análisis-de-datos-con-python.html","id":"agregar-más-variables","chapter":"7 Análisis de Datos con Python","heading":"7.7.2.3 Agregar más variables","text":"Cuando se enfrenta una tarea de modelado predictivo, menudo tendrá varias variables en sus datos que pueden ayudar explicar la variación en la variable de respuesta. Puede incluir más variables explicativas en un modelo de regresión lineal incluyendo más columnas en el marco de datos que pasa la función de entrenamiento del modelo. Hagamos un nuevo modelo que añada la variable de potencia nuestro modelo original:.La mejora de la puntuación R-cuadrado sugiere que la potencia tiene una relación lineal con el mpg. Vamos investigar con un gráfico:Aunque las millas por galón tienden disminuir con los caballos, la relación parece más curvada que lineal, por lo que añadir términos polinómicos nuestro modelo de regresión múltiple podría producir un mejor ajuste:El nuevo R-cuadrado y el menor RMSE sugieren que éste es un modelo mejor que cualquiera de los que hicimos anteriormente y nos preocuparía demasiado el sobreajuste, ya que sólo incluye 2 variables y 2 términos al cuadrado. Hay que tener en cuenta que cuando se trabaja con modelos multidimensionales, resulta difícil visualizar los resultados, por lo que se depende en gran medida de los resultados numéricos.Podríamos seguir añadiendo más variables explicativas para intentar mejorar el modelo. Añadir variables que tienen poca relación con la respuesta o incluir variables demasiado relacionadas entre sí puede perjudicar los resultados cuando se utiliza la regresión lineal. También debe tener cuidado con las variables numéricas que toman pocos valores únicos, ya que pueden actuar más como variables categóricas que numéricas.","code":"# Initialize model\nmulti_reg_model = linear_model.LinearRegression()\n\n# Train the model using the mtcars data\nmulti_reg_model.fit(X = mtcars.loc[:,[\"wt\",\"hp\"]], \n                     y = mtcars[\"mpg\"])\n\n# Check trained model y-intercept\nprint(multi_reg_model.intercept_)\n\n# Check trained model coefficients (scaling factor given to \"wt\")\nprint(multi_reg_model.coef_)\n\n# Check R-squared\nmulti_reg_model.score(X = mtcars.loc[:,[\"wt\",\"hp\"]], \n                      y = mtcars[\"mpg\"])mtcars.plot(kind=\"scatter\",\n           x=\"hp\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\");# Initialize model\nmulti_reg_model = linear_model.LinearRegression()\n\n# Include squared terms\npoly_predictors = pd.DataFrame([mtcars[\"wt\"],\n                                mtcars[\"hp\"],\n                                mtcars[\"wt\"]**2,\n                                mtcars[\"hp\"]**2]).T\n\n# Train the model using the mtcars data\nmulti_reg_model.fit(X = poly_predictors, \n                    y = mtcars[\"mpg\"])\n\n# Check R-squared\nprint(\"R-Squared\")\nprint( multi_reg_model.score(X = poly_predictors , \n                      y = mtcars[\"mpg\"]) )\n\n# Check RMSE\nprint(\"RMSE\")\nprint(rmse(multi_reg_model.predict(poly_predictors),mtcars[\"mpg\"]))"},{"path":"análisis-de-datos-con-python.html","id":"desde-una-perspectiva-económetrica","chapter":"7 Análisis de Datos con Python","heading":"7.7.2.4 Desde una perspectiva económetrica","text":"","code":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n#import plotnine as p\n\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n# read data\ndef read_data(file):\n    return pd.read_csv(\"https://raw.github.com/scunning1975/mixtape/master/\" + file)\n\nnp.random.seed(1)\n\ntb = pd.DataFrame({\n    'x': np.random.normal(size=10000),\n    'u': np.random.normal(size=10000)})\ntb['y'] = 5.5*tb['x'].values + 12*tb['u'].values\n\ntb\n\nreg_tb = sm.OLS.from_formula('y ~ x', \n                             data=tb).fit()\n\nreg_tb\n\nreg_tb.summary()\n\ntb['yhat1'] = reg_tb.predict(tb)\ntb['yhat2'] = 0.1114 + 5.6887*tb['x']\ntb['uhat1'] = reg_tb.resid\ntb['uhat2'] = tb['y'] - tb['yhat2']\n\ntb.describe()\n\ntb = pd.DataFrame({\n    'x': 9*np.random.normal(size=10),\n    'u': 36*np.random.normal(size=10)})\ntb['y'] = 3*tb['x'].values + 2*tb['u'].values\n\nreg_tb = sm.OLS.from_formula('y ~ x', data=tb).fit()\n\ntb['yhat1'] = reg_tb.predict(tb)\ntb['uhat1'] = reg_tb.resid\n\ntb.describe()"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-estimacion-de-regresion-lineal","chapter":"7 Análisis de Datos con Python","heading":"7.7.2.5 Ejemplo: estimacion de regresion lineal","text":"","code":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndata = pd.read_csv('real_estate.csv')\n\ndata.head(5)\n\ndata.shape\n\ndata.describe()\n\ndata.precio.describe()\n\ndata.hist(figsize=(6,6))\n\ndata.superficie.hist()\n\ndata.plot(kind = 'kde',\n          subplots=True,\n          layout = (2,2),\n          sharex=False,figsize=(10,8))\n\ndata.plot(kind='box',subplots=True,layout=(2,2),sharex=False,sharey=False,figsize=(10,8))\n\nplt.scatter(data.superficie,data.precio,s=10,color='purple')\nplt.xlabel('superficie')\nplt.ylabel('precio')\n\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(data,figsize=(10,10))"},{"path":"análisis-de-datos-con-python.html","id":"una-simulación-de-montecarlo","chapter":"7 Análisis de Datos con Python","heading":"7.7.3 Una simulación de montecarlo","text":"\\[y = 3 + 2x + u\n\\]\\[x ∼ N(0,9)\\]\\[u ∼ N(0,36)\\]","code":"coefs = np.zeros(1000)\n\nfor i in range(1000):\n    tb = pd.DataFrame({\n    'x': 9*np.random.normal(size=10000),\n    'u': 36*np.random.normal(size=10000)})\n    tb['y'] = 3 + 2*tb['x'].values + tb['u'].values\n\n    reg_tb = sm.OLS.from_formula('y ~ x', data=tb).fit()\n\n    coefs[i] = reg_tb.params['x']\n\nplt.hist(coefs, bins = 100)\nplt.show()\n\nlen(coefs)\n\ncoefs.mean()\n\ncoefs.std()"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-1-tirar-dados","chapter":"7 Análisis de Datos con Python","heading":"7.7.3.1 Ejemplo 1: Tirar dados","text":"En este juego, la casa tiene más oportunidades de ganar (30 resultados frente los 6 del jugador), lo que significa que la casa tiene bastante ventaja.En este juego, la casa tiene más oportunidades de ganar (30 resultados frente los 6 del jugador), lo que significa que la casa tiene bastante ventaja.Digamos que nuestro jugador comienza con un saldo de 1.000 dólares y está dispuesto perderlo todo, por lo que apuesta 1 dólar en cada tirada (lo que significa que se lanzan ambos dados) y decide jugar 1.000 tiradas.Digamos que nuestro jugador comienza con un saldo de 1.000 dólares y está dispuesto perderlo todo, por lo que apuesta 1 dólar en cada tirada (lo que significa que se lanzan ambos dados) y decide jugar 1.000 tiradas.Como la casa es tan generosa, ofrece pagar 4 veces la apuesta del jugador cuando éste gana. Por ejemplo, si el jugador gana la primera tirada, su saldo aumenta en 4 dólares, y termina la ronda con un saldo de 1.004 dólares.Como la casa es tan generosa, ofrece pagar 4 veces la apuesta del jugador cuando éste gana. Por ejemplo, si el jugador gana la primera tirada, su saldo aumenta en 4 dólares, y termina la ronda con un saldo de 1.004 dólares.Si milagrosamente se produce una racha de 1.000 tiradas ganadas, podría irse casa con 5.000 dólares. Si perdieran todas las rondas, podrían irse casa sin nada. es una mala relación riesgo-recompensa… o quizás sí.Si milagrosamente se produce una racha de 1.000 tiradas ganadas, podría irse casa con 5.000 dólares. Si perdieran todas las rondas, podrían irse casa sin nada. es una mala relación riesgo-recompensa… o quizás sí.","code":"# Importing Packages\nimport matplotlib.pyplot as plt\nimport random# Creating Roll Dice Function\ndef roll_dice():\n    die_1 = random.randint(1, 6)\n    die_2 = random.randint(1, 6)\n\n    # Determining if the dice are the same number\n    if die_1 == die_2:\n        same_num = True\n    else:\n        same_num = False\n    return same_num\n\nroll_dice()\n\n# Inputs\nnum_simulations = 10000\nmax_num_rolls = 1000\nbet = 1\n\n# Tracking\nwin_probability = []\nend_balance = []\n\n# Creating Figure for Simulation Balances\nfig = plt.figure()\nplt.title(\"Monte Carlo Dice Game [\" + str(num_simulations) + \"simulations]\")\nplt.xlabel(\"Roll Number\")\nplt.ylabel(\"Balance [$]\")\nplt.xlim([0, max_num_rolls])\n\n# For loop to run for the number of simulations desired\nfor i in range(num_simulations):\n    balance = [1000]\n    num_rolls = [0]\n    num_wins = 0\n    # Run until the player has rolled 1,000 times\n    while num_rolls[-1] < max_num_rolls:\n        same = roll_dice()\n        # Result if the dice are the same number\n        if same:\n            balance.append(balance[-1] + 4 * bet)\n            num_wins += 1\n        # Result if the dice are different numbers\n        else:\n            balance.append(balance[-1] - bet)\n\n        num_rolls.append(num_rolls[-1] + 1)\n# Store tracking variables and add line to figure\n    win_probability.append(num_wins/num_rolls[-1])\n    end_balance.append(balance[-1])\n    plt.plot(num_rolls, balance)\n\n# Showing the plot after the simulations are finished\nplt.show()\n\n# Averaging win probability and end balance\noverall_win_probability = sum(win_probability)/len(win_probability)\noverall_end_balance = sum(end_balance)/len(end_balance)\n# Displaying the averages\nprint(\"Average win probability after \" + str(num_simulations) + \"runs: \" + str(overall_win_probability))\nprint(\"Average ending balance after \" + str(num_simulations) + \"runs: $\" + str(overall_end_balance))"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-2-modelo","chapter":"7 Análisis de Datos con Python","heading":"7.7.3.2 Ejemplo 2: Modelo","text":"","code":"import numpy as np\nimport statsmodels.api as sm\nimport matplotlib as plt \nnp.random.seed(2021)\nmu = 0 \nsigma = 1 \nn = 100 \n# assumed population parameters\nalpha = np.repeat(0.5, n)\nbeta = 1.5\n\ndef MC_estimation_slope(M):\n    MC_betas = []\n    MC_samples = {}\n\n    for i in range(M):\n        # randomly sampling from normal distribution as error terms\n        e = np.random.normal(mu, sigma, n)\n        # generating independent variable by making sure the variance in X is larger than the variance in error terms\n        X = 9 * np.random.normal(mu, sigma, n)\n        # population distribution using the assumd parameter values alpha/beta\n        Y = (alpha + beta * X + e)\n        \n        # running OLS regression for getting slope parameters\n        model = sm.OLS(Y.reshape((-1, 1)), X.reshape((-1, 1)))\n        ols_result = model.fit()\n        coeff = ols_result.params\n        \n        MC_samples[i] = Y\n        MC_betas.append(coeff)\n    MC_beta_hats = np.array(MC_betas).flatten()\n    return(MC_samples, MC_beta_hats)\n    \nMC_samples, MC_beta_hats = MC_estimation_slope(M = 10000)\nbeta_hat_MC = np.mean(MC_beta_hats)\n\nMC_samples, MC_beta_hats\n\nplt.hist(MC_beta_hats)\n\nbeta_hat_MC"},{"path":"análisis-de-datos-con-python.html","id":"bootstraping","chapter":"7 Análisis de Datos con Python","heading":"7.7.4 Bootstraping","text":"DataConvertimos datos en centimetrosComo podemos ver, la altura máxima y mínima del conjunto de datos son 137,8 cm y 200,6 cm respectivamenteTome 500 alturas seleccionadas al azarSi los datos se distribuyen normalmente, ¿qué pasaría? Para ver esto, vamos trazar la función de distribución acumulativa con la media y la desviación estándar de nuestros datos empíricos.Además de la FCD, trace la función de distribución acumulativa empírica y la función de densidad de probabilidad.Si hubieramos hecho el mismo experimento muchas veces hubiesemos obtenido lo mismo?Necesitaremos una función que genere muestras bootstrap y extraiga réplicas bootstrap de esas muestras tanto como queramos. La función tomará tres parámetros data, func y size. El parámetro ‘func’ corresponde los estadísticos de resumen que querremos utilizar al crear una réplica bootstrap. El parámetro ‘size’ nos indicará cuántas réplicas necesitamos.Ahora utilizaremos nuestra función para obtener 10.000 réplicas bootstrap. Después veremos cómo son su PDF y su ECDF.Comparemos la media empírica y la media de nuestras réplicasIntervalo de confianza?Intervalo de confianza?Medias muy igualesMedias muy igualesEcnontremos el intervalo de confianzaEcnontremos el intervalo de confianzaNecesitamos conocer el area entre los cuantilesNecesitamos conocer el area entre los cuantilesEncontramos un intervalo de confianza entre 167,2cm y 162,5cm.Podemos decir que si elegimos alguien al azar, su altura estará entre este intervalo con un %95 de probabilidad.","code":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt# Import the iris dataset\ndata = pd.read_csv('weight-height.csv')\n\n# Looking at information\nprint(\"\\nInfo:\")\ndisplay(data.info())\n\n# Summary Statistics\nprint(\"\\nSummary Statistics:\")\ndisplay(data.describe())\n\n# Display first 5 rows\ndisplay(data.head())# Convert inches to centimeters\ndata[\"Height(cm)\"] = data[\"Height\"]*2.54\n\n# Get summary statistics of Heights in centimeters\ndisplay(data['Height(cm)'].describe())# Extraemos 500 random\nheights = data['Height(cm)'].sample(500).reset_index(drop=True)\n\n# Eastadisticas\ndisplay(heights.describe())# Obtenga la desviación estándar y la media\n\nheights_std = np.std(heights)\nheights_mean = np.mean(heights)\n\n# Plot Normal CDF\ndef plot_normal_CDF(data,mean,std,label,title):\n    sns.set()\n    # CDF of the data\n    x = np.linspace(min(data),max(data),len(data))\n    cdf = stats.norm.cdf(x,mean,std)\n    \n    plt.plot(x,cdf)\n    plt.xlabel(label)\n    plt.ylabel('percentage')\n    plt.title(title)\n    plt.show()\n    return x,cdf\n\n# CDF of Versicolor\nx_cdf, y_cdf = plot_normal_CDF(heights, heights_mean, heights_std, \"Height (cm)\",\"CDF\")# Create a function to get x, y for of ecdf\ndef get_ecdf(data):\n    \n    # Get lenght of the data into n\n    n = len(data)\n    \n    # We need to sort the data\n    x = np.sort(data)\n    \n    # the function will show us cumulative percentages of corresponding data points\n    y = np.arange(1,n+1)/n\n    \n    return x,y\n\n# Create a function to plot ecdf\ndef plot_ecdf(data,labelx,labely,title,color):\n    \"\"\"Plot ecdf\"\"\"\n    # Call get_ecdf function and assign the returning values\n    x, y = get_ecdf(data)\n    \n    plt.plot(x,y,marker='.',linestyle='none',c=color)\n    plt.xlabel(labelx)\n    plt.ylabel(labely)\n    plt.title(title)\n\n# Create a function overlay ECDF on CDF\ndef plot_overlay_ecdf(data,labelx,labely,title,color,x_cdf, y_cdf):\n    x, y = get_ecdf(data)\n    \n    plt.plot(x,y,marker='.',linestyle='none',c=color,alpha=0.5)\n    plt.plot(x_cdf, y_cdf)\n    plt.xlabel(labelx)\n    plt.ylabel(labely)\n    plt.title(title)\n    \n# Plotting Empirical CDF\nplot_ecdf(heights,\"Height (cm)\",\"percentage\",\"Empirical CDF\",\"r\")\nplt.show()\n\n# Overlap Empirical CDF on CDF\nplot_overlay_ecdf(heights,\"Height (cm)\",\"percentage\",\"Empirical CDF on CDF\",\"y\",x_cdf, y_cdf)\nplt.show()\n\n# Plotting PDF\nsns.distplot(heights,hist=False)\nplt.xlabel(\"Petal length(cm)\")\nplt.ylabel(\"PDF\")\nplt.title(\"Probability Density Function\")\nplt.show()# Plot our original sample ecdf\nplot_ecdf(heights,\"Height (cm)\",\"percentage\",\"ECDF's of bootstrap samples\",\"y\")\n\nfor i in range(1000):\n    \n    # Generate a bootstrap sample\n    bs_sample_heights = np.random.choice(heights,size=len(heights))\n    \n    # Plot ecdf for bootstrap sample\n    x, y = get_ecdf(bs_sample_heights)\n    plt.scatter(x, y,s=1,c='b',alpha=0.3)\n    \nplt.show()def draw_bs_replicates(data,func,size):\n    \"\"\"creates a bootstrap sample, computes replicates and returns replicates array\"\"\"\n    # Create an empty array to store replicates\n    bs_replicates = np.empty(size)\n    \n    # Create bootstrap replicates as much as size\n    for i in range(size):\n        # Create a bootstrap sample\n        bs_sample = np.random.choice(data,size=len(data))\n        # Get bootstrap replicate and append to bs_replicates\n        bs_replicates[i] = func(bs_sample)\n    \n    return bs_replicates# Draw 10000 bootstrap replicates\nbs_replicates_heights = draw_bs_replicates(heights,np.mean,15000)\n\n# Plot probability density function\nplt.hist(bs_replicates_heights,bins=30,density=True)\nplt.axvline(x=np.percentile(bs_replicates_heights,[2.5]), ymin=0, ymax=1,label='2.5th percentile',c='y')\nplt.axvline(x=np.percentile(bs_replicates_heights,[97.5]), ymin=0, ymax=1,label='97.5th percentile',c='r')\nplt.xlabel(\"Height(cm)\")\nplt.ylabel(\"PDF\")\nplt.title(\"Probability Density Function\")\nplt.legend()\nplt.show()\n\n# Plot the ECDF of replicates\nxsbs_ecdf, ysbs_ecdf = get_ecdf(bs_replicates_heights)\nplt.scatter(xsbs_ecdf, ysbs_ecdf,s=5,c='b',alpha=0.5)\nplt.xlabel(\"Height(cm)\")\nplt.ylabel(\"ECDF\")\nplt.title(\"ECDF of Bootstrap Replicates\")\nplt.show()print(\"Empirical mean: \" + str(heights_mean))\n\n# Print the mean of bootstrap replicates\nprint(\"Bootstrap replicates mean: \" + str(np.mean(bs_replicates_heights)))# Get the corresponding values of 2.5th and 97.5th percentiles\nconf_interval = np.percentile(bs_replicates_heights,[2.5,97.5])\n\n# Print the interval\nprint(\"The confidence interval: \",conf_interval)"},{"path":"análisis-de-datos-con-python.html","id":"regresión-logistica","chapter":"7 Análisis de Datos con Python","heading":"7.7.5 Regresión Logistica","text":"La función sigmoide está limitada por debajo por 0 y por encima por 1.En la regresión logística, el resultado se interpreta como una probabilidad: la probabilidad de que una observación pertenezca la segunda de las dos categorías modeladas. Cuando la combinación lineal de variables produce números positivos, la probabilidad resultante es mayor que 0,5 y cuando produce números negativos, la probabilidad es menor que 0,5.profundizaremos en los detalles de cómo funciona la regresión logística, sino que nos centraremos en cómo utilizarla en Python. Lo más importante es saber que la reg<resión logística produce probabilidades que podemos utilizar para clasificar las observaciones.","code":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nplt.figure(figsize=(9,9))\n\ndef sigmoid(t):                          # Define the sigmoid function\n    return (1/(1 + np.e**(-t)))    \n\nplot_range = np.arange(-6, 6, 0.1)       \n\ny_values = sigmoid(plot_range)\n\n# Plot curve\nplt.plot(plot_range,   # X-axis range\n         y_values,          # Predicted values\n         color=\"red\");"},{"path":"análisis-de-datos-con-python.html","id":"titanic","chapter":"7 Análisis de Datos con Python","heading":"7.7.5.1 Titanic","text":"Para el resto de la lección trabajaremos con los datos de entrenamiento de supervivencia del Titanic de Kaggle que vimos en la lección 14. Empezaremos cargando los datos y realizando algunas de las mismas tareas de preprocesamiento que hicimos en la lección 14:Ahora estamos listos para utilizar un modelo de regresión logística para predecir la supervivencia. La biblioteca scikit-learn tiene una función de regresión logística en la subcarpeta learn model.Ahora estamos listos para utilizar un modelo de regresión logística para predecir la supervivencia. La biblioteca scikit-learn tiene una función de regresión logística en la subcarpeta learn model.Hagamos un modelo de regresión logística que sólo utilice la variable Sexo como predictor.Hagamos un modelo de regresión logística que sólo utilice la variable Sexo como predictor.Antes de crear un modelo con la variable sexo, necesitamos convertirla un número real porque las funciones de aprendizaje automático de sklearn sólo tratan con números reales.Antes de crear un modelo con la variable sexo, necesitamos convertirla un número real porque las funciones de aprendizaje automático de sklearn sólo tratan con números reales.Podemos convertir una variable categórica como en un número utilizando la función de preprocesamiento de sklearn LabelEncoder():Podemos convertir una variable categórica como en un número utilizando la función de preprocesamiento de sklearn LabelEncoder():Los coeficientes del modelo de regresión logística son similares los resultados de la regresión lineal. Podemos ver que el modelo produjo un valor de intercepción positivo y un peso de -2,421 en el género. Utilicemos el modelo para hacer predicciones en el conjunto de pruebas:Nota: Utilice model.predict_proba() para obtener las probabilidades de clase predichas. Utilice model.predict() para obtener las clases predichas.La tabla muestra que el modelo predijo una probabilidad de supervivencia de aproximadamente el 19% para los hombres y el 73% para las mujeres. Si utilizamos este modelo simple para predecir la supervivencia, acabaríamos prediciendo que todas las mujeres sobrevivieron y que todos los hombres murieron. Hagamos un modelo más complicado que incluya algunas variables más del conjunto de entrenamiento del titán:continuación, vamos hacer predicciones de clase utilizando este modelo y luego compararemos las predicciones con los valores reales:La tabla anterior muestra las clases que nuestro modelo predijo frente los valores reales de la variable Supervivencia. Esta tabla de valores predichos frente los reales se conoce como matriz de confusión.","code":"import os\nos.chdir(\"datos\")\n\ntitanic_train = pd.read_csv(\"train.csv\")    # Read the data\n\nchar_cabin = titanic_train[\"Cabin\"].astype(str)     # Convert cabin to str\n\nnew_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter\n\ntitanic_train[\"Cabin\"] = pd.Categorical(new_Cabin)  # Save the new cabin var\n\n# Impute median Age for NA Age values\nnew_age_var = np.where(titanic_train[\"Age\"].isnull(), # Logical check\n                       28,                       # Value if check is true\n                       titanic_train[\"Age\"])     # Value if check is false\n\ntitanic_train[\"Age\"] = new_age_var \n\nnew_fare_var = np.where(titanic_train[\"Fare\"].isnull(), # Logical check\n                       50,                         # Value if check is true\n                       titanic_train[\"Fare\"])     # Value if check is false\n\ntitanic_train[\"Fare\"] = new_fare_varfrom sklearn import linear_model\nfrom sklearn import preprocessing\n\n# Initialize label encoder\nlabel_encoder = preprocessing.LabelEncoder()\n\n# Convert Sex variable to numeric\nencoded_sex = label_encoder.fit_transform(titanic_train[\"Sex\"])\n\n# Initialize logistic regression model\nlog_model = linear_model.LogisticRegression(solver = 'lbfgs')\n\n# Train the model\nlog_model.fit(X = pd.DataFrame(encoded_sex), \n              y = titanic_train[\"Survived\"])\n\n# Check trained model intercept\nprint(log_model.intercept_)\n\n# Check trained model coefficients\nprint(log_model.coef_)# Make predictions\npreds = log_model.predict_proba(X= pd.DataFrame(encoded_sex))\npreds = pd.DataFrame(preds)\npreds.columns = [\"Death_prob\", \"Survival_prob\"]\n\n# Generate table of predictions vs Sex\npd.crosstab(titanic_train[\"Sex\"], preds.loc[:, \"Survival_prob\"])# Convert more variables to numeric\nencoded_class = label_encoder.fit_transform(titanic_train[\"Pclass\"])\nencoded_cabin = label_encoder.fit_transform(titanic_train[\"Cabin\"])\n\ntrain_features = pd.DataFrame([encoded_class,\n                              encoded_cabin,\n                              encoded_sex,\n                              titanic_train[\"Age\"]]).T\n\n# Initialize logistic regression model\nlog_model = linear_model.LogisticRegression(solver = 'lbfgs')\n\n# Train the model\nlog_model.fit(X = train_features ,\n              y = titanic_train[\"Survived\"])\n\n# Check trained model intercept\nprint(log_model.intercept_)\n\n# Check trained model coefficients\nprint(log_model.coef_)# Make predictions\npreds = log_model.predict(X= train_features)\n\n# Generate table of predictions vs actual\npd.crosstab(preds,titanic_train[\"Survived\"])"},{"path":"análisis-de-datos-con-python.html","id":"matching","chapter":"7 Análisis de Datos con Python","heading":"7.7.6 Matching","text":"compare results, let’s first look treatment effect identified true experiment.using propensity score methods estimating treatment effects, let’s calculate average treatment effect actual experiment.Using code , calculate NSW job-training program caused real earnings 1978 increase $1,794.343.continuación vamos ver varios ejemplos en los que estimamos el efecto medio del tratamiento o algunas de sus variantes, como el efecto medio del tratamiento en el grupo tratado o el efecto medio del tratamiento en el grupo tratado. Pero aquí, en lugar de utilizar el grupo de control experimental del experimento aleatorio original, utilizaremos el grupo de control experimental de la Current Population Survey. Es muy importante subrayar que, mientras que el grupo de tratamiento es un grupo experimental, el grupo de control consiste ahora en una muestra aleatoria de estadounidenses de ese periodo de tiempo. Por lo tanto, el grupo de control sufre un sesgo de selección extremo, ya que la mayoría de los estadounidenses funcionarían como contrafactuales para el grupo de trabajadores en apuros que se seleccionó en el programa NSW. continuación, añadiremos los datos de la CPS los datos experimentales y estimaremos la puntuación de propensión utilizando logit para ser coherentes con Dehejia y Wahba (2002).Recordemos lo que hace la ponderación probabilística inversa. Está ponderando las unidades de tratamiento y control según\nla puntuación p, lo que hace que las unidades con valores muy pequeños de la puntuación de propensión se disparen y se vuelvan inusualmente influyentes en el cálculo del TCA.","code":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\ndef read_data(file):\n    full_path = \"https://raw.github.com/scunning1975/mixtape/master/\" + file\n    \n    return pd.read_stata(full_path)nsw_dw = read_data('nsw_mixtape.dta')\n\nmean1 = nsw_dw[nsw_dw.treat==1].re78.mean()\nmean0 = nsw_dw[nsw_dw.treat==0].re78.mean()\nate = np.unique(mean1 - mean0)[0]\nprint(\"The experimental ATE estimate is {:.2f}\".format(ate))# Prepare data for logit \nnsw_dw_cpscontrol = read_data('cps_mixtape.dta')\n\nnsw_dw_cpscontrol = pd.concat((nsw_dw_cpscontrol, nsw_dw))\nnsw_dw_cpscontrol[['u74', 'u75']] = 0\nnsw_dw_cpscontrol.loc[nsw_dw_cpscontrol.re74==0, 'u74'] = 1\nnsw_dw_cpscontrol.loc[nsw_dw_cpscontrol.re75==0, 'u75'] = 1\n# estimating propensity score\nlogit_nsw = smf.glm(formula=\"\"\"treat ~ age + age**2 + age**3 + educ + educ**2 + \n                    marr + nodegree + black + hisp + re74 + re75 + u74 + u75 + educ*re74\"\"\", \n                    family=sm.families.Binomial(),\n                   data=nsw_dw_cpscontrol).fit()\n                  \nnsw_dw_cpscontrol['pscore'] = logit_nsw.predict(nsw_dw_cpscontrol)\n\nnsw_dw_cpscontrol\n\nnsw_dw_cpscontrol.groupby('treat')['pscore'].mean()\n\nnsw_dw_cpscontrol[\"pscore\"][nsw_dw_cpscontrol[\"treat\"] == 1]\n\n# histogram\nplt.hist(nsw_dw_cpscontrol[\"pscore\"][nsw_dw_cpscontrol[\"treat\"] == 1])\n\nplt.hist(nsw_dw_cpscontrol[\"pscore\"][nsw_dw_cpscontrol[\"treat\"] == 0])# continuation\nN = nsw_dw_cpscontrol.shape[0]\n\n# Manual with non-normalized weights using all data\nnsw_dw_cpscontrol = nsw_dw_cpscontrol \nnsw_dw_cpscontrol['d1'] = nsw_dw_cpscontrol.treat/nsw_dw_cpscontrol.pscore\nnsw_dw_cpscontrol['d0'] = (1-nsw_dw_cpscontrol.treat)/(1-nsw_dw_cpscontrol.pscore)\n\n\ns1 = nsw_dw_cpscontrol.d1.sum()\ns0 = nsw_dw_cpscontrol.d0.sum()\n\nnsw_dw_cpscontrol['y1'] = nsw_dw_cpscontrol.treat * nsw_dw_cpscontrol.re78 / nsw_dw_cpscontrol.pscore\nnsw_dw_cpscontrol['y0'] = (1 - nsw_dw_cpscontrol.treat) * nsw_dw_cpscontrol.re78 / (1 - nsw_dw_cpscontrol.pscore)\nnsw_dw_cpscontrol['ht'] = nsw_dw_cpscontrol['y1'] - nsw_dw_cpscontrol['y0']\n\nte_1 = nsw_dw_cpscontrol.ht.mean()\n\nprint(\"Treatment Effect (non-normalized, all data): {:.2f}\".format(te_1))\n\nnsw_dw_cpscontrol['y1'] = nsw_dw_cpscontrol.treat * nsw_dw_cpscontrol.re78 / nsw_dw_cpscontrol.pscore\nnsw_dw_cpscontrol['y1'] /= s1/N\nnsw_dw_cpscontrol['y0'] = (1 - nsw_dw_cpscontrol.treat) * nsw_dw_cpscontrol.re78 / (1 - nsw_dw_cpscontrol.pscore)\nnsw_dw_cpscontrol['y0'] /= s0/N\nnsw_dw_cpscontrol['ht'] = nsw_dw_cpscontrol['y1'] - nsw_dw_cpscontrol['y0']\n\nte_2 = nsw_dw_cpscontrol.ht.mean()\n\nprint(\"Treatment Effect (normalized, all data): {:.2f}\".format(te_2))\n\nnsw_dw_trimmed = nsw_dw_cpscontrol.drop(['d1', 'd0', 'y1', 'y0'], axis=1)\nnsw_dw_trimmed = nsw_dw_trimmed[nsw_dw_trimmed.pscore.between(.1, .9)]\nN = nsw_dw_trimmed.shape[0]\n\nnsw_dw_trimmed['y1'] = nsw_dw_trimmed.treat * nsw_dw_trimmed.re78 / nsw_dw_trimmed.pscore\nnsw_dw_trimmed['y0'] = (1 - nsw_dw_trimmed.treat) * nsw_dw_trimmed.re78 / (1 - nsw_dw_trimmed.pscore)\nnsw_dw_trimmed['ht'] = nsw_dw_trimmed['y1'] - nsw_dw_trimmed['y0']\n\nte_3 = nsw_dw_trimmed.ht.mean()\n\nprint(\"Treatment Effect (non-normalized, trimmed data): {:.2f}\".format(te_3))\n\nnsw_dw_trimmed['y1'] = nsw_dw_trimmed.treat * nsw_dw_trimmed.re78 / nsw_dw_trimmed.pscore\nnsw_dw_trimmed['y1'] /= s1/N\nnsw_dw_trimmed['y0'] = (1 - nsw_dw_trimmed.treat) * nsw_dw_trimmed.re78 / (1 - nsw_dw_trimmed.pscore)\nnsw_dw_trimmed['y0'] /= s0/N\nnsw_dw_trimmed['ht'] = nsw_dw_trimmed['y1'] - nsw_dw_trimmed['y0']\n\nte_4 = nsw_dw_trimmed.ht.mean()\n\nprint(\"Treatment Effect (normalized, trimmed data): {:.2f}\".format(te_4))"},{"path":"análisis-de-datos-con-python.html","id":"análisis-de-datos-parte-iii","chapter":"7 Análisis de Datos con Python","heading":"7.8 Análisis de Datos (Parte III)","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"bootstraping-en-el-contexto-de-mco","chapter":"7 Análisis de Datos con Python","heading":"7.8.1 Bootstraping en el contexto de MCO","text":"Los datos anteriores pintan una bonita imagen del bootstrapping paramétrico.Los datos anteriores pintan una bonita imagen del bootstrapping paramétrico.Sin embargo, si por casualidad, y esto es lo más probable, tuviéramos datos dispersos, podría existir la posibilidad de que nuestra selección aleatoria de puntos se encontrara totalmente en una zona y en otra - recordemos la mención de que el valor atípico puede ser muestreado varias veces pesar de ser un único punto atípico.Sin embargo, si por casualidad, y esto es lo más probable, tuviéramos datos dispersos, podría existir la posibilidad de que nuestra selección aleatoria de puntos se encontrara totalmente en una zona y en otra - recordemos la mención de que el valor atípico puede ser muestreado varias veces pesar de ser un único punto atípico.Para luchar contra esto, podemos aplicar un tipo diferente de bootstrapping, llamado “bootstrapping paramétrico”, por el que aplicamos el bootstrapping en los residuos, y en el propio parámetro.Para luchar contra esto, podemos aplicar un tipo diferente de bootstrapping, llamado “bootstrapping paramétrico”, por el que aplicamos el bootstrapping en los residuos, y en el propio parámetro.En resumen…En resumen…Bootstrap y la simulación de Montecarlo de una estadística: ambos se basan en la repetición del muestreo y en el examen directo de los resultados.Bootstrap y la simulación de Montecarlo de una estadística: ambos se basan en la repetición del muestreo y en el examen directo de los resultados.Sin embargo, una gran diferencia entre los métodos es que el bootstrap utiliza la muestra inicial original como población de la que se vuelve tomar la muestra, mientras que la simulación de Montecarlo se basa en establecer un proceso de generación de datos (con valores conocidos de los parámetros).Sin embargo, una gran diferencia entre los métodos es que el bootstrap utiliza la muestra inicial original como población de la que se vuelve tomar la muestra, mientras que la simulación de Montecarlo se basa en establecer un proceso de generación de datos (con valores conocidos de los parámetros).Mientras que Monte Carlo se utiliza para probar los estimadores de la unidad, los métodos de bootstrap pueden utilizarse para estimar la variabilidad de un estadístico y la forma de su distribución muestral.Mientras que Monte Carlo se utiliza para probar los estimadores de la unidad, los métodos de bootstrap pueden utilizarse para estimar la variabilidad de un estadístico y la forma de su distribución muestral.La función sigmoide está limitada por debajo por 0 y por encima por 1.La función sigmoide está limitada por debajo por 0 y por encima por 1.En la regresión logística, el resultado se interpreta como una probabilidad: la probabilidad de que una observación pertenezca la segunda de las dos categorías modeladas. Cuando la combinación lineal de variables produce números positivos, la probabilidad resultante es mayor que 0,5 y cuando produce números negativos, la probabilidad es menor que 0,5.En la regresión logística, el resultado se interpreta como una probabilidad: la probabilidad de que una observación pertenezca la segunda de las dos categorías modeladas. Cuando la combinación lineal de variables produce números positivos, la probabilidad resultante es mayor que 0,5 y cuando produce números negativos, la probabilidad es menor que 0,5.profundizaremos en los detalles de cómo funciona la regresión logística, sino que nos centraremos en cómo utilizarla en Python. Lo más importante es saber que la reg<resión logística produce probabilidades que podemos utilizar para clasificar las observaciones.","code":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.formula.api as sm\n\n# synthetic sample data\nn_points = 25\nx = np.linspace(0, 10, n_points)\ny = x + (np.random.rand(len(x)) * 5)\ndata_df = pd.DataFrame({\"x\": x, \"y\": y})\nols_model = sm.ols(formula = \"y ~ x\", data=data_df)\nresults = ols_model.fit()\n# coefficients\nprint(\"Intercept, x-Slope : {}\".format(results.params))\ny_pred = ols_model.fit().predict(data_df[\"x\"])\n# plot results\nplt.scatter(x, y)\nplt.plot(x, y_pred, linewidth=2)\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"x vs y\")\nplt.show()\n\n# resample with replacement each row\nboot_slopes = []\nboot_interc = []\nn_boots = 100\nplt.figure()\n\nfor _ in range(n_boots):\n # sample the rows, same size, with replacement\n sample_df = data_df.sample(n=n_points, replace=True)\n # fit a linear regression\n ols_model_temp = sm.ols(formula = \"y ~ x\", data=sample_df)\n results_temp = ols_model_temp.fit()\n \n # append coefficients\n boot_interc.append(results_temp.params[0])\n boot_slopes.append(results_temp.params[1])\n \n # plot a greyed out line\n y_pred_temp = ols_model_temp.fit().predict(sample_df[\"x\"])\n plt.plot(sample_df[\"x\"], y_pred_temp, color=\"grey\", alpha=0.2)\n# add data points\nplt.scatter(x, y)\nplt.plot(x, y_pred, linewidth=2)\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"x vs y\")\nplt.show()\n\nsns.distplot(boot_slopes)\nplt.show()\n\nsns.distplot(boot_interc)\nplt.show()import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats"},{"path":"análisis-de-datos-con-python.html","id":"logit-probabilidad-de-sobrevivir-al-hundimiento-del-titanic","chapter":"7 Análisis de Datos con Python","heading":"7.8.2 Logit: Probabilidad de sobrevivir al hundimiento del Titanic","text":"Ahora estamos listos para utilizar un modelo de regresión logística para predecir la supervivencia.Ahora estamos listos para utilizar un modelo de regresión logística para predecir la supervivencia.La biblioteca scikit-learn tiene una función de regresión logística en la subcarpeta linear model.La biblioteca scikit-learn tiene una función de regresión logística en la subcarpeta linear model.Hagamos un modelo de regresión logística que sólo utilice la variable Sexo como predictor.Hagamos un modelo de regresión logística que sólo utilice la variable Sexo como predictor.Antes de crear un modelo con la variable sexo, necesitamos convertirla un número real porque las funciones de aprendizaje automático de sklearn sólo tratan con números reales.Antes de crear un modelo con la variable sexo, necesitamos convertirla un número real porque las funciones de aprendizaje automático de sklearn sólo tratan con números reales.Podemos convertir una variable categórica como en un número utilizando la función de preprocesamiento de sklearn LabelEncoder():Podemos convertir una variable categórica como en un número utilizando la función de preprocesamiento de sklearn LabelEncoder():Los coeficientes del modelo de regresión logística son similares los resultados de la regresión lineal. Podemos ver que el modelo produjo un valor de intercepción positivo y un peso de -2,421 en el género. Utilicemos el modelo para hacer predicciones en el conjunto de pruebas:Nota: Utilice model.predict_proba() para obtener las probabilidades de clase predichas. Utilice model.predict() para obtener las clases predichas.La tabla muestra que el modelo predijo una probabilidad de supervivencia de aproximadamente el 19% para los hombres y el 73% para las mujeres. Si utilizamos este modelo simple para predecir la supervivencia, acabaríamos prediciendo que todas las mujeres sobrevivieron y que todos los hombres murieron. Hagamos un modelo más complicado que incluya algunas variables más del conjunto de entrenamiento del titán:continuación, vamos hacer predicciones de clase utilizando este modelo y luego compararemos las predicciones con los valores reales:La tabla anterior muestra las clases que nuestro modelo predijo frente los valores reales de la variable Supervivencia. Esta tabla de valores predichos frente los reales se conoce como matriz de confusión.","code":"import os\nos.chdir(\"datos\")\n\ntitanic_train = pd.read_csv(\"train.csv\")    # Read the data\n\nchar_cabin = titanic_train[\"Cabin\"].astype(str)     # Convert cabin to str\n\nnew_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter\n\ntitanic_train[\"Cabin\"] = pd.Categorical(new_Cabin)  # Save the new cabin var\n\n# Impute median Age for NA Age values\nnew_age_var = np.where(titanic_train[\"Age\"].isnull(), # Logical check\n                       28,                       # Value if check is true\n                       titanic_train[\"Age\"])     # Value if check is false\n\ntitanic_train[\"Age\"] = new_age_var \n\nnew_fare_var = np.where(titanic_train[\"Fare\"].isnull(), # Logical check\n                       50,                         # Value if check is true\n                       titanic_train[\"Fare\"])     # Value if check is false\n\ntitanic_train[\"Fare\"] = new_fare_var\n\ntitanic_trainfrom sklearn import linear_model\nfrom sklearn import preprocessing\n\n# Initialize label encoder\nlabel_encoder = preprocessing.LabelEncoder()\n\n# Convert Sex variable to numeric\nencoded_sex = label_encoder.fit_transform(titanic_train[\"Sex\"])\n\n# Initialize logistic regression model\nlog_model = linear_model.LogisticRegression(solver = 'lbfgs')\n\n# Train the model\nlog_model.fit(X = pd.DataFrame(encoded_sex), \n              y = titanic_train[\"Survived\"])\n\n# Check trained model intercept\nprint(log_model.intercept_)\n\n# Check trained model coefficients\nprint(log_model.coef_)encoded_sex\n\n# Make predictions\npreds = log_model.predict_proba(X= pd.DataFrame(encoded_sex))\npreds = pd.DataFrame(preds)\npreds.columns = [\"Death_prob\", \"Survival_prob\"]\n\n# Generate table of predictions vs Sex\npd.crosstab(titanic_train[\"Sex\"], preds.loc[:, \"Survival_prob\"])\n\npreds# Convert more variables to numeric\nencoded_class = label_encoder.fit_transform(titanic_train[\"Pclass\"])\nencoded_cabin = label_encoder.fit_transform(titanic_train[\"Cabin\"])\n\ntrain_features = pd.DataFrame([encoded_class,\n                              encoded_cabin,\n                              encoded_sex,\n                              titanic_train[\"Age\"]]).T\n\n# Initialize logistic regression model\nlog_model = linear_model.LogisticRegression(solver = 'lbfgs')\n\n# Train the model\nlog_model.fit(X = train_features ,\n              y = titanic_train[\"Survived\"])\n\n# Check trained model intercept\nprint(log_model.intercept_)\n\n# Check trained model coefficients\nprint(log_model.coef_)# Make predictions\npreds = log_model.predict(X= train_features)\n\n# Generate table of predictions vs actual\npd.crosstab(preds,titanic_train[\"Survived\"])"},{"path":"análisis-de-datos-con-python.html","id":"gmm","chapter":"7 Análisis de Datos con Python","heading":"7.8.3 GMM","text":"El método generalizado de momentos (GMM) es un método para construir estimadores, análogo al de máxima verosimilitud (ML). El GMM utiliza suposiciones sobre momentos específicos de las variables aleatorias en lugar de suposiciones sobre toda la distribución, lo que hace que el GMM sea más robusto que el ML, costa de cierta eficiencia. Los supuestos se denominan condiciones de momento.continuación se utiliza una función invocable para producir la salida iteración por iteración cuando se utiliza el optimizador lineal.Se define la matriz G, que es la derivada de los momentos del GMM con respecto los parámetros.continuación, se importan los datos y se selecciona un subconjunto de las carteras de prueba para que la estimación sea más rápida.Los valores iniciales de las cargas de los factores y las primas de riesgo se estiman mediante MCO y medias simples.Los valores de partida se calculan las estimaciones del primer paso se encuentran utilizando el optimizador lineal.La matriz de ponderación inicial es sólo la matriz de identificación.Por último, se calcula el VCV de las estimaciones de los parámetros.Premio al riesgo con sus test - t","code":"# GMM\n\nfrom numpy import hstack, ones, array, mat, tile, reshape, squeeze, eye, asmatrix\nfrom numpy.linalg import inv\nfrom pandas import read_csv, Series \nfrom scipy.linalg import kron\nfrom scipy.optimize import fmin_bfgs\nimport numpy as np\nimport statsmodels.api as smdef gmm_objective(params, pRets, fRets, Winv, out=False):\n    global lastValue, functionCount\n    T,N = pRets.shape\n    T,K = fRets.shape\n    beta = squeeze(array(params[:(N*K)]))\n    lam = squeeze(array(params[(N*K):]))\n    beta = reshape(beta,(N,K))\n    lam = reshape(lam,(K,1))\n    betalam = beta @ lam\n    expectedRet = fRets @ beta.T\n    e = pRets - expectedRet\n    instr = tile(fRets,N)\n    moments1  = kron(e,ones((1,K)))\n    moments1 = moments1 * instr\n    moments2 = pRets - betalam.T\n    moments = hstack((moments1,moments2))\n\n    avgMoment = moments.mean(axis=0)\n    \n    J = T * mat(avgMoment) * mat(Winv) * mat(avgMoment).T\n    J = J[0,0]\n    lastValue = J\n    functionCount += 1\n    if not out:\n        return J\n    else:\n        return J, momentsdef gmm_G(params, pRets, fRets):\n    T,N = pRets.shape\n    T,K = fRets.shape\n    beta = squeeze(array(params[:(N*K)]))\n    lam = squeeze(array(params[(N*K):]))\n    beta = reshape(beta,(N,K))\n    lam = reshape(lam,(K,1))\n    G = np.zeros((N*K+K,N*K+N))\n    ffp = (fRets.T @ fRets) / T\n    G[:(N*K),:(N*K)]=kron(eye(N),ffp)\n    G[:(N*K),(N*K):] = kron(eye(N),-lam)\n    G[(N*K):,(N*K):] = -beta.T\n    \n    return Gos.chdir(\"datos\")\n\ndata = read_csv('FamaFrench.csv')\n\n# Split using both named colums and ix for larger blocks\ndates = data['date'].values\nfactors = data[['VWMe','SMB','HML']].values\nriskfree = data['RF'].values\nportfolios = data.iloc[:,5:].values\n\nT,N = portfolios.shape\nportfolios = portfolios[:,np.arange(0,N,2)]\nT,N = portfolios.shape\nexcessRet = portfolios - np.reshape(riskfree,(T,1))\nK = np.size(factors,1)\n\nportfolios.shape\n\ndatabetas = []\n\nfor i in range(N):\n    res = sm.OLS(excessRet[:,i],sm.add_constant(factors)).fit()\n    betas.append(res.params[1:])\n\navgReturn = excessRet.mean(axis=0)\navgReturn.shape = N,1\nbetas = array(betas)\nres = sm.OLS(avgReturn, betas).fit()\nriskPremia = res.params\n\nriskPremiariskPremia.shape = 3\nstartingVals = np.concatenate((betas.flatten(),riskPremia))\n\nWinv = np.eye(N*(K+1))\nargs = (excessRet, factors, Winv)\niteration = 0\nfunctionCount = 0\nstep1opt = fmin_bfgs(gmm_objective, startingVals, args=args)\n\nstep1opt\n\npremia = step1opt[-3:]\npremia\n\npremia = step1opt[-3:]\npremia = Series(premia,index=['VWMe', 'SMB', 'HML'])\nprint('Annualized Risk Premia (First step)')\nprint(12 * premia)\n\nout = gmm_objective(step1opt, excessRet, factors, Winv, out=True)\nS = np.cov(out[1].T)\nWinv2 = inv(S)\nargs = (excessRet, factors, Winv2)\n\niteration = 0\nfunctionCount = 0\nstep2opt = fmin_bfgs(gmm_objective, step1opt, args=args)out = gmm_objective(step2opt, excessRet, factors, Winv2, out=True)\nG = gmm_G(step2opt, excessRet, factors)\nS = np.cov(out[1].T)\nvcv = inv(G @ inv(S) @ G.T)/Tpremia = step2opt[-3:]\npremia = Series(premia,index=['VWMe', 'SMB', 'HML'])\npremia_vcv = vcv[-3:,-3:]\nprint('Annualized Risk Premia')\nprint(12 * premia)\n\npremia_stderr = np.diag(premia_vcv)\npremia_stderr = Series(premia_stderr,index=['VWMe', 'SMB', 'HML'])\nprint('T-stats')\nprint(premia / premia_stderr)"},{"path":"análisis-de-datos-con-python.html","id":"diferencias-en-diferencias","chapter":"7 Análisis de Datos con Python","heading":"7.8.4 Diferencias en Diferencias","text":"Pregunta clave: ¿cómo saber si el aumento o disminución de algún fenomeno que observamos es una tendencia natural? En otras palabras, ¿cómo se puede saber el contrafactual \\(Y_0\\) de lo que habría sucedido si se hubiera pasado algo?Pregunta clave: ¿cómo saber si el aumento o disminución de algún fenomeno que observamos es una tendencia natural? En otras palabras, ¿cómo se puede saber el contrafactual \\(Y_0\\) de lo que habría sucedido si se hubiera pasado algo?Una técnica para responder este tipo de preguntas es la Diferencia en Diferencia. La diferencia en la diferencia se utiliza habitualmente para evaluar el efecto de las macrointervenciones, como el efecto de la inmigración en el desempleo, el efecto de los cambios en la ley de armas en los índices de delincuencia o simplemente la diferencia en el compromiso de los usuarios debido una campaña de marketing.Una técnica para responder este tipo de preguntas es la Diferencia en Diferencia. La diferencia en la diferencia se utiliza habitualmente para evaluar el efecto de las macrointervenciones, como el efecto de la inmigración en el desempleo, el efecto de los cambios en la ley de armas en los índices de delincuencia o simplemente la diferencia en el compromiso de los usuarios debido una campaña de marketing.En todos estos casos, se tiene un periodo antes y después de la intervención y se desea desentrañar el impacto de la intervención de una tendencia general. Como ejemplo motivador, veamos una pregunta similar la que tuve que responder.En todos estos casos, se tiene un periodo antes y después de la intervención y se desea desentrañar el impacto de la intervención de una tendencia general. Como ejemplo motivador, veamos una pregunta similar la que tuve que responder.Vamos ver un ejemplo del impacto de una publicidad. En este ejemplo se colocaron Vamos colocar 3 vallas en la ciudad de Porto Alegre, la capital del estado de Rio Grande Sul. Queremos ver si eso aumentaba los depósitos en nuestra cuenta de ahorros. Como nota para los que estén muy familiarizados con la geografía brasileña, Rio Grande Sul forma parte del sur del país, una de las regiones más desarrolladas.Vamos ver un ejemplo del impacto de una publicidad. En este ejemplo se colocaron Vamos colocar 3 vallas en la ciudad de Porto Alegre, la capital del estado de Rio Grande Sul. Queremos ver si eso aumentaba los depósitos en nuestra cuenta de ahorros. Como nota para los que estén muy familiarizados con la geografía brasileña, Rio Grande Sul forma parte del sur del país, una de las regiones más desarrolladas.Teniendo esto en cuenta, decidimos también mirar los datos de otra capital del sur. Colocamos la valla publicitaria en Porto Alegre durante todo el mes de junio. Los datos que tenemos son los siguientes:Teniendo esto en cuenta, decidimos también mirar los datos de otra capital del sur. Colocamos la valla publicitaria en Porto Alegre durante todo el mes de junio. Los datos que tenemos son los siguientes:Recordemos que los depósitos son nuestra variable de resultado, la que deseamos aumentar con las vallas publicitarias. POA es un indicador ficticio de la ciudad de Porto Alegre. Cuando es cero, significa que las muestras son de Florianópolis. Jul es una variable ficticia para el mes de julio, o para el período posterior la intervención. Cuando es cero, se refiere las muestras de mayo, el período anterior la intervención.","code":"# Commented out IPython magic to ensure Python compatibility.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\n\n# %matplotlib inline\n\nstyle.use(\"fivethirtyeight\")\n\ndata = pd.read_csv(\"billboard_impact.csv\")\ndata.head()"},{"path":"análisis-de-datos-con-python.html","id":"estimador-did","chapter":"7 Análisis de Datos con Python","heading":"7.8.4.1 Estimador DID","text":"Para evitar la confusión entre Tiempo y Tratamiento, partir de ahora, utilizaré D para denotar el tratamiento y T para denotar el tiempo. Sea \\(Y_D(T)\\) el resultado potencial del tratamiento D en el período T. En un mundo ideal en el que tenemos la capacidad de observar el contrafactual, estimaríamos el efecto del tratamiento de una intervención de la siguiente manera:$\n= E[Y_1(1) - Y_0(1)|D=1]\n$El efecto causal es el resultado en el periodo posterior la intervención en el caso de un tratamiento menos el resultado también en el periodo posterior la intervención, pero en el caso de tratamiento. Por supuesto, podemos medir esto porque \\(Y_0(1)\\) es contrafactual.Una forma de evitarlo es comparar el antes y el después.$\n= E[Y(1)|D=1] - E[Y(0)|D=1]\n$En nuestro ejemplo, compararíamos los depósitos medios de POA antes y después de la colocación del cartel publicitario.Este estimador nos dice que debemos esperar que los depósitos aumenten 41,04 R después de la intervención. ¿Pero podemos confiar en esto?Obsérvese que \\(E[Y(0)|D=1]=E[Y_0(0)|D=1]\\), es decir, el resultado observado para la unidad tratada antes de la intervención es igual al resultado contrafactual para la unidad tratada también antes de la intervención. Como estamos utilizando esto para estimar el resultado contrafactual después de la intervención \\(E[Y_0(1)|D=1]\\), esta estimación anterior supone que \\(E[Y_0(1)|D=1] = E[Y_0(0)|D=1]\\).Está diciendo que en el caso de que haya intervención, el resultado en el último período sería el mismo que el resultado del período inicial. Obviamente, esto sería falso si la variable de resultado sigue algún tipo de tendencia. Por ejemplo, si los depósitos están subiendo en POA, \\(E[Y_0(1)|D=1] > E[Y_0(0)|D=1]\\), es decir, el resultado del último período sería mayor que el del período inicial incluso en ausencia de la intervención. Con un argumento similar, si la tendencia de Y es descendente, \\(E[Y_0(1)|D=1] < E[Y_0(0)|D=1]\\). Esto es para mostrar que esto del antes y el después es un gran estimador.Otra idea es comparar el grupo tratado con un grupo tratado que recibió la intervención:$\n= E[Y(1)|D=1] - E[Y(1)|D=0]\n$En nuestro ejemplo, sería comparar los depósitos de POA con los de Florianópolis en el período posterior la intervención.Este estimador nos dice que la campaña es perjudicial y que los clientes disminuirán los depósitos en R$ 119,10.Observe que \\(E[Y(1)|D=0]=E[Y_0(1)|D=0]\\). Y como estamos utilizando \\(E[Y(1)|D=0]\\) para estimar el contrafactual para los tratados después de la intervención, estamos asumiendo que podemos reemplazar el contrafactual faltante así: \\(E[Y_0(1)|D=0] = E[Y_0(1)|D=1]\\). Pero nótese que esto sólo sería cierto si ambos grupos tienen un nivel de base muy similar. Por ejemplo, si Florianópolis tiene muchos más depósitos que Porto Alegre, esto sería cierto porque \\(E[Y_0(1)|D=0] > E[Y_0(1)|D=1]\\). Por otro lado, si el nivel de depósitos es menor en Florianópolis, tendríamos \\(E[Y_0(1)|D=0] < E[Y_0(1)|D=1]\\).De nuevo, esto es una gran idea. Para resolver esto, podemos utilizar tanto la comparación espacial como la temporal. Esta es la idea del enfoque de la diferencia en la diferencia. Funciona sustituyendo el contrafactual que falta de la siguiente manera:$\nE[Y_0(1)|D=1] = E[Y_1(0)|D=1] + (E[Y_0(1)|D=0] - E[Y_0(0)|D=0])\n$Lo que esto hace es tomar la unidad tratada antes de la intervención y añadirle un componente de tendencia, que se estima utilizando el control \\(E[Y_0(1)|D=0] - E[Y_0(0)|D=0]\\). En palabras, está diciendo que el tratado después de la intervención, si hubiera sido tratado, se parecería al tratado antes del tratamiento más un factor de crecimiento que es el mismo que el del control.Es importante observar que esto supo ne que las tendencias del tratamiento y del control son las mismas:$\nE[Y_0(1) - Y_0(0)|D=1] = E[Y_0(1) - Y_0(0)|D=0]\n$donde el lado izquierdo es la tendencia contrafactual. Ahora, podemos reemplazar el contrafactual estimado en la definición del efecto del tratamiento \\(E[Y_1(1)|D=1] - E[Y_0(1)|D=1]\\)$\n= E[Y(1)|D=1] - (E[Y(0)|D=1] + (E[Y(1)|D=0] - E[Y(0)|D=0])\n$Si reordenamos los términos, obtenemos el estimador clásico Diff--Diff.$\n= (E[Y(1)|D=1] - E[Y(1)|D=0]) - (E[Y(0)|D=1] - E[Y(0)|D=0])\n$Recibe ese nombre porque obtiene la diferencia entre el tratamiento y el control después y antes del tratamiento.Esto es lo que se ve en el código.Diff--Diff nos dice que deberíamos esperar que los depósitos aumenten en 6,52 R$ por cliente. Obsérvese que el supuesto que hace diff--diff es mucho más plausible que los otros 2 estimadores. Simplemente asume que el patrón de crecimiento entre las 2 ciudades es el mismo. Pero requiere que tengan el mismo nivel de base ni que la tendencia sea cero.Para visualizar lo que hace diff--diff, podemos proyectar la tendencia de crecimiento de la ciudad tratada la tratada para ver el contrafactual, es decir, el número de depósitos que deberíamos esperar si hubiera intervención.¿Ves esa pequeña diferencia entre las líneas discontinuas rojas y amarillas? Si te concentras bien, puedes ver el pequeño efecto del tratamiento en Porto Alegre.Ahora bien, lo que se puede preguntar es “¿hasta qué punto puedo confiar en este estimador? Tengo derecho que informen de los errores estándar”. Lo cual tiene sentido, ya que los estimadores sin ellos parecen tontos. Para ello, utilizaremos un ingenioso truco que utiliza la regresión. En concreto, estimaremos el siguiente modelo lineal$\nY_i = _0 + _1 POA_i + _2 Jul_i + _3 POA_i*Jul_i + e_i\n$Obsérvese que \\(\\beta_0\\) es la línea de base del control. En nuestro caso, es el nivel de depósitos en Florianópolis en el mes de mayo. Si activamos la variable ficticia de la ciudad tratada, obtenemos \\(\\beta_1\\). Así, \\(\\beta_0 + \\beta_1\\) es la línea de base de Porto Alegre en mayo, antes de la intervención, y \\(\\beta_1\\) es el aumento de la línea de base de Porto Alegre sobre Florianópolis. Si desactivamos la variable ficticia del POA y activamos la de julio, obtenemos \\(\\beta_0 + \\beta_2\\), que es el nivel de Florianópolis en julio, después del período de intervención. \\(\\beta_2\\) es entonces la tendencia del control, ya que lo añadimos la línea de base para obtener el nivel del control en el periodo posterior la intervención. Recapitulando, \\(\\beta_1\\) es el incremento que obtenemos al pasar del control al tratamiento, \\(\\beta_2\\) es el incremento que obtenemos al pasar del periodo anterior al periodo posterior la intervención. Por último, si activamos ambas variables ficticias, obtenemos \\(\\beta_3\\). \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\\) es el nivel en Porto Alegre después de la intervención. Así que \\(\\beta_3\\) es el impacto incremental cuando se pasa de mayo julio y de Florianópolis POA. En otras palabras, es el estimador de la diferencia en la diferencia.Si cree, compruébelo usted mismo. Deberías obtener exactamente el mismo número que obtuvimos arriba. Y también observe cómo obtenemos nuestros tan deseados errores estándar.","code":"poa_before = data.query(\"poa==1 & jul==0\")[\"deposits\"].mean()\n\npoa_after = data.query(\"poa==1 & jul==1\")[\"deposits\"].mean()\n\npoa_after - poa_beforefl_after = data.query(\"poa==0 & jul==1\")[\"deposits\"].mean()\npoa_after - fl_afterfl_before = data.query(\"poa==0 & jul==0\")[\"deposits\"].mean()\n\ndiff_in_diff = (poa_after-poa_before)-(fl_after-fl_before)\ndiff_in_diffplt.figure(figsize=(10,5))\nplt.plot([\"May\", \"Jul\"], [fl_before, fl_after], label=\"FL\", lw=2)\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_after], label=\"POA\", lw=2)\n\nplt.plot([\"May\", \"Jul\"], [poa_before, poa_before+(fl_after-fl_before)],\n         label=\"Counterfactual\", lw=2, color=\"C2\", ls=\"-.\")\n\nplt.legend();smf.ols('deposits ~ poa*jul', data=data).fit().summary().tables[1]"},{"path":"análisis-de-datos-con-python.html","id":"plotly","chapter":"7 Análisis de Datos con Python","heading":"7.8.5 Plotly","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"introducción-tablas-y-gráficos-interactivos","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.1 Introducción: tablas y gráficos interactivos","text":"plotly es una librería de python gráficos interactivos, en el ámbito estadístico, financiero, geográfico, científico.plotly es una librería de python gráficos interactivos, en el ámbito estadístico, financiero, geográfico, científico.Es muy útil para hacer gráficos que puedan interactuar con el usuario/aEs muy útil para hacer gráficos que puedan interactuar con el usuario/aTambién podemos hacer tablas interactivas","code":"import plotly.graph_objects as go\nfig = go.Figure(data=go.Bar(y=[2, 3, 1]))\nfig.show()import plotly.graph_objects as go\n\n# Valores a mostrar \n\nencabezado = ['A Scores', 'B Scores']\nvalores = [[100, 90, 80, 90], [95, 85, 75, 95]]\n\n# Crear figura\n# 1. llamar librería y función: go.Figure()\n# 2. Definir datos: data=[]\n# 3. Usar go.Table() para definir encabezado (header) y valores(cells)\n# 4. Guardamos el resultado en una variable -> alpicamos fig.show()\nfig = go.Figure(data=[go.Table(header=dict(values=encabezado),\n                               cells=dict(values=valores))\n                     ])\nfig.show()\n\n# Podemos hacer modificaciones \n\nfig = go.Figure(data=[go.Table(\n    #Encabezado\n    header=dict(values=encabezado,\n                line_color='red', #Color borde: cambiar rojo\n                fill_color='lightskyblue', #Color fondo\n                align='left'), #Alineación\n    #Valores\n    cells=dict(values=valores,\n               line_color='darkslategray',#Color borde\n               fill_color='lightcyan', #Color rojo\n               align='left')) #Alineación\n])\n\n#Una vez creado el fig podemos editar su tamaño\nfig.update_layout(width=300, height=300)\nfig.show()\n\n\"\"\"+ Para modificar el formato de los datos, ejemplo número de decimales, podemos usar formatencabezado = ['A Scores', 'B Scores', 'C Scores']\nvalores = [[100, 90, 80, 90], [95, 85, 75, 95], [95.12345, 85.12345, 75.12345, 95.12345]]\n\nfig = go.Figure(data=[go.Table(\n    #Encabezado\n    header=dict(values=encabezado,\n                line_color='darkslategray', #Color borde: cambiar rojo\n                fill_color='lightskyblue', #Color fondo\n                align='left'), #Alineación\n    #Valores\n    cells=dict(values=valores,\n               line_color='darkslategray',#Color borde\n               fill_color='lightcyan', #Color rojo\n               align='left', #Alineación\n               format = [None, \".1f\", \".2f\"])) #Formato: en una lista va según el número de columnas\n                ])\n\n#Una vez creado el fig podemos editar su tamaño\nfig.update_layout(width=400, height=300)\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"graficos-de-lineas","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.2 Graficos de lineas","text":"Para este ejemplo vamos usar la librería plotly.express. Dentro de esta tenemos el DataFrame gapminder datos de expectativa de vida, población, PIB por país.Usamos plotly.express cuando hacemos un ejercicio sencillo. Para comenzar ejemplos más genéricos o en algunos casos, más complejos, vamos usar go.ScatterComo primer ejemplo, vamos usar una función de los DataFrame llamada pivot_table que nos va permitir modificar nuestra serie del tipo [Fecha, PIB, Pais] [Fecha, PIB, Argentina, Chile, Uruguay]","code":"#Importamos la librería\nimport plotly.express as px\n\n#Usamos un subconjunto de los datos, seleccionamos América\ndf = px.data.gapminder().query(\"continent=='Americas'\")\ndf.head()\n\nfig = px.line(df, x=\"year\", \n              y=\"lifeExp\", \n              color='country')\nfig.show()\n\ndf_sur = df[(df.country==\"Argentina\" )| (df.country==\"Chile\") | (df.country==\"Uruguay\")]\nfig = px.line(df_sur, x=\"year\", y=\"gdpPercap\", color='country')\nfig.show()df.head()\n\nimport pandas as pd\ndf_sur2 = pd.pivot_table(df_sur, \n                         values=[\"gdpPercap\"], \n                         index=[\"year\"], \n                         columns=\"country\" )\ndf_sur2.head()\n\n#Importamos la librería\nimport plotly.graph_objects as go\n\n#Creamos figura\nfig = go.Figure()\n#Primer plot\n\n#go.Scatter(x=variable_x, y=variable_y)\nfig.add_trace(go.Scatter(x=df_sur2.index, y=df_sur2.gdpPercap.Argentina,\n                    mode='lines',\n                    name='Argentina'))\n\n#Segundo plot\nfig.add_trace(go.Scatter(x=df_sur2.index, y=df_sur2.gdpPercap.Chile,\n                    mode='lines+markers',\n                    name='Chile'))\n\n# #Tercer plot\nfig.add_trace(go.Scatter(x=df_sur2.index, y=df_sur2.gdpPercap.Uruguay,\n                    mode='markers', name='Uruguay'))\n\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"grafico-de-puntos","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.3 Grafico de puntos","text":"Podemos agregar una tercera dimensión, representada en el tamaño de la burbuja. Para esto usamos el argumento size y podemos transformlo en un gráfico de burbujas.Otro ejemplo es agregar una escala de colores para resaltar una categoría","code":"#Definimos el tipo gráfico\nfig = px.scatter(df_sur, x=\"gdpPercap\", y=\"lifeExp\", color = \"country\")\nfig.show()#Definimos el tipo gráfico\nfig = px.scatter(df_sur, x=\"gdpPercap\", y=\"lifeExp\", color = \"country\", size=\"pop\")\nfig.show()import plotly.graph_objects as go\nimport numpy as np\n\ndf_chile = df[(df.country==\"Chile\")]\n\nfig = go.Figure(data=go.Scatter(\n    x = df_chile.gdpPercap,\n    y = df_chile.lifeExp,\n    mode='markers',\n    marker=dict(\n        size=16,\n        color=df_chile.lifeExp, #set color equal to a variable\n        colorscale='Viridis', # one of plotly colorscales\n        showscale=True\n    )\n))\n\nfig.show()\n\nimport plotly.express as px\n\ndf = px.data.gapminder().query(\"year==2007 and continent=='Americas'\")\n\nfig = px.scatter(df, x=\"gdpPercap\", \n                 y=\"lifeExp\", \n                 text=\"country\", \n                 log_x=True, \n                 size_max=60)\n\nfig.update_traces(textposition='top center')\n\nfig.update_layout(\n    height=800,\n    title_text='GDP and Life Expectancy (Americas, 2007)'\n)\n\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"grafico-de-barras","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.4 Grafico de barras","text":"","code":"fig = px.bar(df_chile, x='year', y='pop')\nfig.show()\n\ndf_total = px.data.gapminder()\ndf_2007 = df_total[df_total.year==2007]\ndf_2007.head()\n\nfig = px.bar(df_2007, \n             x=\"continent\", \n             y=\"pop\", \n             title=\"Wide-Form Input\")\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"grafico-de-torta","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.5 Grafico de torta","text":"","code":"df.loc[df['pop'] < 8.e6, 'country'] = 'Other countries' # Represent only large countries\nfig = px.pie(df, values='pop', \n             names='country', \n             title='Población en América')\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"animación","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.6 Animación","text":"","code":"df.head()\n\nimport plotly.express as px\ndf = px.data.gapminder()\n\nfig = px.scatter(df, x=\"gdpPercap\", y=\"lifeExp\", \n                 animation_frame=\"year\", \n                 animation_group=\"country\",\n                 size=\"pop\", \n                 color=\"continent\", \n                 hover_name=\"country\", \n                 facet_col=\"continent\",\n                 log_x=True, size_max=45, \n                 range_x=[100,100000], \n                 range_y=[25,90]\n                )\nfig.show()\n\nfig = px.bar(df, x=\"continent\", y=\"pop\", \n             color=\"continent\",\n             animation_frame=\"year\", \n             animation_group=\"country\", \n             range_y=[0,4000000000])\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"mapas-de-calor","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.7 Mapas de calor","text":"","code":"df = px.data.gapminder().query(\"continent=='Americas'\")\ndf.head()\n\nfig = go.Figure(data=go.Heatmap(\n        z=df.lifeExp,\n        x=df.year,\n        y=df.country,\n        colorscale='Viridis'\n        ))\n\nfig.update_layout(\n    title='Expectativa de vida',\n    xaxis_nticks=13)\n\nfig.update_layout(width=750, height=750)\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"sunburst-charts","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.8 Sunburst charts","text":"","code":"import plotly.express as px\n\ndf = px.data.gapminder().query(\"year == 2007\")\n\n\nfig = px.sunburst(df, path=['continent', 'country'], values='pop', color='lifeExp',\n                  hover_data=['iso_alpha'])\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"treemap","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.9 Treemap","text":"","code":"df.head()\n\nimport plotly.express as px\nimport numpy as np\ndf = px.data.gapminder().query(\"year == 2007\")\n\nfig = px.treemap(df, path=[px.Constant('world'), 'continent', 'country'], values='pop',  \n                  hover_data=['iso_alpha'])\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"histograma-1","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.10 Histograma","text":"","code":"import plotly.express as px\ndf = px.data.tips()\nfig = px.histogram(df, x=\"total_bill\", y=\"tip\", color=\"sex\", marginal=\"rug\", hover_data=df.columns)\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"caja-y-bigote","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.11 Caja y bigote","text":"","code":"import plotly.express as px\ndf = px.data.tips()\nfig = px.box(df, x=\"day\", y=\"total_bill\", color=\"smoker\", notched=True)\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"mapas","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.12 Mapas","text":"Un mapa Choropleth es un mapa compuesto por polígonos de colores.Se utiliza para representar las variaciones espaciales de una\ncantidad.Vamos utilizar la función px.choropleth_mapboxLos principales parámetros de una mapa son:\nGeoJson- formatted: información geometrica donde cada caracteristica tiene un id o identificador que es posible mapear\nLista de valores indexados al ID.\nGeoJson- formatted: información geometrica donde cada caracteristica tiene un id o identificador que es posible mapearLista de valores indexados al ID.Vamos cargar un archivo GeoJSON que contiene información geometrica\npara los condados en Estados Unidos.","code":"from urllib.request import urlopen\nimport json\nwith urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n\ncounties[\"features\"][0]\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/fips-unemp-16.csv\",\n                   dtype={\"fips\": str})\n\ndf.head()\n\nimport plotly.express as px\n\nfig = px.choropleth_mapbox(df, geojson=counties, locations='fips', color='unemp',\n                           color_continuous_scale=\"Viridis\",\n                           range_color=(0, 12),\n                           mapbox_style=\"carto-positron\",\n                           zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n                           opacity=0.5,\n                           labels={'unemp':'unemployment rate'}\n                          )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"mapas-de-puntos","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.13 Mapas de puntos","text":"","code":"import pandas as pd\nus_cities = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\")\n\nimport plotly.express as px\n\nfig = px.scatter_mapbox(us_cities, lat=\"lat\", lon=\"lon\", hover_name=\"City\", hover_data=[\"State\", \"Population\"],\n                        color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"mapas-de-burbujas","chapter":"7 Análisis de Datos con Python","heading":"7.8.5.14 Mapas de burbujas","text":"","code":"df = px.data.gapminder().query(\"year==2007\")\nfig = px.scatter_geo(df, locations=\"iso_alpha\", color=\"continent\",\n                     hover_name=\"country\", size=\"pop\",\n                     projection=\"natural earth\")\nfig.show()\n\nimport plotly.express as px\ndf = px.data.gapminder()\nfig = px.scatter_geo(df, locations=\"iso_alpha\", color=\"continent\",\n                     hover_name=\"country\", size=\"pop\",\n                     animation_frame=\"year\",\n                     projection=\"natural earth\")\nfig.show()\n\nimport plotly.graph_objects as go\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_us_cities.csv')\ndf.head()\n\ndf['text'] = df['name'] + '<br>Population ' + (df['pop']/1e6).astype(str)+' million'\nlimits = [(0,2),(3,10),(11,20),(21,50),(50,3000)]\ncolors = [\"royalblue\",\"crimson\",\"lightseagreen\",\"orange\",\"lightgrey\"]\ncities = []\nscale = 5000\n\nfig = go.Figure()\n\nfor i in range(len(limits)):\n    lim = limits[i]\n    df_sub = df[lim[0]:lim[1]]\n    fig.add_trace(go.Scattergeo(\n        locationmode = 'USA-states',\n        lon = df_sub['lon'],\n        lat = df_sub['lat'],\n        text = df_sub['text'],\n        marker = dict(\n            size = df_sub['pop']/scale,\n            color = colors[i],\n            line_color='rgb(40,40,40)',\n            line_width=0.5,\n            sizemode = 'area'\n        ),\n        name = '{0} - {1}'.format(lim[0],lim[1])))\n\nfig.update_layout(\n        title_text = '2014 US city populations<br>(Click legend to toggle traces)',\n        showlegend = True,\n        geo = dict(\n            scope = 'usa',\n            landcolor = 'rgb(217, 217, 217)',\n        )\n    )\n\nfig.show()"},{"path":"análisis-de-datos-con-python.html","id":"automatización","chapter":"7 Análisis de Datos con Python","heading":"7.8.6 Automatización","text":"","code":"import pandas as pd\nimport openpyxl\nfrom openpyxl import load_workbook\nfrom openpyxl.styles import Font\nfrom openpyxl.chart import BarChart, Reference\nimport string"},{"path":"análisis-de-datos-con-python.html","id":"tablas-ancha","chapter":"7 Análisis de Datos con Python","heading":"7.8.6.1 Tablas ancha","text":"","code":"excel_file = pd.read_excel('datasets\\\\supermarket_sales.xlsx')\nexcel_file[['Gender', 'Product line', 'Total']]\n\n# Confeccionamos tabla\nreport_table = excel_file.pivot_table(index='Gender',\n                                      columns='Product line',\n                                      values='Total',\n                                      aggfunc='sum').round(0)\nreport_table\n\n# Exportamos tabla\n\nreport_table.to_excel('report_2021.xlsx',\n                      sheet_name='Report',\n                      startrow=4)"},{"path":"análisis-de-datos-con-python.html","id":"hacer-el-reporte-con-openpyxl","chapter":"7 Análisis de Datos con Python","heading":"7.8.6.2 Hacer el reporte con Openpyxl","text":"Crear filas y columnas de referencia:Añadir graficos:Aplicar formulas manualmente:Aplicar formulas con celdas de referencias:","code":"wb = load_workbook('report_2021.xlsx')\nsheet = wb['Report']\n\n# Celdas de referencia \nmin_column = wb.active.min_column\nmax_column = wb.active.max_column\nmin_row = wb.active.min_row\nmax_row = wb.active.max_row\n\nprint(f'Min Columns: {min_column}')\nprint(f'Max Columns: {max_column}')\nprint(f'Min Rows: {min_row}')\nprint(f'Max Rows: {max_row}')wb = load_workbook('report_2021.xlsx')\nsheet = wb['Report']\n\n# Grafico de barras \n\nbarchart = BarChart()\n\n# Localizar datos y categorias \ndata = Reference(sheet,\n                 min_col=min_column+1,\n                 max_col=max_column,\n                 min_row=min_row,\n                 max_row=max_row)\ncategories = Reference(sheet,\n                       min_col=min_column,\n                       max_col=min_column,\n                       min_row=min_row+1,\n                       max_row=max_row) \n# Agregar datos y categorias\nbarchart.add_data(data, titles_from_data=True)\nbarchart.set_categories(categories)\n#location chart\nsheet.add_chart(barchart, \"B12\")\nbarchart.title = 'Sales by Product line'\nbarchart.style = 5 #choose the chart style\nwb.save('report_2021.xlsx')wb = load_workbook('report_2021.xlsx')\nsheet = wb['Report']\n\nsheet['B7'] = '=SUM(B5:B6)'\nsheet['B7'].style = 'Currency'\n\nwb.save('report_2021.xlsx')import string\nalphabet = list(string.ascii_uppercase)\nexcel_alphabet = alphabet[0:max_column] #note: Python lists start on 0 -> A=0, B=1, C=2. #note2 the [a:b] takes b-a elements\nexcel_alphabet\n\nwb = load_workbook('report_2021.xlsx')\nsheet = wb['Report']\n\n# Suma en columnas B - G: \nfor i in excel_alphabet:\n    if i!='A':\n        sheet[f'{i}{max_row+1}'] = f'=SUM({i}{min_row+1}:{i}{max_row})'\n        sheet[f'{i}{max_row+1}'].style = 'Currency'\n\n# adding total label\nsheet[f'{excel_alphabet[0]}{max_row+1}'] = 'Total'\nwb.save('report_2021.xlsx')\n\n# Formatear la tabla\n\nwb = load_workbook('report_2021.xlsx')\nsheet = wb['Report']\n\nsheet['A1'] = 'Sales Report'\nsheet['A2'] = '2021'\nsheet['A1'].font = Font('Arial', bold=True, size=20)\nsheet['A2'].font = Font('Arial', bold=True, size=10)\n\nwb.save('report_2021.xlsx')"},{"path":"análisis-de-datos-con-python.html","id":"automatizar-el-reporte-con-python","chapter":"7 Análisis de Datos con Python","heading":"7.8.6.3 Automatizar el reporte con Python","text":"","code":"import pandas as pd\nimport openpyxl\nfrom openpyxl import load_workbook\nfrom openpyxl.styles import Font\nfrom openpyxl.chart import BarChart, Reference\nimport string\n\ndef automate_excel(file_name):\n    \"\"\"The file name should have the following structure: sales_month.xlsx\"\"\"\n    # read excel file\n    excel_file = pd.read_excel(file_name)\n    # make pivot table\n    report_table = excel_file.pivot_table(index='Gender', columns='Product line', values='Total', aggfunc='sum').round(0)\n    # splitting the month and extension from the file name\n    month_and_extension = file_name.split('_')[1]\n    # send the report table to excel file\n    report_table.to_excel(f'report_{month_and_extension}', sheet_name='Report', startrow=4)\n    # loading workbook and selecting sheet\n    wb = load_workbook(f'report_{month_and_extension}')\n    sheet = wb['Report']\n    # cell references (original spreadsheet)\n    min_column = wb.active.min_column\n    max_column = wb.active.max_column\n    min_row = wb.active.min_row\n    max_row = wb.active.max_row\n    # adding a chart\n    barchart = BarChart()\n    data = Reference(sheet, min_col=min_column+1, max_col=max_column, min_row=min_row, max_row=max_row) #including headers\n    categories = Reference(sheet, min_col=min_column, max_col=min_column, min_row=min_row+1, max_row=max_row) #not including headers\n    barchart.add_data(data, titles_from_data=True)\n    barchart.set_categories(categories)\n    sheet.add_chart(barchart, \"B12\") #location chart\n    barchart.title = 'Sales by Product line'\n    barchart.style = 2 #choose the chart style\n    # applying formulas\n    # first create alphabet list as references for cells\n    alphabet = list(string.ascii_uppercase)\n    excel_alphabet = alphabet[0:max_column] #note: Python lists start on 0 -> A=0, B=1, C=2. #note2 the [a:b] takes b-a elements\n    # sum in columns B-G\n    for i in excel_alphabet:\n        if i!='A':\n            sheet[f'{i}{max_row+1}'] = f'=SUM({i}{min_row+1}:{i}{max_row})'\n            sheet[f'{i}{max_row+1}'].style = 'Currency'\n    sheet[f'{excel_alphabet[0]}{max_row+1}'] = 'Total'\n    # getting month name\n    month_name = month_and_extension.split('.')[0]\n    # formatting the report\n    sheet['A1'] = 'Sales Report'\n    sheet['A2'] = month_name.title()\n    sheet['A1'].font = Font('Arial', bold=True, size=20)\n    sheet['A2'].font = Font('Arial', bold=True, size=10)\n    wb.save(f'report_{month_and_extension}')\n    return\n\n# Aplicar la función a un archivo\nautomate_excel('datasets\\\\sales_2021.xlsx')\n\n# Aplicar la función a múltiples archivos \n\n# cada uno por separado\nautomate_excel('datasets\\\\sales_january.xlsx')\nautomate_excel('datasets\\\\sales_february.xlsx')\nautomate_excel('datasets\\\\sales_march.xlsx')\n\n# option 2: todos juntos \nexcel_file_1 = pd.read_excel('datasets\\\\sales_january.xlsx')\nexcel_file_2 = pd.read_excel('datasets\\\\sales_february.xlsx')\nexcel_file_3 = pd.read_excel('datasets\\\\sales_march.xlsx')\n\nnew_file = pd.concat([excel_file_1,\n                      excel_file_2,\n                      excel_file_3], ignore_index=True)\nnew_file.to_excel('sales_2021.xlsx')\nautomate_excel('sales_2021.xlsx')"},{"path":"análisis-de-datos-con-python.html","id":"web-scraping-en-python","chapter":"7 Análisis de Datos con Python","heading":"7.9 Web Scraping en Python","text":"","code":""},{"path":"análisis-de-datos-con-python.html","id":"introducción-2","chapter":"7 Análisis de Datos con Python","heading":"7.9.1 Introducción","text":"Web scrapping o escrapeo de datos es un conjunto de herramientas que permiten obtener datos que esten disponibles en sitios web.Web scrapping o escrapeo de datos es un conjunto de herramientas que permiten obtener datos que esten disponibles en sitios web.Estos datos pueden estar estructurados (datos enfocados en el analisis de datos) o estructurados (datos previos un formato de analisis de datos).Estos datos pueden estar estructurados (datos enfocados en el analisis de datos) o estructurados (datos previos un formato de analisis de datos).Vamos ver algunos conceptos básicos, solo con el fin de que los conozcan, y luego vamos ver varios ejemplos.Vamos ver algunos conceptos básicos, solo con el fin de que los conozcan, y luego vamos ver varios ejemplos.","code":""},{"path":"análisis-de-datos-con-python.html","id":"html","chapter":"7 Análisis de Datos con Python","heading":"7.9.1.1 HTML","text":"Hypertext Markup Language\nEs el lenguaje utilizado para diseñar sitios web.\nContiene varias etiquetas que almacenan todo tipo de información contenido en una página web.\nCada etiqueta tiene un rol especifico. Para ver detalles: w3schools.com/tags/default.asp\nEsta organizada jerarquicamente (parecen ramas de arboles). Es importante entender este concepto\nRecuerden que nuestro objetivo es entender esta estructura para poder extraer la información.\nHypertext Markup LanguageEs el lenguaje utilizado para diseñar sitios web.Contiene varias etiquetas que almacenan todo tipo de información contenido en una página web.Cada etiqueta tiene un rol especifico. Para ver detalles: w3schools.com/tags/default.aspEsta organizada jerarquicamente (parecen ramas de arboles). Es importante entender este conceptoRecuerden que nuestro objetivo es entender esta estructura para poder extraer la información.","code":""},{"path":"análisis-de-datos-con-python.html","id":"estructura","chapter":"7 Análisis de Datos con Python","heading":"7.9.1.2 Estructura","text":"01figuras/image.pnghtml: inicio/fin archivo htmlbody: cuerpodiv: seccionesp: parrafos.01figuras/image.png01figuras/image.png","code":"# Crear un HTML\nhtml = '''\n<html>\n  <head>\n    <title>Intro HTML<\/title>\n  <\/head>\n  <body>\n    <p>Chao Chao!<\/p>\n    <p>Hola Hola!<\/p>\n  <\/body>\n<\/html>\n'''\n\nhtml"},{"path":"análisis-de-datos-con-python.html","id":"css-selectors","chapter":"7 Análisis de Datos con Python","heading":"7.9.1.3 CSS selectors","text":"Cascading Style SheetsAl igual que HTML sirve para generar sitios web.En CSS hay algunos selectores adicionales, tres principales: class, id y tag.Estos atributos son subconjuntos de información especifica contenida en cada sección","code":"# HTML/CSS con atributos\nhtml = '''\n<html>\n  <body>\n    <div class=\"class1\" id=\"div1\">\n      <p class=\"class2\">atributos<\/p>\n    <\/div>\n    <div class=\"holaholabola\">\n      <p class=\"class2\">holaholahola!<\/p>\n    <\/div>\n  <\/body>\n<\/html>\n'''"},{"path":"análisis-de-datos-con-python.html","id":"xpath","chapter":"7 Análisis de Datos con Python","heading":"7.9.1.4 XPATH","text":"Es un lenguaje que permite construir expresiones que recorren y procesan un documento.Es parecido la idea de expresiones regulares al trabajar con texto.-Tiene en cuenta la estructura jerarquica./: sirve para moverse de una generación otraetiqueta: indica donde buscarCon doble // llamo todas las generacionesimage.pngMás detalles en: https://www.geeksforgeeks.org/introduction--xpath/Ejercicio 4.9.1: Para el siguiente html escriba el xpath que le permita ir la frase “aprendo python haciendo”","code":"xpath = \"/html/body/div[2]\"html = '''\n<html>\n  <body>\n    <div>\n      <p>Me gusta el chocolate<\/p>\n      <p>Aquí no<\/p>\n    <\/div>\n    <div>\n      <p>Aprendo python haciendo<\/p>\n    <\/div>\n  <\/body>\n<\/html>\n'''\n\nxpath = '/html/body/div[2]/p[1]'"},{"path":"análisis-de-datos-con-python.html","id":"muchos-ejemplos-practicos-de-web---scraping","chapter":"7 Análisis de Datos con Python","heading":"7.9.2 Muchos ejemplos practicos de Web - scraping","text":"Todos los conceptos anteriores son muy útiles cuando se trabajan con sitios web complejos. Es probable que si profundizan en el uso de web-scraping los vayan necesitar. Sin embargo, en la practica vamos utilziar programas auxiliares que nos permiten conocer las rutasTodos los conceptos anteriores son muy útiles cuando se trabajan con sitios web complejos. Es probable que si profundizan en el uso de web-scraping los vayan necesitar. Sin embargo, en la practica vamos utilziar programas auxiliares que nos permiten conocer las rutasAhora, vamos mirar como utilizar la siguiente extensión: https://selectorgadget.com/Ahora, vamos mirar como utilizar la siguiente extensión: https://selectorgadget.com/WS va utilizar los selectores de CSS y algunas librerías para obtener información guardada jerarquicamente en algún sitio web.WS va utilizar los selectores de CSS y algunas librerías para obtener información guardada jerarquicamente en algún sitio web.Imaginemos queremos obtener información de los precios de productos en Amazon, los pasos que necesitamos son:\nPrimero, vamos hacer una conexión con la página web desde el código de Python.\nUna vez conectada, vamos acceder la información neta de la págica utilizando alguna librearía.\nVamos rastrear los selectores que contienen la información que deseamos obtener (en este caso el precio)\nUna vez reconocido, utilizaremos distintas funciones para extraer la información.\nUna ves onbtenida la información la ordenamos con funciones que ya conocemos (ej. utilizando Pandas).\nImaginemos queremos obtener información de los precios de productos en Amazon, los pasos que necesitamos son:Primero, vamos hacer una conexión con la página web desde el código de Python.Una vez conectada, vamos acceder la información neta de la págica utilizando alguna librearía.Vamos rastrear los selectores que contienen la información que deseamos obtener (en este caso el precio)Una vez reconocido, utilizaremos distintas funciones para extraer la información.Una ves onbtenida la información la ordenamos con funciones que ya conocemos (ej. utilizando Pandas).Vamos revisar tres librerías: () Beatiful Soap; (ii) Selenium.Vamos revisar tres librerías: () Beatiful Soap; (ii) Selenium.Van avanzando en nivel de dificultad.Van avanzando en nivel de dificultad.","code":""},{"path":"análisis-de-datos-con-python.html","id":"beautiful-soap","chapter":"7 Análisis de Datos con Python","heading":"7.9.2.1 Beautiful Soap","text":"Beautiful Soup es un paquete de python que se utiliza para analizar documentos HTML y XML.Es una de las bibliotecas más simples y amigables de python.Beautiful soup crea un árbol de análisis para diferentes páginas analizadas que se utilizan para extraer datos de la fuente (HTML).Sintaxis Básica:html.parser se utiliza para analizar documentos HTML.soup es un nombre de variable general que se utiliza para la definición del código fuente.Find es una función incorporada en la biblioteca que se utiliza para obtener el contenido de la fuente. Encuentra la primera ocurrencia del elemento o selector. find se utiliza principalmente para extraer encabezados, títulos, nombres de productos, etc. de la página web. Ejemplos.Find_all encuentra todas las ocurrencias del elemento o selector.Se utiliza principalmente para rastrear los datos de las tablas, reseñas de productos, detalles y productos listados en una página web.Devuelve la salida en forma de lista.Select es una de las funciones más potente.Selecciona el elemento o selector directamente en forma de lista.Utiliza selectores CSS y selectores de etiquetas.También devuelve la salida en forma de lista.","code":"from bs4 import BeautifulSoup\nimport requests           ##A Python HTTP library\n\n#url = 'WEBSITE_URL' (link)\n#res = requests.get(url)  ## HTTP Requests\n#print(res)            ## if o/p is <200> then connection established\n#soup = BeautifulSoup(res.text,'html.parser')\n#print(soup)          ## source code# Find, Find All y Select## Retorna el título\nprint(soup.find('title').text)\n\n## Retorna el texto del primer encabezado  \nprint(soup.find('h1').text)\n\n## Retorna el contenido de la primera etiqueta div con los atributos tags\nprint(soup.find('div', {'class': 'tags'}))\n\n## Retorna el texto de etiqueta \"a\", pero con atributo \"tag\". \nprint(soup.find('a',{'class':'tag'}).text)## Retorna todos los div con el atributo/clase product name \n\nprint(soup.find_all('div', {'class': 'productname'}))\nproductnames = soup.find_all('div', {'class': 'productname'})\n\nfor name in productnames:\n   print(name.text)## Retorna todas las etiquetas con la clase \"productname\" en una lista \n\n# print(soup.select(.productname))\n\n## Retorna el primer div con el id product en formato texto \n\nprint(soup.select(div #productkey)[0].get_text())"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-1-scraping-para-wikipedia---conociendo-villasana-de-mena","chapter":"7 Análisis de Datos con Python","heading":"7.9.2.2 Ejemplo 1: Scraping para Wikipedia - conociendo Villasana de Mena","text":"","code":"# Cargamos librerías \n\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Establecemos conexión\nres = requests.get('https://es.wikipedia.org/wiki/Villasana_de_Mena')\nres\n\n# Obtenemos la codificación de la página web\nsoup = BeautifulSoup(res.text,'html.parser')\n#print(soup)\n\nnombre = soup.select(\"#firstHeading\")[0].getText().strip()\nprint(nombre)\n\nprimerparrafo = soup.select('.ambox-content+ p')[0].get_text().strip()\nprimerparrafo\n\ntodoslosparrafos = [soup.select('p')[i].get_text().strip() for i in range(1,len(soup.select('p')))]\ntodoslosparrafos\n\nmapa = soup.select(\"tr:nth-child(5) img\")[0]\nmapa\n\nimage_url = mapa.attrs[\"src\"]\nimage_url\n\nurl_base = \"https:\"\nimage_url = mapa.attrs[\"src\"]\nurl_final = url_base + image_url \nurl_final\n\nimg_data = requests.get(url_final).content\n#img_data\n\nimg_data = requests.get(url_final).content\n\nwith open('m1.jpg', 'wb') as handler:\n    handler.write(img_data)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg = mpimg.imread('m1.jpg')\nimgplot = plt.imshow(img)\nplt.show()\n\n# Importemos todas las imagenes en un iterador\n\nimagenes = [soup.select(\"td :nth-child(1) .image img\")[i] for i in range(0,len(soup.select(\"td :nth-child(1) .image img\")))]\nimagenes\n\nurl_1 = \"https:\" + imagenes[0].attrs[\"src\"]\n\nurl_2= \"https:\" + imagenes[1].attrs[\"src\"]\n\nurl_3 = \"https:\" +imagenes[2].attrs[\"src\"]\n\nurls = [url_1, url_2, url_3]\nurls\n\nfor i in range(0,3): \n    img_data = requests.get(urls[i]).content\n    with open(\"imagen\" + str(i) + \".jpg\", \"wb\") as handler:\n        handler.write(img_data)"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-2-datos-de-libros","chapter":"7 Análisis de Datos con Python","heading":"7.9.2.3 Ejemplo 2: Datos de libros","text":"Ejercicio 4.9.2:  Arme un código que le permita extraer todas las portadas. Utilice la lista enlace_final que acaba de crear","code":"# importing the libraries\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\n\n# Request\nr = requests.get('http://books.toscrape.com/')\n\n# Obtenemos los datos\nsoup = BeautifulSoup(r.text, 'html.parser')\n#print(soup)\n\n# Generamos una variable que tenga toda la info con el atributo produc_pod\nresults = soup.find_all(attrs={'class':'product_pod'})\n\n# let's check how many results we got\nlen(results)\n#print(results)\n\n# Exploremos el primer libro\n\nfirst_book = results[0]\nfirst_book\n\n# Obtenemos el titulo\nfirst_book.h3.a.get('title')\n\n# su precio\nfirst_book.find('p', class_=\"price_color\").text[2:] # selecting from the 3rd charcater on to get just the price number\n\n# Ahora hagamoslos para todos los libros \n\nnames= []\nprices= []\n\n# Rango de páginmas para trabajar \n\npages = [str(i) for i in range(1,51)]\n\nfor page in pages:\n  response = requests.get('http://books.toscrape.com/catalogue/page-' + page + '.html')\n\n  page_html = BeautifulSoup(response.text, 'html.parser')\n\n  book_containers = page_html.find_all(attrs={'class':'product_pod'})\n\n  for book in book_containers:\n    names.append(book.h3.a.get('title'))\n    prices.append(book.find('p', class_=\"price_color\").text[2:])\n\n# Ahora vamos a crear una base de datos con todos los libros \n\nbooks_df = pd.DataFrame(list(zip(names, prices)), columns=['titles','prices (£)'])\n\nbooks_df\n\n# Ahora hagamoslos para todos los links\n\nlinks = []\n\n# Rango de páginmas para trabajar \n\npages = [str(i) for i in range(1,51)]\n\nfor page in pages:\n  response = requests.get('http://books.toscrape.com/catalogue/page-' + page + '.html')\n\n  page_html = BeautifulSoup(response.text, 'html.parser')\n\n  book_containers = page_html.find_all(attrs={'class':'product_pod'})\n\n  for book in book_containers:\n    links.append(book.h3.a.get(\"href\"))\n\nlinks[100]\n\nenlace_final = []\n\nfor i in range(0,len(links)):\n    enlace_final.append(\"http://books.toscrape.com/catalogue/\" + links[i])\n\n#print(enlace_final)enlace_final[0]\n\nportadas = []\nresponse = requests.get(enlace_final[0])\npage_html = BeautifulSoup(response.text, 'html.parser')    \nbook_containers = page_html.find_all(\"img\")\nbook_containers[0].get(\"src\")\nenlace_book = \"http://books.toscrape.com/\" + book_containers[0].get(\"src\")\nenlace_book\n\n#enlace_final\n\n# Ahora hagamoslos para todos las portadas \n\nportadas = []\n\n# Rango a trabajar \n\nfor i in range(0,100):\n  response = requests.get(enlace_final[i])\n\n  page_html = BeautifulSoup(response.text, 'html.parser')\n\n  book_containers = page_html.find_all(\"img\")\n\n  enlace_book = \"http://books.toscrape.com/\" + book_containers[0].get(\"src\")\n  portadas.append(enlace_book)\n\n# Exportemos \n\nfor i in range(0,10): \n    img_data = requests.get(portadas[i]).content\n    with open(\"imagen\" + str(i) + \".jpg\", \"wb\") as handler:\n        handler.write(img_data)"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-3-datos-de-supermercado","chapter":"7 Análisis de Datos con Python","heading":"7.9.2.4 Ejemplo 3: Datos de supermercado","text":"Vamos operar en dos pasos para que vean una forma alternativa de hacer un scrappingVamos operar en dos pasos para que vean una forma alternativa de hacer un scrappingVamos scrapiar la página de frutas del lider # importing libraries\nbs4 import BeautifulSoup\nimport requests\nimport pandas pdVamos scrapiar la página de frutas del lider # importing libraries\nbs4 import BeautifulSoup\nimport requests\nimport pandas pdAhora vamos operar en el segundo paso. Para cada enlace vamos crear una base de datos con la información que contiene.En este caso vamos iterar sobre los enlaces de los productos","code":"# importing the libraries\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\n\n# Request\nr = requests.get('https://www.lider.cl/supermercado/category/Frutas-y-Verduras/Frutas/_/N-1hd567c')\n\n# Obtenemos los datos\nsoup = BeautifulSoup(r.text, 'html.parser')\n#print(soup)\n\n# Generamos una variable que tenga toda la info con el atributo produc_pod\ncontenedor = soup.find_all(attrs={'class':'product-image'})\npproducto = contenedor[0]\npproducto\n\n# Obtenemos el nombre del producto \nnombre = pproducto.find('img').get(\"alt\")\nnombre\n\n# Vamos a obtener los links\nenlace = \"https://www.lider.cl\" + pproducto.find('a', class_=\"product-link\").get(\"href\")\nenlace\n\n# Ahora para todos los productos de la pagina \n\nnombre = []\nenlaces = []\n\nresponse = requests.get(\"https://www.lider.cl/supermercado/category/Frutas-y-Verduras/Frutas/_/N-1hd567c\")\n\npage_html = BeautifulSoup(response.text, 'html.parser')\n\nproduct_containers = page_html.find_all(attrs={'class':'product-image'})\n\nfor product in product_containers:\n    nombre.append(product.find('img').get(\"alt\"))\n    enlaces.append(\"https://www.lider.cl\" + product.find('a', class_=\"product-link\").get(\"href\"))\n\nnombre\n\nenlaces\n\nproductos_df = pd.DataFrame(list(zip(nombre, enlaces)), columns=['Nombre Producto','enlace'])\n\nproductos_df.head()#enlaces\n\n# Nombre producto \n\nresponse = requests.get(\"https://www.lider.cl/supermercado/product/Palta-Palta-Hass-Malla/4545\")\nsoup = BeautifulSoup(response.text, 'html.parser')\ncontenedor = soup.find_all(attrs={'class':'product-info'})\n\ncontenedor\n\n# Nombre del producto\nnombre = soup.find(\"span\", class_=\"product-descript\").get_text().strip()\nnombre\n\nprecio = soup.find(\"p\", class_=\"price\").get_text().strip()\nprecio\n\npuntos = soup.find(class_=\"miclub\").get_text().strip().replace(u'\\xa0', u' ')\npuntos\n\n#enlaces\n\n# Ahora escalemos...\n\nnombres = []\nprecios = []\npuntos =  []\n\nfor i in range(0,10):\n    response = requests.get(enlaces[i])\n    soup = BeautifulSoup(response.text, 'html.parser')\n    nombres.append(soup.find(\"span\", class_=\"product-descript\").get_text().strip())\n    precios.append(soup.find(\"p\", class_=\"price\").get_text().strip())\n    puntos.append(soup.find(class_=\"miclub\").get_text().strip().replace(u'\\xa0', u' '))\n\npuntos\n\nproductos_final_df = pd.DataFrame(list(zip(nombres, precios, puntos)), columns=['Nombre Producto','Precio', \"Puntos\"])\nproductos_final_df\n\n# Hacemos un merge \n\ndf = productos_df.merge(productos_final_df, \n          on = \"Nombre Producto\")\ndf"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-4-datos-de-transparencia","chapter":"7 Análisis de Datos con Python","heading":"7.9.2.5 Ejemplo 4: Datos de transparencia","text":"","code":"# importing the libraries\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\n\n# Request\nr = requests.get('http://transparencia.mineduc.cl/2018/dotacion/enero/per_planta.html')\n\n# Obtenemos los datos\nsoup = BeautifulSoup(r.text, 'html.parser')\n#print(soup)\n\nprint('Classes of each table:')\nfor table in soup.find_all('table'):\n    print(table.get('class'))\n# Creating list with all tables\ntables = soup.find_all('table')\n\n# Definimos dataset\ndf = pd.DataFrame(columns=['Estamento', 'Profesion', 'Region','Sueldo'])\ndf\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor row in table.tbody.find_all('tr'):    \n    \n    columns = row.find_all('td')\n    \n    if(columns != []):\n        estamento= columns[0].text.strip()\n        calif = columns[5].text.strip()\n        region = columns[7].text.strip()\n        plata = columns[10].text.strip()\n  \n        df = df.append({'Estamento': estamento,  'Profesion': calif, 'Region': region, 'Sueldo': plata}, ignore_index=True)\n\ndf"},{"path":"análisis-de-datos-con-python.html","id":"selenium","chapter":"7 Análisis de Datos con Python","heading":"7.9.3 Selenium","text":"Nos permite hacer web-scraping más sofisticados. Por ejemplo, si deseo hacer un click en algo.Veremos en detalle un ejemplo que muestra la utilidad de Selenium para automatizar procesos de extracción de datos","code":""},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-1-obteniendo-información-de-tripadvisor","chapter":"7 Análisis de Datos con Python","heading":"7.9.3.1 Ejemplo 1: Obteniendo información de TripAdvisor","text":"De esta forma podemos ir automatizando el scraping de diversas formasPara cualquier proceso de obtención de datos que requiera manipulación más complejas: cambio de páginas, ingresar contraseñas, entre otras. Selenium es una buena opción","code":"from selenium import webdriver\nimport csv\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Ejemplo: obtener los reviews desde Trip Advisor \n\n# Importamos librearía principal\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait \nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Definimos link que vamos a mirar \nURL = \"https://www.tripadvisor.com/Restaurant_Review-g190328-d8867662-Reviews-Storie_Sapori_La_Valletta-Valletta_Island_of_Malta.html\"\n\n# Abrimos el navegador que vamos a utilizar \ndriver = webdriver.Chrome()\n# Abrimos la página web que vamos a utilizar \ndriver.get(URL)\n\n# Hacemos que la página web espere 10 segundos, la idea es que pueda cargar el cookie\nwait = WebDriverWait(driver, 10)\nwait.until(EC.element_to_be_clickable((By.XPATH, \"//button[text()='I Accept']\"))).click()\n\n# Vamos a abrir un archivo csv \ncsvFile = open(\"reviews.csv\", \n               \"w\", \n               newline='', \n               encoding=\"utf-8\")\n\ncsvWriter = csv.writer(csvFile)\n\n# Vamos a incluir las columnas para las cuales queremos obtener información \ncsvWriter.writerow(['Score','Date','Title','Review'])\n\n# Ahora, tengo que ir a las recomendaciones y apretar en \"more reviews\" para \n# cargar todas las revisiones\n\ndriver.find_element_by_xpath('//*[@id=\"taplc_location_reviews_list_resp_rr_resp_0\"]/div/div[17]').click()\ntime.sleep(5) # Wait for reviews to load\n\n# Vamos a revisar paso a paso como logramos esto\n\n# Ahora vamos a buscar los elementos contenidos en las reviews\n\nreviews = driver.find_elements_by_xpath(\"//div[@class='ui_column is-9']\")\nnum_page_items = min(len(reviews), 10)\n\n# Vemos la cantidad de reviews de esta pagina\nnum_page_items\n\n# Iteramos sobre la cantidad de reviews. Para cada review vamos a ir obteniendo la información\nfor i in range(num_page_items):\n    # Obtenemos el puntaje, la fecha, el titulo y la review\n    score_class = reviews[i].find_element_by_xpath(\".//span[contains(@class, 'ui_bubble_rating bubble_')]\").get_attribute(\"class\")\n    score = score_class.split(\"_\")[3]\n    date = reviews[i].find_element_by_xpath(\".//span[@class='ratingDate']\").get_attribute(\"title\")\n    title = reviews[i].find_element_by_xpath(\".//span[@class='noQuotes']\").text\n    review = reviews[i].find_element_by_xpath(\".//p[@class='partial_entry']\").text.replace(\"\\n\", \"\")\n    # Escribimos sobre el CSV\n    csvWriter.writerow((score, date, title, review))\n\n# Cerramos CSV\ncsvFile.close()\n\n# Ahora esperamos unos segundos \n\n# Apretamos la pagina siguiente \ndriver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/div[6]/div/div[1]/div[4]/div/div[5]/div/div[18]/div/div/div/a[2]\").click()\ntime.sleep(3)\n\n# Repitamos el proceso \ncsvFile = open(\"reviews2.csv\", \n               \"w\", \n               newline='', \n               encoding=\"utf-8\")\n\ncsvWriter = csv.writer(csvFile)\n\n# Vamos a incluir las columnas para las cuales queremos obtener información \ncsvWriter.writerow(['Score','Date','Title','Review'])\ndriver.find_element_by_xpath('//*[@id=\"taplc_location_reviews_list_resp_rr_resp_0\"]/div/div[17]').click()\ntime.sleep(5) # Wait for reviews to load\nreviews = driver.find_elements_by_xpath(\"//div[@class='ui_column is-9']\")\nnum_page_items = min(len(reviews), 10)\nfor i in range(num_page_items):\n    # Obtenemos el puntaje, la fecha, el titulo y la review\n    score_class = reviews[i].find_element_by_xpath(\".//span[contains(@class, 'ui_bubble_rating bubble_')]\").get_attribute(\"class\")\n    score = score_class.split(\"_\")[3]\n    date = reviews[i].find_element_by_xpath(\".//span[@class='ratingDate']\").get_attribute(\"title\")\n    title = reviews[i].find_element_by_xpath(\".//span[@class='noQuotes']\").text\n    review = reviews[i].find_element_by_xpath(\".//p[@class='partial_entry']\").text.replace(\"\\n\", \"\")\n    # Escribimos sobre el CSV\n    csvWriter.writerow((score, date, title, review))\n\n# Cerramos CSV\ncsvFile.close()\n\n# Cerramos navegador \ndriver.close()"},{"path":"análisis-de-datos-con-python.html","id":"ejemplo-2-ingresar-una-contraseña","chapter":"7 Análisis de Datos con Python","heading":"7.9.3.2 Ejemplo 2: Ingresar una contraseña","text":"","code":"import time\nfrom selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.get(\"https://mail.rediff.com/cgi-bin/login.cgi\")\n\n# Identificamos la posición del nombre de usuario, la contrase{a y señalamos que poner\ndriver.find_element_by_name(\"login\").send_keys(\"ncamposb\")\ntime.sleep(0.2)\ndriver.find_element_by_name(\"passwd\").send_keys(\"contraeñaultrasecreta23\")\ntime.sleep(0.4)\n# Le indicamos que aprete sobre sign in \ndriver.find_element_by_class_name(\"signinbtn\").click()\n# Cerramos el navegador"},{"path":"análisis-de-datos-con-r.html","id":"análisis-de-datos-con-r","chapter":"8 Análisis de datos con R","heading":"8 Análisis de datos con R","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"introducción-a-r-y-rstudio","chapter":"8 Análisis de datos con R","heading":"8.1 Introducción a R y RStudio","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"por-qué-utilizar-r-como-lenguaje-de-programación","chapter":"8 Análisis de datos con R","heading":"8.1.1 ¿Por qué utilizar R como lenguaje de programación?","text":"Código abierto: Licencia tiene costo.Código abierto: Licencia tiene costo.Versátil: análisis estadístico y econométrico + tareas de\nprogramación útiles (ej. web - scraping).Versátil: análisis estadístico y econométrico + tareas de\nprogramación útiles (ej. web - scraping).Flexible y reproducible: garantiza reproducibilidad, es flexible\nante cambios y detección de errores.Flexible y reproducible: garantiza reproducibilidad, es flexible\nante cambios y detección de errores.Demandado: Lenguajes de programación de código abierto son solicitados cada vez más en el mercado laboral.Demandado: Lenguajes de programación de código abierto son solicitados cada vez más en el mercado laboral.","code":""},{"path":"análisis-de-datos-con-r.html","id":"términos-clave-y-recurrentes-a-lo-largo-del-curso","chapter":"8 Análisis de datos con R","heading":"8.1.2 Términos clave y recurrentes a lo largo del curso","text":"RStudio: Interfaz Gráfica para el usuario. Esta diseñada para un\nuso más sencillo de R.Objetos: Cualquier cosa que guardes en R. Por ejemplo: bases de\ndatos, variables, listas de nombres, gráficos.Funciones: Una operación producida por un código tal que acepta\ninsumos y retorna productos.Paquetes: Es un conjunto de funciones agrupadas según su objetivo.Scripts/Códigos: Documento que contiene todos los comandos\nutilizados en un proceso de análisis de datos.","code":""},{"path":"análisis-de-datos-con-r.html","id":"recursos-de-aprendizaje","chapter":"8 Análisis de datos con R","heading":"8.1.3 Recursos de Aprendizaje","text":"Hojas de resumen: material disponible con un resumen de los\nprincipales comandos/paquetes/funciones para distintos temas.\nDisponibles en: https://www.rstudio.com/resources/cheatsheets/Foros y sitios web:\nStackoverflow\nMedium\nEl Blog de RStudio\nStackoverflowMediumEl Blog de RStudioLibros:\nR Data Science\nRecursos sobre R en\nespañol.\nBookdown\nR Data ScienceRecursos sobre R en\nespañol.Bookdown","code":""},{"path":"análisis-de-datos-con-r.html","id":"interfaz-rstudio","chapter":"8 Análisis de datos con R","heading":"8.1.4 Interfaz RStudio","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"cuatro-secciones-en-rstudio","chapter":"8 Análisis de datos con R","heading":"8.1.4.1 Cuatro secciones en Rstudio","text":"Las fundamentales son: Scripts, Console,\nEnvironment/Files/Help.Scripts: Donde desarrollaremos los códigos. Es un cuarderno.Console: Donde veremos los resultados.Environment: Donde veremos los datos y la información que vamos generando.Files: Donde veremos los archivos asociados nuestros proyectos.Help: Donde desarrollaremos los códigos. Es un cuarderno.Para personalizar RStudio hay que ir : Tools -> Global Options.\nAllí pueden cambiar el color del ambiente, tipo de letra, orden. La\nidea es utilizar RStudio de la forma mas intuitiva posible.","code":""},{"path":"análisis-de-datos-con-r.html","id":"escribir-comentarios","chapter":"8 Análisis de datos con R","heading":"8.1.4.2 Escribir comentarios","text":"Es muy importante escribir comentarios lo largo de los códigos. Tambíen es importante saber escribir buenos comentarios. Una buena guía que habla un poco de eso y de muchas cosas más es: Code Data Social Sciences: Practitioner’s Guide.(#) para escribir comentarios en el script. Si quiero marcar varias\nlineas como comentario: ctrl + shift + c","code":""},{"path":"análisis-de-datos-con-r.html","id":"índices","chapter":"8 Análisis de datos con R","heading":"8.1.4.3 Índices","text":"Es importante escribir índices para documentar sus códigos. Para\nobservarlo debo escribir: ctrl + shift + oLos títulos pueden estar jerarquizados:Es importante actualizarlo constantemente para perder funcionalidad.","code":"\n# El mas importante\n## Este es un poco menos importante\n### Este es un poco menos importante que el anterior\n#### El menos importante"},{"path":"análisis-de-datos-con-r.html","id":"instalación-de-paquetes-y-ayuda","chapter":"8 Análisis de datos con R","heading":"8.1.5 Instalación de paquetes y ayuda","text":"Los paquetes son basicamente conjuntos de funciones. Los paquetes se\ninstalan una vez, pero se llaman siempre que se vayan utilizar.","code":""},{"path":"análisis-de-datos-con-r.html","id":"librerías","chapter":"8 Análisis de datos con R","heading":"8.1.5.1 Librerías","text":"Esto se puede hacer de dos maneras:Desde la pestaña packages en la esquina inferior derecha.Desde la pestaña packages en la esquina inferior derecha.Con comandosCon comandosVarios paquetes la vez:Error común:Por lo general es bueno instalarlas desde el inicio del trabajo dado que\ncomúnmente usamos las mismas librerías al realizar análisis de datos.\nCon esta función le decimos R que instale el paquete si este está\ninstalado (algo típico cuando cambiamos de computador):Cargar librerías:Si queremos ver que hay dentro de cada paquete:Importante: el paquete debe ser instalado una vez, pero cargado cada\nvez que se utilice. Muchas veces hay actualizaciones. Para revisar e\ninstalarlas.Alternativamente puedo conectar paquete con función utilizando ::. Si\nhago esto es necesario llamar la librería para utilizar esa función\nen particular. obstante, lo recomendable es cargar todas las\nlibrerías de los paquetes que voy utilizar al inicio.","code":"\ninstall.packages(\"dplyr\")\ninstall.packages(c(\"dplyr\",\"ggplot2\",\"rio\"))\ninstall.packages(\"dplyr\", \"ggplot2\")\nif(!require(dplyr)) {install.packages(\"dplyr\")}\nlibrary(dplyr)\nls(\"package:dplyr\", all = TRUE) #ls = list objects\nupdate.packages()"},{"path":"análisis-de-datos-con-r.html","id":"función-de-ayuda","chapter":"8 Análisis de datos con R","heading":"8.1.5.2 Función de ayuda","text":"Sobre una función en particular:Sobre un paquete en particular:","code":"\nhelp(mean) \n?mean \nmean #pulsar la tecla F1 \nsd\nhelp(\"dplyr\") \nlibrary(help=\"dplyr\")"},{"path":"análisis-de-datos-con-r.html","id":"shortcuts-útiles","chapter":"8 Análisis de datos con R","heading":"8.1.5.3 Shortcuts útiles","text":"Esc: interrumpir el comando actualCtrl + s: guardartab: autocompletarCtrl + Enter: ejecutar líneaCtrl + Shift + C: comentar<-: Alt + - / option + -%>%: ctrl + shift + m (pipe)Ctrl + l: limpiarCtrl + alt + b: ejecutar todo hasta aquí (flechas en la consola \npermiten ver los últimos comandos utilizados).Shift + lineas: seleccionar varias lineasCtrl + f: buscar/remplazarCtrl + \"flecha arriba\" en la consola: ver comandos utilizados.","code":""},{"path":"análisis-de-datos-con-r.html","id":"limpiar-environment-en-r","chapter":"8 Análisis de datos con R","heading":"8.1.5.4 Limpiar “environment” en R","text":"Eliminar todos los objetos:Eliminar sólo un objeto:Si quiero limpiar la consola tengo que apretar Ctrl + L.","code":"\nrm(list=ls())\nrm(data1)"},{"path":"análisis-de-datos-con-r.html","id":"identificar-el-paquete-de-una-función","chapter":"8 Análisis de datos con R","heading":"8.1.5.5 Identificar el paquete de una función","text":"Hay ocasiones en que queremos saber de que paquete es una función\ndeterminada. Para ello, revisar:\nhttps://sebastiansauer.github.io/finds_funs/","code":""},{"path":"análisis-de-datos-con-r.html","id":"error-común","chapter":"8 Análisis de datos con R","heading":"8.1.5.6 Error común","text":"Por ejemplo:Noten que en la consola aparece un signo +. En estos casos RStudio se\ndetiene porque probablemente se les olvido un ) o bien un #. En\nestos casos hay que corregir el error para ejecutar nuevamente y luego\napretar esc en la consola para seguir ejecutando los comandos.","code":"\nx <- \"hola\""},{"path":"análisis-de-datos-con-r.html","id":"manipulación-de-objetos","chapter":"8 Análisis de datos con R","heading":"8.1.6 Manipulación de objetos","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"usar-r-como-calculadoraejecutar-comandos","chapter":"8 Análisis de datos con R","heading":"8.1.6.1 Usar R como calculadora/ejecutar comandos","text":"De manera separada (seleccionar las ordenes+ctrl+Enter):Ejecutar todas las instrucciones:Incluso grandes operaciones:Incluso uso de números imaginarios:","code":"\n2+2 \n3*5^(1/2)\n2+2 ; 3*5^(1/2)\n3+4 \n5*4 \n8/4 \n6^7\n6^77\n\nlog(10) \nlog(1)\n\nsqrt(91) # raiz cuadrada\n\nround(7.3) # redondear\nsqrt(91) + 4892788*673 - (log(4)*round(67893.9181, digits = 2))\n2i+5i+sqrt(25i)"},{"path":"análisis-de-datos-con-r.html","id":"creacion-de-objetos-asignaciones-y-funciones","chapter":"8 Análisis de datos con R","heading":"8.1.6.2 Creacion de objetos: asignaciones y funciones","text":"con el signo <- asignamos valores. También se puede utilizar =, pero\nes recomendable, ya que confunde.Las asignaciones son MUDAS. Si las llamo, aparecen en la consola.\nLo anterior es una operación sencilla, pero lo que queremos es generar\nasignaciones con funciones.Podemos utilizar funciones. Las funciones son la parte central del uso\nde R. Algunas funciones vienen instaladas en R. Otras funciones hay que\nobtenerlas desde paquetes. También es posible escribir tus propias\nfunciones (). Las funciones estan por lo general escritas en\nparéntesis, por ejemplo filter(). Hay ocasiones en que las funciones\nestan relacionadas con un paquete específico dplyr::filter().Ejemplo 1: función simpleEjemplo 2: sobre una base de datosNota: mtcars viene incluidad en R.Para ver mas:Otra función:Uso de funciones aritméticas:Relaciones entre objetos:Creación de objetos por asignación:Comparar objetos:Notar que las asignaciones son silenciosas:Creación de objetos usando funciones:Podemos escribir un promedio aquí usando función mean():Limpiamos datos nuevamente:","code":"\ny <- 2 + 4 \ny\nsqrt(49)\nsummary(mtcars$mpg)\ndata()\nx <- 2 \ny <- 3\nz <-c(x,y) \nz\nmean(z) \nmedian(z)\nw <- mean(z)\na <- 3+10 \nb <- 2*4\na > b\na \nb \n# o altenativamente utilizar print \nprint(a) \nprint(b)\nvalores <- c(a,b)\npromedio <- mean(valores)\nprint(promedio)\npromedio\na <- 2 \nb <- 5\nvalores1 <- c(a,b)\npromedio1 <-mean(valores1)\nprint(promedio1)\nrm(list = ls()) \nrm(promedio)"},{"path":"análisis-de-datos-con-r.html","id":"creacion-de-objetos-y-asignaciones","chapter":"8 Análisis de datos con R","heading":"8.1.6.3 Creacion de objetos y asignaciones","text":"Es importante espaciar codigos. Definimos dos vectores utilizando la\nfunción c()Calculamos promedio, desviación estándar y correlaciónGraficamosEstimar una regresión linealEjercicio 2.1.1: Nombrar objetosDe los siguientes ejemplos, ¿Cuáles son nombres de variables válidas en\nR?","code":"\neduc <- c(8,12,8,11,16,14,8,10,14,12)\ningreso <- c(325,415,360,380,670,545,350,420,680,465)\nmean(ingreso)\npromedioingreso <- mean(ingreso)\n\nsd(ingreso)\nsdingreso <- sd(ingreso)\n\ncor(educ, ingreso)\ncoreduing <- cor(educ,ingreso)\nplot(educ, ingreso)\nlm(ingreso ~ educ)\n# min_height\n# max.height\n# _age\n# .mass\n# MaxLength\n# Min-length\n# 2widths\n# Calsius2kelvin"},{"path":"análisis-de-datos-con-r.html","id":"tipos-de-objetos","chapter":"8 Análisis de datos con R","heading":"8.1.7 Tipos de objetos","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"vectores","chapter":"8 Análisis de datos con R","heading":"8.1.7.1 Vectores","text":"R opera componente por componente, por lo que es muy sencillo poder\ntrabajar con vectores y matrices.Para crear un vector utilizamos la funcion c()Veamos los vectoresSupongamos los siguientes vectores:¿Cuál es su longitud?Si tienen la misma longitud, ¿cual sería el resultado de x + y?IMPORTANTE: En este caso R realiza la operación de todos modos, pero\nnos indica que hay una advertencia de que sus dimensiones difieren. Lo\nrelevante de los vectores es que sólo se puede concatenar elementos del\nmismo tipo, de lo contrario R nos arroja error.Subconjunto de un vector","code":"\nx <- c(1,2,3,4,5)\n#o bien\ny <- c(6:8)\nz <- x + y \nz\nx<-c(1:4)\ny<-c(1:3)\nlength(x)\nlength(y)\nz <- x + y \nz\nx <- rep(1.5:9.5,4) #genera repeticiones de los valores definidos \ny <- c(20:30)\nx1 <- c(1,2)\nx2 <- c(3,4)\nx3 <- c(x1,x2)\nx4 <- c(c(1,2), c(3,4))\ny[3] # obtener el tercer elemento \ny[2:4] \ny[4:2]\ny[c(2,6)]\ny[c(2,16)]"},{"path":"análisis-de-datos-con-r.html","id":"matrices-1","chapter":"8 Análisis de datos con R","heading":"8.1.7.2 Matrices","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"definir-matrices","chapter":"8 Análisis de datos con R","heading":"8.1.7.2.1 Definir matrices","text":"Sintaxis generalPara crear matrices utilizamos la función matrix()es necesario poner data=, pero por orden mental es mejor hacerlo.Notar que por DEFECTO rellena columna por columna. Podemos explicitar\nque queremos realizar la matriz fila por filaPodemos saber cual es la dimensión de xLos va repetir!Notar que el orden en cualquier matrix es filas x columnas. Podemos\ntambién omitir la cantidad de filas o columnas en la matriz y obtenemos\nel mismo resultadoEn el caso de crear matrices vacías hay que definir las dimensionesDarle nombre las filas y columnasSe puede realizar desde las funciones colnames y rownamesAñadir filas o columnas una matriz","code":"\nmi.matriz <- matrix(vector,\n                    ncol = num_columnas, nrow = num_filas,\n                    byrow = valor_logico,\n                    dimnames = list(vector_nombres_filas,\n                                   vector_nombres_columnas)\n)\nx <- matrix (data = c(1,2,3,4), \n             nrow = 2, \n             ncol = 2) \n\n\nx1 <- matrix(c(1,2,3,4), \n             2, \n             2)\nx \nx1\ny <- matrix(data = c(1:4),\n          nrow = 3, \n          ncol = 2, \n          byrow = TRUE)\ny\ndim(y)\ndim(y)[1] # cantidad de filas\ndim(y)[2] # cantidad de columnas\n\ny<- matrix(c(1,2,3,4), nrow=2, ncol=2, byrow=2)\ny\ny <- matrix(c(1,2,3,4), nrow=2, ncol=3, byrow= 2) \ny\ny <- matrix(c(1:4), 2, byrow=T)\ny \ny <- matrix(nrow=3, ncol=3)\ny #útil para los loops\ny <- matrix (c(1:4), 2, byrow = FALSE, \n             dimnames=list(c(\"X1\",\"X2\"),c(\"Y1\", \"Y2\")))\ny\ncolnames(x) <- c(\"Variable 1\", \"Variable 2\")\nrownames(x) <- c(\"a1\", \"a2\")\nx\nw <- c(5,6)"},{"path":"análisis-de-datos-con-r.html","id":"unir-matrices","chapter":"8 Análisis de datos con R","heading":"8.1.7.2.2 Unir matrices","text":"Unir mediante filas (queda con el nombre del vector la observación)Unir mediante columnas¿Y si tienen diferente cantidad de filas y/o columnas? repite el vector\no observaci?n con menor longitudPodemos pasar un vector una matrizTrasponer matrices:Potencialmente se pueden hacer muchas más operaciones que involucren\nmatrices. Por ejemplo, subconjuntos de una matriz: segundo y cuarto\nelemento de la segunda fila:","code":"\nz <- rbind(x,w) \nz\nz <- cbind(x,w) \nz\nx <- matrix(c(1:9),3)\nx\ny <- c(5,6)\ny\n\nz<-rbind(x,y)\nz \nx<-1:10\nx\n\ndim(x)<-c(2,5)\nx\nx <- matrix(c(1:9),3)\nxtraspuesta <- t(x)\nM <- matrix(1:8, nrow=2)\nM\nM[1,1]\nM[1,]\nM[,2]\nM[2,c(2,4)]"},{"path":"análisis-de-datos-con-r.html","id":"herramientas-de-programación-1","chapter":"8 Análisis de datos con R","heading":"8.2 Herramientas de programación","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"objetos-adicionales","chapter":"8 Análisis de datos con R","heading":"8.2.1 Objetos adicionales","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"arreglos","chapter":"8 Análisis de datos con R","heading":"8.2.1.1 Arreglos","text":"Crear arreglos: La única diferencia con matrices es que acepta mas de\ndos dimensiones.Para generarlos:Para nombrarlos definimos etiquetas y luego las agregamos. Es mucho\nmejor y mas ordenado hacerlo así:Ejemplo de arreglo: Defino un arreglo de 3 matrices de 2 (filas) x 4\n(columnas)Notar la tercera dimensión!Subconjuntos de un arreglo: x[Fila,Columna, Matriz]","code":"\nmi.arreglo <-array(vector, \n                   dimensiones, \n                   dimnames = etiquetas_dim)\ndim1 <- c(\"A1\", \"A2\")\ndim2 <- c(\"B1\", \"B2\", \"B3\", \"B4\")\ndim3 <- c(\"C1\", \"C2\", \"C3\")\nx <- array(1:24,\n           c(2,4,3),\n           dimnames = list(dim1, dim2, dim3))\nx\nx[1,2,3]       # 1 fila, 2da columna, 3ra matriz\nx[,,3]         # 3era matriz completa\nx[,4,]         # 4ta columna de todas las matrices\nx[,-1,2]       # Todas las filas, sin la primera columna, de la matriz 2. \nx[-1,c(1,2),3] # Todas las columnas, sin la fila 1, de la matriz 3. \nx[-1,1:2,3]    # Columnas 1 y 2, sin la fila 1, de la matriz 3. \nx[,,1]*2"},{"path":"análisis-de-datos-con-r.html","id":"listas-1","chapter":"8 Análisis de datos con R","heading":"8.2.1.2 Listas","text":"Las listas contienen elementos de diferente tipo: matrices, objetos,\ndataframes, vectores, etc.Para acceder un objeto dentro de la lista se debe utilizar DOBLE\nCORCHETE:Veamos un ejemplo de cómo trabaja:Al nombrar los componentes dentro de la lista, podemos llamarlos sin\nnecesidad de los corchetes:Como los componentes dentro de las listas tienen definida una\nnaturaleza, podemos hacer operaciones con ellas también:Finalmente, tambien podemos crear listas vacias (útil para rellenar\nutilizando iteradores):Ejemplo de una lista: Definimos una listaLlamar subconjuntosNotar que es importante saber el tipo de dato.","code":"\nx <- list(c(1:8), \"R\", TRUE, 2+3i, 5L)\nx #separa todo componente\nx[[5]]\nx[5]\nx <- list(Titulacion = c(\"Economia\", \"Administracion\", \"Politica\"), \n           Edad =c(25,23,27))\nx\nx$Titulacion\nx[[1]]\nx[[\"Titulacion\"]]\n\nx$Edad\nx[[2]]\nx[[\"Edad\"]]*3\n\nx$Edad[c(1,3)]*3\nx <-vector(\"list\", length = 10)\nmilista <- list(\n  \n  # Primer elemento sera un vector\n  hospitales = c(\"Van Buren\", \"Gustavo Fricke\", \"Salvador\"),\n  \n  # Segundo elemento sera un dataframe\n  direccion   = data.frame(\n    calle = c(\"San Ignacio\", \"Av. Alvarez\", \"Av. Salvador\"),\n    ciudad   = c(\"Valparaíso\", \"Viña del Mar\", \"Santiago\")\n  )\n)\n\nmilista\n# Retorna el elemento de la lista\nmilista$hospitales\nmilista[2]\n\n# Retorna el elemento, pero sin nombre y como vector\nmilista[[1]]\nmilista[[\"hospitales\"]]\n\n# Elementos particulares\nmilista[[1]][3]\nmilista[[2]][,1]\nmilista[[2]][1,]\nmilista[[2]][1,2]"},{"path":"análisis-de-datos-con-r.html","id":"dataframes","chapter":"8 Análisis de datos con R","heading":"8.2.1.3 Dataframes","text":"Crear un dataframe: Es básicamente una matriz donde filas y columnas\ntienen significado. Contiene tanto valores numericos, carácteres,\nvariables categóricas, etc.Esto es lo mas cercano una base de datos. Es uno de los objetos\nbásicos que veremos muchas veces lo largo del curso.Dimensiones de un dataframe: Para ver las dimensiones hay que\nutilizar nrow(), ncol() o bien dim().Notar que nrow() y ncol() tambien sirven para matrices.Subconjuntos de un dataframe: Al igual que en las matrices,\nutilizamos [ ] para acceder elementos dentro de la base.Tibbles: Una version mas moderna de dataframes. Es la misma idea:\nlista de vectores con nombres.Paquete para trabajar con bases de datos (más detalles en secciones 2.3\ny 2.4)Agregar columnas un dataframe: Podemos agregar variables la\nbase. Dos opciones:Attach:Notación muy inconvenientePara evitar escribir datos cada vez que quiera llamar una variable voy\nutilizar la funcion attach():Esta es una forma mucho mas conveniente de trabajar con dataframes o\ntibble en R.Ejercicio 2.2.1: Crear un dataframeRespuesta:","code":"\ndatos <- data.frame(Titulacion = c(\"Economia\", \"Administracion\", \n                                   \"Administracion\"),\n                    Edad =c(25,23,27), \n                    ocupacion = c(1,0,1))\ndatos\nnrow(datos)\nncol(datos)\ndim(datos)\ndatos[1,1]\ndatos[,1]\ndatos[,-2]\ndatos[, c(1:2)]\ndatos[, c(\"Titulacion\")]\n\n# Tambien puedo ocupar $ para llamar. \ndatos$Titulacion\n\n# Ojo que esto lo puedo hacer con todo\nsummary(datos[,2])\nsummary(datos[,2])[3]\nsummary(datos[,2])[\"Max.\"]\nAltura <- c(168, 177, 177, 177, 178, 172, 165, 171, 178, 170)\nPeso  <- c(88, 72, 85, 95, 71, 69, 61, 61, 51, 75)\nM <- cbind(Altura, Peso)\ninstall.packages(\"tidyverse\") \nlibrary(tibble) # Una libreria dentro de tidyverse. \nmisdatos <- as_tibble(M)\nmisdatos\n\n# Podemos ver los nombres o bien estadistica básica de cada variable. \nnames(misdatos)\nsummary(misdatos)\n# Opción 1: utilizando $nombre_columna<-vector\ndatos$id <- c(1:3)\ndatos\n\n# Opción 2: utilizando el cbind (al igual que con matrices)\nid2<-c(1:3)\ndatos1 <- cbind(id2,datos) \ndatos1\nmisdatos$Altura\nmisdatos$Peso\nattach(datos) # para comenzar\n\n# Ahora puedo llamar a sus variables sin utilizar datos\nnueva <- Edad * ocupacion \nnueva <- datos$Edad * datos$ocupacion\nnueva \n\ndetach(datos) # para terminar\nejer <- data.frame(Tipo_animal = c(\"Perro\", \"Perro\",\"Gato\",\"Perro\", \n                                   \"Gato\",\"Gato\",\"Gato\"),\n                   Color =c(\"Café\",\"Blanco\",\"Negro\",\"Manchas\",\"Café\",\"Tricolor\",\"Negro\"), \n                   Peso = c(7,5,3,4,2,5,4),\n                   Pasea=c(\"Sí\",\"Sí\",\"Sí\",\"No\",\"No\",\"No\",\"Sí\"))"},{"path":"análisis-de-datos-con-r.html","id":"acceder-a-elementos-de-un-objeto","chapter":"8 Análisis de datos con R","heading":"8.2.2 Acceder a elementos de un objeto","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"vectores-1","chapter":"8 Análisis de datos con R","heading":"8.2.2.1 Vectores","text":"Para acceder los elementos de un objeto debemos utilizar los corchetes\n[ ]De esta manera podemos extraerlo o utilizarlo en diferentes operaciones.\nPero sólo debe ser entre observaciones numericas:","code":"\n# Veamos en un vector\n\nx<-c(\"T\", \"FALSE\", 1:9,1+2i,\"t\", \"c\",\"a\",6)\nclass(x)\n\n# Podemos llamar a un objeto de un vector \nx[1]\nx[3]\nx[1]+x[6] #de lo contrario R nos arroja un error \ny<-c(1:6, 1+2i)\ny[3]+y[7]\n\n#O podemos visualizar a x, pero sin el primer objeto\nx[-1]\n\n#O eliminar el primer objeto \nx<-x[-1]\nx1 <- x[-1]"},{"path":"análisis-de-datos-con-r.html","id":"matrices-2","chapter":"8 Análisis de datos con R","heading":"8.2.2.2 Matrices","text":"Si analizamos los objetos de una matriz:","code":"\nw<-matrix(1:9,3)\nw\n\n# Elemento [1,1]\nw[1,1]\n# Toda la primera columna \nw[,1]\n# Toda la segunda fila\nw[2,]\n# Dos columnas\nw[,1:2] # w[,-3]\n# Todas las filas menos la primera\nw[-1,]"},{"path":"análisis-de-datos-con-r.html","id":"listas-2","chapter":"8 Análisis de datos con R","heading":"8.2.2.3 Listas","text":"Si queremos llamar un objeto dentro de una lista:Para acceder un objeto dentro de la lista se debe utilizar doble\ncorchete:","code":"\nz <- list(c(1:8), \"R\", T, 2+3i, 5L)\nz #separa todo componente\nz[[1]]\n\n#Y algo dentro de ese objeto \nz[[1]][5]\n\n#Tambien podemos analizar su clasificacion \nclass(z[[1]]) #numerico \n\n#Si es numerico, entonces tambien podemos hacer operaciones \nz[[1]][3]*z[[1]][5]"},{"path":"análisis-de-datos-con-r.html","id":"tipos-de-objetos-y-datos","chapter":"8 Análisis de datos con R","heading":"8.2.3 Tipos de objetos y datos","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"identificar-tipos-de-objetos","chapter":"8 Análisis de datos con R","heading":"8.2.3.1 Identificar tipos de objetos","text":"Veamos en un vector cualquieraComo bien sabemos todo en R es un objeto. Para saber que tipo de objeto\nes puedo utilizar la función class():Note que class() identifico que es un “data.frame” y una lista.\nTambién puedo ver los tipos de elementos dentro de un determinado\nobjeto. Para datos, existen 5 tipos principales, llamados:carácteres: texto. Se escriben con comillas (ej: “3”, “swc”).numéricos: numeros reales (ej. 2, 15.5).enteros: numeros enteros (“L” le dice R que guarde esto como\nun entero).logical: valores logicos (ej. TRUE, FALSE).complejos: 1 + 4i (Numero complejos).","code":"\nx<-c(\"T\", FALSE, 1:9,1+2i,\"t\", \"c\",\"a\",6)\nclass(x)\nclass(datos)\nclass (x)"},{"path":"análisis-de-datos-con-r.html","id":"identificar-tipos-de-datos","chapter":"8 Análisis de datos con R","heading":"8.2.3.2 Identificar tipos de datos","text":"Hay más clases: Date(fechas), Factor(variables categóricas), data.frame,\ntibble, list.","code":"\n# 1. Carácteres\nz<-c(\"a\",\"b\")\nclass(z)\n\n# 2. Números enteros\nw<-c(1L,2L,3L) #la L es para obligar que sea entero\nclass(w)\n\n# 3. Numéricos \nw1 <- c(1,2,3)\nclass(w1)\n\n# Notar que la L lo obliga a ser entero ¿Qué ocurre si no coloco la L? \n\n# 4. Valores logicos\nv1 <- c(TRUE, FALSE)\nclass(v1)\n\n# Notar que si estan entre comillas son caracteres, no logical!. \nv<-c(\"TRUE\", \"FALSE\")\nclass(v)\n\n# 5. Números complejos \nt<-c(1+2i,1+3i)\nclass(t)"},{"path":"análisis-de-datos-con-r.html","id":"importan-los-tipos-de-los-objetos","chapter":"8 Análisis de datos con R","heading":"8.2.3.3 ¿Importan los tipos de los objetos?","text":"R intenta mantener dentro de un vector el tipo de objeto. Si es que\ntratamos de juntar distintos tipos de datos en un vector. Igualar la\nclase. Por ejemplo, si intentamos generar un vector con texto y\nnúmero/logical todo será texto:Si intentamos juntar un “Logical” con un número todo será número:Aquí carácter todo se va caracter:Si utilizo el operador : para crear vectores lo interpreta como un\ninteger.","code":"\nh <- c(TRUE,\"a\", TRUE, 2)\nh\nclass(h)\nh1 <- c(TRUE,2,3)\nh1\nclass(h1)\nx <- c(\"T\", \"FALSE\", 1:3, 1+2i, \"t\", \"c\", \"a\")\nclass(x)\n\ny <- c(1:4)\nclass(y)\n\ny <- c(1,2,3,4)\nclass(y)"},{"path":"análisis-de-datos-con-r.html","id":"forzar-a-r-para-que-utilice-un-tipo-de-dato","chapter":"8 Análisis de datos con R","heading":"8.2.3.4 Forzar a R para que utilice un tipo de dato","text":"Forzar R llevar el vector solo numérico:.numeric() fuerza al vector solo tener números. Noten que remplaza\nlos que son numérico por NA.Forzar R llevar el vector solo carácteres:Noten que ahora se agregaron las dobles comillas.Forzar R llevar el vector sólo carácter.Noten que “T” fue aceptado como TRUE!","code":"\nz <- as.numeric(x)\nclass(z)\nz \nz1 <- as.character(y)\nclass(z1)\nz1\nx\nz2 <- as.logical(x)\nclass(z2)\nz2 "},{"path":"análisis-de-datos-con-r.html","id":"cómo-saber-que-tipo-de-datos-tengo","chapter":"8 Análisis de datos con R","heading":"8.2.3.5 ¿Cómo saber que tipo de datos tengo?","text":"","code":"\n# Puedo preguntar a R sobre el tipo. \n\nis.numeric(y)   # Es numerico?\nis.character(y) # Es caracter?\nis.logical(y)   # Es logico?\n\n# La respuesta a esta pregunta va a ser un valor lógico\nclass(is.numeric(y))\n\n# También se puede utilizar la opcion `typeof`\ntypeof(y)"},{"path":"análisis-de-datos-con-r.html","id":"mayúsculas-importan","chapter":"8 Análisis de datos con R","heading":"8.2.3.6 Mayúsculas importan","text":"Como vimos en lasección anterior, R es sensible las mayúsculas. Esto\naplica para nombres de funciones, paquetes, comando y también para los\ndatos. Por ejemplo:Para R las minúsculas y mayúsculas SI IMPORTAN!.","code":"\nx <- c(\"A\", \"a\",\"a\",\"a\", \"B\", \"B\",\"b\", \"A\")\nlength(x)\n\nsum(x == 'a')\nsum(x == 'b')"},{"path":"análisis-de-datos-con-r.html","id":"carácteres-especiales","chapter":"8 Análisis de datos con R","heading":"8.2.3.7 Carácteres especiales","text":"Vimos que existen nombres de variables validos en R. Tambien existe\nun conjunto de caracteres invalidos.NA: Available (missing values)NaN: Numbers (ej. 0/0)Inf: Infinito (1/0)-Inf: Menos infinito","code":"\n0/0\n-1/0 "},{"path":"análisis-de-datos-con-r.html","id":"missing-values-3","chapter":"8 Análisis de datos con R","heading":"8.2.3.8 Missing values","text":"En R, los valores perdidos (“missing values”) se representan con el\nvalor especial NA (letras mayusculas N y - sin comillas). Para saber\nsi tengo valores NA en un objeto, puedo ocupar la funcion .na():funciona en el primer caso, si en el segundo. Veremos mas detalles\nsobre como tratar missing values (ej. recodificar) en las clases de\nmanipulacion y analisis de datos.","code":"\nejemplo <- c(1,3,NA,4)\nis.na(ejemplo)\n\nejemplo[!is.na(ejemplo)]\n\n# El resultado es un vector. Noten que es importante para utilizar algunas funciones:\n\nmean(ejemplo)\nmean(ejemplo, na.rm = TRUE)"},{"path":"análisis-de-datos-con-r.html","id":"funciones-útiles","chapter":"8 Análisis de datos con R","heading":"8.2.3.9 Funciones útiles","text":"","code":"\n# Redondear\nround(x,digits = n)\nround(c(2.53, 3.52), 1)\n\n# Estadística simple\nx <- c(1,2,3,4,5,6)\n\nmean(x)    # promedio\nmedian(x)  # mediana\nsd(x)      # desviacion estandar\nsum(x)     # suma del vector\nmin(x)     # Valor minimo\nmax(x)     # Valor maximo\nrange(x)   # rango \nsummary(x) # resumen \n\n# Notar que tienen que ser vectores!\nmean(10, 6, 12, 10, 5, 0) \nmean(c(10, 6, 12, 10, 5, 0))\n\n# En el primer caso sólo toma el primer valor! Ojo, siempre un vector. \n\n# Otras funciones\nseq(1,10,2 )                        # Crear secuencias\nrep(c(1,2,3),10)                    # Repetir\ncut(x,2)                            # subdividir\nsample(x, size = 3, replace = TRUE) # Generar un aleatorio\n\n# El analisis de datos es...\n\nAltura = c(168, 177, 177, 177, 178, 172, 165, 171, 178, 170)\nPeso  = c(88, 72, 85, 95, 71, 69, 61, 61, 51, 75)\nM = cbind(Altura, Peso)\n\n# Paquete para trabajar con bases de datos (más detalles en secciones 2.3 y 2.4)\ninstall.packages(\"tidyverse\") \nlibrary(tibble) # Una libreria dentro de tidyverse. \nmisdatos <- as_tibble(M)\nmisdatos\n\nattach(misdatos)\nmax(Altura)\nmin(misdatos$Peso)\ndetach(misdatos)"},{"path":"análisis-de-datos-con-r.html","id":"herramientas-en-r","chapter":"8 Análisis de datos con R","heading":"8.2.4 Herramientas en R","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"escribir-códigos","chapter":"8 Análisis de datos con R","heading":"8.2.4.1 Escribir Códigos","text":"Algunos comentarios con respecto la escritura de codigos:Siempre escribir comentarios autocontenidos.Siempre utilizar índice.permite reducir el código.permite ver donde estoy también en un código largo.Puedo verlo con ctrl + shift + O o bien en la esquina inferior\nizquierda del script.Separar códigos largos en varios códigos. Cada código debe tener un\nobjetivo claro que debe explicar en una oración.Escribir códigos en bloques.Tres consejos básicos de estilo:utilizar _ para generar variables o bien otra convención.separar entre objetos y operaciones.","code":""},{"path":"análisis-de-datos-con-r.html","id":"condicionales-y-controladores-de-flujo","chapter":"8 Análisis de datos con R","heading":"8.2.4.2 Condicionales y controladores de flujo","text":"Operadores básicos:Operador %%: Un operador muy útil para comparar valores y para evaluar\nrápidamente si un valor está dentro de un vector o marco de datos.Operadores para seleccionar subconjuntos de datos:Operadores lógicos:","code":"\nrm(list=ls())\n\n# Asignador\na <- 2 * 3\na \n\na1 = 2 * 3\na1 \n\n# Nota: Nunca ocupar igual. \n\n# Igualdad \nTRUE == TRUE \nTRUE == FALSE\n\n# Nota: Para comparar elementos se utiliza doble igual \"==\".  \n\n\n# Desigualdad (!=)\nTRUE != FALSE \n\"Hola\" != \"Chao\"\n\n# Otros comparadores: <, > (>=), (<=) \n3 < 5\n5 > 8\n5 >= 5\n\n# Nota: No confundir con <- que es para asignar. \n\nTRUE > FALSE\n\n# Noten que la respuesta en un logical. \n\n# En vectores y matrices\nvector1 <- c(16,9,13)\nvector2 <- c(10,12,15)\n\n# Comparar contra un escalar\nvector1 > 10\nvector2 < 10\n\n# Compararlos entre ellos \nvector1 < vector2\n\n# Veamos en una matriz \nmatrix <- matrix(c(vector1, vector2), \n                 byrow = TRUE, \n                 nrow = 2)\nmatrix\n\nmatrix > 10\nrm(list=ls())\n\nv1 <- 3\nv2 <- 101\nt <- c(1,2,3,4,5,6,7,8)\n\n# El valor v1 ¿se encuentra dentro de t?\nv1 %in% t \n\n# Otro ejemplo....\nmivector <- c(\"a\", \"b\", \"c\", \"d\")\n\"a\" %in% mivector\n\n# Si no  está en el vector da un FALSE\n\"h\" %in% mivector\n\n# Si le quiero preguntar si NO está, coloco un signo de exclamación al frente. \n!\"a\" %in% mivector\n\n# Un vector en otro vector \na <- seq(12, 19, 1)\nb <- seq(1, 16, 1)\n\n# Veamos si los elementos de un vector \"largo\" están en uno \"corto\" \nb %in% a\n\n# Esto va a ser muy útil cuando trabajemos con bases de datos. \ny <- c(2,3,3,4,NA,8)\ny\n\n# Vamos a seleccionar solo los que no son NA\ny1 <- y[!is.na(y)]\ny1 \n\n# Noten que utilizamos la funcion is.NA y un operador (!). Más detalles en parte de análisis de datos, pero noten que la idea principal está aquí. \n# Operador (&): operador \"y\"\nTRUE & TRUE \nTRUE & FALSE\nFALSE & TRUE\nFALSE & FALSE \n\n# Ejemplo \nx <- 12 \nx > 5 & x < 11  \n\n# Operador (|): operador \"o\". Condiciones no excluyentes.\nTRUE  | TRUE \nTRUE  | FALSE \nFALSE | TRUE\nFALSE | FALSE \n\n# Ejemplo\ny <- 4\ny < 5 | y > 15 \n\n# Operador de negación (!)\n!TRUE \n!FALSE \n\n# Combinarlo con funciones \n!is.numeric(5)\n!is.numeric(\"Hello\")\n\n# Operadores con vectores \nc(TRUE, TRUE, FALSE) & c(TRUE, FALSE, FALSE)\nc(TRUE, TRUE, FALSE) | c(TRUE, FALSE, FALSE)\n!c(TRUE,TRUE,FALSE)\n\n# Si utilizo dos \"&&\" sólo compara el primer elemento del vector\nc(TRUE,FALSE,FALSE) || c(TRUE,FALSE,FALSE)\nc(TRUE,FALSE,FALSE) && c(TRUE,TRUE, TRUE)"},{"path":"análisis-de-datos-con-r.html","id":"condicionales","chapter":"8 Análisis de datos con R","heading":"8.2.4.3 Condicionales","text":":Else:Elseif:","code":"\nx <- 3\n  \n#if (condicion){\n\n#  cualquier cosa que quiero que se haga si la condición se cumple \n\n#}\n\nif (x > 0) {\n  \n  print(\"x es número mayor que cero\")\n  \n}\n\nif (x > 0){\n  \n  print(\"x es un número mayor que cero \")\n  \n}\n\nif (x < 0){\n  \n  print(\"x es un número menor que cero\")\n  \n}\nx <- 0\n\nif (x > 0){\n  \n  print(\"x es un número positivo\")\n  \n} else{\n  \n  print(\"x es un número negativo\")\n  \n}\nx <- 0\n\nif (x > 0){\n  \n  print(\"x es mayor que cero\")\n  \n} else if (x == 0){\n  \n  print(\"x es igual a cero\")\n} else{\n  \n  print(\"x es menor que cero\")\n  \n}\n\n# Importante: `(%%) corresponde al resto de una división`\n\nx <- 6 \n\nif (x %% 2 == 0){\n  \n  print(\"x es divisible por 2\")\n  \n} else if (x %% 3 == 0){\n  \n  print(\"x es divisible por 3\")\n} else {\n  \n  print(\"x no es divisible ni por 2 ni por 3\")\n  \n}\n\n# Notar que si la primera condición se cumple, la segunda no se ejecuta aunque sea cierta.  "},{"path":"análisis-de-datos-con-r.html","id":"funciones-3","chapter":"8 Análisis de datos con R","heading":"8.2.4.4 Funciones","text":"Idea principal: \\(f(x) = 2x + 1\\), \\(x \\R\\) \\(f(x) = 'hola' + x\\),\n\\(x \\('pepe', 'pepa', 'marta')\\)Algunas cosas adicionales sobre funciones: n permite ver los\nargumentos de una función, sin necesidad de ver la documentación. Útil \nveces.Ejercicio 2.2.2: FuncionesGenere una función que sea igual la división de dos elementos. Coloque\nun mensaje que indique cuando la división es indeterminada.Respuesta:","code":"\nvalores <-  c(1,2,3,4)\nmean(valores)\n# Escribir funciones con un argumento\ntriple <- function(x){\n  \n  y <- 3 * x\n  return(y)\n  \n}\n\n\ntriple <- function(x){\n  x/2\n\n  \n}\n\ntriple(500)\n\n# Escribir funciones con mas de un argumento\noperacion <- function(a,b){\n  \n  a*b + a/b\n  \n  \n}\n\noperacion(4,2)\n\n# Escribir funciones fijando opción por defecto.\noperacion_defecto <- function(a,b = 1){\n  a*b + a/b\n  \n  \n}\n\noperacion_defecto(4)\noperacion_defecto(4,0)\n\n# Escribir funciones utilizando if y return.\noperacion_condicionales <- function(a,b = 1){\n  if (b == 0){\n    return(0)\n    \n  }\n  a*b + a/b  \n  \n}\n\noperacion_condicionales(4,0)\n\n# Funciones con texto \ntexto <- function(){\n  print(\"Hola mundo!\")\n  return(TRUE)\n}\n\ntexto()\n\n# Funciones por defecto en ambos casos\noperacion_dosdefectos <- function(a = 1,b = 1){\n  if (b == 0){\n    return(0)\n    \n  }\n  \n  a*b + a/b  \n  \n}\n\noperacion_dosdefectos()\noperacion_indeterminada <- function(a,b){\n  a/b \n  if (b == 0){\n    print(\"Es indeterminado\")\n  }\n}\n\noperacion_indeterminada(1,0)"},{"path":"análisis-de-datos-con-r.html","id":"manipulación-de-bases-de-datos-parte-i-1","chapter":"8 Análisis de datos con R","heading":"8.3 Manipulación de bases de datos (Parte I)","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"principios-de-programación","chapter":"8 Análisis de datos con R","heading":"8.3.1 Principios de programación","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"iteradores-1","chapter":"8 Análisis de datos con R","heading":"8.3.1.1 Iteradores","text":"loop:loop con opciones:Flexibilizar el iterador:: mientras que…","code":"\n# Imaginen que queremos mostrar los nombres de un vector de forma reiterada\nciudades <- c(\"Nueva York\", \"Paris\", \"Santiago\", \"Rancagua\")\n\nprint(ciudades[1])\nprint(ciudades[2])\nprint(ciudades[3])\nprint(ciudades[4])\n\n# Lo anterior se puede hacer utilizando un iterador.\nfor (i in 1:4){\n  print(ciudades[i])\n}\n\n# Utilizando variables ocultas...\nfor (.j in 1:4){\n  print(ciudades[.j])\n}\n\nfor (.j in 1:length(ciudades)){\n  print(ciudades[.j])\n}\n\n# o escrito señalando el nombre de cada elemento de un vector. \nfor (ciudad in ciudades){\n  print(ciudad)\n}\n\n# Otro ejemplo:\nsemana <- c(\"Domingo\",\n            \"Lunes\",\n            \"Martes\",\n            \"Miercoles\",\n            \"Jueves\",\n            \"Viernes\",\n            \"Sabado\")\n\nfor (dia in semana)\n{\n  print(dia)\n}\nciudades <- c(\"Nueva York\", \"Paris\", \"Santiago\", \"Tokio\")\n\n# Agregar opcion break: quiebra el loop\nfor (ciudad in ciudades){\n  if(nchar(ciudad) == 8){\n    break\n  }\n  print(ciudad)\n}\n\n# Agregar  Opción next: se salta ese elemento\nfor (ciudad in ciudades){\n  if (nchar(ciudad) == 8){\n    next \n  }\n  print(ciudad)\n}\nciudades <- c(\"Nueva York\", \"Paris\", \"Santiago\", \n              \"Tokio\", \"Rancagua\", \"Roma\")\n\n# Ahora el tamaño del loop es flexible. Esto es muy inportante para cuando trabajemos con bases de datos. \n\nfor (i in 1:length(ciudades)){\n  print(ciudades[i])\n}\n\n# Noten que ahora llamo a los elementos dentro de un loop como subconjuntos de un vector. \n\n# En resumen, dos versiones de lo mismo \n\nfor (i in 1:length(ciudades)){\n  print(ciudades[i])\n}\n\nfor (ciudad in ciudades){\n  \n  print(ciudad)\n}\n\n# Ejemplo: para dejar mensajes \n\nfor (i in 1:length(ciudades)){\n  print(paste(ciudades[i], \"esta en la posicion\",i, \n              \"en el vector ciudades\"))\n}\n\n# Lo anterior es aplicable para inspecciones de bases de datos, por ejemplo, podemos dejar un mensaje\n\nfor (i in 1:length(mtcars)){\n  print(paste(\"el promedio de la variable\",\n              names(mtcars[i]), \"es\",\n              mean(mtcars[,i])))\n}\nx <- 1 \n\nwhile (x <= 7){\n  print(paste(\"x es\", x))\n  \n# Actualización \n  x <- x + 1 \n}\n\nwhile (x <= 700){\n  print(paste(\"x es\", x))\n  # Actualizacion \n  x <- x + 1 \n}\n\n# Muy importante la actualización."},{"path":"análisis-de-datos-con-r.html","id":"lapply-sapply-y-vapply","chapter":"8 Análisis de datos con R","heading":"8.3.1.2 Lapply, Sapply y Vapply","text":"Lapply:Equivalencia loop y lapply. Quiero saber todas las clases de la\nlista:Resultado de lapply con vector:Lapply como función:Sapply: Es una variacion de lapply que sirve para simplificar\nlapply. Ahora el resultado es un vector, una lista.Vapply: Es una variación que sirve para definir explícitamente el\ntipo de objeto del resultado.","code":"\nnyc <- list(poblacion = 8405837, \n            barrios = c(\"Manhattan\", \"Bronx\", \n                        \"Brooklyn\", \"Queens\", \n                        \"Staten Island\"))\nnyc\n# Puedo hacer esto con un loop \nfor (objeto in nyc){\n  print(class(objeto))\n}\n\nfor (i in 1:length(nyc)){\n  print(class(nyc[[i]]))\n}\n\n# Pero, si utilizo `lapply()` puedo hacerlo mucho más eficiente. \n# Lapply: que aplica esto como si fuese un `for`. \nlapply(nyc,class)\n# Lapply ejecuta la función, en este caso class, para todo elemento del objeto, en este caso nyc. \n\n# Si quiero saber el número de carácteres\ncities <- c(\"New york\",\"Paris\", \"Tokyo\", \" Rio de Janeiro\")\nlapply(cities,nchar)\nclass(lapply(cities,nchar))\n\n# Noten que el resultado aquí es una lista. Si quiero que sea un vector, puedo utilizar la función `unlist()`. \nunlist(lapply(cities,nchar))\nclass(unlist(lapply(cities,nchar)))\nprecios <- list(2.25, 2.18, 2.89, 2.84, 2.89)\n\n# Creamos una función\nmultiplicar <- function(x,factor){\n  x * factor\n}\n\n# Ahora, podemos aplicar lapply y agregar opciones de la función\ntresveces <- lapply(precios, multiplicar, factor = 3)\ntresveces <- unlist(tresveces)\n\n# Noten que la sintaxis es: objeto, función, opciones. Es decir, igual a lo anterior, pero pudiendo agregar opcionales. \ncities <- c(\"New york\",\"Paris\", \"Tokyo\", \" Rio de Janeiro\")\n\nlapply(cities,nchar)\nsapply(cities,nchar)\n\n# Noten que es bastante ordenado. Sin embargo, falla cuando no es fácil ordenar el resultado. \nvapply(cities, nchar, numeric(1))"},{"path":"análisis-de-datos-con-r.html","id":"manipulacion-de-bases-de-datos-en-r","chapter":"8 Análisis de datos con R","heading":"8.3.2 Manipulacion de bases de datos en R","text":"Analizar datos es parte importante de las labores que uno desea realizar\nal utilizar R. Vamos revisar cuatro aspectos iniciales de cualquier\ntrabajo con datos:Importar datos en distintos formatos.Inspeccionar y limpiar los datos que tenemos.Transformar los datos con el fin de crear nuevas variables.Juntar bases de datos de distinto tipo.","code":""},{"path":"análisis-de-datos-con-r.html","id":"importar-bases-de-datos","chapter":"8 Análisis de datos con R","heading":"8.3.2.1 Importar bases de datos","text":"Para poder utilizar, cargar, renovar datos es importante tenerlos\ntodos en un solo lugar.Para poder utilizar, cargar, renovar datos es importante tenerlos\ntodos en un solo lugar.Esto también aplica para los resultados y codigos.Esto también aplica para los resultados y codigos.Para que esto efectivamente ocurra necesitamos decirle R cual va \nser nuestro directorio de trabajo. Es decir, el lugar donde\nguardaremos los datos que queremos trabajar, los resultados de\nnuestros análisis y nuestros códigos.Para que esto efectivamente ocurra necesitamos decirle R cual va \nser nuestro directorio de trabajo. Es decir, el lugar donde\nguardaremos los datos que queremos trabajar, los resultados de\nnuestros análisis y nuestros códigos.Directorios de trabajo en R:Ejemplo de ordenar carpetas:Proyectos de R:Proyecto: piensen en muchos proyectos con muchos códigos.Es una herramienta incorporada en RStudio que permite manejar un\nproyecto de análisis de datos.Permite dividir el trabajo en múltiples contextos cada uno con su\npropio directorio de trabajo, espacio de trabajo e historial.¿Por qué utilizarlo?\nMantiene códigos y datos en la misma carpeta.\nMantiene códigos y datos separados de otros proyectos: evita\nconfusiones o errores.\nIdentifica automáticamente el directorio de trabajo, facilitando\ncooperación.\nMantiene códigos y datos en la misma carpeta.Mantiene códigos y datos separados de otros proyectos: evita\nconfusiones o errores.Identifica automáticamente el directorio de trabajo, facilitando\ncooperación.Pasos para crear un proyecto:Dos opciones:\nR crea una carpeta de trabajo.\nCrear una carpeta primero, y luego decirle R que la\nidentifique.\nR crea una carpeta de trabajo.Crear una carpeta primero, y luego decirle R que la\nidentifique.Todos los nuevos archivos serán automáticamente guardados en la\ncarpeta del proyecto.Una vez hecho el proyecto se genera un archivo con extensión\nR.proj.Aquí R guarda información del proyecto: historial, datos, etc.Lo más importante es que fija el directorio de trabajo de forma tal\nde que siempre sea el mismo una vez que se abre un proyecto. Evita\ncambiar directorios de trabajo al colaborar.Package :Package rio: R tiene bases de datos propias. Sin embargo, muchas veces\nvan querer trabajar con datos propios. o bien de datos públicos que\nmuchas veces estan en distintos formatos. Rio package es una forma\nflexible de importar datos en distintos.Rio debe su nombre por R input/output. Dos funciones principales:\nimport() y export(). Además, cuando se le indica la extensión \nrio este leerá y utilizará la herramienta indicada para leer esos\ndatos.Hay otras opciones tambien para importar datos. Por ejemplo,\nread.csv() (\"base R\"); read.xlsx (\"openxlsx\"). El problema es que\nson difíciles de recordar. Mejor utilizar una.Importar datos en distintos formatos: Utilizar import() para importar\nun conjunto de datos es bastante sencillo. Basta con proporcionar la\nruta del archivo (incluyendo el nombre y la extensión del archivo) entre\ncomillas.Opciones:","code":"\n# Me dice donde estoy \ngetwd()\n\n# Si quiero indicarle otra ruta tengo dos opciones: \n\n# Opcion 1: Indicar a R la ruta/carpeta donde tengo mis datos \nsetwd(\"C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3\")\n\n# Opcion 2: Lo mismo que opcion 1, pero más ordenado. \nruta <-\"C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3\"\nsetwd(ruta)\n\n# Noten que es necesario ocupar `/`. Por defecto las rutas al copiarlas vienen con otro tipo de \"slash\". Para cambiarlas fácilmente, y no una por una, vamos a utilizar `ctrl + f`. \n\n# Con este atajo podemos remplazar varios elementos a la vez. Cuidado!! Es importante decirle si queremos que modifique todo el documento o bien solo una parte. \ndir(ruta)\nruta <- \"C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3\"\n\n# Codigos \ncodigos <- \"C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3/codigos\"\ncodigos <- paste(ruta,\"/\",\"codigos\", sep = \"\")\nsetwd(codigos)\ndir(codigos)\n\n# Datos \ndatos <- \"C:/Users/catab/Dropbox/Curso R - Agosto 2022/Clase 3/datos\"\ndatos <- paste(ruta,\"/\",\"datos\",sep=\"\")\ndir(datos)\n\n# Datos raw \ndatosraw <- \"C:/Users/catab/Dropbox/Cursos de R/Curso R - Noviembre 2022/Clase 3/datos/raw\"\ndir(datosraw)\n\n# Todo lo anterior no es muy recomendado. \ninstall.packages(\"pacman\")\npacman::p_load(\"here\")\n\nhere()\nhere(\"datos\")\nlibrary(pacman)\np_load(here, # Modificar carpetas\n       rio)  # Importar/exportar datos\n# Ocupamos R project, here e import para importar datos fácilmente\n\ndatos_xlsx <- import(here(\"datos\", \"Data.xlsx\"))\ndatos_dta  <- import(here(\"datos\", \"Data.dta\"))\ndatos_txt  <- import(here(\"datos\", \"Data.txt\"))\ndatos_csv  <- import(here(\"datos\", \"Data.csv\"))\n# 1. Importar distintas hojas: Por defecto se importa la primer hoja de una base de datos, con \"which\" puedo elegir la hoja.\n\ndatos_xlsx_h1 <- import(here(\"datos\", \"Data.xlsx\"), \n                        which = \"hoja1\") \ndatos_xlsx_h2 <- import(here(\"datos\", \"Data.xlsx\"), \n                        which = \"hoja2\") \n\n# 2. Puedo decirle a priori que valores son missings. \n\n# Especificar un missing\ndatos_xlsx <- import(here(\"datos\", \"Data.xlsx\"), na = \"2018\") \n\n# Especificar varios a la vez..\ndatos_xlsx <- import(here(\"datos\", \"Data.xlsx\"), \n                     na = c(\"Missing\", \"\", \" \"))\n\n# 3. Saltar filas.\ndatos_xlsx <- import(here(\"datos\", \"Data.xlsx\"), skip = 1)\n\n# Noten que puedo hacerlo con cualquier tipo de datos, ya que son opciones de la función `rio`. \n\ndatos_dta  <- import(here(\"datos\", \"Data.dta\") , skip = 1)\n\n# En la tabla de este link pueden ver los distintos paquetes que soporta `rio`, junto a ejemplo adicionales. \nbrowseURL(\"https://cran.r-project.org/web/packages/rio/vignettes/rio.html\") "},{"path":"análisis-de-datos-con-r.html","id":"manipulación-de-bases-de-datos-parte-ii-1","chapter":"8 Análisis de datos con R","heading":"8.4 Manipulación de bases de datos (Parte II)","text":"Ya sabemos cargar datos. Ahora vamos hacer el primer proceso para\nanalizar cualquier base de datos.","code":""},{"path":"análisis-de-datos-con-r.html","id":"funciones-clave","chapter":"8 Análisis de datos con R","heading":"8.4.1 Funciones clave","text":"Tidyverse es una colección de paquetes de R. Tidyverse contiene\nmúltiples paquetes que iremos utilizando. Incluye: dplyr, ggplot2,\ntidyr, stringr, tibble, purrr, magrittr y forcats. Una\npaquete clave es dplyr que contiene muchas funciones para trabajar con\nbases de datos.Instalamos tidyverse:","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(pacman)\np_load(tidyverse)"},{"path":"análisis-de-datos-con-r.html","id":"operador-piping","chapter":"8 Análisis de datos con R","heading":"8.4.2 Operador piping","text":"Piping %>%: crtrl + shift + m. Paquete asociado: magrittrOperador que permite encadenar las funciones para realizar de manera\nsencilla transformaciones complejas en las bases de datos.Lo que dice es pasar el elemento que esta su izquierda como un\nargumento de la función que tiene la derecha.Coloca el énfasis en las acciones. Pasa un output intermedio de una\nfunción la siguiente.magrittr es el paquete que permite ocupar piping. piping se\nutiliza mucho con las librerias de tidyverse y dplyr enfocadas\nen análisis de datos.Excelente cuando existe una secuencia de acciones/operaciones que\nqueremos realizar.","code":"\np_load(magrittr)\ndata(iris)\n\n# Ejemplo 1: usar pipe como encadenador \nhead(iris, n = 4)\n\n# Con piping\niris %>% head(n = 4)\n\n# Ejemplo 2: obtener número total de observaciones y un promedio. \nsummarize(mtcars, \n           media = mean(disp))\n\nmtcars %>% summarize(promedio = mean(disp))\n\n# Con piping \nmtcars %>%\n  filter(mpg > 20) %>% \n  summarise(promedio = mean(disp))\n\npromedio_mpg_20 <- mtcars %>%\n                   filter(mpg > 20) %>% \n                   summarise(promedio = mean(disp))\npromedio_mpg_20 "},{"path":"análisis-de-datos-con-r.html","id":"proceso-de-análisis-de-datos-parte-i-cargar-inspeccionar-y-limpiar","chapter":"8 Análisis de datos con R","heading":"8.4.3 Proceso de análisis de datos (Parte I): cargar, inspeccionar y limpiar","text":"Preámbulo:Inspeccionar:Ejercicio 2.4.1: Utilizando el operador pipping, mostrar las\núltimas 2 filas de las primeras 11 filas.Limpiar:Renombrar variables:Seleccionar variables o columnas:Remover columnas:Mirar si hay duplicados. En ocasiones es importante revisar si hay\nduplicados. El paquete dplyr contiene distinct(). Esta función\nexamina cada fila y reduce los datos solo las que sean valores\ndiferentes:","code":"\n# Limpiar\nrm(list = ls())\n\n# Cargamos paquetes que vamos a utilizar\nlibrary(pacman)\np_load(\n  rio,        # Importar/Exportar datos \n  here,       # Determinar las rutas de mi carpeta \n  tidyverse,  # Analisis de datos y visualización\n  magrittr,   # Para utilizar operador %>%  \n  janitor     # Para análisis de datos\n)\n\n# Importamos datos\ndatos <- import(here(\"datos\", \"Data.xlsx\"), which = \"hoja1\") \n# Visión general \ndatos \nview(datos)\n\n# Para una inspección detallada \np_load(skimr)\nskim(datos)\nstr(datos)\n\n# Mirar algunas filas, columnas, etc.\n\nhead(datos)     # muestra las 6 primeras filas \ntail(datos)     # últimas 6 filas \nhead(datos, 11) # podemos pedir m?s de 6 \ntail(datos, 3)  # o menos de 6\n\n# Mirar nombre variables/columnas\nnames(datos)\ndatos %>% names()\n\n# `names()` también puede ser \nnames(datos) <- c(\"YEAR\", \"GDP\", \"GROSS_EXPORTS\", \n                  \"GROSS_IMPORTS\", \"NET_EXPORTS\" )\nnames(datos)\n\n# Mirar filas \nrow.names(datos)\nrow.names(mtcars)\n\n# Mirar la cantidad de variables de la base de datos\nlength(datos) # columnas o variables\ndim(datos) \nncol(datos) \nnrow(datos)\n\n# ¿Qué tipo de objeto es? \nclass(datos)\n\n# También podemos inspeccionar elementos específicos\ndatos<- import(here(\"datos\", \"Data.xlsx\"), which = \"hoja1\") \n\n# Si queremos ver la columna de GDP\ndatos$gdp\ndatos[2]\ndatos[,2]\n\n# Para llamarlos solo por su nombre recordar ocupar `attach()`\nattach(datos)\ngdp\ndetach(datos)\n\n# Si queremos un objeto dentro de una variable \ndatos$gdp[7]\ndatos[7,2]      #[fila, columna]\ndatos[7,\"gdp\"]\n\n# Si queremos seleccionar parte de la columna \ndatos$gdp[5:10]\n\n# o bien (solo por `attach()`)\nattach(datos)\ngdp[1:2]\n\n# o bien \ndatos[1:2,2] # mostrar las filas 1:2, de la columna 2\n\n# `table()` me permite hacer una tabla sencilla de frecuencias\ntable(datos$year)\nattach(datos)\ntable(year)\ndetach(datos)\nnames(datos) <- c(\"YEAR\", \"GDP\", \"GROSS_EXPORTS\", \n                  \"GROSS_IMPORTS\", \"NET_EXPORTS\" )\nnames(datos)\n\n# La función `clean_names()` del paquete `janitor` estandariza nombres: \n\ndatos_nuevos <- clean_names(datos)\nnames(datos)\nnames(datos_nuevos)\nrm(list = ls())\n\ndatos<- import(here(\"datos\", \"Data.xlsx\"), which = \"hoja1\") \n\n# Función para renombrar variables. Sintaxis: rename(nuevonombre = viejonombre)\n\n# Escribamos esto, pero con `piping`\ndatos_renombrados <- datos %>% \n                     rename(tiempo = year, \n                     pib = gdp, \n                     exportaciones = gross_exports)\nnames(datos_renombrados)\nrm(datos_renombrados)\nrm(list= ls())\ndatos<- import(here(\"datos\", \"Data.xlsx\"), which = \"hoja1\") \n\n# select() de `dplyr` permite seleccionar variables\ndatos_select <- datos %>% \n                select(year, gdp)  \n\ndatos_select1 <- datos %>% \n  select(c(1:4))\n\ndatos_select1 <- datos %>% \n  select(c(1:ncol(datos)-1))  \n\n# También puedo seleccionar en base a un criterio (Sólo sirve para string)\ndatos_select2 <- datos %>% \n  select(year, contains(\"Gross\"))\n\n# Con `select` también se puede renombrar \ndatos_select_renombrados <- datos %>% \n                            select(tiempo = year, \n                                    pib = gdp)\n\n# Una opción es ocupar select con `everything()` para ordeanar mis columnas. \ndatos_select_ordenados <- datos %>% \n                        select(gdp, year, everything()) \n# La idea aqui es decir: todas se quedan, menos las que pongo aquí. \ndatos_select_remover <- datos %>% \n                      select(-c(gdp, year)) \ndatos1 <- datos %>% distinct()\n\n# ¿Cuantos duplicados?\nnrow(datos)\nnrow(datos1)\n\ndif <- nrow(datos) - nrow(datos1) \ndif"},{"path":"análisis-de-datos-con-r.html","id":"proceso-de-análisis-de-datos-parte-ii-cargar-inspeccionar-y-limpiar","chapter":"8 Análisis de datos con R","heading":"8.4.4 Proceso de análisis de datos (Parte II): cargar, inspeccionar y limpiar","text":"Cargar:Inspeccionar datos:Limpiar bases de datos (continuación):Seleccionar columnas:Manipulación de NAs: Con vectores. Los NA son parte importante de la\nlimpieza de los datos. Recordemos que la función .na() nos\npermite identificarlos.Manipulación de NAs: con complete cases()","code":"\n# Limpiamos consola\nrm(list = ls())\n\n# Cargamos paquetes que vamos a utilizar\npacman::p_load(\n  rio,        # importar/exportar datos. \n  here,       # escribir rutas de las carpetas.  \n  janitor,    # limpiar datos y tablas.\n  tidyverse,  # Manejo de bases de datos y visualización. \n  magrittr,    # Permite utilizar operador %>% (piping). \n  skimr,       # Inspeccionar datos\n  inspectdf,   # Inspeccionar datos\n  gapminder    # Base de datos con información de países. \n)\ndata(\"gapminder\")\ngapminder %>% view()\n\n# Cargamos datos que vienen incluidos en R. \ndata(starwars) \nstarwars \n#  Inspect_cat() retorna una base de datos que resume características de un data.frame.\ninspeccion <- inspect_cat(starwars)\ninspeccion\nclass(inspeccion)\n\n# Las columnas son: \n\n# col_names: nombre de cada columna\n# cnt: número de valores únicos por nivel\n# common: el nivel más común\n# common_pcnt: el porcentaje de ocurrencia del nivel más común.\n# levels: una lista de dataframes (tibbles) cada uno con tablas de frecuencia para todos los niveles. \n\n# Notar que una de las columnas de un data frame pueden ser listas: \ninspeccion[1,5]\ninspeccion[2,5]\ninspeccion[3,5]\ninspeccion[4,5]\ninspeccion[5,5]\n\n# Ahora, si quiero ver el contenido, debo utilizar doble paréntesis cuadrado\ninspeccion[1,5]      # Con el nombre\ninspeccion[[1,5]]    # El contenido\ninspeccion[[5]]      # El contenido de todas las filas, en este caso, listas\ninspeccion[[5]][[1]]\ninspeccion[[5]][[2]]\n\n# Una forma más simple de mirar esta información es:\ninspeccion$levels$eye_color\ninspeccion[[5]][[1]]\n\n# Otra cosa interesante de este paquete es la función `show_plot()`\nstarwars %>% inspect_cat() %>% show_plot()\n\n# Esta función permite ver las categorías de cada variable categórica. Noten que las zonas en gris son los NA. \nrm(list= ls())\n\ndatos<- import(here(\"datos\", \"Data.xlsx\"), which = \"hoja1\") \n# Vimos que `select()` es parte importante. `select()` puede ser utilizado con varias funciones adicionales: \n\n# 1. everything () - todas las otras columnas no mencionadas.\ndatos_everything <- datos %>% \n  select (year, gdp, everything()) \n\n# 2. last_col () - la última columna.\ndatos_last_col <- datos %>% \n  select(year, last_col())\n\n# 3. where () - aplicar una función a todas las columnas y \n#               selecciona solo las que cumple esta condición \n#               (es decir, cuando es verdadera). \ndatos_where <- datos %>% \n  select(where(is.logical))\n\n# 4. contains () - columns containing a character string\ndatos_contains <- datos %>% \n  select(contains(\"exports\"))\n\n# 5. starts_with () - selecciona la variable si se tiene un prefijo determinado\ndatos_starts_with <- datos %>% \n                     select(starts_with(\"gross_\")) \n\n# 6. ends_with () - selecciona la variable si se tiene un sufijo determinado\ndatos_ends_with <- datos %>% \n  select(ends_with(\"_imports\")) \n\n# 7. matches () - aplicar una expresión regular \ndatos_matches <- datos %>% \n  select(matches(\"gross|gdp\"))\n\n# 8. any_of () - la selecciona si la columna existe pero NO retorna error \n#                si no la encuentra. \ndatos_any_of  <-  datos %>% \n  select(any_of(c(\"year\", \"gdp\", \"cualquiercosa\")))\nx <- c(1,2,NA,NA,5)\nmalos <- is.na(x)  \nmalos\nclass(malos)\n\n# Si queremos eliminar los NA es cosa de colocar un vector sobre otro\nx[!malos]\nx[!is.na(x)]\n\nx <- x[!malos]\nx\n# Miremos un caso más práctico con iris\nsummary(iris)\nskim(iris)\ndata(\"iris\")\n\n# Generar NA en la base de datos (después veremos esto en detalle ahora concéntrense solo en los NA)\niris$Sepal.Length<-ifelse(iris$Sepal.Length<5, NA,iris$Sepal.Length)\n\n# Dos opciones: \n\n  # (i) como antes\n  malos <- is.na(iris$Sepal.Length)\n  iris2 <- iris[!malos,] \n  iris2\n\n  # (ii) utilizando `complete.cases()`\n  completos <- complete.cases(iris$Sepal.Length)\n  head(completos, 100) \n\n  # Creamos una base nueva solo con casos completos. \n  iris3 <- iris[completos,] \n  head(iris3)  \n  summary(iris3) "},{"path":"análisis-de-datos-con-r.html","id":"manipulación-de-bases-de-datos-parte-iii","chapter":"8 Análisis de datos con R","heading":"8.5 Manipulación de bases de datos (Parte III)","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"crear-variables","chapter":"8 Análisis de datos con R","heading":"8.5.1 Crear Variables","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"crear-variables-binarias","chapter":"8 Análisis de datos con R","heading":"8.5.1.1 Crear variables binarias","text":"Ahora vamos ver como crear variables dicótomas o binarias. Estas\nvariables son muy importantes para hacer análisis de datos. Vamos \nutilizar la función ifelse().Ejercicio 2.5.1: Escriba lo anterior utilizando mutate.Respuesta:","code":"\nrm(list= ls())\ndata <- import(here(\"datos\", \"Data.xlsx\"), which = \"hoja1\") \n\n# Asignar valor 1 si estamos en democracia, 0 caso contrario\ndata <- data %>% mutate(demo = ifelse(year>=1990,1,0))\ntable(data$demo)\n\n# Tabla 1: ¿Cual fue el pib promedio en democracia y en dictadura?\ntabla1 <- data %>% group_by(demo) %>% \n                   summarize(promediopib = mean(gdp, na.rm = TRUE)) \ntabla1 \n\n# Asignar valor 1 si estamos en democracia y el pib es mayor a su media\ndata <- data %>%  mutate(avance = ifelse(demo == 1 & gdp > mean(gdp),\n                                         1,0))\nmean(data$gdp)\n\n# Asignar valor 1 si estamos en democracia y si dentro de esos periodos\n# las exportaciones netas son mayores a la mediana de las importaciones \ndata$expo <-ifelse(data$demo==1, ifelse(data$gross_exports>\n                                          median(data$gross_imports),1,0),0)\ntable(data$expo)\ndata <- data %>% mutate(expo = ifelse(demo==1, \n                                      ifelse(gross_exports>\n                                          median(gross_imports),1,0),0))"},{"path":"análisis-de-datos-con-r.html","id":"crear-variables-categóricas","chapter":"8 Análisis de datos con R","heading":"8.5.1.2 Crear variables categóricas","text":"También podemos crear variables categóricas utilizando ifelse(). De\nmanera muy sencilla podemos generar variables según ciertas condiciones:","code":"\n# De manera muy sencilla podemos generar variables según ciertas condiciones  \ndata$tipo<-ifelse(data$year<1980 & data$year>1959,1,\n                  ifelse(data$year<2000 & data$year>1979,2,\n                         ifelse(data$year<2010 & data$year>1999,3,4)\n                  )\n)\ntable(data$tipo)  \n\n#  Nota: no es recomendable utilizar o escribir tantos `ifelse()` juntos.\n# Es mejor utilizar `case_when()` que veremos un más adelante. "},{"path":"análisis-de-datos-con-r.html","id":"factores","chapter":"8 Análisis de datos con R","heading":"8.5.2 Factores","text":"R puede codificar automáticamente una variable categorica (factor) con\nun número entero. Sirven para hacer estadisticas o estimar regresiones.\nLos factores, pueden ser ordenados o ordenados, se utilizan para\nrepresentar variables que se agrupan en categorías.Veamos un ejemplo:","code":"\nx <- rep(c(\"Ford\",\"BMW\",\"Peugeot\"),10)\nx\n\nclass(x)\n\n# `factor()`\nfactor_nominal <- factor(x)\nfactor_nominal\nclass(factor_nominal)\nlevels(factor_nominal) #para analizar que niveles tiene el objeto\ntable(factor_nominal)  # resume la cantidad de observaciones por nivel"},{"path":"análisis-de-datos-con-r.html","id":"funciones-útiles-para-transformar-bases-de-datos","chapter":"8 Análisis de datos con R","heading":"8.5.3 Funciones útiles para transformar bases de datos","text":"across(): Algunas veces queremos aplicar una función múltiples\nvariables, para ello vamos utilizar la función across() y\nespecificar la función con fns. Por ejemplo:cumsum(): Suma acumuladaRecodificar variables: continuación se presentan algunos escenarios en\nlos que es necesario recodificar (cambiar) los valores:Editar un valor específico (por ejemplo, una fecha con un año o\nformato incorrecto)Para conciliar valores que se escriben igualCrear una nueva columna de valores categóricasCrear una nueva columna de categorías numéricas (por ejemplo,\ncategorías de edad)recode (): Cambiar valoresreplace(): Reemplazar valoresreplace_na(): Para cambiar los valores perdidos (NA) por un valor\nespecífico, como “Missing”, utilice la función dplyr replace_na()\ndentro de mutate():Recordatorio: Esta función cambia los missing values solo por\nvalores de la misma clase de la variable/columna, por lo tanto, si\ntrabajamos con valores de clase numérica los reemplazos deben ser\ntambién numéricos.case_when(): una función de dplyr. Es útil para asignar múltiples\nvalores. Sirve si se necesita recodificar muchos grupos:","code":"\ndatos2 <- data %>% \n          mutate(across(cols = everything(), fns = as.numeric))\n\n# Aquí `across()`, que es una función de \"dplyr\", permite ser utilizada con `mutate()`, `select()`, `filter()`, `summarise()`, etc. \ndatos3 <- data %>% \n  mutate(across(.cols = contains(\"gross\"), .fns = as.numeric))\n# Para hacer operaciones acumuladas\nsum(c(2,4,15,10))     # Retorna la suma del vector\ncumsum(c(2,4,15,10))  # Retorna la suma acumulada del vector\n\n# Puedo utilizarla con mutate ()\ndatos_acumulados <- data %>% \n  arrange(year) %>% \n  count(gdp) %>% \n  mutate(gdp_acumulado = cumsum(n))\ndatos_acumulados\nrm(list= ls())\ndata <- import(here(\"datos\", \"Data.xlsx\"), which = \"hoja1\") \ndata <- data %>% mutate(demo = ifelse(year>=1990,1,0))\n\n# Seguimos con data \nnames(data)\n\n# Creamos una nueva variable \ndatos <- data %>% mutate(regimen = ifelse(demo == 1, \"Democracia\", \"Dictadura\"))\ndatos \n\n# Recodificamos (cambiar nombres)\ndatos <- datos %>% \n         mutate(regimen = recode(regimen, \"Democracia\" = \"Demo\", \n                          \"Dictadura\" = \"Dict\"))\ndatos \n\n# Notar que es vieja variable por nueva variable\ndatos <- datos %>%  mutate(avance = ifelse(demo == 1 & gdp > mean(gdp),\n                                           1,0))\n\n# Similar sintaxis a la de recode\ndatos <- datos %>% \n         mutate(regimen = replace(regimen, avance == 0, \"No cambio en el pib\"))\n\n# Sintaxis\n# mutate(columna a cambiar = replace(columna a cambiar, \n#                                      criterio para las filas, \n#                                      nuevo valor)).\n\n# Un equivalente a `replace()` es utilizar []. \ndatos$regimen[datos$avance == 0] <- \"No cambio en el pib\"\ndatos_ficticios <- data.frame(var1 = c(seq(1,10), NA),\n                              var2 = c(rep(NA,11)))\ndatos_ficticios\n\ndatos_ficticios1 <- datos_ficticios %>% \n                    mutate(var1 = replace_na(var1, 0))\ndatos_ficticios1\nrm(list = ls())\n\ndatosedad <- data.frame(edad = c(2,3,4,1,500,2330,8,10,12), \n                        unidad = c(\"años\",\"años\", \"años\", NA, \"meses\", \"meses\", \n                                   \"años\", \"años\", \"semanas\"))\ndatosedad\n\n# Imaginemos que queremos tener una medida comparable de edad, para ello\n# podemos utilizar `case_when()`. \ndatos_case_when <-  datosedad %>% \n  mutate(edad_años = case_when(\n    unidad == \"años\"  ~ edad, \n    unidad == \"meses\" ~ edad/12, \n    unidad == \"semanas\" ~ edad/52, \n    is.na(unidad) ~ edad))\n\ndatos_case_when"},{"path":"análisis-de-datos-con-r.html","id":"cambio-de-formato-de-los-datos","chapter":"8 Análisis de datos con R","heading":"8.5.4 Cambio de formato de los datos","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"de-ancho-a-largo","chapter":"8 Análisis de datos con R","heading":"8.5.4.1 De ancho a largo","text":"En en siguiente ejemplo, los datos están guardaos en “wide” para las\ncolumnas que tienen el número de casos de malaria por tramos de edad.\nPara un trabajo de análisis de datos es importante transformar los datos\n“long”.pivot_longer(): función del paquete tidyr. Paquete incluido en\ntidyverse(). Transforma los datos de wide long.","code":"\nrm(list = ls())\ncount_data <- import(here(\"datos\",\"malaria_facility_count_data.rds\"))\nhead(count_data)\ndf_long <- count_data %>% \n  pivot_longer(\n    cols = c(\"malaria_rdt_0-4\", \n             \"malaria_rdt_5-14\", \n             \"malaria_rdt_15\", \n             \"malaria_tot\")\n  )\n\ndf_long \n\n# Una mejor opción es con la función starts_with(): \ncount_data %>% \n  pivot_longer(\n    cols = starts_with(\"malaria_\")\n  )\n\n# Para agregar nombres a las nuevas variable creadas\ndf_long1 <- count_data %>% \n  pivot_longer(\n    cols = starts_with(\"malaria_\"),\n    names_to = \"grupo_edad\",\n    values_to = \"casos_malaria\"\n  )\n\ndf_long1"},{"path":"análisis-de-datos-con-r.html","id":"de-largo-a-ancho","chapter":"8 Análisis de datos con R","heading":"8.5.4.2 De largo a ancho","text":"pivot_wider(): Transforma los datos de long wide. Útil si quiero\nhacer una tabla mas amigable para el o la lectora.Otra opción para hacer esto mismo es gather() y spread():","code":"\nrm(list = ls())\nlinelist <- import(here(\"datos\",\"linelist_cleaned.rds\"))\n\n df_wide <- linelist %>%\n   count(age_cat, gender)\n df_wide\n\n# En un mejor formato\n table_wide <- df_wide %>%\n  pivot_wider(\n     id_cols = age_cat,\n     names_from = gender,\n     values_from = n\n   )\n\n table_wide\nrm(list = ls())\ndatos <- import(here(\"datos\",\"Cuadro_1.xls\"), skip = 1) \n\n# Cargamos datos que estan en formato ancho y los preparamos un poco...\ndatos <- datos[-c(1:2)]  \ncolnames(datos) <- c(1960:2018)\ncolnames(datos)\n\n# 1. Como vemos, los años estan en las columnas, y queremos pasar a long data, \n# de forma tal que los años esten en una columna y el pib en otra \n# 2. La funciom \"gather()\" transforma los datos de formato ancho (wide) a \n# formato largo (long)\n\n# De ancho a largo: utilizando la función gather()\n# La primera es la variable \"clave\" y la segunda es la del valor\ndata_long <- datos %>% \n  gather(año, pib, 1:59)\ndata_long\n\n# De largo a ancho: utilizando la función spread()\ndata_wide <- data_long %>% \n  spread(año,pib)\ndata_wide"},{"path":"análisis-de-datos-con-r.html","id":"juntar-bases-de-datos","chapter":"8 Análisis de datos con R","heading":"8.5.5 Juntar bases de datos","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"pegar-hacia-el-lado-por-columnas","chapter":"8 Análisis de datos con R","heading":"8.5.5.1 Pegar hacia el lado (por columnas)","text":"En el siguiente ejemplo, tenemos datos de hospitales.Sintaxis: Imaginemos que tenemos dos bases de datos “df1”, “df2”. df1\ntienen una columna llamada “ID”. df2 tiene una columna que se llama\nidentificador.Left y Right join:Full Join: El más inclusivo de los “joins”. Retorna todas las\nobservaciones/filas:Inner join: El más restrictivo de los “join”. Retorna solo las filas\nque hicieron match entre ambas bases de datos.Semi - join: Mantiene todas las observaciones de la base de\nreferencia que tengan un “match” en la base secundaria, pero agrega\nnuevas columnas ni duplicados en casos de multiples “match”.Anti- join: Se le llama, al igual que semi-join “join de filtros”.\nRetorna las observaciones/filas en la base de datos de referencia que \nhacen “match” en base de datos secundaria.La función merge:","code":"\nrm(list = ls())\nhosp_info <- import(here(\"datos\",\"hosp_info_final.xlsx\"))\nlinelist_mini <- import(here(\"datos\",\"linelist_mini_final.xlsx\"))\n# Caso 1: nombres de identificadores distintos.\ndata_junta <- join(df1,\n                   df2,\n                   by = c(\"ID\" = \"identificador\"))\n\n# Caso 2: imaginemos ambas bases de datos (df1, df2) tienen un\n#         identificador llamado \"ID\"\n\ndata_junta <- join(df1,\n                        df2,\n                        by = \"ID\")\n\n# Caso 3: imaginemos queremos pegar bases de datos considerando más\n#         de un identificador.\ndata_junta <- join(df1, df2, by = c(\"nombre\" = \"primernombre\",\n                                         \"apellido\" = \"primerapellido\",\n                                         \"Edad\" = \"edad\"))\n# Left join: la primera base de datos que aparece es la referencia.\n# Right join: la segunda base de datos que aparece es la referencia.\n\n# Left_join\nleft_join_ex1 <- left_join(linelist_mini,\n                           hosp_info,\n                           by = c(\"hospital\" = \"hosp_name\"))\n\n\nright_join_ex1 <- right_join(hosp_info,\n                             linelist_mini,\n                             by = c(\"hosp_name\" = \"hospital\"))\n\n# Ambos son equivalentes. \nright_join_ex2 <- right_join(linelist_mini,\n                             hosp_info,\n                             by = c(\"hospital\" = \"hosp_name\"))\n\nleft_join_ex2 <- linelist_mini %>%\n  left_join(hosp_info,\n            by = c(\"hospital\" = \"hosp_name\"))\n\n# Notas:\n\n# 1. Todas las filas/observaciones de la base de datos de referencia\n#    se mantienen.\n# 2. Si hay más de un match, se duplican las observaciones.\n# 3. Los identificadores se combinan. Según el nombre\n#     de la columna de la base de datos de referencia.\n# 4. Cuando no hay un \"match\" las columnas se llenan con un NA\n#     para las observaciones de la base de referencia.\n# 5. No \"match\" por la base que no es de referencia se borran.\nfull_join_ex3 <- full_join(linelist_mini,\n                           hosp_info,\n                           by = c(\"hospital\" = \"hosp_name\"))\n\nfull_join_ex3 <- linelist_mini %>%\n  full_join(hosp_info,\n            by = c(\"hospital\" = \"hosp_name\"))\n# Su análogo en stata es:  merge 1:1, keep _merge==3\n# Los que se pegaron perfectamente entre ambas bases.\n\ninner_join_example <- linelist_mini %>%\n  inner_join(hosp_info,\n             by = c(\"hospital\" = \"hosp_name\"))\nsemi_join_example1 <- semi_join(hosp_info, linelist_mini, \n                                by = c(\"hosp_name\" =\"hospital\"))\n\n\nsemi_join_example <- hosp_info %>%\n                     semi_join(linelist_mini,\n                     by = c(\"hosp_name\" = \"hospital\"))\nanti_join_example <- hosp_info %>%\n  anti_join(linelist_mini,\n            by = c(\"hosp_name\" = \"hospital\"))\nrm(list = ls())\n\nauthors <- data.frame(\n  surname = (c(\"Tukey\", \"Venables\", \"Tierney\", \"Ripley\", \"McNeil\")),\n  nationality = c(\"US\", \"Australia\", \"US\", \"UK\", \"Australia\"),\n  deceased = c(\"yes\", rep(\"no\", 4)))\nauthors\n\nbooks <- data.frame(\n  name = (c(\"Tukey\", \"Venables\", \"Tierney\",\n             \"Ripley\", \"Ripley\", \"McNeil\", \"R Core\")),\n  title = c(\"Exploratory Data Analysis\",\n            \"Modern Applied Statistics ...\",\n            \"LISP-STAT\",\n            \"Spatial Statistics\", \"Stochastic Simulation\",\n            \"Interactive Data Analysis\",\n            \"An Introduction to R\"),\n  other.author = c(NA, \"Ripley\", NA, NA, NA, NA,\n                   \"Venables & Smith\"))\nbooks\n\nm0 <- merge(authors,\n            books,\n            by.x = \"surname\", by.y = \"name\")\nm0\n\n# Por defecto solo mantiene las que hicieron match.\nm1 <-  merge(authors,\n             books,\n             by.x = \"surname\", by.y = \"name\", all = TRUE)\nm1"},{"path":"análisis-de-datos-con-r.html","id":"pegar-hacia-abajo-por-filas","chapter":"8 Análisis de datos con R","heading":"8.5.5.2 Pegar hacia abajo (por filas)","text":"Otra forma de unir bases de datos es agregar filas. Similar lo que se\nhace con append() en Stata. Vamos utilizar la función bind_rows()\ndesde el paquete “dplyr”.bind_rows() es bastante inclusivo. Cualquier columna presente en\nlas bases de datos se incluye en el output.Si ambas columnas se llaman igual, se alinearan correctamente.Adicionalmente, podemos agregar el argumento .id=. Este argumento\ngenera una nueva columna que sirve para identificar de donde\nproviene la informacion.Ejemplo 1: caso sencilloEjemplo 2: ¿Qué ocurre si hay más de un archivo?Ejemplo 3: ¿Qué ocurre si tengo muchos archivos?","code":"\ncontinente_resumen <- gapminder %>%\n  group_by(continent) %>%\n  summarise(\n    cases = n(),\n    gdpPercapmedian = median(gdpPercap, na.rm = TRUE))\ncontinente_resumen\n\n# Crear Tabla 2: sin agrupar\ntotales <- gapminder %>%\n  summarise(\n    cases = n(),\n    gdpPercapmedian =  median(gdpPercap, na.rm=T)\n  )\ntotales\n\n# Ahora las podemos pegar\ncombinadas <- bind_rows(continente_resumen, totales)\ncombinadas\n\n# ¿Como cambiar ese NA? \ncombinadas <- combinadas %>% \n              mutate(continent = replace_na(\"total\"))\ncombinadas\n\n# Muy util colocar \"id\"\ncombinadas_id <- bind_rows(continente_resumen, \n                           totales, \n                           .id = \"id\")\ncombinadas_id\n# Base de datos maestra\ntrial <- data.frame(\n  year    = c(2016, 2017, 2018, 2019),\n  n       = c(501, 499, 498, 502),\n  outcome = c(51, 52, 49, 50)\n) %>%\n  print()\n\n# Base de datos 1\ntrial_2020 <- data.frame(\n  year    = 2020,\n  n       = 500,\n  outcome = 48\n) %>%\n  print()\n\n# Base de datos 2\ntrial_2021 <- data.frame(\n  year      = 2021,\n  n         = 598,\n  outcome   = 57\n) %>%\n  print()\n\n# Para combinar mas de una base de datos\ntrial1<- bind_rows(trial,trial_2020, trial_2021)\nrm(list=ls())\nlibrary(pacman)\np_load(plyr)    # Recomendable ocupar paquete plyr.\n\nallfiles <- list.files(path = \"datos\",\n                       pattern = \".csv\",\n                       full.names = TRUE)\nallfiles\n\n# Append data\ncombined_data<- ldply(allfiles, read_csv)\ncombined_data\n\n# Transformar a un data.frame.\ncombined_data_sep <- separate(data = combined_data,\n                              col = \"year;n;outcome\",\n                              into = c(\"year\", \"n\", \"outcome\"),\n                              sep = \";\")\ncombined_data_sep"},{"path":"análisis-de-datos-con-r.html","id":"análisis-de-datos","chapter":"8 Análisis de datos con R","heading":"8.6 Análisis de datos","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"inspección-de-datos","chapter":"8 Análisis de datos con R","heading":"8.6.1 Inspección de datos","text":"Preámbulo:Ahora vamos crear tablas de estadística descriptiva que nos interesen\npara el análisis de datos.Transformamos variables de interés:","code":"\n# Limpiamos consola\nrm(list = ls())\n\n# Cargamos paquetes que vamos a utilizar\npacman::p_load(\n  rio,        # importar/exportar datos. \n  here,       # escribir rutas de las carpetas.  \n  janitor,    # limpiar datos y tablas.\n  tidyverse,  # Manejo de bases de datos y visualización. \n  magrittr,    # Permite utilizar operador %>% (piping). \n  skimr,       # Inspeccionar datos\n  inspectdf,   # Inspeccionar datos\n  gapminder    # Base de datos con información de países. \n)\n# Importamos datos\ndatos <- import(here(\"datos\",\"linelist_cleaned.rds\"))\n\n#Inspeccionamos los datos\n\n# a.  Visión general de la base de datos\nskim(datos)\n\n# b. Información sobre cada columna\nsummary(datos)\n\n\n# c. Información sobre cada variable categórica. \ninsp <- inspect_cat(datos)\ninsp\ninsp_figura <- insp %>% show_plot()\ninsp_figura\n\n# d. Miramos nombre de las variables\nnames(datos)\n\n# Nota 1: Tenemos 29 variables y el id es igual a case_id. \n# a. Miramos la clase de cada variable \nunlist(lapply(datos,class))\n\n# Nota 2: gender esta como texto, podría estar como factor. Lo mismo ocurre con outcome. \n\n# b. Transformamos gender/outcome en factores \ndatos$gender <- as.factor(datos$gender)\nlevels(datos$gender)\n\ndatos$outcome<- as.factor(datos$outcome)\nlevels(datos$outcome)\n\n# Miramos como esta codificado ahora \nunlist(lapply(datos,class))[\"gender\"]\nskim(datos)\n\n# c. Seleccionamos solo variables que nos interesa ocupar: reducir el problema!\ndatos_trabajo <- datos %>% select(case_id, outcome, gender, age, \n                                  age_years, age_cat, hospital, wt_kg:temp, \n                                  days_onset_hosp)\n\n# d. Vemos que hay variables que están codificadas como \"yes\" y \"no\". \n#    Vamos a crearlas como variables binarias. \n\ndatos_trabajo <-  datos_trabajo %>%  \n  mutate(chills =  ifelse(chills == \"yes\",1,0),\n         cough  =  ifelse(cough == \"yes\",1,0), \n         aches  =  ifelse(aches == \"yes\",1,0), \n         vomit  =  ifelse(vomit == \"yes\",1,0))\n\nsummary(datos_trabajo)\n\n# Limpiamos para quedarnos solo con los datos que nos interesan. \nrm(insp, insp_figura)\n\n# Inspeccionamos nuevamente\nskim(datos_trabajo)\n\n# e. Ahora vamos a dejar una base de datos unicamente con valores completos\ncompletos <- complete.cases(datos_trabajo)\ncompletos \ndatos_trabajo <- datos_trabajo[completos,]\nskim(datos_trabajo)            \n\n# f. Finalmente, vamos a renombrar la base de datos con la que vamos a trabajar.  \nrm(datos, completos)\ndatos <- datos_trabajo\nrm(datos_trabajo)"},{"path":"análisis-de-datos-con-r.html","id":"estadística-descriptiva","chapter":"8 Análisis de datos con R","heading":"8.6.2 Estadística descriptiva","text":"Describiendo los datos:","code":"\npacman::p_load(rstatix)\n\n# Opcion 1: utilizar get_summary_stats() del paquete \"rstatix\". \n# El resultado se guarda en un dataframe. \ntabla1 <- datos  %>% \n  get_summary_stats(\n    everything(),  \n    type = \"full\")                    \ntabla1 \n\n# Puedo exportar esta tabla en excel utilizando `rio` y `here`.\nexport(tabla1, here(\"resultados\", \n                    \"ejercicio1\", \n                    \"tabla1.xlsx\"))"},{"path":"análisis-de-datos-con-r.html","id":"tablas-de-frecuencia","chapter":"8 Análisis de datos con R","heading":"8.6.3 Tablas de frecuencia","text":"El paquete “janitor” ofrece la función tabyl() para producir\ntabulaciones simples y tabulaciones cruzadas, que pueden ser “adornadas”\no modificadas con funciones de ayuda para mostrar porcentajes,\nproporciones, recuentos, etc.El uso por defecto de tabyl() en una columna específica produce los\nvalores únicos, los recuentos y los “porcentajes” de la columna (en\nrealidad proporciones). Las proporciones pueden tener muchos dígitos.\nPuede ajustar el número de décimales con la función adorn_rounding()\ncomo se describe continuación.","code":""},{"path":"análisis-de-datos-con-r.html","id":"tablas-de-frecuencias-con-una-entrada","chapter":"8 Análisis de datos con R","heading":"8.6.3.1 Tablas de frecuencias con una entrada","text":"Ahora voy generar tantas tablas como variables categóricas para ello\nvamos hacer uso de iteraciones. Creo una lista vacía, donde voy \nguardar las tablas (data frames) que se generen.Ahora tengo una lista de tablas. Cada tabla habla de un variable\ncategórica. Si vemos son equivalente generarlas de otra forma. Si\ntengo pocas variables categóricas, esta forma puede ser relevante,\nsin embargo, si tengo muchas, puede ser una buena forma de generar\nestadística descriptiva.","code":"\n# Frecuencia para age_cat \ntabla2a <- datos %>% tabyl(outcome)\ntabla2a \n\ntabla2b <- datos %>% tabyl(gender)\ntabla2b \n\ntabla2c <- datos %>% tabyl(age_cat)\ntabla2c \n\nrm(tabla2a, tabla2b, tabla2c)\n\n# ¿Qué pasa si queremos hacer una tabla de frecuencias para \n# todas las variables que son factores? -> iteradores! \n\n# Nombre de todas las variables de la base de datos\nnombres <- names(datos)\nnombres \n\n# Selecciono solo a las que son categóricas (factores)\nselect <- unlist(lapply(datos, is.factor))\nselect \n\nnombres <- nombres[select == TRUE]\nnombres \ntabla2 <-  list()\n\ntabla2a <- datos %>% tabyl(outcome)\ntabla2a \n\ntabla2b <- datos %>% tabyl(gender)\ntabla2b \n\ntabla2c <- datos %>% tabyl(age_cat)\ntabla2c \n\nfor (i in 1:length(nombres)){\n  \n  tabla2[[i]] <- datos %>% tabyl(nombres[i])\n}\n# Comparamos \ndatos %>% tabyl(outcome,show_na = FALSE)\ntabla2[[1]]\n\ndatos %>% tabyl(gender,show_na = FALSE)\ntabla2[[2]]\n\n# Exportamos utilizando `rio` y `here` de nuevo \nexport(tabla2[[1]],here(\"resultados\", \"ejercicio1\", \"tabla2-1-freq-outcomes.xlsx\"))\nexport(tabla2[[2]],here(\"resultados\", \"ejercicio1\", \"tabla2-2-freq-gender.xlsx\"))\nexport(tabla2[[3]],here(\"resultados\", \"ejercicio1\", \"tabla2-3-freq-age_cat.xlsx\"))"},{"path":"análisis-de-datos-con-r.html","id":"tablas-de-frecuencias-con-más-de-una-entrada","chapter":"8 Análisis de datos con R","heading":"8.6.3.2 Tablas de frecuencias con más de una entrada","text":"Imaginen que estamos interesados o interesadas en saber cuantas\nobservaciones por categoría de edad por genero. Para ello nos gustaría\nhacer una tabla de doble entrada. Podemos ocupar tabyl() para hacer\neso fácilmente:","code":"\ntabla3 <- datos  %>% tabyl(age_cat, gender, show_na = FALSE)\ntabla3 \nrm(tabla3)\n\n# Hacemos una lista \ntabla3 <- list()\n\ntabla3[[1]] <- datos %>% tabyl(outcome, gender)\ntabla3[[2]] <- datos %>% tabyl(outcome, age_cat)\ntabla3[[3]] <- datos %>% tabyl(age_cat, gender)\n\ntabla3[[1]]\ntabla3[[2]]\ntabla3[[3]]\n\n# Exportamos pero a diferentes hojas\nexport(list(\"tabla 3.1\" = tabla3[[1]],\"tabla 3.2\" = tabla3[[2]],\n            \"tabla 3.3\" = tabla3[[3]]),   #Podemos asignarles nombres\n       here(\"resultados\", \"ejercicio1\", \"tabla3_cruzadas.xlsx\"))"},{"path":"análisis-de-datos-con-r.html","id":"tablas-mejoradas-con-opciones","chapter":"8 Análisis de datos con R","heading":"8.6.3.3 Tablas mejoradas con opciones","text":"","code":"\n# Tabular los recuentos y las proporciones por categoría de edad\n\n# Con porcentajes para una de una entrada\ntabla4 <- datos %>%               \n  tabyl(age_cat) %>%       \n  adorn_pct_formatting()   # convertir proporciones en porcentajes\ntabla4\n\n# Con porcentajes por fila\ntabla5 <- datos  %>%                                  \n  tabyl(age_cat, gender) %>%                  # contar para edad y género\n  adorn_totals(where = \"row\") %>%             # agregar totales\n  adorn_percentages(denominator = \"row\") %>%  # cambiar a porcentaje\n  adorn_pct_formatting(digits = 2)            # numero de digitos\ntabla5 \n\n# Nota: ¿Qué ocurre si cambio col/row en adorn_percentages? \n\n# Con porcentajes  por columna \ntabla6 <- datos  %>%                                  \n  tabyl(age_cat, gender) %>%                  # contar para edad y genero\n  adorn_totals(where = \"row\") %>%             # agregar totales\n  adorn_percentages(denominator = \"col\") %>%  # cambiar a porcentaje\n  adorn_pct_formatting(digits = 2)            # nùmero de digitos\n\ntabla6 \n\n\n# Con total  por columna y fila \ntabla7 <- datos  %>%                                  \n  tabyl(age_cat, gender) %>%                  # contar para edad y genero\n  adorn_totals(where = list(\"row\",\"col\")) %>%             # agregar totales\n  adorn_percentages(denominator = \"col\") %>%  # cambiar a porcentaje\n  adorn_pct_formatting(digits = 2)            # nùmero de digitos\ntabla7 \n\n# ¿Como se diferencian estas tablas? \ntabla4\ntabla5 # por fila\ntabla6 # por la columna\ntabla7 # agrega dos totales\n\n# Agregamos titulos \ntabla8 <- datos %>%                                 \n  tabyl(age_cat, gender) %>%                  \n  adorn_totals(where = \"row\") %>%            \n  adorn_percentages(denominator = \"row\") %>%  # \n  adorn_pct_formatting(digits = 2) %>%        \n  adorn_ns(position = \"rear\") %>%  # front para colocar parentesis en %        \n  adorn_title(                     # Agregar titulos            \n    row_name = \"Cat. edad\",\n    col_name = \"Genero\")\ntabla8\n\n# Exportamos tablas utilizando `rio` y `here`. \nlista <- list(tabla4,tabla5,tabla6,tabla7,tabla8)\nlista\n\nfor (i in 1:5){\n  export(lista[i],here(\"resultados\", \"ejercicio1\", \n                       paste(\"tabla\",i + 3,\".xlsx\",sep=\"\")))\n}\n\npacman::p_load(flextable)\n\n# Exportamos la tabla directamente desde flextable\ntabla8_imagen_word <- datos %>%\n  tabyl(age_cat, gender) %>% \n  adorn_totals(where = \"col\") %>% \n  adorn_percentages(denominator = \"col\") %>% \n  adorn_pct_formatting(digits = 2) %>% \n  adorn_ns(position = \"front\") %>% \n  adorn_title(\n    row_name = \"Categoria Edad\",\n    col_name = \"Genero\",\n    placement = \"combined\") %>% \n  flextable::flextable() %>%                  \n  flextable::autofit() %>%                       \n  flextable::save_as_docx(path = \"tabla8_imagen.docx\")  "},{"path":"análisis-de-datos-con-r.html","id":"estadística-descriptiva-con-dplyr","chapter":"8 Análisis de datos con R","heading":"8.6.4 Estadística descriptiva con “dplyr”","text":"Ahora vamos volver utilizar los verbos del paquete “dplyr” que vimos\nla sección anterior. Vamos generar las mismas cosas, pero de otra\nforma.Frecuencias:Porcentajes:Resumen:Estadísticas condicionales:Percentiles:Resumir datos agregados:Juntar tablas:","code":"\n# Removemos todo, menos datos \nrm(list=ls()[! ls() %in% c(\"datos\")])\n\n# 1. ¿Cuál es el total de datos?\ntabla1 <- datos %>%                 \n          summarise(n_rows = n())   \ntabla1 \n\n# 2. ¿Cuál es el total por categorías de edad? (con group_by y con summarize)?\ntabla2 <- datos %>% \n  group_by(age_cat) %>%     \n  summarise(n_rows = n())   \ntabla2 \n\n# 3. ¿Cuál es el total por categorías de edad (con count)?\ntabla3 <- datos %>% \n    count(age_cat)\ntabla3 \n\n# 4. ¿Cuál es el total por categorías de edad y outcome (con count)?\ntabla4 <- datos %>% \n  count(age_cat, outcome)\ntabla4 \n\n# 5. Equivalente con group_by()\ntabla4a <- datos %>% \n  group_by(age_cat, outcome) %>% \n  summarise(n_rows = n())\ntabla4a\ntabla5 <- datos %>% \n  count(outcome, age_cat) %>%                     \n  mutate(                            \n    percent = (n / sum(n))*100) \ntabla5\n\ntabla6 <- datos %>%                  \n  group_by(outcome) %>%                          \n  count(age_cat) %>%                            \n  mutate(percent = (n / sum(n)*100)) \ntabla6 \ntabla7 <- datos %>%         # armamos la tabla como un nuevo objeto\n  group_by(hospital) %>%    # agrupamos todos los cálculos por hospital\n  summarise(                # generamos la estadística que nos interesa\n    cases       = n(),      # número de observaciones por grupo\n    delay_max   = max(days_onset_hosp, na.rm = TRUE),  # maximo retraso\n    delay_mean  = round(mean(days_onset_hosp, na.rm = TRUE), digits = 1),  # retraso promedio\n    delay_sd    = round(sd(days_onset_hosp, na.rm = TRUE), digits = 1),  # desviacion estandar retraso\n    delay_3     = sum(days_onset_hosp >= 3, na.rm = TRUE),   # número de hospitales on retrasos mayores a tres dias\n    pct_delay_3 = (delay_3 / cases))                    # porcentaje de lo anterior \ntabla7\ntabla8 <- datos %>% \n  group_by(hospital) %>% \n  summarise(\n    max_temp_fvr = max(temp[fever == \"yes\"], na.rm = TRUE),\n    max_temp_no = max(temp[fever == \"no\"], na.rm = TRUE)\n  )\ntabla8\n\n# ¿Cuál es la temperatura promedio para los pacientes que tuvieron fiebre y para los que no?\ntabla9 <- datos %>% \n  group_by(hospital) %>% \n  summarise(\n    promedio_temp_fvr = mean(temp[fever == \"yes\"], na.rm = TRUE),\n    promedio_temp_no = mean(temp[fever == \"no\"], na.rm = TRUE)\n  )\ntabla9\n# Percentiles por defecto de la variable edad (0%, 25%, 50%, 75%, 100%). \n# Usamos la función `quantile()`. \ntabla10 <- datos %>% \n  summarise(percentiles_edad = quantile(age_years, na.rm = TRUE))\ntabla10\n\n# Agregarlos manualmente\ntabla11 <- datos %>% \n  summarise(\n    percentiles_edad = quantile(\n      age_years,\n      probs = c(.05, 0.5, 0.75, 0.98), \n      na.rm=TRUE)\n  )\ntabla11 \n\n# También los podemos calcular combinando con otro verbos y mas especifico\ntabla12 <- datos %>% \n  group_by(hospital,outcome) %>% \n  summarise(\n    p05 = quantile(age_years, probs = 0.05, na.rm=TRUE),\n    p50 = quantile(age_years, probs = 0.5, na.rm=TRUE),\n    p75 = quantile(age_years, probs = 0.75, na.rm=TRUE),\n    p98 = quantile(age_years, probs = 0.98, na.rm=TRUE)\n  )\ntabla12\n\n# También podemos hacerlo utilizando `get_summary_stats()`\ntabla13 <- datos %>% \n  group_by(hospital) %>% \n  rstatix::get_summary_stats(age, type = \"quantile\")\ntabla13 \n# Contamos y quitamos los NA\ntabla14 <- datos %>% \n  drop_na(gender, outcome) %>% \n  count(outcome, gender)\ntabla14\n\n# Contamos por grupos y con condiciones. \ntabla15 <- tabla14 %>% \n  group_by(outcome) %>% \n  summarise(\n    total_cases  = sum(n, na.rm=T),\n    male_cases   = sum(n[gender == \"m\"], na.rm=T),\n    female_cases = sum(n[gender == \"f\"], na.rm=T))\ntabla15\n\n# Hacer lo mismo, pero para mas grupos y variables\ntabla16 <- datos %>% \n  group_by(outcome) %>% \n  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columnas\n                   .fns = mean,                               # funcion\n                   na.rm = TRUE))                             # opciones\ntabla16\n\n# Para todas las variables numéricas \ntabla17 <-\n  datos %>% \n  group_by(outcome) %>% \n  summarise(across(\n    .cols = where(is.numeric), \n    .fns = mean,\n    na.rm=T))\ntabla17 \n\n# De long a wide\npacman::p_load(scales)\n\ntabla18 <- datos %>%                  \n  group_by(outcome) %>%                         \n  count(age_cat) %>%                            \n  mutate(percent = scales::percent(n / sum(n))) \ntabla18 \n\n# La tabla esta en formato long! \ntabla19 <- tabla18 %>%  \n  select(-percent) %>%   # no quiero porcentaje\n  pivot_wider(names_from = age_cat, \n              values_from = n)  \ntabla19\n\n# Tabla con totales\ntabla20 <- datos %>% \n  group_by(gender) %>%\n  summarise(\n    known_outcome = sum(!is.na(outcome)),           # Número de filas del grupo en las que no falta el resultado\n    n_death  = sum(outcome == \"Death\", na.rm=T),    # Número de filas en el grupo donde el resultado es Muerte\n    n_recover = sum(outcome == \"Recover\", na.rm=T), # Número de filas del grupo cuyo resultado es Recuperado\n  ) %>% \n  adorn_totals() %>%                                # Adornar la fila total (suma de cada columna numérica)\n  adorn_percentages(\"row\") %>%                      # Proporciones\n  adorn_pct_formatting() %>%                        # porcetnaje\n  adorn_ns(position = \"rear\")                      # ()\ntabla20 \n\n# Exportamos todas las tablas \nlista_tablas <- list(tabla1,tabla2,tabla3,tabla4,tabla5,\n                     tabla6,tabla7,tabla8,tabla9,tabla10,\n                     tabla11,tabla12,tabla13,tabla14,tabla15,\n                     tabla16,tabla17,tabla18,tabla19,tabla20)\nlista_tablas\n\nfor (i in 1:20){\n  export(lista_tablas[i],here(\"resultados\", \"ejercicio2\", \n                              paste(\"tabla\",i,\".xlsx\",sep=\"\")))\n}\n# Algunas estadísticas por hospital y outcome\ntabla21 <-datos %>% \n  filter(!is.na(outcome) & hospital != \"Missing\") %>%  # Dejo todo lo que no sea missing\n  group_by(hospital, outcome) %>%                      # agrupo por hospital y outcome\n  summarise(                                            \n    N = n(),                                                 \n    ct_value = median(ct_blood, na.rm=T))               \ntabla21\n\n# Algunas estadísticas solo por outcome \ntabla22 <- datos %>% \n  filter(!is.na(outcome) & hospital != \"Missing\") %>%\n  group_by(outcome) %>%                            # Ahora agrupo solo por outcome, no hospital    \n  summarise(\n    N = n(),                                       # Estadisticas solo por outcome    \n    ct_value = median(ct_blood, na.rm=T))\ntabla22\n\n# Juntando \ntable_long <- bind_rows(tabla21, tabla22) %>% \n  mutate(hospital = replace_na(hospital, \"Total\"))\ntable_long\n\n# A formato long y a exportar \ntable_long %>% \n  mutate(hospital = replace_na(hospital, \"Total\")) %>% \n  pivot_wider(                                         # de largo a ancho\n    values_from = c(ct_value, N),                       # cambio valores\n    names_from = outcome) %>%                           # cambio columnas\n  mutate(                                              # agrego nuevas columnas\n    N_Known = N_Death + N_Recover,                               # numero total\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # % casos que murieron\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # % casos recuperados\n  select(                                              # reordeno columnas\n    hospital, N_Known,                                   # totales\n    N_Recover, Pct_Recover, ct_value_Recover,            # recuperado\n    N_Death, Pct_Death, ct_value_Death)  %>%             # muertes\n  arrange(N_Known)  %>%                                  # Ordeno \n  flextable::flextable() %>%                     # a imagen\n  flextable::autofit() %>%                       # una linea por fila \n  flextable::save_as_docx(path = here(\"resultados\",\"ejercicio2\",\"tablefinal.docx\"))  # exporto"},{"path":"análisis-de-datos-con-r.html","id":"visualización","chapter":"8 Análisis de datos con R","heading":"8.7 Visualización","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"visualización-r-base","chapter":"8 Análisis de datos con R","heading":"8.7.1 Visualización: R base","text":"Vamos hacer graficos unicamente utilizando R básico.","code":""},{"path":"análisis-de-datos-con-r.html","id":"plot","chapter":"8 Análisis de datos con R","heading":"8.7.1.1 Plot","text":"","code":"\n# Para visualizar datos \naltura <- c(168, 177, 177, 177, 178, 172, 165, 171, 178, 170)\npeso <-  c(88, 72, 85, 52, 71, 69, 61, 61, 51, 75)\n\n# Gráfico simple \nplot(altura,\n     peso, \n     ylab = \"Peso (kg)\", \n     xlab = \"Altura (cm)\")\n\n# Ahora lo vamos a exportar\nsetwd(here(\"figuras\"))\npdf(\"g1.pdf\")\nplot(altura,peso)\ndev.off()\nrm(altura,peso)"},{"path":"análisis-de-datos-con-r.html","id":"histograma-2","chapter":"8 Análisis de datos con R","heading":"8.7.1.2 Histograma","text":"Opciones:","code":"\n# Buscar ayuda: \n?hist()\n\n# Implementarla \ng2 <- hist(wage,\n      breaks = 20,   # número de intervalos\n      main = \"Distribución del Salario (dólares por hora)\",    # título del gráfico\n      xlab = \"Salario\",                                        # título del eje x\n      ylab = \"Número de personas\"                              # título del eje y\n)\ng2\n\n# Podemos agregar colores\ng2 <- hist(wage,\n     breaks = 20,   # n?mero de intervalos\n     main = \"Distribución del Salario (dólares por hora)\",    # título del gráfico\n     xlab = \"Salario\",                                        # título del eje x\n     ylab = \"Numero de personas\",                              # título del eje y\n     col = \"pink\"\n)\n\ng2\n\n# También podemos agregar límites en los ejes\ng2 <- hist(wage,\n     breaks = 20,   # número de intervalos\n     main = \"Distribución del Salario (dólares por hora)\",    \n     xlab = \"Salario\",                                        \n     ylab = \"Número de personas\",                              \n     col=\"pink\",\n     xlim = c(0,50),# Límites del histograma\n     freq = TRUE #TRUE = freq. absoluta, FALSE: relativa. \n)\ng2\n\n\n# Podemos hacer lo mismo, pero  agregar la distribución empirica\npdf(\"g3.pdf\")\ng3 <- hist(wage,\n     breaks = 20,   # número de intervalos\n     main = \"Distribución del Salario (dólares por hora)\",    \n     xlab = \"Salario\",                                        \n     ylab = \"Número de personas\",                              \n     col=\"pink\",\n     xlim = c(0,30),\n     ylim = c(0, .12), # Límites del histograma\n     freq = FALSE, #TRUE = freq. absoluta, FALSE: relativa. \n)\n\n\n# Agrego la distribución empirica de los datos \nlines(density(wage), col=\"blue\", lwd=2)  \n# lwd = line width\n# col = color\n# lty = line type\n# opciones de lty (0 = blank,  1 = solid (default), \n#                              2 = dashed, 3 = dotted, \n#                              4 = dotdash, 5 = longdash, \n#                              6 = twodash) \n\n# Agregar más de una distribución \nlines(density(wage, adjust=2), col=\"red\", lwd=2, lty=2) # adjust=2 lo que hace es suavizar un poco la curva \n\n# Agregar el promedio y la mediana \n\n# Promedio \nabline(v = mean(wage), \n       lwd = 2, \n       lty = 3, \n       col=\"darkgreen\")\n\n# Mediana \nabline(v = median(wage), \n       lwd = 2, \n       lty = 3, \n       col=\"darkblue\")    \ndev.off()\n\n# Notar que solo hemos utilizado paquetes básicos de R.\n# Ahora, podemos exportar el gráfico a la carpeta."},{"path":"análisis-de-datos-con-r.html","id":"gráfico-de-dispersión","chapter":"8 Análisis de datos con R","heading":"8.7.1.3 Gráfico de dispersión","text":"","code":"\n# Básico \ndev.off()\nplot(experience, wage)\nplot(wage~experience)\n\n# Mejorando la presentación \nplot (wage ~ experience,  \n      main = \"Salario en función de la experiencia\",\n      xlab = \"Experiencia (en años)\",\n      ylab = \"Salario\"\n)\n\n# Podemos agrupar \nCPS1985 <- CPS1985 %>% mutate(sexo = as.numeric(gender))\n\npdf(\"g4.pdf\")\ng4 <- plot (wage ~ experience, \n      data = CPS1985, \n      pch = sexo, #Diferente simbolo dependiendo del género\n      col = sexo,  #Color diferenciado por género\n      main = \"Salario en función de la experiencia\",\n      xlab = \"Experiencia (en años)\",\n      ylab = \"Salario\"\n)\n\n# Agregamos Leyenda \nlegend(\"topright\", \n       legend=c(\"Hombres\",\"Mujeres\"), \n       pch=1:2, \n       col=1:2,\n       bty=\"n\") #Caja de la leyenda\n\nlegend(\"topleft\", legend=c(\"Hombres\",\"Mujeres\"), pch=1:2, col=1:2)\n\n# Agregamos una regresión \nwith(CPS1985[gender==\"male\",], \n     abline(lm(wage~experience), col=\"black\"))\n\nwith(CPS1985[gender==\"female\",], \n     abline(lm(wage~experience), col=\"red\"))\ndev.off()"},{"path":"análisis-de-datos-con-r.html","id":"gráfico-de-barras","chapter":"8 Análisis de datos con R","heading":"8.7.1.4 Gráfico de barras","text":"","code":"\n# Primero tenemos que crear las frecuencias \nfrecuencias <- table(occupation)  # con variable categórica\nfrecuencias\n\n# Básico\nbarplot(frecuencias,\n        col = c(1:6)\n)\n\n# Ahora con educación \neducacion <- table(education)\npdf(\"g5.pdf\")\ng5 <-  barplot(educacion,\n        col = 1:dim(educacion),  # para establecer el número de diferentes colores  (=diferentes valores)\n        horiz = FALSE,            # orientación de las barras\n        ylim = c(0,250)\n)\n\n# Agregamos un título\ntitle(\"Distribución de la educación\", \n      xlab= \"Educación (en años)\", \n      ylab = \"Número de personas\")\ndev.off()"},{"path":"análisis-de-datos-con-r.html","id":"gráfico-de-torta","chapter":"8 Análisis de datos con R","heading":"8.7.1.5 Gráfico de torta","text":"","code":"\ndatos <- table(ethnicity)\ndatos\npie(datos, \n       labels=c(\"Caucasicos\", \"Hispanos\", \"Otros\"), \n       col = heat.colors(3),\n       main = \"Frecuencia de etnias\",\n       clockwise = FALSE)\n\n# Podemos cambiar los colores \ncolores<-c(\"darkred\",\"pink\",\"red\")\n\npdf(\"g6.pdf\")\ng6 <- pie(datos, \n    labels = levels(ethnicity), \n    col = colores,\n    main = \"Frecuencia etnias\")\ndev.off()"},{"path":"análisis-de-datos-con-r.html","id":"boxplot","chapter":"8 Análisis de datos con R","heading":"8.7.1.6 Boxplot","text":"Algunas cosas que tener en cuenta sobre un boxplot:Se le llama grafico de caja y bigote.Primer quartil (1): borde inferior de la caja.Mediana: Linea de al medio de la caja.Tercer quartil: Borde superior de la caja.Rango intercuartil: Diferencia entre el tercer cuartil y el primero.Sobre el gráfico:Caja más grande implica que los datos están mas dispersos.Tamaño de la caja es el rango intercuartil.Si la mediana esta al centro, la distribución es simétrica.¿Que pasa si la parte superior (sobre la linea) es más grande? Los\ndatos se concentran en la parte baja de la distribución.Los bigotes determinan el límite para valores atípicos.Su longitud\nmáxima es de un 150% del Rango intercuartil (RIC).","code":"\ndev.off()\nboxplot(wage,\n          main = \"Salario (dólares por hora)\",\n          ylab = \"Salario\",\n          col = \"pink\",\n          border = \"red\")\n\n# Separando por sexo\nboxplot(wage ~ gender,\n        main = \"Salario (dólares por hora) según sexo\",\n        ylab = \"Salario\",\n        col = c(\"pink\",\"darkgreen\"),\n        border = \"black\")\n\n\n#Horizontal\nboxplot(wage ~ gender,\n        main = \"Salario (dólares por hora) según sexo\",\n        ylab = \"Salario\",\n        names = c(\"Hombres\",\"Mujeres\"),    #Cambiar nombres\n        horizontal = T,                    #Posición horizontal\n        col = rainbow(2, alpha=0.8),       #Paleta rainbow (aleatoria) y transparencia (alpha)\n        border = \"black\")"},{"path":"análisis-de-datos-con-r.html","id":"uso-paquete-ggplot","chapter":"8 Análisis de datos con R","heading":"8.7.2 Uso paquete ggplot","text":"Ahora vamos utilizar un paquete avanzado de visualización. Primero,\nalgunos aspectos relacionados con la lógica de su uso.Utilizar ggplot2 es básicamente ir agregando capas. Entre cada\ncosa que quiero agregar al gráfico debo colocar un signo +.La sintaxis básica incluye:Comience con el comando ggplot() de la línea de base, esto\n“abre” el ggplot y permite que las funciones subsecuentes sean\nagregadas con +. Normalmente el conjunto de datos también se\nespecifica en este comando.\nComience con el comando ggplot() de la línea de base, esto\n“abre” el ggplot y permite que las funciones subsecuentes sean\nagregadas con +. Normalmente el conjunto de datos también se\nespecifica en este comando.Añadir capas “geom”, estas funciones visualizan los datos como\ngeometrías (formas), por ejemplo, como un gráfico de barras, un\ngráfico de líneas, un gráfico de dispersión, un histograma (¡o\nuna combinación!). Todas estas funciones comienzan con geom_\ncomo prefijo.\nAñadir capas “geom”, estas funciones visualizan los datos como\ngeometrías (formas), por ejemplo, como un gráfico de barras, un\ngráfico de líneas, un gráfico de dispersión, un histograma (¡o\nuna combinación!). Todas estas funciones comienzan con geom_\ncomo prefijo.Añade elementos de diseño al gráfico, como etiquetas de ejes,\ntítulos, fuentes, tamaños, esquemas de color, leyendas o\nrotación de ejes.\nAñade elementos de diseño al gráfico, como etiquetas de ejes,\ntítulos, fuentes, tamaños, esquemas de color, leyendas o\nrotación de ejes.Ejemplo:Ahora ya entendemos la sintaxis general. Para cambiar de tipo de\ngráficos tenemos distintas geometrías.","code":"\nggplot(datos, aes(x = experience, y = wage)) +                               \n  geom_point( color = \"blue\") +                          \n  labs()+                                    \n  theme()  \n  # geom_point (para puntos)\n  # geom_line (para lineas)\n  # geom_histogram (para histograma)\n  # geom_boxplot (para boxplot)\n  # geom_bar o geom_col() (para barras)\n  # geom_smooth (lineas suavizadas)"},{"path":"análisis-de-datos-con-r.html","id":"geometrías","chapter":"8 Análisis de datos con R","heading":"8.7.2.1 Geometrías","text":"Otros tienen más sentido con una variable","code":"\nggplot(datos, aes(x = experience))+                               \n  geom_histogram() +                         \n  labs()+                                   \n  theme() \n\nggplot(datos, mapping = aes(x = experience, y = wage))+                               \n  geom_smooth(color = \"red\") +                         \n  labs()+                                   \n  theme() "},{"path":"análisis-de-datos-con-r.html","id":"aesthetics","chapter":"8 Análisis de datos con R","heading":"8.7.2.2 Aesthetics","text":"Otro aspecto importante es lo que se denomina “aesthetics”. Estas son\nlas características visuales de los datos. En ggplot estos son\nmodificados dentro de la opción theme(). todas las geometrías\ntienen las mismas opciones. Sin embargo, algunas comunes son:shape: Mostrar un punto con geom_point() como punto, estrella,\ntriángulo o cuadrado.fill: El color interior (por ejemplo, de una barra o boxplot)color: La línea exterior de una barra, boxplot, etc., o el color\ndel punto si se utiliza geom_point()size: Tamaño (por ejemplo, grosor de la línea, tamaño del punto)alpha: Transparencia (1 = opaco, 0 = invisible)binwidtho: Ancho de los bins o cajas del histogramawidth: Anchura de las columnas del “bar plot”.linetype: Tipo de línea (por ejemplo, sólida, discontinua,\npunteada)Los “aesthetics” pueden ser asignados valores o vectores.Ejemplo:","code":"\n# Visualización de puntos \nggplot(datos, aes(x = experience, y = wage))+                               \n  geom_point(color = \"darkgreen\",\n             size  = 2.8, \n             alpha = 0.2) + theme()\n\n# Histograma \nggplot(datos, mapping = aes(x = wage))+                               \n  geom_histogram(color = \"white\", \n                 fill  = \"pink\", \n                 binwidth = 5 , \n                 alpha = 1)"},{"path":"análisis-de-datos-con-r.html","id":"escalar-los-valores-o-agrupar","chapter":"8 Análisis de datos con R","heading":"8.7.2.3 Escalar los valores o agrupar","text":"","code":"\n# Si queremos diferenciar alguna variable categorica por color (agrupar)\nggplot(datos, aes(experience, wage, color = gender)) + \n  geom_point()\n\n# Si queremos diferenciar alguna variable continua por tamaño (escalar)\nggplot(datos, aes(x = experience,\n                  y = wage,\n                  size = age)) + \n  geom_point(shape = \"circle\",\n             alpha = 0.3)\n\n# Ambos (agrupar y escalar)\nggplot(data = datos, \n       mapping = aes(x = experience,\n                     y = wage,\n                     size = age,\n                     color = gender)) + \n  geom_point(shape = \"circle\",\n             alpha = 0.3)"},{"path":"análisis-de-datos-con-r.html","id":"equivalencia-sintaxis-ggplot","chapter":"8 Análisis de datos con R","heading":"8.7.2.4 Equivalencia sintaxis ggplot","text":"","code":"\n# Notar que estas tres sintaxis son equivalentes\nggplot(data = datos, \n       mapping = aes(x = age))+\n  geom_histogram()\n\nggplot(data = datos)+\n  geom_histogram(mapping = aes(x = age))\n\nggplot()+\n  geom_histogram(data = datos, mapping = aes(x = age))"},{"path":"análisis-de-datos-con-r.html","id":"combinar-gráficos","chapter":"8 Análisis de datos con R","heading":"8.7.2.5 Combinar gráficos","text":"facet_wrap(): Mostrar un panel diferente para cada nivel de una sola\nvariable. Un ejemplo de esto podría ser mostrar una figura diferente\npara cada region de un país. Las facetas se ordenan alfabéticamente, \nmenos que la variable sea un factor con otro ordenamiento definido. Por\nejemplo:facet_grid(): Se utiliza cuando se quiere introducir una segunda\nvariable en la disposición de las facetas. Aquí cada panel de una\ncuadrícula muestra la intersección entre los valores de dos columnas.\nPor ejemplo:","code":"\n# Datos \nmalaria_data <- import(here(\"datos\", \"malaria_facility_count_data.rds\")) %>%  \n  select(-submitted_date, -Province, -newid)                                 \n\n# A plot with facets by district\nggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n    geom_col(width = 1, fill = \"darkred\") +   # graficamos datos como columnas\n    theme_minimal() +         # simplificamos la parte de atras\n    labs(x = \"Fecha\",         #Etiquetas/labels\n         y = \"Casos de Malaria\",\n         title = \"Casos de Malaria por distrito\") +\n    facet_wrap(~District)   # creamos las facetas\nmalaria_age <- malaria_data %>%\n  select(-malaria_tot) %>% \n  pivot_longer(\n    cols = c(starts_with(\"malaria_rdt_\")),  \n    names_to = \"grupo_edad\",      \n    values_to = \"num_casos\"      \n  ) %>% \n  mutate(\n    age_group = str_replace(grupo_edad, \"malaria_rdt_\", \"\"))\n\nggplot(malaria_age, aes(x = data_date, y = num_casos)) +\n  geom_col(fill = \"darkred\", width = 1) +\n  theme_minimal()+\n  labs( \n    x = \"Fechas\",\n    y = \"Casos\",\n    title = \"Casos de Malaria por grupos de edad y distrito\") +\n  facet_grid(District ~ age_group)"},{"path":"análisis-de-datos-con-r.html","id":"modificar-y-guardar","chapter":"8 Análisis de datos con R","heading":"8.7.2.6 Modificar y guardar","text":"Modificar graficos: Una cosa muy positiva de ggplot() es que uno puede\ndefinirlo con un nombre y luego ir sobre-escribiendolo. Eso es muy útil\npara la sintaxis.Exportar gráficos: Para los gráfico que se generen con ggplot() es\nposible utilizar la opción ggsave().","code":"\n# Gráfico original \nhistograma <- ggplot(data = datos, mapping = aes(x = age))+\n              geom_histogram()\nhistograma\n\n# Gráfico modificado. \nhistograma_modificado <- histograma +\n                         geom_vline(xintercept = 50)\nhistograma_modificado\n# Noten que solamente hemos agregado la nueva opción sobre el objeto asignado. \n# Muy útil y recomendable para hacer visualizaciones en R. \nggsave(here(\"figuras\", \"histograma.pdf\"), histograma_modificado)"},{"path":"análisis-de-datos-con-r.html","id":"labels","chapter":"8 Análisis de datos con R","heading":"8.7.2.7 Labels","text":"","code":"\ndistribucion_salarios <- ggplot(\n  data = datos,      # datos\n  mapping = aes(     # ejes\n    x = age,                        \n    y = wage,         \n    color = occupation))+     # agrupo\n  geom_point()+               # geometria\n  labs(\n    title = \"Salarios por edad\",\n    subtitle = \"Estados Unidos, 1985\",\n    x = \"Edad en años\",\n    y = \"Salario en dólares por hora\",\n    color = \"Ocupación\",\n    caption = stringr::str_glue(\"Maxima edad es: {max(datos$age, na.rm=TRUE)}\"))\n\ndistribucion_salarios"},{"path":"análisis-de-datos-con-r.html","id":"temas","chapter":"8 Análisis de datos con R","heading":"8.7.2.8 Temas","text":"","code":"\n# Clasico \ndistribucion_salarios + theme_classic()\n\n# Mínimo \ndistribucion_salarios + theme_minimal()\n\n# Oscuro \ndistribucion_salarios + theme_dark()\n\n# Claro\ndistribucion_salarios + theme_light()\n\n# Gris \ndistribucion_salarios + theme_grey()\n\n# Blanco y negro \ndistribucion_salarios + theme_bw()"},{"path":"análisis-de-datos-con-r.html","id":"piping-y-ggplot","chapter":"8 Análisis de datos con R","heading":"8.7.2.9 Piping y ggplot","text":"","code":"\nrm(list = ls())\n  datos <- import(here(\"datos\", \"linelist_cleaned.rds\"))\n  \n  datos %>%                                                     \n    select(c(case_id, fever, chills, cough, aches, vomit)) %>%    \n    pivot_longer(                                                 \n      cols = -case_id,                                  \n      names_to = \"symptom_name\",\n      values_to = \"symptom_is_present\")%>%\n    mutate(                                                      \n      symptom_is_present = replace_na(symptom_is_present, \"unknown\")) %>% \n      ggplot(                                                        \n      mapping = aes(x = symptom_name, fill = symptom_is_present))+\n    geom_bar(position = \"fill\", col = \"black\") +                    \n    theme_minimal() +\n    labs(\n      x = \"Sintoma\",\n      y = \"Sintoma (status)\"\n    )"},{"path":"análisis-de-datos-con-r.html","id":"gráficos-para-variables-continuas","chapter":"8 Análisis de datos con R","heading":"8.7.2.10 Gráficos para variables continuas","text":"Histograma:Gráfico de densidad:Diagrama de dispersión (scatterplot):Boxplot:Gráfico de barras:","code":"\nrm(list = ls())\ndata(\"CPS1985\")\ndata <- CPS1985\nrm(CPS1985)\nattach(data)\n\n#Gráfico absoluto\nggplot(data, aes(x=wage)) + \n  geom_histogram()\n\n#Gráfico relativo\nggplot(data, aes(x=wage)) + \n  geom_histogram(aes(y=..density..))\n\n# Histograma con 20 intervalos\nggplot(data, aes(x=wage)) + \n  geom_histogram(bins=100, \n                 color=\"black\", \n                 fill=\"blue\", \n                 alpha = 0.8)\n\n#Agregando títulos y limites\nggplot(data, aes(x=wage)) + \n  geom_histogram(bins=20, color=\"white\", fill=\"blue\") +\n  labs(title = \"Distribución del salario (dólares por hora)\",\n       x = \"Salario\",\n       y = \"Número de empleados\") + \n  xlim(0,30)\n\n#Gráficos de subconjuntos\nggplot(data, aes(x=wage)) + \n  geom_histogram(bins=20, color=\"white\", fill=\"pink\") +\n  facet_grid(gender~.)\n\n# U horizontal\nggplot(data, aes(x=wage)) + \n  geom_histogram(bins=20, color=\"white\", fill=\"pink\") +\n  facet_wrap(~gender)+\n  labs(title = \"Distribución del salario (dólares por hora)\",\n       x = \"Salario\",\n       y = \"Número de empleados\") + xlim(0,30) + theme_minimal()\n\n# O ambos juntos \nggplot(data, aes(x=wage)) +\n  geom_histogram(bins=20, aes(fill=gender), \n                 position=\"fill\", \n                 alpha=0.6) +\n  labs(title = \"Distribución del salario (dólares por hora)\",\n       x= \"Salario\", \n       y=\"Empleados\", \n       fill=\"Género\") +  # títulos de ejes y leyenda\n  scale_fill_discrete(labels=c(\"Hombre\",\"Mujer\")) +    # títulos claves leyenda\n  xlim(0,30)\nggplot(data, aes(x=wage, color=gender)) +\n  geom_freqpoly(bins=20, aes(y=..density..)) +\n  labs(color=\"Género\")\n\n# Uniendo a y b\nggplot(data, aes(x=wage)) + \n  geom_histogram(aes(y=..density..), \n                 bins=20, \n                 color=\"white\", \n                 fill=\"pink\") + \n  #stat_function(fun = dnorm, colour = \"red\",\n  #              args = list(mean = mean(wage, na.rm = TRUE),\n  #                          sd = sd(wage, na.rm = TRUE))) +\n  geom_density(color=\"blue\")+\n  labs(title=\"Distribución del salario (dólares por hora)\",\n       x= \"Salario\", \n       y=\"Empleados\")\nggplot(data, aes(experience, log(wage))) +\n  geom_point() +\n  labs(title=\"Diagrama de dispersión\",\n       subtitle= \"ScatterPlot\",\n       caption=\"Fuente: CPS1985 (paquete AER)\",\n       x=\"Experiencia (en años)\",\n       y=\"Salario (en logaritmo)\")\n\n# Formas de darle color al gráfico\n\n  # (1) Desde geom_()\n  ggplot(data, aes(experience, log(wage))) +\n    geom_point(aes(color=gender))\n\n  # (2) Desde ggplot()\n  ggplot(data, aes(experience, log(wage), color=\"red\")) +\n    geom_point()\n\n# Función jitter (shortcut geom_point(position = \"jitter\")\nggplot(data, aes(experience,log(wage))) +\n  geom_jitter(width=.2, alpha=0.5) +               #Suavizar la visualización de solapamiento\n  labs(title=\"Diagrama de dispersión\",\n       subtitle= \"ScatterPlot\",\n       caption=\"Fuente: CPS1985 (paquete AER)\",\n       x=\"Experiencia (en años)\",\n       y=\"Salario (en logaritmo)\")\n\n# Diviendo por categorías\nggplot(data, aes(experience,log(wage), color=gender)) +\n  geom_jitter() +\n  geom_smooth(method=\"lm\") + \n  labs(title=\"Diagrama de dispersión\",\n       x=\"Experiencia (en años)\",\n       y=\"Salario (en logaritmo)\") +\n  scale_color_discrete(name=\"Género\", labels=c(\"Hombre\",\"Mujer\"))\n\n# Nota: Ojo con realizar de otra manera, cambia la recta de regresión\n\nggplot(data, aes(experience,log(wage))) +\n  geom_point(aes(color=gender)) +\n  geom_smooth(method=\"lm\") +\n  labs(title=\"Diagrama de dispersión\",\n       x=\"Experiencia (en años)\",\n       y=\"Salario (en logaritmo)\") +\n  scale_color_discrete(\"Género\", labels=c(\"Hombre\",\"Mujer\"))\n# Básico\nggplot(data, aes(x=gender, y=wage, fill=gender)) +\n  geom_boxplot() +\n  labs(title=\"Boxplot\",\n       x=\"Género\", \n       y=\"Salario\", \n       fill=\"Género\") +   # titulo ejes y leyenda\n  scale_x_discrete(labels=c(\"Hombre\",\"Mujer\")) +   # etiquetas del eje x\n  scale_fill_discrete(labels=c(\"Hombre\",\"Mujer\"))  # etiquetas claves leyenda\n\n#Visualizando todos los datos con paletas creadas (armamos una función)\nts4_colors <- c(\n  `pink`        = \"#B40586\",\n  `light blue`       = \"#00A2E9\",\n  `green`      = \"#00AC8B\",\n  `orange`     = \"#f37735\",\n  `red`     = \"#FF1B6B\",\n  `purple` = \"#805FAA\",\n  `blue`  = \"#0059ff\",\n  `light green` = \"#7EAF64\")\n\nts4_cols <- function(...) {\n  cols <- c(...)\n  if (is.null(cols)){\n    return (ts4_colors)\n  }\n  ts4_colors[cols]\n}\nts4_cols(\"red\")\nts4_cols()\n\nggplot(data, aes(x=gender, y=wage, fill=gender) ) +\n  geom_boxplot(alpha=0.3, \n               fill=ts4_cols(\"pink\",\"light green\"), \n               outlier.colour = ts4_cols(\"blue\")) +\n  labs(title=\"Boxplot\",x=\"Género\", y=\"Salario\") +\n  scale_x_discrete(labels=c(\"Hombre\",\"Mujer\")) +\n  guides(fill=FALSE) +\n  coord_flip() +              \n  geom_point(stat= \"summary\", shape=16, size=4, colour=ts4_cols(\"purple\")) +\n  geom_jitter(width = 0.1, alpha = 0.2, colour=ts4_cols(\"orange\"))\nggplot(data, aes(ethnicity)) +\n  geom_bar() +\n  labs(title=\"Diagrama de barras\",\n       x= \"Raza\",\n       y=\"Empleados\") +\n  scale_x_discrete(labels=c(\"Caucásico\", \"Hispano\", \"Otros\"))\n\n# Distintos colores\nggplot(data, aes(ethnicity, fill=ethnicity)) +\n  geom_bar() +\n  labs(title=\"Distribución de empleadores según etnia\",\n       x= \"Raza\",\n       y=\"Empleados\") +\n  scale_x_discrete(labels=c(\"Caucásico\", \"Hispano\", \"Otros\")) + guides(fill=FALSE)  # Sin/con leyenda\n\n# ¿Cómo se distribuye el género sobre la etnia?\nggplot(data, aes(ethnicity, fill=gender)) +\n  geom_bar() +\n  labs(title=\"Diagrama de barras\",\n       x= \"Etnia\",\n       y=\"Empleados\") +\n  scale_x_discrete(labels=c(\"Caucásico\", \"Hispano\", \"Otros\")) +\n  scale_fill_discrete(\"Género\", labels=c(\"Hombre\",\"Mujer\"))\n\n# Separado por género, pero paralelo\nggplot(data, aes(ethnicity, fill=gender)) +\n  geom_bar(position=\"dodge\") +            # también: position=position_dodge()\n  labs(title=\"Distribución de empleadores según etnia\",\n       x= \"Raza\",\n       y=\"Empleados\") +\n  scale_x_discrete(labels=c(\"Caucásico\", \"Hispano\", \"Otros\")) +\n  scale_fill_brewer(palette = \"Accent\",                             # Definir la paleta manualmente \n                    \"Género\",\n                    labels=c(\"Hombre\",\"Mujer\"))"},{"path":"análisis-de-datos-con-r.html","id":"gráficos-con-plotly","chapter":"8 Análisis de datos con R","heading":"8.7.2.11 Gráficos con plotly","text":"Ejemplo 1:Ejemplo 2:Recordatorio: Set seed o sembrar la semilla nos permite generar\ndatos seudo aleatorizados pero replicables, de manera que cualquier\npersona que acceda al script sea capaz de llegar los mismos resultados\nque nosotros. El número que se incluye dentro de la función actua como\nun “código” que genera estos datos aleatorios, y puede ser cualquiera.Ejemplo 3: Ocupar directamente plotly","code":"\ng1 <- ggplot(data, aes(ethnicity, fill=gender)) +\n  geom_bar(position=\"dodge\") +            # también: position=position_dodge()\n  labs(title=\"Distribución de empleadores según etnia\",\n       x= \"Raza\",\n       y=\"Empleados\") +\n  scale_x_discrete(labels=c(\"Caucásico\", \"Hispano\", \"Otros\")) +\n  scale_fill_brewer(palette = \"Accent\",     #Definir la paleta manualmente \n                    \"Género\",\n                    labels=c(\"Hombre\",\"Mujer\"))\ng1\n\nggplotly(g1)\nset.seed(100) #semilla\n\nd <- diamonds[sample(nrow(diamonds), 1000), ]\n\np <- ggplot(data = d, aes(x = carat, y = price)) +\n  geom_point(aes(text = paste(\"Clarity:\", clarity)), size = 1) +\n  geom_smooth(aes(colour = cut, fill = cut)) + facet_wrap(~ cut)\np\n\nggplotly(p)\ndata(iris)\n\nfig <- plot_ly(data = iris, \n               x = ~Sepal.Length, \n               y = ~Petal.Length, \n               color=~Species)\nfig\n\n# Sacar barra de opciones y zoom\nfig %>%\n  layout(yaxis = list(fixedrange = TRUE),\n         showlegend = TRUE,\n         xaxis = list(fixedrange = TRUE)) %>%\n  config(displayModeBar = FALSE)\n\n#Line plot\nx <- c(1:50)\nrandom_y <- rnorm(50, mean = 0)\ndata <- data.frame(x, random_y)\n\nfig <- plot_ly(data, x = ~x, \n               y = ~random_y, \n               type = 'scatter', \n               mode = 'lines')\nfig\n\n# Histogramas\nfig <- plot_ly(x = ~rnorm(50), \n               type = \"histogram\")\nfig\n\n# Con 2 grupos\nfig <- plot_ly(alpha = 0.6)\nfig <- fig %>% add_histogram(x = ~rnorm(500))\nfig <- fig %>% add_histogram(x = ~rnorm(500) + 1)\nfig <- fig %>% layout(barmode = \"overlay\")\nfig\n\n# Boxplot\nfig <- plot_ly(y = ~rnorm(50), type = \"box\")\nfig <- fig %>% add_trace(y = ~rnorm(50, 1))\nfig"},{"path":"análisis-de-datos-con-r.html","id":"análisis-de-datos-y-generación-de-informes-con-r-markdown","chapter":"8 Análisis de datos con R","heading":"8.8 Análisis de datos y generación de informes con R Markdown","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"test-de-hipótesis","chapter":"8 Análisis de datos con R","heading":"8.8.1 Test de hipótesis","text":"Imaginen que una compañia quiere testear la efectividad de una\nestrategia de venta. La estrategia consiste en colocar un letrero que\ndiga “oferta final”. La estrategia B consiste en dejar todo como\nsiempre. Aleatoriamente se eligen los días y lugares en donde se\nutilizara la estrategia y los días en que se utilizara la estrategia\nB. Luego, se registran los datos de ventas y se observan que las ventas\nde la compañía con la estrategia B fueron un 43,4% mayores las tiendas\ncon la estrategia . La intuición se contradice con los datos, sin\nembargo, ¿Es esto debido al azar? ¿o bien es un resultado\nestadísticamente significativo?","code":""},{"path":"análisis-de-datos-con-r.html","id":"test-de-diferencia-de-media","chapter":"8 Análisis de datos con R","heading":"8.8.1.1 Test de diferencia de media","text":"Una de las pruebas más comunes en estadística, la prueba t, se utiliza\npara determinar si las medias de dos grupos son iguales entre sí. La\nhipótesis de la prueba es que ambos grupos proceden de distribuciones\nnormales con varianzas iguales. La hipótesis nula es que las dos medias\nson iguales, y la alternativa es que lo son. Se sabe que bajo la\nhipótesis nula, podemos calcular un estadístico t que seguirá una\ndistribución t.También existe una modificación ampliamente utilizada de la prueba t,\nconocida como prueba t de Welch, que ajusta el número de grados de\nlibertad cuando se cree que las varianzas son iguales entre sí. Este\ntutorial cubre los fundamentos de la realización de pruebas t en R.t.test() puede utilizarse para realizar pruebas t de una y dos\nmuestras en vectores de datos. La función contiene una variedad de\nargumentos. La sintaxis es:Aquí \\(x\\) es un vector numérico de valores de datos e \\(y\\) es un vector\nnumérico opcional de valores de datos. Si se excluye \\(y\\), la función\nrealiza una prueba t de una muestra sobre los datos contenidos en \\(x\\),\nsi se incluye realiza una prueba t de dos muestras utilizando tanto \\(x\\)\ncomo \\(y\\).El argumento \\(mu\\) proporciona un número que indica el verdadero valor de\nla media (o la diferencia de medias si se realiza una prueba de dos\nmuestras) bajo la hipótesis nula. Por defecto, la prueba realiza una\nprueba t de dos caras; sin embargo, puede realizar una hipótesis\nalternativa cambiando el argumento alternativo “mayor” o “menor”\ndependiendo de si la hipótesis alternativa es que la media es mayor o\nmenor que \\(mu\\), respectivamente.Por ejemplo:Realiza una prueba t de una muestra sobre los datos contenidos en \\(x\\)\ndonde la hipótesis nula es que \\(mu = 25\\) y la alternativa es menor que\n25. El argumento var.equal indica si se deben asumir o varianzas\niguales al realizar una prueba t de dos muestras. Por defecto se asume\nuna varianza desigual.Por último, el argumento conf.level determina el nivel de confianza\ndel intervalo de confianza.","code":"\n# Datos \ndata(\"midwest\")\nnames(midwest)\nhead(midwest)\nskim(midwest)\n\nt.test(x, y = NULL, \n       alternative = c(\"two.sided\", \"less\", \"greater\"), \n       paired = FALSE,\n       mu = 0,\n       var.equal = FALSE, \n       conf.level = 0.95)\nt.test(x, alternative = \"less\", mu = 25)"},{"path":"análisis-de-datos-con-r.html","id":"t-test-de-una-cola","chapter":"8 Análisis de datos con R","heading":"8.8.1.2 t-test de una cola","text":"La prueba t de una muestra compara la media de una muestra con un valor\nconocido, cuando la varianza de la población es desconocida.\nConsideremos que queremos evaluar el porcentaje de adultos con estudios\nuniversitarios en el “midwest” y compararlo con un valor determinado.Por ejemplo, supongamos que la media nacional de adultos con estudios\nuniversitarios es del 32% (licenciatura o superior) y queremos ver si la\nmedia del “midwest” es significativamente diferente (en terminos\nestadísticos) de la media nacional. En concreto, queremos comprobar si\nla media del medio oeste es inferior la media nacional.El resultado nos indica que p < 0.001 lo que nos dice que rechazamos la\nhipótesis nula (de que es igual la nacional) lo que nos da evidencia\nestadística de que la media poblacional del “midwest” es menor que un\n32%. Pero, como vimos anteriormente, tenemos problemas de normalidad\n(recuerden el gráfico con las distribuciones empíricas). Para\nasegurarnos de que nuestra conclusión es correcta vamos efectuar dos\nversiones alternativas del test:Ambos resultados apoyan nuestra conclusión inicial de que el porcentaje\nde adultos con estudios universitarios en el medio oeste es\nestadísticamente inferior la media nacional.","code":"\nsummary(midwest$percollege)\n# Gráficamente \n\n# En escala absoluta\np1 <- ggplot(midwest, aes(percollege)) + \n      geom_histogram(fill = \"white\", color = \"grey30\")\n\n# En escala logarítmica \np2 <- ggplot(midwest, aes(percollege)) + \n  geom_histogram(fill = \"white\", color = \"grey30\") +\n  scale_x_log10()\n\n# La escala logarítmica es una transformación que sirve justificar el supuesto de normalidad. \n\n# La escala logarítmica hace que la distancia entre 10 y 100 sea la misma que entre 100 y 1000.       \n\ngrid.arrange(p1, p2, ncol = 2)\n# 1. Recordemos que queremos testear si el promedio del \n#    \"midwest\" es menor que el promedio nacional. \n# 2. Hacemos tres test: \nt.test(midwest$percollege, \n       mu = 32, \n       alternative = \"less\")\n# La misma versión, pero en logaritmos... \nt.test(log(midwest$percollege), \n       mu = log(32), \n       alternative = \"less\")\n\n# Wilcox test sirve cuando no deseamos asumir que los datos provienen de una distribución normal. \nwilcox.test(midwest$percollege, \n            mu = 32, \n            alternative = \"less\")"},{"path":"análisis-de-datos-con-r.html","id":"t-test-de-dos-colas","chapter":"8 Análisis de datos con R","heading":"8.8.1.3 t-test de dos colas","text":"Ahora digamos que queremos comparar las diferencias entre el porcentaje\npromedio de adultos con estudios universitarios en Ohio y en Michigan.\nEn este caso, queremos realizar una prueba t de dos muestras.Podemos ver que Ohio parece tener ligeramente menos adultos con estudios\nuniversitarios que Michigan. obstante, el gráfico nos dice si es\nestadísticamente significativo o .Los resultados que aparecen continuación muestran un valor \\(p < 0.01\\)\nque apoya la hipótesis alternativa de que “la verdadera diferencia de\nmedias es igual 0”; esencialmente afirma que existe una diferencia\nestadística entre las dos medias.","code":"\n# Seleccionamos los datos \ndf <- midwest %>%\n      filter(state == \"OH\" | state == \"MI\") %>%\n      select(state, percollege)\n\n# Estadisticas para ohio\nsummary(df %>% filter(state == \"OH\") %>% .$percollege)\n\n# Estadisticas para michigan \nsummary(df %>% filter(state == \"MI\") %>% .$percollege)\nggplot(df, aes(state, percollege)) +\n  geom_boxplot()\n\np1 <- ggplot(df, aes(percollege)) +\n  geom_histogram(fill = \"white\", color = \"grey30\") +\n  facet_wrap(~ state)\n\n# En logaritmos \np2 <- ggplot(df, aes(percollege)) +\n  geom_histogram(fill = \"white\", color = \"grey30\") +\n  facet_wrap(~ state) +\n  scale_x_log10()\ngrid.arrange(p1, p2, nrow = 2)\n\n# Realizamos los test \n\n# Sintaxis basica \nt.test(percollege ~ state, \n       data = df)\n\n# Transformando los datos\nt.test(log(percollege) ~ state, \n       data = df)\n\n# Sin asumir normalidad\nwilcox.test(percollege ~ state, \n            data = df)"},{"path":"análisis-de-datos-con-r.html","id":"paired-t---test","chapter":"8 Análisis de datos con R","heading":"8.8.1.4 paired t - test","text":"Sirve para hacer test de diferencias de media para dos grupos que son\nestadisticamente iguales, pero que fueron sometidos un tratamientom\ndistinto. Lo que se quiere testear es si efectivamente la diferencia de\npromedios de alguna variable puede estar asociada este tratamiento.Vamos ocupar la base de datos de R llamada “sleep”. Imaginen queremos\ntestear si una droga particular tiene un efecto estadisticamente\nsignificativo sobre las horas que se duerme. En particular, vamos ver\nsi la variable “extra” es diferente entre ambos grupos. Vamos utilizar\nt.test() nuevamente, pero con la opción paired = TRUE.","code":"\n# Miramos los datos graficamente \nggplot(sleep, aes(group, extra)) +\n  geom_boxplot()\n\n# Testeamos \nt.test(extra ~ group, \n       data = sleep, \n       paired = TRUE)"},{"path":"análisis-de-datos-con-r.html","id":"regresión-lineal-en-r","chapter":"8 Análisis de datos con R","heading":"8.8.2 Regresión Lineal en R","text":"","code":""},{"path":"análisis-de-datos-con-r.html","id":"mco-con-datos-generados","chapter":"8 Análisis de datos con R","heading":"8.8.2.1 MCO con datos generados","text":"En resumen…. Para estimar:\nmco <- lm(y ~ x1 + x2 + x3, data = misdatos) Para mirar resultados:\nsummary(mco) Para mirar gráficamente:\nggplot() + geom_point() + geom_smooth(method = \"lm\")","code":"\n# Limpiar \nrm(list = ls())\n\n# Fijar la semilla (Nos permite generar resultados reproducibles)\nset.seed(1234)\n\n# Genero número de observaciones \nN <- 50 \n\n# Simular una variable x con una distribución uniforme runif(N,min,max)\nx <- runif(N,1,40)\nx\n\n# Simular y rnorm(N,media,sd)\ny <- 40 + 0.5*x + rnorm(N,0,2)\ny\n\n# Crear una base de datos (data.frame) con lo que yo genere. \nmco_data <- data.frame(x,y)\nmco_data\n\n# Ahora, vamos a estimar por Minimos Cuadrados ordinarios \n# el modelo\nmco <- lm(y ~ x, \n          data = mco_data)\nmco \nsummary(mco)\n\n# También lo podemos ver gráficamente \nggplot(mco_data, aes(x,y))+ \n  geom_point(with=.2, alpha=0.5) + \n  geom_smooth(method = \"lm\", se = TRUE) + \n  labs(title = \"Diagrama de dispersión\", \n       subtitle = \"ScatterPlot\", \n       x = \"x\", \n       y = \"y\") + theme_minimal()"},{"path":"análisis-de-datos-con-r.html","id":"mco-con-datos-reales","chapter":"8 Análisis de datos con R","heading":"8.8.2.2 MCO con datos reales","text":"","code":"\nrm(list = ls())\ndata(\"CPS1985\")\nnames(CPS1985)\n\n# Correlación entre variables \nCPS1985 %>%  \n  summarize(cor1 = cor(wage, education), \n            cor2 = cor(wage, experience))\n\n# Gráficamente \nggplot(CPS1985, aes(education, log(wage))) + \n  geom_point() +\n  geom_smooth(method = 'lm', se = T) +\n  labs(title=\"Diagrama de dispersión\",\n       subtitle= \"ScatterPlot\",\n       caption=\"Fuente: CPS1985 (paquete AER)\",\n       x=\"Educación en años\",\n       y=\"Salario (en logaritmo)\")\n\n# Estimación \n\n  # Creamos variables que utilizaremos en las estimaciones \n\n  # Crear dos variables: log(wage) y experiencia al cuadrado. \n  CPS1985$lnwage <- log(CPS1985$wage)\n  CPS1985$exp2 <- CPS1985$exp * CPS1985$exp \n\n  # R no nos permite elevar variables al cuadrado dentro de la función\n  # Generamos variables categóricas que queramos incluir en la regresión\n\n  # Variables dicótomicas (sobre variable binarias).  \n  table(CPS1985$gender)\n  CPS1985$sexo <- ifelse(CPS1985$gender == \"male\", 1, 0)\n  table(CPS1985$union)\n  CPS1985$sind <- ifelse(CPS1985$union == \"yes\", 1, 0)\n\n  # Variables dicótomas (sobre variables categóricas)\n  CPS1985 <- cbind(CPS1985, \n                 dummy(factor(CPS1985$occupation), sep = \"_\"))\n\n  # Generando formas editables \n  f0 <- wage ~ education \n  f1 <- lnwage ~ education \n  f2 <- lnwage ~ education + experience\n  f3 <- lnwage ~ education + experience + exp2\n  f4 <- lnwage ~ education + experience + exp2 + sexo\n  f5 <- lnwage ~ education + experience + exp2 + sexo + sind\n  f6 <- lnwage ~ education + experience + exp2 + sexo + sind + CPS1985_services\n\n  # Estimaciones \n  mco_0 <- lm(f0, data=CPS1985, na.action = na.exclude)\n  mco_1 <- lm(f1, data=CPS1985, na.action = na.exclude)\n  mco_2 <- lm(f2, data=CPS1985, na.action = na.exclude)\n  mco_3 <- lm(f3, data=CPS1985, na.action = na.exclude)\n  mco_4 <- lm(f4, data=CPS1985, na.action = na.exclude)\n  mco_5 <- lm(f5, data=CPS1985, na.action = na.exclude)\n  mco_6 <- lm(f6, data=CPS1985, na.action = na.exclude)\n\n\n# Mirar los resultados de mis estimaciones \nsummary(mco_0)\nsummary(mco_1)\nsummary(mco_2)\nsummary(mco_3)\nsummary(mco_4)\nsummary(mco_5)\nsummary(mco_6)\n\n# Exportar resultados \n\n# El paquete se llama \"stargazer\"\nsetwd(here(\"resultados\"))\n\nstargazer(mco_1, mco_2, mco_3, # Modelos\n          type = \"text\", \n          title = \"Estimación MCO salarios vs. educ\", \n          style = \"qje\", # Estilos \n          notes = \"\\\\footnotezie ***p=.01; ** p =.05; *p=.1\", \n          notes.append = F, # Para remplazar notas puestas anteriormente. \n          single.row = F, # Para que los errores estandar esten en la misma linea que los coeficientes \n          no.space = T, # No se generen espacios entre los regresores.\n          out = \"est1-3.doc\", \n          digits = 2)"},{"path":"análisis-de-datos-con-r.html","id":"r-markdown","chapter":"8 Análisis de datos con R","heading":"8.8.3 R Markdown","text":"Herramienta para crear informes que sean automáticos, reproducibles\ny con datos que se puedan actualizar en el tiempo.Se pueden generar informes en formato word, pdf, html.R markdown (Rmd) es básicamente una intersección entre texto\nnarrativo y código que genera visualizaciones y estimaciones con el\nfin de analizar datos.Rmd puede generar: texto plano, segmentos de co ́digo, gr ́aficos,\ntablas, tableros interactivos.R markdown es especialmente útil:\nInformes rutinarios: Por ejemplo, informe semanal sobre un\nconjunto de análisis que se actualiza en el tiempo.\nInformes de análisis para un subconjunto de datos: Por\nejemplo, informes por país de una base de datos que contiene\ninformación de distintos países.\nInformes rutinarios: Por ejemplo, informe semanal sobre un\nconjunto de análisis que se actualiza en el tiempo.Informes de análisis para un subconjunto de datos: Por\nejemplo, informes por país de una base de datos que contiene\ninformación de distintos países.","code":""},{"path":"análisis-de-datos-con-r.html","id":"conceptos-básicos","chapter":"8 Análisis de datos con R","heading":"8.8.3.1 Conceptos Básicos","text":"Markdown es el lenguaje que permite escribir documentos en texto\nplano. Los archivos escritos en Markdown tienen la extensión .md.R Markdown es la variación especifica para R. Permite escribir\ntexto plano con Markdown y adjuntar código proveniente de R. Los\narchivos escritos en R Markdown tienen la extensión .Rmd.knirt: Paquete de R. Sirve para leer los segmentos de código que\nqueremos introducir en los reportes.Pandoc: Sirve para convertir el output en un archivo con formato\nword/pdf/html. Es un software que viene instalado automáticamente en\n\n","code":""},{"path":"análisis-de-datos-con-r.html","id":"proceso","chapter":"8 Análisis de datos con R","heading":"8.8.3.2 Proceso","text":"Fuente: https://rmarkdown.rstudio.com/authoring quick\ntour.html","code":""},{"path":"análisis-de-datos-con-r.html","id":"primer-archivo-en-r-markdown","chapter":"8 Análisis de datos con R","heading":"8.8.3.3 Primer archivo en R Markdown","text":"Opciones para elegir el tipo de documento y si quiero confeccionar\nun documento/presentación/un tablero u otros tipos de archivo más\ndetallados.Opciones para cambiar título y autor().","code":""},{"path":"análisis-de-datos-con-r.html","id":"directorio-de-trabajo","chapter":"8 Análisis de datos con R","heading":"8.8.3.4 Directorio de trabajo","text":"El directorio de trabajo de un archivo .Rmd será donde esta\nguardado el archivo con esa extensión.De este modo, R buscará los archivos en la carpeta en donde este\nguardado el archivo .Rmd.Para este ejercicio simplemente dejaremos los datos que utilizaremos\njunto al archivo.","code":""},{"path":"análisis-de-datos-con-r.html","id":"componentes-de-r-markdown","chapter":"8 Análisis de datos con R","heading":"8.8.3.5 Componentes de R Markdown","text":"YAML: fijar título, fecha y tipo de output.Markdown text: introducir texto.Code Chunk: cargar paquetes, datos, visualizaciones.","code":""}]
